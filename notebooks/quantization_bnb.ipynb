{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2db72f9",
   "metadata": {},
   "source": [
    "# A 4 bit quantization transform for Thunder\n",
    "\n",
    "\n",
    "In this notebook, we will implement a seamless 4 bit quantization transform for thunder.\n",
    "We will use `bitsandbytes`for the 4 bit quantization and kernel.\n",
    "\n",
    "First let's import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3b8246e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes\n",
    "import thunder\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f32b2b9",
   "metadata": {},
   "source": [
    "To have something concrete to work with, let's take [LitGPT](https://github.com/Lightning-AI/litgpt)'s quantized generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca09a898",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning import fabric\n",
    "from lightning.fabric.plugins import BitsandbytesPrecision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990663a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61515bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1e08170",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model '/home/tv/data/firma/grid/thunder/litgpt/checkpoints/meta-llama/Meta-Llama-3-8B-Instruct/lit_model.pth' with {'name': 'Llama-3-8B-Instruct', 'hf_config': {'name': 'Meta-Llama-3-8B-Instruct', 'org': 'meta-llama'}, 'scale_embeddings': False, 'block_size': 8192, 'vocab_size': 128000, 'padding_multiple': 512, 'padded_vocab_size': 128256, 'n_layer': 32, 'n_head': 32, 'head_size': 128, 'n_embd': 4096, 'rotary_percentage': 1.0, 'parallel_residual': False, 'bias': False, 'lm_head_bias': False, 'n_query_groups': 8, 'shared_attention_norm': False, 'norm_class_name': 'RMSNorm', 'norm_eps': 1e-05, 'mlp_class_name': 'LLaMAMLP', 'gelu_approximate': 'none', 'intermediate_size': 14336, 'rope_condense_ratio': 1, 'rope_base': 500000, 'n_expert': 0, 'n_expert_per_token': 0, 'rope_n_elem': 128}\n",
      "Time to instantiate model: 0.22 seconds.\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Time to load the model weights: 5.00 seconds.\n",
      "Seed set to 1234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "You are a helpful assistant.\n",
      "user\n",
      "\n",
      "What food do llamas eat? Answer in German.\n",
      "assistant\n",
      "\n",
      "Die Lamas fressen Gräser und Pflanzen, insbesondere Gras, Blätter, Triebe und Blumen. Sie sind Omnivoren und können auch Obst und Getreide konsumieren, wenn es ihnen zur Verfügung steht.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time for inference 1: 1.49 sec total, 37.47 tokens/sec\n",
      "Memory used: 7.57 GB\n"
     ]
    }
   ],
   "source": [
    "# Copyright Lightning AI. Licensed under the Apache License 2.0, see LICENSE file.\n",
    "\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, Literal, Optional\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "import torch._dynamo.config\n",
    "import torch._inductor.config\n",
    "from lightning.fabric.plugins import BitsandbytesPrecision\n",
    "\n",
    "from litgpt import GPT, Config, PromptStyle, Tokenizer\n",
    "from litgpt.prompts import has_prompt_style, load_prompt_style\n",
    "from litgpt.utils import CLI, check_valid_checkpoint_dir, get_default_supported_precision, load_checkpoint\n",
    "\n",
    "\n",
    "\n",
    "def multinomial_num_samples_1(probs: torch.Tensor) -> torch.Tensor:\n",
    "    if torch._dynamo.is_compiling():\n",
    "        # Faster alternative to `torch.multinomial(probs, num_samples=1)` that is also CUDAGraph friendly\n",
    "        distribution = torch.empty_like(probs).exponential_(1)\n",
    "        return torch.argmax(probs / distribution, dim=-1, keepdim=True)\n",
    "    return torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "\n",
    "def sample_top_p(logits: torch.Tensor, top_p: float) -> torch.Tensor:\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=False)\n",
    "    cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
    "    # Example:\n",
    "    # sorted_probs=[0.1, 0.15, 0.2, 0.25, 0.3] -> sorted_cumprobs=[0.1, 0.25, 0.45, 0.7, 1.0]\n",
    "    # sorted_indices_to_remove = [1, 1, 0, 0, 0] if top_p=0.7\n",
    "    sorted_indices_to_remove = cumulative_probs <= (1 - top_p)\n",
    "    # Keep at least 1 token always to prevent the case where no token is selected\n",
    "    # In this case the most probable one is always kept\n",
    "    sorted_indices_to_remove[-1:] = 0\n",
    "    indices_to_remove = sorted_indices_to_remove.scatter(0, sorted_indices, sorted_indices_to_remove)\n",
    "    logits = logits.masked_fill(indices_to_remove, float(\"-inf\"))\n",
    "    return logits\n",
    "\n",
    "\n",
    "def sample(\n",
    "    logits: torch.Tensor, temperature: float = 1.0, top_k: Optional[int] = None, top_p: float = 1.0\n",
    ") -> torch.Tensor:\n",
    "    if top_p < 0.0 or top_p > 1.0:\n",
    "        raise ValueError(f\"top_p must be in [0, 1], got {top_p}\")\n",
    "    logits = logits[0, -1]\n",
    "    # optionally crop the logits to only the top k options\n",
    "    if top_k is not None:\n",
    "        v, i = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "        # do not use `torch.where` as in nanogpt because it will repeat top-k collisions\n",
    "        logits = torch.full_like(logits, float(\"-inf\")).scatter_(-1, i, v)\n",
    "    # optionally scale the logits and sample from a probability distribution\n",
    "    if temperature > 0.0 or top_p > 0.0:\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "        # optionally crop the logits to smallest set of logits with a cumulative probability above top_p\n",
    "        if top_p < 1.0:\n",
    "            logits = sample_top_p(logits, top_p)\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        return multinomial_num_samples_1(probs)\n",
    "    return torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "def next_token(model: GPT, input_pos: torch.Tensor, x: torch.Tensor, **kwargs: Any) -> torch.Tensor:\n",
    "    logits = model(x, input_pos)\n",
    "    next = sample(logits, **kwargs)\n",
    "    return next.to(dtype=x.dtype)\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate(\n",
    "    model: GPT,\n",
    "    prompt: torch.Tensor,\n",
    "    max_returned_tokens: int,\n",
    "    *,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: Optional[int] = None,\n",
    "    top_p: float = 1.0,\n",
    "    eos_id: Optional[int] = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Takes a conditioning sequence (prompt) as input and continues to generate as many tokens as requested.\n",
    "\n",
    "    The implementation of this function is modified from A. Karpathy's nanoGPT.\n",
    "\n",
    "    Args:\n",
    "        model: The model to use.\n",
    "        prompt: Tensor of shape (T) with indices of the prompt sequence.\n",
    "        max_returned_tokens: The maximum number of tokens to return (given plus generated).\n",
    "        temperature: Scales the predicted logits by 1 / temperature.\n",
    "        top_k: If specified, only sample among the tokens with the k highest probabilities.\n",
    "        top_p: If specified, it represents the cumulative probability threshold to consider in the sampling process.\n",
    "            In top-p sampling, the next token is sampled from the highest probability tokens\n",
    "            whose cumulative probability exceeds the threshold `top_p`. When specified,\n",
    "            it must be `0 <= top_p <= 1`. Here, `top_p=0` is equivalent\n",
    "            to sampling the most probable token, while `top_p=1` samples from the whole distribution.\n",
    "            It can be used in conjunction with `top_k` and `temperature` with the following order\n",
    "            of application:\n",
    "\n",
    "            1. `top_k` sampling\n",
    "            2. `temperature` scaling\n",
    "            3. `top_p` sampling\n",
    "\n",
    "            For more details, see https://arxiv.org/abs/1904.09751\n",
    "            or https://huyenchip.com/2024/01/16/sampling.html#top_p\n",
    "        eos_id: If specified, stop generating any more token once the <eos> token is triggered.\n",
    "    \"\"\"\n",
    "    T = prompt.size(0)\n",
    "    assert max_returned_tokens > T\n",
    "    if model.max_seq_length < max_returned_tokens - 1:\n",
    "        # rolling the kv cache based on the `input_pos` value would be necessary. However, doing so would introduce a\n",
    "        # data dependency on the `input_pos` tensor and impact model compilation. Since this setting is uncommon, we do\n",
    "        # not support it to avoid negatively impacting the overall speed\n",
    "        raise NotImplementedError(f\"max_seq_length {model.max_seq_length} needs to be >= {max_returned_tokens - 1}\")\n",
    "\n",
    "    device = prompt.device\n",
    "    tokens = [prompt]\n",
    "    input_pos = torch.tensor([T], device=device)\n",
    "    token = next_token(\n",
    "        model, torch.arange(0, T, device=device), prompt.view(1, -1), temperature=temperature, top_k=top_k, top_p=top_p\n",
    "    ).clone()\n",
    "    tokens.append(token)\n",
    "    for _ in range(2, max_returned_tokens - T + 1):\n",
    "        token = next_token(\n",
    "            model, input_pos, token.view(1, -1), temperature=temperature, top_k=top_k, top_p=top_p\n",
    "        ).clone()\n",
    "        tokens.append(token)\n",
    "        if token == eos_id:\n",
    "            break\n",
    "        input_pos = input_pos.add_(1)\n",
    "    return torch.cat(tokens)\n",
    "\n",
    "\n",
    "with torch.inference_mode():\n",
    "    prompt: str = \"What food do llamas eat? Answer in German.\"\n",
    "    num_samples: int = 1\n",
    "    max_new_tokens: int = 256\n",
    "    top_k: Optional[int] = 50\n",
    "    top_p: float = 1.0\n",
    "    temperature: float = 0.8\n",
    "    checkpoint_dir: Path = Path(\"/home/tv/data/firma/grid/thunder/litgpt/checkpoints/meta-llama/Meta-Llama-3-8B-Instruct/\")\n",
    "    quantize: Optional[Literal[\"bnb.nf4\", \"bnb.nf4-dq\", \"bnb.fp4\", \"bnb.fp4-dq\", \"bnb.int8\"]] = \"bnb.nf4\"\n",
    "    precision: Optional[str] = \"bf16-true\"\n",
    "    compile: bool = False\n",
    "# litgpt generate base --quantize bnb.nf4 --checkpoint_dir checkpoints/tiiuae/falcon-7b --precision bf16-true --max_new_tokens 256\n",
    "\n",
    "    \"\"\"Generates text samples based on a pre-trained model and tokenizer.\n",
    "\n",
    "    Args:\n",
    "        prompt: The prompt string to use for generating the samples.\n",
    "        num_samples: The number of text samples to generate.\n",
    "        max_new_tokens: The number of generation steps to take.\n",
    "        top_k: The number of top most probable tokens to consider in the sampling process.\n",
    "        top_p: If specified, it represents the cumulative probability threshold to consider in the sampling process.\n",
    "            In top-p sampling, the next token is sampled from the highest probability tokens\n",
    "            whose cumulative probability exceeds the threshold `top_p`. When specified,\n",
    "            it must be `0 <= top_p <= 1`. Here, `top_p=0` is equivalent\n",
    "            to sampling the most probable token, while `top_p=1` samples from the whole distribution.\n",
    "            It can be used in conjunction with `top_k` and `temperature` with the following order\n",
    "            of application:\n",
    "\n",
    "            1. `top_k` sampling\n",
    "            2. `temperature` scaling\n",
    "            3. `top_p` sampling\n",
    "\n",
    "            For more details, see https://arxiv.org/abs/1904.09751\n",
    "            or https://huyenchip.com/2024/01/16/sampling.html#top_p\n",
    "        temperature: A value controlling the randomness of the sampling process. Higher values result in more random\n",
    "            samples.\n",
    "        checkpoint_dir: The checkpoint directory to load.\n",
    "        quantize: Whether to quantize the model and using which method:\n",
    "            - bnb.nf4, bnb.nf4-dq, bnb.fp4, bnb.fp4-dq: 4-bit quantization from bitsandbytes\n",
    "            - bnb.int8: 8-bit quantization from bitsandbytes\n",
    "            for more details, see https://github.com/Lightning-AI/litgpt/blob/main/tutorials/quantize.md\n",
    "        precision: Indicates the Fabric precision setting to use.\n",
    "        compile: Whether to compile the model.\n",
    "    \"\"\"\n",
    "    precision = precision or get_default_supported_precision(training=False)\n",
    "\n",
    "    plugins = BitsandbytesPrecision(mode='nf4', dtype=torch.bfloat16)\n",
    "    precision = None\n",
    "\n",
    "    fabric = L.Fabric(devices=1, precision=precision, plugins=plugins)\n",
    "\n",
    "    check_valid_checkpoint_dir(checkpoint_dir)\n",
    "    config = Config.from_file(checkpoint_dir / \"model_config.yaml\")\n",
    "\n",
    "    checkpoint_path = checkpoint_dir / \"lit_model.pth\"\n",
    "\n",
    "    tokenizer = Tokenizer(checkpoint_dir)\n",
    "    prompt_style = (\n",
    "        load_prompt_style(checkpoint_dir) if has_prompt_style(checkpoint_dir) else PromptStyle.from_config(config)\n",
    "    )\n",
    "\n",
    "    prompt = prompt_style.apply(prompt)\n",
    "    encoded = tokenizer.encode(prompt, device=fabric.device)\n",
    "    prompt_length = encoded.size(0)\n",
    "    max_returned_tokens = prompt_length + max_new_tokens\n",
    "\n",
    "    fabric.print(f\"Loading model {str(checkpoint_path)!r} with {config.__dict__}\", file=sys.stderr)\n",
    "    t0 = time.perf_counter()\n",
    "    with fabric.init_module(empty_init=True):\n",
    "        model = GPT(config)\n",
    "    fabric.print(f\"Time to instantiate model: {time.perf_counter() - t0:.02f} seconds.\", file=sys.stderr)\n",
    "    with fabric.init_tensor():\n",
    "        # set the max_seq_length to limit the memory usage to what we need\n",
    "        model.max_seq_length = max_returned_tokens\n",
    "        # enable the kv cache\n",
    "        model.set_kv_cache(batch_size=1)\n",
    "    model.eval()\n",
    "\n",
    "    if compile:\n",
    "        torch._dynamo.config.automatic_dynamic_shapes = True\n",
    "        torch._inductor.config.triton.unique_kernel_names = True\n",
    "        torch._inductor.config.coordinate_descent_tuning = True\n",
    "        global next_token\n",
    "        next_token = torch.compile(next_token, mode=\"reduce-overhead\")\n",
    "\n",
    "    model = fabric.setup_module(model)\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    load_checkpoint(fabric, model, checkpoint_path)\n",
    "    fabric.print(f\"Time to load the model weights: {time.perf_counter() - t0:.02f} seconds.\", file=sys.stderr)\n",
    "\n",
    "    L.seed_everything(1234)\n",
    "    for i in range(num_samples):\n",
    "        t0 = time.perf_counter()\n",
    "        y = generate(model, encoded, max_returned_tokens, temperature=temperature, top_k=top_k, top_p=top_p, eos_id=tokenizer.eos_id)\n",
    "        t = time.perf_counter() - t0\n",
    "        for block in model.transformer.h:\n",
    "            block.attn.kv_cache.reset_parameters()\n",
    "        fabric.print(tokenizer.decode(y))\n",
    "        tokens_generated = y.size(0) - prompt_length\n",
    "        fabric.print(\n",
    "            f\"Time for inference {i + 1}: {t:.02f} sec total, {tokens_generated / t:.02f} tokens/sec\", file=sys.stderr\n",
    "        )\n",
    "    if fabric.device.type == \"cuda\":\n",
    "        fabric.print(f\"Memory used: {torch.cuda.max_memory_allocated() / 1e9:.02f} GB\", file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ff8603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96bf1de8",
   "metadata": {},
   "source": [
    "# Excursion: How does quantization work\n",
    "\n",
    "To take a detailed look at how quantization works, let's see how the quantization works. First we quantize to 4 bit using `quantize_4bit`. It returns a quantized tensor and a status object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89d7e3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.randn(200, 100, device=\"cuda\")\n",
    "qt, st = bitsandbytes.functional.quantize_4bit(w, quant_type='nf4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d805a41",
   "metadata": {},
   "source": [
    "Then we can use `matmul_4bit` to multiply a (floating-point) activation with the quantized amtrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ceeff98",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(300, 100, device=\"cuda\")\n",
    "out = bitsandbytes.matmul_4bit(x, qt.t(), bias=None, quant_state=st)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6003d2",
   "metadata": {},
   "source": [
    "We can compare the quantized matmul to the original floating point one. There is quite a difference (due to 4 bits not being very precise)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11f71b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7309, device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(out - x @ w.t()).abs().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd536bc4",
   "metadata": {},
   "source": [
    "...but when comparing to the magnitude of the result, we see that everything is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e850cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.9811, device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.abs().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bcea01",
   "metadata": {},
   "source": [
    "But so how does it work?\n",
    "\n",
    "Let's re-do the `matmul_4bit` manually in PyTorch.\n",
    "\n",
    "The quantized tensor is a `uint8` tensor, each byte in the tensor encodes two 4-bit-entries in the original matrix. The tensor has been flattened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa9ac17e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 90],\n",
       "        [ 37],\n",
       "        [102],\n",
       "        ...,\n",
       "        [107],\n",
       "        [136],\n",
       "        [180]], device='cuda:0', dtype=torch.uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdc5b93",
   "metadata": {},
   "source": [
    "The quantization state that has been returned by `quantize_4bit` contains the information to dequantize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bca60c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bitsandbytes.functional.QuantState at 0x7f8032a06810>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8fa90d",
   "metadata": {},
   "source": [
    "Key properties of the quantization state are\n",
    "\n",
    "- `.blocksize` defines the blocksize, values within a block share the same scale from `.absmax`.\n",
    "- `.code` is a \"dictionary\" to index into with the 4 bit value `0` ... `15`. The code seems to be normalized to the range -1 ... 1 and is not equidistributed.\n",
    "- `.absmax` contains scales for all blocks (of size `.blocksize`),\n",
    "- `.dtype` is the original dtype,\n",
    "- `.shape` is the original shape.\n",
    "\n",
    "We can look at the `.code` dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8395f856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f802fe03990>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(st.code)\n",
    "from matplotlib import pyplot\n",
    "pyplot.plot(st.code.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c84cbc1",
   "metadata": {},
   "source": [
    "And we can decode the tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9094ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_dequant = torch.stack((st.code[(qt.squeeze().int() >> 4) & 0xf], st.code[qt.squeeze().int() & 0xf]), -1).view(-1)\n",
    "for i, m in zip(range(0, w_dequant.size(0), st.blocksize), st.absmax):\n",
    "    w_dequant[i: i + st.blocksize] *= m\n",
    "    \n",
    "w_dequant = w_dequant.view(st.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ef9121",
   "metadata": {},
   "source": [
    "We can see whether the output computed by `matmul_4bit` with a matrix multiplication with our manually dequantized matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7775b7fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out - x @ w_dequant.t()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b07569",
   "metadata": {},
   "source": [
    "## 4 bit matmul in Thunder\n",
    "\n",
    "But now, let's get the 4 bit matmul into Thunder.\n",
    "Happily, Thunder is easily extensible, so we can just define a nf8 meta and implementation, instantiate an `OperatorExecutor` and register our new symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88ba472e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3436d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import thunder\n",
    "ex = thunder.extend.OperatorExecutor('quant_bnb', version=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fb4e464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bnb_matmul_nf4_meta(x, qweight, bias, absmax, quant_map, blocksize, dtype, shape):\n",
    "    assert isinstance(shape, Sequence) and len(shape) == 2\n",
    "    assert x.shape[-1] == shape[1], f\"{x.shape=}, rhs {shape=}\"\n",
    "    return thunder.TensorProxy(like=x, shape=(*x.shape[:-1], shape[0]))\n",
    "\n",
    "def bnb_matmul_nf4_impl(x, qweight, bias, absmax, quant_map, blocksize, dtype, shape):\n",
    "    qs = bitsandbytes.functional.QuantState(absmax, shape=shape, blocksize=blocksize, code=quant_map,\n",
    "                                            quant_type='nf4', dtype=dtype)\n",
    "    \n",
    "    return bitsandbytes.matmul_4bit(x, qweight.t(), bias=bias, quant_state=qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d402a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_matmul_nf4 = ex.register_operator('bnb_matmul_nf4', meta=bnb_matmul_nf4_meta, fn=bnb_matmul_nf4_impl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12314f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Symbol name=bnb_matmul_nf4]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb_matmul_nf4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716068b7",
   "metadata": {},
   "source": [
    "Let's try this on a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5802a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn(x, qweight, qs):\n",
    "    return bnb_matmul_nf4(x, qweight, None, qs.absmax, qs.code, qs.blocksize, qs.dtype, qs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5634b73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "jfn = thunder.jit(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20bf3316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -5.6017,  -6.9423,  14.8765,  ...,   6.7327,  -0.9416, -13.1131],\n",
       "        [ 11.6984,  -1.3968,  -1.5242,  ...,   5.7076,  -5.6656,  -7.5721],\n",
       "        [ -7.2364,   0.3612,   4.7494,  ...,  13.5308,  12.4456,  16.8248],\n",
       "        ...,\n",
       "        [ 19.8676,  13.6140, -17.0503,  ...,  -6.3070,   4.9783,  -0.5094],\n",
       "        [-17.1213,  -5.3133, -22.2789,  ...,   2.8354,   2.1703,  -8.1751],\n",
       "        [  8.0881,   8.1432,  -5.1421,  ...,  -7.8354,   3.0699,  -8.4743]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jfn(x, qt, st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60fd8287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import torch\n",
       "from thunder.executors.torchex import no_autocast\n",
       "\n",
       "@torch.no_grad()\n",
       "@no_autocast\n",
       "def computation(x, qweight, t_2_absmax, t_2_code):\n",
       "  # x: \"cuda:0 f32[300, 100]\"\n",
       "  # qweight: \"cuda:0 ui8[10000, 1]\"\n",
       "  # t_2_absmax: \"cuda:0 f32[313]\"\n",
       "  # t_2_code: \"cuda:0 f32[16]\"\n",
       "\n",
       "  # /tmp/ipykernel_1517946/3597101269.py:2: \t    return bnb_matmul_nf4(x, qweight, None, qs.absmax, qs.code, qs.blocksize, qs.dtype, qs.shape)\n",
       "  t3 = bnb_matmul_nf4(x, qweight, None, t_2_absmax, t_2_code, 64, torch.float32, (200, 100))  # t3: \"cuda:0 f32[300, 200]\"\n",
       "  return t3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thunder.last_traces(jfn)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0ff6e7",
   "metadata": {},
   "source": [
    "## Quantizing a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8a5261e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, thunder, bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69dc0e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#qt, st = bitsandbytes.functional.quantize_4bit(tm, , quant_type='nf4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cbb9900f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(import thunder\n",
       " import thunder.core.prims as prims\n",
       " import torch\n",
       " from thunder.executors.torchex import no_autocast\n",
       " \n",
       " @torch.no_grad()\n",
       " @no_autocast\n",
       " def prologue(*args, **kwargs):\n",
       "   # args: \"Any\"\n",
       "   prims.check_len(args, 1)\n",
       "   # kwargs: \"Any\"\n",
       "   prims.check_len(kwargs, 0)\n",
       "   input: \"cuda:0 f32[4, 128]\" = args[0]\n",
       "   p0: \"Any\" = globals()['__function_obj']\n",
       "   module: \"Any\" = thunder.core.module.get_thunder_module(p0)\n",
       "   t_0_weight: \"cuda:0 f32[256, 128]\" = module.get_parameter('0.weight')\n",
       "   t_2_weight: \"cuda:0 f32[128, 256]\" = module.get_parameter('2.weight')\n",
       "   prims.check_tensor_metadata(input, (4, 128), 'cuda:0', torch.float32, False)\n",
       "   m: \"Any\" = module.get_submodule('0')\n",
       "   b0: \"bool True\" = m.training\n",
       "   prims.check_number_type_and_value(b0, True)\n",
       "   i1: \"int 128\" = m.in_features\n",
       "   prims.check_number_type_and_value(i1, 128)\n",
       "   i2: \"int 256\" = m.out_features\n",
       "   prims.check_number_type_and_value(i2, 256)\n",
       "   prims.check_tensor_metadata(t_0_weight, (256, 128), 'cuda:0', torch.float32, True)\n",
       "   p1: \"Any\" = module.get_submodule('1')\n",
       "   s4: \"str\" = p1.approximate\n",
       "   prims.check_string_value(s4, 'none')\n",
       "   p2: \"Any\" = module.get_submodule('2')\n",
       "   b10: \"bool True\" = p2.training\n",
       "   prims.check_number_type_and_value(b10, True)\n",
       "   i11: \"int 256\" = p2.in_features\n",
       "   prims.check_number_type_and_value(i11, 256)\n",
       "   i12: \"int 128\" = p2.out_features\n",
       "   prims.check_number_type_and_value(i12, 128)\n",
       "   prims.check_tensor_metadata(t_2_weight, (128, 256), 'cuda:0', torch.float32, True)\n",
       "   cache_info: \"Any\" = thunder._get_cache_info()\n",
       "   cache_info_is_autocast_enabled: \"bool False\" = cache_info['is_autocast_enabled']\n",
       "   prims.check_number_type_and_value(cache_info_is_autocast_enabled, False)\n",
       "   cache_info_no_grad_sync: \"bool False\" = cache_info['no_grad_sync']\n",
       "   prims.check_number_type_and_value(cache_info_no_grad_sync, False)\n",
       "   return (input, t_0_weight, t_2_weight),\n",
       " import thunder\n",
       " import thunder.torch as ltorch\n",
       " import torch\n",
       " from thunder.executors.torchex import no_autocast\n",
       " \n",
       " @torch.no_grad()\n",
       " @no_autocast\n",
       " def computation(input, t_0_weight, t_2_weight):\n",
       "   # input: \"cuda:0 f32[4, 128]\"\n",
       "   # t_0_weight: \"cuda:0 f32[256, 128]\"\n",
       "   # t_2_weight: \"cuda:0 f32[128, 256]\"\n",
       " \n",
       "   # /usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:116: \t        return F.linear(input, self.weight, self.bias)\n",
       "   t3 = ltorch.linear(input, t_0_weight, None)  # t3: \"cuda:0 f32[4, 256]\"\n",
       "     # t3 = prims.linear(input, t_0_weight, None)  # t3: \"cuda:0 f32[4, 256]\"\n",
       " \n",
       "   # /usr/local/lib/python3.11/dist-packages/torch/nn/modules/activation.py:696: \t        return F.gelu(input, approximate=self.approximate)\n",
       "   t9 = ltorch.gelu(t3, approximate='none')  # t9: \"cuda:0 f32[4, 256]\"\n",
       "     # t5 = ltorch.true_divide(t3, 1.4142135623730951)  # t5: \"cuda:0 f32[4, 256]\"\n",
       "       # t5 = prims.div(t3, 1.4142135623730951)  # t5: \"cuda:0 f32[4, 256]\"\n",
       "     # t6 = ltorch.erf(t5)  # t6: \"cuda:0 f32[4, 256]\"\n",
       "       # t6 = prims.erf(t5)  # t6: \"cuda:0 f32[4, 256]\"\n",
       "     # t7 = ltorch.mul(0.5, t6)  # t7: \"cuda:0 f32[4, 256]\"\n",
       "       # t7 = prims.mul(0.5, t6)  # t7: \"cuda:0 f32[4, 256]\"\n",
       "     # t8 = ltorch.add(0.5, t7, alpha=None)  # t8: \"cuda:0 f32[4, 256]\"\n",
       "       # t8 = prims.add(0.5, t7)  # t8: \"cuda:0 f32[4, 256]\"\n",
       "     # t9 = ltorch.mul(t3, t8)  # t9: \"cuda:0 f32[4, 256]\"\n",
       "       # t9 = prims.mul(t3, t8)  # t9: \"cuda:0 f32[4, 256]\"\n",
       " \n",
       "   # /usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:116: \t        return F.linear(input, self.weight, self.bias)\n",
       "   t13 = ltorch.linear(t9, t_2_weight, None)  # t13: \"cuda:0 f32[4, 128]\"\n",
       "     # t13 = prims.linear(t9, t_2_weight, None)  # t13: \"cuda:0 f32[4, 128]\"\n",
       "   return t13)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.nn.Sequential(\n",
    "    torch.nn.Linear(128, 256, bias=False),\n",
    "    torch.nn.GELU(),\n",
    "    torch.nn.Linear(256, 128, bias=False)\n",
    ").cuda()\n",
    "tm = thunder.jit(m)\n",
    "a = torch.randn(4, 128, device='cuda')\n",
    "tm(a);\n",
    "computation_trace = thunder.last_traces(tm)[0]\n",
    "prologue_trace = thunder.last_prologue_traces(tm)[0]\n",
    "prologue_trace, computation_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "816b517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from thunder.core.transform_common import EarlyTransform\n",
    "\n",
    "class LinearQuant4bit(EarlyTransform):\n",
    "    def __init__(self):\n",
    "        self.quant_states = {}\n",
    "\n",
    "    def transform_module(self, model: thunder.ThunderModule):\n",
    "        def convert_linear_submodule(tm, name):    \n",
    "            weight_name = f\"{name}.weight\"\n",
    "            w = tm.get_parameter(weight_name)\n",
    "            # device!\n",
    "            qw, qs = bitsandbytes.functional.quantize_4bit(w.to('cuda'), quant_type='nf4')\n",
    "            tm._overrides_parameters[weight_name] = qw\n",
    "            tm._overrides_parameters[f\"{weight_name}.absmax\"] = qs.absmax\n",
    "            tm._overrides_parameters[f\"{weight_name}.code\"] = qs.code\n",
    "            self.quant_states[weight_name] = {'dtype': qs.dtype, 'shape': qs.shape, 'blocksize': qs.blocksize}\n",
    "\n",
    "        for n, submodule in model._model.named_modules():\n",
    "            if isinstance(submodule, torch.nn.Linear):\n",
    "                convert_linear_submodule(model, n)\n",
    "\n",
    "    def transform_traces(self, prologue_trace, computation_trace, epilogue_trace, **kwargs):\n",
    "        from thunder.core.trace import tracectx\n",
    "        def get_orig_and_thunder_module_proxies_from_prologue(prologue_trace): \n",
    "            modules_and_thunder_modules = [\n",
    "                (bsym.args[0], bsym.output)\n",
    "                for bsym in prologue_trace.bound_symbols\n",
    "                if bsym.sym is prims.unpack_thunder_module\n",
    "            ]\n",
    "\n",
    "            if len(modules_and_thunder_modules) != 1:\n",
    "                raise NotImplementedError(\"cannot deal with modules other than the compiled module\")\n",
    "\n",
    "            ((orig_module_proxy, thunder_module_proxy),) = modules_and_thunder_modules\n",
    "            if prologue_producers[orig_module_proxy].sym is not prims.unpack_function_obj:\n",
    "                raise NotImplementedError(\"original module does not match the compiled module\")\n",
    "\n",
    "            return orig_module_proxy, thunder_module_proxy\n",
    "\n",
    "        from thunder.core import utils\n",
    "        from thunder.core import prims\n",
    "\n",
    "        def get_checks(prologue_trace):\n",
    "            check_dict = {}\n",
    "            prologue_producers, prologue_consumers = utils.producers_and_consumers(prologue_trace)\n",
    "            for bsym in prologue_trace.bound_symbols:\n",
    "                if bsym.sym == prims.unpack_parameter or bsym.sym == prims.unpack_buffer:\n",
    "                    param_thunder_module, param_name = bsym.args\n",
    "                    checks = [bsym2 for bsym2 in prologue_consumers[bsym.output] if bsym2.sym == prims.check_tensor_shape_and_metadata]\n",
    "                    assert len(checks) == 1, f\"expected each parameter and buffer to have exactly one checker, but {bsym.output} has {len(checks)}\"\n",
    "                    assert isinstance(param_name, str)\n",
    "                    check_dict[param_name] = (checks[0], bsym)\n",
    "            return check_dict\n",
    "\n",
    "        def add_trace_output(trace, output):\n",
    "            ret_node = trace.bound_symbols[-1]\n",
    "            assert ret_node.sym == prims.python_return\n",
    "            assert len(ret_node.args) == 1\n",
    "            ret_node.args = ((*ret_node.args[0], output),)\n",
    "\n",
    "\n",
    "        checks = get_checks(prologue_trace)\n",
    "\n",
    "        compute_producers, compute_consumers = utils.producers_and_consumers(computation_trace)\n",
    "\n",
    "        output_idxes = {id(o): i for i, o in enumerate(prologue_trace.output)}\n",
    "\n",
    "        computation_trace.push_scope([])\n",
    "        quantized_proxies : dict[int, str]= {}  # id -> name\n",
    "\n",
    "        new_bsyms = []\n",
    "        new_compute_inputs = []\n",
    "        for n, qs in self.quant_states.items():\n",
    "            param = tm.get_parameter(n)\n",
    "            n_absmax = f\"{n}.absmax\"\n",
    "            n_code = f\"{n}.code\"\n",
    "            param_absmax = tm.get_parameter(n_absmax)\n",
    "            param_code = tm.get_parameter(n_code)\n",
    "            check, get_param = checks[n]\n",
    "            quantized_proxies[id(get_param.output)] = n\n",
    "            # check has args: tensor, shape, device, dtype, requires_grad\n",
    "            proxy, _, _, _, requires_grad = check.args\n",
    "            thunder_device = thunder.devices.to_device(param.device)\n",
    "            thunder_device_str = str(thunder_device)\n",
    "            check.args = (proxy, (*param.shape,), thunder_device_str, param.dtype, False) \n",
    "\n",
    "            output_idx = output_idxes.get(id(get_param.output))\n",
    "            if output_idx is not None:\n",
    "                with tracectx(prologue_trace):\n",
    "                    # better way\n",
    "                    proxy_absmax = thunder.TensorProxy(name=f\"{get_param.output.name}_absmax\",\n",
    "                                                 shape=param_absmax.shape, dtype=thunder.dtypes.to_dtype(param_absmax.dtype),\n",
    "                                                 device=thunder.devices.to_device(param_absmax.device), requires_grad=False)\n",
    "                    proxy_code = thunder.TensorProxy(name=f\"{get_param.output.name}_code\",\n",
    "                                                 shape=param_code.shape, dtype=thunder.dtypes.to_dtype(param_code.dtype),\n",
    "                                                 device=thunder.devices.to_device(param_code.device), requires_grad=False)\n",
    "                    # get_param.sym = unpack_buffer/parameter as needed\n",
    "                    new_bsyms.append(get_param.sym.bind(get_param.args[0], n_absmax, output=proxy_absmax))\n",
    "                    new_bsyms.append(get_param.sym.bind(get_param.args[0], n_code, output=proxy_code))\n",
    "                    add_trace_output(prologue_trace, proxy_absmax)\n",
    "                    add_trace_output(prologue_trace, proxy_code)\n",
    "                    new_compute_inputs.append(proxy_absmax)\n",
    "                    new_compute_inputs.append(proxy_code)\n",
    "                    qs['proxy_absmax'] = proxy_absmax\n",
    "                    qs['proxy_code'] = proxy_code\n",
    "                compute_input = computation_trace.args[output_idx]\n",
    "\n",
    "        prologue_trace.bound_symbols[-1:-1] = new_bsyms\n",
    "\n",
    "\n",
    "        with tracectx(computation_trace):\n",
    "            new_bindings = [thunder.core.prims.unpack_trivial.bind(i, output=i) for i in new_compute_inputs]\n",
    "\n",
    "        new_computation_trace = thunder.core.trace.from_trace(computation_trace)\n",
    "        new_computation_trace.args = (*new_computation_trace.args, *new_compute_inputs)\n",
    "        new_computation_trace._siginfo.args = [(a.name, None) for a in new_computation_trace.args]\n",
    "        for idx, bsym in enumerate(computation_trace.bound_symbols):\n",
    "            if bsym.sym != prims.unpack_trivial:\n",
    "                break\n",
    "            new_computation_trace.bound_symbols.append(bsym.from_bsym())\n",
    "        new_computation_trace.bound_symbols += new_bindings\n",
    "        proxies_to_replace = {}\n",
    "        for bsym in computation_trace.bound_symbols[idx:]:\n",
    "            if bsym.sym == thunder.torch.linear and id(bsym.args[1]) in quantized_proxies:\n",
    "                assert len(bsym.args) == 3 # torch.linear(input, weight, bias) \n",
    "                n = quantized_proxies[id(bsym.args[1])]\n",
    "                qs = self.quant_states[n]\n",
    "                # bnb_matmul_nf4(x, qweight, bias, absmax, quant_map, blocksize, dtype, shape)\n",
    "                #  how to replace the _call_ctx(???)\n",
    "                new_args = (*bsym.args[:3], qs['proxy_absmax'], qs['proxy_code'], qs['blocksize'], qs['dtype'], qs['shape'])\n",
    "                new_computation_trace.bound_symbols.append(bsym.from_bsym(sym=bnb_matmul_nf4,\n",
    "                                                                          _call_ctx={'bnb_matmul_nf4': bnb_matmul_nf4_impl}, \n",
    "                                                                          subsymbols=[], args=new_args))\n",
    "            else:\n",
    "                new_computation_trace.bound_symbols.append(bsym.from_bsym())\n",
    "\n",
    "        new_computation_trace.set_provenance(thunder.core.trace.TraceProvenance(\"quant pass\"))\n",
    "        return prologue_trace, new_computation_trace, epilogue_trace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7bb75d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(4, 128, device='cuda')\n",
    "\n",
    "expected = m(a)\n",
    "\n",
    "tm = thunder.jit(m, executors=(ex, *thunder.get_default_executors()))\n",
    "\n",
    "transform = LinearQuant4bit()\n",
    "transform.transform_module(tm)\n",
    "tm2 = thunder.core.transforms.add_transform(tm, early_transform=transform)\n",
    "\n",
    "actual = tm2(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83922be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# Constructed by quant pass\n",
       "import thunder\n",
       "import thunder.torch as ltorch\n",
       "import torch\n",
       "from thunder.executors.torchex import no_autocast\n",
       "\n",
       "@torch.no_grad()\n",
       "@no_autocast\n",
       "def computation(input, t_0_weight, t_2_weight, t_0_weight_absmax, t_0_weight_code, t_2_weight_absmax, t_2_weight_code):\n",
       "  # input: \"cuda:0 f32[4, 128]\"\n",
       "  # t_0_weight: \"cuda:0 f32[256, 128]\"\n",
       "  # t_2_weight: \"cuda:0 f32[128, 256]\"\n",
       "  # t_0_weight_absmax: \"cuda:0 f32[512]\"\n",
       "  # t_0_weight_code: \"cuda:0 f32[16]\"\n",
       "  # t_2_weight_absmax: \"cuda:0 f32[512]\"\n",
       "  # t_2_weight_code: \"cuda:0 f32[16]\"\n",
       "\n",
       "  # /usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:116: \t        return F.linear(input, self.weight, self.bias)\n",
       "  t3 = bnb_matmul_nf4(input, t_0_weight, None, t_0_weight_absmax, t_0_weight_code, 64, torch.float32, (256, 128))  # t3: \"cuda:0 f32[4, 256]\"\n",
       "\n",
       "  # /usr/local/lib/python3.11/dist-packages/torch/nn/modules/activation.py:696: \t        return F.gelu(input, approximate=self.approximate)\n",
       "  t9 = ltorch.gelu(t3, approximate='none')  # t9: \"cuda:0 f32[4, 256]\"\n",
       "    # t5 = ltorch.true_divide(t3, 1.4142135623730951)  # t5: \"cuda:0 f32[4, 256]\"\n",
       "      # t5 = prims.div(t3, 1.4142135623730951)  # t5: \"cuda:0 f32[4, 256]\"\n",
       "    # t6 = ltorch.erf(t5)  # t6: \"cuda:0 f32[4, 256]\"\n",
       "      # t6 = prims.erf(t5)  # t6: \"cuda:0 f32[4, 256]\"\n",
       "    # t7 = ltorch.mul(0.5, t6)  # t7: \"cuda:0 f32[4, 256]\"\n",
       "      # t7 = prims.mul(0.5, t6)  # t7: \"cuda:0 f32[4, 256]\"\n",
       "    # t8 = ltorch.add(0.5, t7, alpha=None)  # t8: \"cuda:0 f32[4, 256]\"\n",
       "      # t8 = prims.add(0.5, t7)  # t8: \"cuda:0 f32[4, 256]\"\n",
       "    # t9 = ltorch.mul(t3, t8)  # t9: \"cuda:0 f32[4, 256]\"\n",
       "      # t9 = prims.mul(t3, t8)  # t9: \"cuda:0 f32[4, 256]\"\n",
       "\n",
       "  # /usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:116: \t        return F.linear(input, self.weight, self.bias)\n",
       "  t13 = bnb_matmul_nf4(t9, t_2_weight, None, t_2_weight_absmax, t_2_weight_code, 64, torch.float32, (128, 256))  # t13: \"cuda:0 f32[4, 128]\"\n",
       "  return t13"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thunder.last_traces(tm2)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f464fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21de3bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
