{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thunder functional jit\n",
    "\n",
    "This notebook shows how to use thunder's \"functional jit\" entrypoint, thunder.functional.jit. This function takes a \"functional\" Python function and returns another \"jitted\" Python function with the same signature. When the jitted function is called, thunder executes the its understanding of the program. If there are no \"sharp edges\" (more on that below), then the jitted function will compute the same result as the original function.\n",
    "\n",
    "Before getting into the details, let's see a simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import thunder\n",
    "from thunder.functional import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo(a, b):\n",
    "    return a + b\n",
    "\n",
    "jfoo = jit(foo)\n",
    "\n",
    "a = torch.randn((2, 2))\n",
    "b = torch.randn((2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5103, -0.0213],\n",
       "        [ 1.1842,  0.7658]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jfoo(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5103, -0.0213],\n",
       "        [ 1.1842,  0.7658]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here a function foo that just adds its inputs together is jitted, and we can verify that the result of the jitted function is the same as the result of the original function. We can also inspect what jfoo actually ran by using last_traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# Constructed by Delete Last Used (took 0 milliseconds)\n",
       "import torch\n",
       "from thunder.executors.torchex import no_autocast\n",
       "\n",
       "@torch.no_grad()\n",
       "@no_autocast()\n",
       "def computation(a, b):\n",
       "  # a \n",
       "  # b \n",
       "  t0 = torch.add(a, b)  # t0\n",
       "    # t0 = ltorch.add(a, b, alpha=None)  # t0\n",
       "      # t0 = prims.add(a, b)  # t0\n",
       "  del a, b\n",
       "  return t0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traces = thunder.last_traces(jfoo)\n",
    "traces[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see the computation that jfoo performed, which adds two tensors together using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functional jit can execute \"functional\" Python functions with input values that are PyTorch tensors, numbers, strings, PyTorch dtypes, PyTorch devices, Nones, slice objects, ellipses objects, PyTorch size objects and tuples, lists, and dicts of those values. It cannot accept other types as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot unpack object of type <class '__main__.TensorPair'>. Please file an issue requesting support.\n"
     ]
    }
   ],
   "source": [
    "# Simple class that holds a pair of tensors as \"a\" and \"b\"\n",
    "class TensorPair:\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "tp = TensorPair(a, b)\n",
    "\n",
    "def bar(tp):\n",
    "    return tp.a + tp.b\n",
    "\n",
    "jbar = jit(bar)\n",
    "\n",
    "# Attempting to pass a TensorPair object to the jitted function results\n",
    "#   in a ValueError\n",
    "try:\n",
    "    jbar(tp)\n",
    "except ValueError as ve:\n",
    "    print(ve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5103, -0.0213],\n",
       "        [ 1.1842,  0.7658]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A workaround for custom inputs is to translate them to accepted values and collections\n",
    "\n",
    "def tensorpair_wrapper(tp):\n",
    "    return jfoo(tp.a, tp.b)\n",
    "\n",
    "tensorpair_wrapper(tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functional jit will translate PyTorch functions to thunder operations by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5103, -0.0213],\n",
       "        [ 1.1842,  0.7658]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def foo_torch(a, b):\n",
    "    return torch.add(a, b)\n",
    "\n",
    "jfoo_torch = jit(foo_torch)\n",
    "jfoo_torch(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, the functional jit is intended to jit \"functional\" Python functions without \"sharp edges.\" A \"sharp edge\" is any behavior in the original Python function that will not be translated to the jitted function. Sharp edges are:\n",
    "\n",
    "- Inputs that aren't from the function's signature\n",
    "- Attempts to modify inputs\n",
    "- Calling non-functional operations and/or operations with side effects\n",
    "\n",
    "The following cells provide examples of sharp edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encountered exception ValueError: tensor([[ 0.1333, -1.0425],\n",
      "        [ 0.1407, -0.5683]]) had an unexpected type <class 'torch.Tensor'>. Supported types are (<class 'thunder.core.proxies.TensorProxy'>, <class 'numbers.Number'>) while tracing <function partial_add at 0x169b8fbe0>:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inputs that aren't from the function's signature\n",
    "\n",
    "from thunder.core.interpreter import JITError\n",
    "\n",
    "# partial_add loads the value b, which is not a signature input\n",
    "def partial_add(a):\n",
    "    return a + b\n",
    "\n",
    "jpartial_add = jit(partial_add)\n",
    "\n",
    "# The value b will cause an error, as it has not been \"proxied\" by the functional jit.\n",
    "#   Behind the scenes, the functional jit replaces its inputs with \"proxies\" to observe\n",
    "#   how they're used in the program.\n",
    "try:\n",
    "    jpartial_add(a)\n",
    "except JITError as je:\n",
    "    print(je)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to an input list is not yet supported\n"
     ]
    }
   ],
   "source": [
    "# Attempts to modify inputs\n",
    "\n",
    "def list_sum(lst):\n",
    "    accum = lst[0]\n",
    "\n",
    "    for x in lst[1:]:\n",
    "        accum = accum + x\n",
    "\n",
    "    lst.append(accum)\n",
    "\n",
    "jlist_sum = jit(list_sum)\n",
    "\n",
    "try: \n",
    "    jlist_sum([a, b])\n",
    "except NotImplementedError as nie:\n",
    "    print(nie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.6113, 1.2556],\n",
       "        [1.2779, 1.5685]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calling non-functional operations and/or operations with side effects\n",
    "import random\n",
    "\n",
    "def add_random(a):\n",
    "    return a + random.random()\n",
    "\n",
    "jadd_random = jit(add_random)\n",
    "\n",
    "jadd_random(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above example will not throw an error, even though the jitted function does not properly emulate the original function. This can be seen by looking at its last computation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# Constructed by Delete Last Used (took 0 milliseconds)\n",
       "import torch\n",
       "from thunder.executors.torchex import no_autocast\n",
       "\n",
       "@torch.no_grad()\n",
       "@no_autocast()\n",
       "def computation(a):\n",
       "  # a \n",
       "  t0 = torch.add(a, 0.4846238608207385)  # t0\n",
       "    # t0 = ltorch.add(a, 0.4846238608207385, alpha=None)  # t0\n",
       "      # t0 = prims.add(a, 0.4846238608207385)  # t0\n",
       "  del a\n",
       "  return t0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thunder.last_traces(jadd_random)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last trace, we see that the value returned from random.random() is treated as a compile-time constant, even though it's generated at runtime. This means that the jitted function will use the same value on every call, and not generate a new value using random.random(). random.random() is a non-functional operation that accepts an implicit random state input, and it also has a side effect of mutating Python's random state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, function calls are actually loads of global variables, and these are technically "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3109",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
