{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FSDP Tutorial\n",
    "\n",
    "In this tutorial, we will walk through the implementation of Fully Sharded Data Parallel (FSDP) with Zero2 sharding strategy in `thunder`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    "In recent times, the LLM models have grown so large that all the model parameters don't fit on a single GPU. To circumvent this problem, there are various strategies like Tensor Parallel, Pipeline Parallel, Fully Sharded Data Parallel, etc to train these large models. In this tutorial, we discuss and implement Zero2 strategy for Fully Sharded Data Parallel (FSDP).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Zero2 strategy for FSDP?\n",
    "\n",
    "In this strategy, we shard the model parameters across all the availabe GPUs. That is each GPU holds onto only a chunk of the parameter. During the forward pass, all GPUs call `all_gather` communication primitive to gather the parameters from other GPUs. Unlike Zero3 strategy which frees the parameter after forward pass, we save these unsharded parameters for backward pass. This is to save the overhead of extra communication. In the backward pass, we utilize the saved parameters and compute the gradients. Once the gradients are computed, we use `reduce_scatter` communication primitive to reduce (average) the gradients across all GPUs and scatter those gradients so that a given GPU holds only a chunk of gradient.\n",
    "\n",
    "For more information on FSDP, we recommend reading\n",
    "\n",
    "1. PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel - [Link](https://arxiv.org/abs/2304.11277)\n",
    "2. ZeRO: Memory Optimizations Toward Training Trillion Parameter Models - [Link](https://arxiv.org/abs/1910.02054)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Model\n",
    "\n",
    "For this example we will have a simple model `Linear(Tanh(Linear(x)))` which will be sharded over 2 GPUs\n",
    "\n",
    "**NOTE**: We are generating the abstract trace so we don't actually need a system with 2 GPUs for this. It is only required when we execute this trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed\n",
    "import thunder\n",
    "import thunder.distributed\n",
    "from looseversion import LooseVersion\n",
    "from IPython.display import Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda'\n",
    "dim = 64\n",
    "def create_model():\n",
    "    layers = [torch.nn.Linear(dim, dim, bias=False),\n",
    "              torch.nn.Tanh(),\n",
    "              torch.nn.Linear(dim, dim, bias=False)]\n",
    "    return torch.nn.Sequential(*layers).to(device)\n",
    "\n",
    "# Model\n",
    "model = create_model()\n",
    "# Input\n",
    "x = torch.randn(dim, dim, device=device)\n",
    "\n",
    "\n",
    "# we want to obtain a functional version of our model. The JIT does that internally and we reach into those\n",
    "# internals here\n",
    "thunder_model = thunder.jit(model)\n",
    "cache_rec, i_, _ = thunder.compile_data(thunder_model).get_computation_and_inputs(x)\n",
    "computation_trace = cache_rec.computation_traces[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_as_highlighted_code(trace):\n",
    "    return Code(str(trace), language=\"python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can show the functional version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrap_as_highlighted_code(computation_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 : Configuration\n",
    "\n",
    "For our implementation of FSDP, we will generate the trace where we are sharding our model over 2 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FSDP Config\n",
    "# Usually these values are set in the environment by `torchrun` but for this example\n",
    "# we will set them ourselves\n",
    "world_size = 2  # We have two processes.\n",
    "global_rank = 0  # Current process is the very first process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Function to shard parameters\n",
    "\n",
    "Next step is to write a function which will actually shard the parameters over 0-dim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: We shard over 0th dimension of the param.\n",
    "def shard_param(param: torch.Tensor, rank: int, world_size: int, name: str) -> None:\n",
    "    # We will keep it simple and error if param's 0th dim is not divisible by ``world_size``.\n",
    "    # Alternative is that we can pad our parameters so that they are divisible by `world_size`.\n",
    "    assert param.shape[0] % world_size == 0,(\n",
    "        f\"Current sharding requires the first dimension of the parameter {name!r} ({param.shape[0]})\"\n",
    "        f\" to be divisible by the world size ({world_size})\"\n",
    "    )\n",
    "    chunk_size = param.shape[0] // world_size\n",
    "\n",
    "    # rank helps us determine which chunk of the parameter we will hold.\n",
    "    shard = param.data.narrow(0, chunk_size * rank, chunk_size).clone()\n",
    "    param.data = shard\n",
    "\n",
    "# Shard each parameter of the model\n",
    "for param_name, param in model.named_parameters():\n",
    "    shard_param(param, global_rank, world_size, param_name)\n",
    "    # Mark the param to denote that it is sharded.\n",
    "    # This is required by the synchronization primitive we will use below.\n",
    "    param.distparallel_type = thunder.core.proxies.DistParallelType.FULLY_SHARDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify our model looks as expected\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us verify that we have actually sharded the parameters.\n",
    "# Checking if the weight of 1st Linear layer is sharded over 0th dim.\n",
    "assert model[0].weight.shape == (dim / world_size, dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Add an operation to synchronize the parameters before calling the model.forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to create a process group. This is needed because the synchronization primitive `synchronize` that we will use to gather and scatter our weights in forward and backward requires a process group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a process group\n",
    "\n",
    "if LooseVersion(torch.__version__) > LooseVersion(\"2.6\"):\n",
    "    # ProcessGroup constructor has been updated since https://github.com/pytorch/pytorch/pull/135653/\n",
    "    process_group = torch.distributed.distributed_c10d.ProcessGroup(torch.distributed.distributed_c10d.Store(),\n",
    "                                                                    global_rank, world_size)\n",
    "else:\n",
    "    options = torch.distributed.distributed_c10d.ProcessGroup.Options(backend=\"nccl\")\n",
    "    process_group = torch.distributed.distributed_c10d.ProcessGroup(torch.distributed.distributed_c10d.Store(),\n",
    "                                                                    global_rank, world_size, options)\n",
    "torch.distributed.distributed_c10d.GroupMember.WORLD = process_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have a  functional version of the model which\n",
    "# takes as inputs the expected arguments and all the parameters.\n",
    "functional_forward = computation_trace.python_callable()\n",
    "\n",
    "# This function creates a model with synchronization\n",
    "# before calling the forward pass.\n",
    "def model_with_syncs(x, *params):\n",
    "    # We call `prims.synchronize` on all the parameters.\n",
    "    # This is essentially calling `all_gather` so that we have the complete\n",
    "    # parameter before we actually to the forward computation.\n",
    "    unsharded_params = []\n",
    "    for param in params:\n",
    "        unsharded_params.append(thunder.distributed.prims.synchronize(param, process_group))\n",
    "\n",
    "    return functional_forward(x, *unsharded_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now see what the trace of our model looks like with all the synchronization.\n",
    "\n",
    "Two main observations regarding the below trace \n",
    "1. We can observe the `prims.synchronize` that we inserted using `model_with_syncs`.\n",
    "2. Output of the `prims.synchronize` have the shape of unsharded (original) parameter.\n",
    "\n",
    "With this, we have implemented the FSDP for the forward pass of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = thunder.trace()(model_with_syncs, x, *model.parameters())\n",
    "\n",
    "wrap_as_highlighted_code(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For backward, we don't have to do anything because `thunder` already knows how to compute the backward of `prims.synchronize`. We can verify that by using the `value_and_grad` transform to generate the complete forward and backward trace together.\n",
    "\n",
    "Observations for the trace below:\n",
    "1. `prims.synchronize` from previous trace is now decomposed into `prims.all_gather` and `prims.wait`. So, we can clearly see that we make a communication call to gather the parameter (which is asynchronous) and wait till we have the complete parameter.\n",
    "2. At the end of the trace (after the forward and the backward computation), we see calls to `prims.reduce_scatter` and `prims.wait`. This takes care of reducing the gradients across all the GPUs and sharding them. One thing to note, for averaging gradients with low dynamic range dtype like `float16`, if we naively sum the gradients across GPUs before dividing by `world_size`, it can lead to overflows. So we scale the gradient tensor with `world_size`, before calling `reduce_scatter` with `sum` reduction to effectively average the gradients without overflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thunder.core.transforms import value_and_grad\n",
    "\n",
    "forward_and_backward_model = value_and_grad(model_with_syncs)\n",
    "\n",
    "forward_backward_trace = thunder.trace()(forward_and_backward_model, x, *model.parameters())\n",
    "\n",
    "wrap_as_highlighted_code(forward_backward_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above trace, only contains primitive which specifies the semantic of an operation abstractly but doesn't perform the actual computation.\n",
    "\n",
    "Now we will generate the execution trace which can actually perform the compute.\n",
    "\n",
    "In the execution trace generated below, we can see that all the primitives have been replaced with actually PyTorch operations. Also, our synchronization primitives have been replaced with PyTorch implementation provided by thunder i.e. `torch_all_gather_prim_impl`, `torch_reduce_scatter_prim_impl`, `torch_wait_prim_impl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_trace = thunder.transform_for_execution(forward_backward_trace, executors_list=thunder.get_always_executors())\n",
    "\n",
    "# Grab the final trace\n",
    "exec_trace = optimized_trace[-1]\n",
    "wrap_as_highlighted_code(exec_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 : Running the actual computation\n",
    "\n",
    "Running the actual computation will require setting up 2 processes and running our above code in both those processes (which can be tricky with Jupyter Notebook). Instead, we will write a small script and run it with `torchrun` which takes care of setting up the processes and relevant state.\n",
    "\n",
    "**NOTE**: This requires device running this notebook to have at least 2-GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below, we will use `thunder.distributed.fsdp` which does the same as what we did above (with some extra checks). The code below should look familiar as it is roughly all the above pieces in a single script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile thunder_fsdp_simple_example.py\n",
    "\n",
    "# imports\n",
    "from thunder.tests.litgpt_model import GPT, Config\n",
    "import torch\n",
    "import torch.distributed\n",
    "import thunder\n",
    "import thunder.distributed\n",
    "import os\n",
    "\n",
    "# # # # # # # #\n",
    "# Create Model\n",
    "# # # # # # # #\n",
    "\n",
    "# NOTE: We create the model on CPU.\n",
    "device='cpu'\n",
    "dim = 64\n",
    "def create_model():\n",
    "    layers = []\n",
    "    layers.append(torch.nn.Linear(dim, dim))\n",
    "    layers.append(torch.nn.ReLU())\n",
    "    layers.append(torch.nn.Linear(dim, dim))\n",
    "    return torch.nn.Sequential(*layers).to(device)\n",
    "\n",
    "# Model\n",
    "model = create_model()\n",
    "# Input\n",
    "x = torch.randn(dim, dim, device=device)\n",
    "\n",
    "# # # # # # # #\n",
    "# Setup for distributed\n",
    "# # # # # # # #\n",
    "torch.distributed.init_process_group(backend='nccl')\n",
    "\n",
    "rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "\n",
    "device = f\"cuda:{rank}\"\n",
    "\n",
    "# # # # # # # #\n",
    "# Move inputs to correct device\n",
    "# # # # # # # #\n",
    "x = x.to(device)\n",
    "\n",
    "# # # # # # # #\n",
    "# Wrap the model in thunder.distributed.fsdp\n",
    "# # # # # # # #\n",
    "\n",
    "# thunder.distributed.fsdp takes care of moving the parameter\n",
    "# shard to the correct GPU for the current process.\n",
    "cmodel = thunder.jit(thunder.distributed.fsdp(model))\n",
    "\n",
    "# Run the forward pass.\n",
    "cmodel(x)\n",
    "\n",
    "# # # # # # # #\n",
    "# Check the traces\n",
    "# # # # # # # #\n",
    "fwd_traces = thunder.last_traces(cmodel)\n",
    "bwd_traces = thunder.last_backward_traces(cmodel)\n",
    "\n",
    "# # # # # # # #\n",
    "# Print and check to see if they match ours\n",
    "# # # # # # # #\n",
    "if rank == 0:\n",
    "    print(fwd_traces[-1])\n",
    "    print(\"*******\"* 8)\n",
    "    print(bwd_traces[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us run the above script and check what the trace looks like.\n",
    "\n",
    "We can observe that forward trace has `torch_all_gather_prim_impl` to gather the parameter before forward pass and the backward trace has `torch_reduce_scatter_prim_impl` to reduce and scatter the gradients back to different GPUs. This is similar to our implementation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun --nproc_per_node=2 thunder_fsdp_simple_example.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "We have created our implementation of FSDP to shard our model across multiple GPUs. In the process, we also learned that:\n",
    "\n",
    "1. `thunder` provides us with primitives for synchronization across mutiple GPUs.\n",
    "2. `thunder` also takes care of implementing the backward support for the synchronization primitives, so we don't have to explicitly do anything to get the backward working.\n",
    "3. We can just easily apply `thunder.distributed.fsdp` to our model and it will take care of sharding the parameters and also adding synchronizations to our model. Also, we can easily check the modifications by inspecting the traces."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
