{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FSDP Tutorial\n",
    "\n",
    "In this tutorial, we will walk through the implementation of Fully Sharded Data Parallel (FSDP) with Zero2 sharding strategy in `thunder`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    "In recent times, the LLM models have grown so large that all the model parameters don't fit on a single GPU. To circumvent this problem, there are various strategies like Tensor Parallel, Pipeline Parallel, Fully Sharded Data Parallel, etc to train these large models. In this tutorial, we discuss and implement Zero2 strategy for Fully Sharded Data Parallel (FSDP).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Zero2 strategy for FSDP?\n",
    "\n",
    "In this strategy, we shard the model parameters across all the availabe GPUs. That is each GPU holds onto only a chunk of the parameter. During the forward pass, all GPUs call `all_gather` communication primitive to gather the parameters from other GPUs. Unlike Zero3 strategy which frees the parameter after forward pass, we save these unsharded parameters for backward pass. This is to save the overhead of extra communication. In the backward pass, we utilize the saved parameters and compute the gradients. Once the gradients are computed, we use `reduce_scatter` communication primitive to reduce (average) the gradients across all GPUs and scatter those gradients so that a given GPU holds only a chunk of gradient.\n",
    "\n",
    "For more information on FSDP, we recommend reading\n",
    "\n",
    "1. PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel - [Link](https://arxiv.org/abs/2304.11277)\n",
    "2. ZeRO: Memory Optimizations Toward Training Trillion Parameter Models - [Link](https://arxiv.org/abs/1910.02054)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Model\n",
    "\n",
    "For this example we will have a simple model `Linear(Tanh(Linear(x)))` which will be sharded over 2 GPUs\n",
    "\n",
    "**NOTE**: We are generating the abstract trace so we don't actually need a system with 2 GPUs for this. It is only required when we execute this trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed\n",
    "import thunder\n",
    "import thunder.distributed\n",
    "from IPython.display import Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda'\n",
    "dim = 64\n",
    "def create_model():\n",
    "    layers = [torch.nn.Linear(dim, dim, bias=False),\n",
    "              torch.nn.Tanh(),\n",
    "              torch.nn.Linear(dim, dim, bias=False)]\n",
    "    return torch.nn.Sequential(*layers).to(device)\n",
    "\n",
    "# Model\n",
    "model = create_model()\n",
    "# Input\n",
    "x = torch.randn(dim, dim, device=device)\n",
    "\n",
    "\n",
    "# we want to obtain a functional version of our model. The JIT does that internally and we reach into those\n",
    "# internals here\n",
    "thunder_model = thunder.jit(model)\n",
    "cache_rec, i_, _ = thunder.compile_data(thunder_model).get_computation_and_inputs(x)\n",
    "computation_trace = cache_rec.computation_traces[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_as_highlighted_code(trace):\n",
    "    return Code(str(trace), language=\"python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can show the functional version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"c1\"># Constructed by Dead Code Elimination (took 0 milliseconds)</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">torch.nn.functional</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">thunder.executors.torchex</span> <span class=\"kn\">import</span> <span class=\"n\">no_autocast</span>\n",
       "\n",
       "<span class=\"nd\">@torch</span><span class=\"o\">.</span><span class=\"n\">no_grad</span><span class=\"p\">()</span>\n",
       "<span class=\"nd\">@no_autocast</span><span class=\"p\">()</span>\n",
       "<span class=\"k\">def</span> <span class=\"nf\">augmented_forward_fn</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">):</span>\n",
       "  <span class=\"c1\"># args: &quot;Collection&quot; </span>\n",
       "  <span class=\"n\">t0</span><span class=\"p\">,</span> \\\n",
       "  <span class=\"n\">t1</span><span class=\"p\">,</span> \\\n",
       "  <span class=\"n\">t2</span><span class=\"p\">,</span> \\\n",
       "  <span class=\"o\">=</span> <span class=\"n\">args</span>\n",
       "  <span class=\"n\">t3</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">functional</span><span class=\"o\">.</span><span class=\"n\">linear</span><span class=\"p\">(</span><span class=\"n\">t0</span><span class=\"p\">,</span> <span class=\"n\">t1</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">)</span>  <span class=\"c1\"># t3: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t3 = ltorch.linear(t0, t1, None)  # t3: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "      <span class=\"c1\"># t3 = prims.linear(t0, t1, None)  # t3: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"p\">[</span><span class=\"n\">t4</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">nvFusion0</span><span class=\"p\">(</span><span class=\"n\">t3</span><span class=\"p\">)</span>\n",
       "    <span class=\"c1\"># t4 = prims.tanh(t3)  # t4: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t5</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">functional</span><span class=\"o\">.</span><span class=\"n\">linear</span><span class=\"p\">(</span><span class=\"n\">t4</span><span class=\"p\">,</span> <span class=\"n\">t2</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">)</span>  <span class=\"c1\"># t5: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t5 = ltorch.linear(t4, t2, None)  # t5: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "      <span class=\"c1\"># t5 = prims.linear(t4, t2, None)  # t5: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"k\">return</span> <span class=\"p\">{</span><span class=\"s1\">&#39;output&#39;</span><span class=\"p\">:</span> <span class=\"n\">t5</span><span class=\"p\">,</span> <span class=\"s1\">&#39;flat_args&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"n\">t0</span><span class=\"p\">,</span> <span class=\"n\">t1</span><span class=\"p\">,</span> <span class=\"n\">t2</span><span class=\"p\">],</span> <span class=\"s1\">&#39;flat_output&#39;</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"n\">t5</span><span class=\"p\">,)},</span> <span class=\"p\">((</span><span class=\"n\">t0</span><span class=\"p\">,</span> <span class=\"n\">t2</span><span class=\"p\">,</span> <span class=\"n\">t4</span><span class=\"p\">),</span> <span class=\"p\">())</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{c+c1}{\\PYZsh{} Constructed by Dead Code Elimination (took 0 milliseconds)}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{torch}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{nn}\\PY{n+nn}{.}\\PY{n+nn}{functional}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{thunder}\\PY{n+nn}{.}\\PY{n+nn}{executors}\\PY{n+nn}{.}\\PY{n+nn}{torchex} \\PY{k+kn}{import} \\PY{n}{no\\PYZus{}autocast}\n",
       "\n",
       "\\PY{n+nd}{@torch}\\PY{o}{.}\\PY{n}{no\\PYZus{}grad}\\PY{p}{(}\\PY{p}{)}\n",
       "\\PY{n+nd}{@no\\PYZus{}autocast}\\PY{p}{(}\\PY{p}{)}\n",
       "\\PY{k}{def} \\PY{n+nf}{augmented\\PYZus{}forward\\PYZus{}fn}\\PY{p}{(}\\PY{o}{*}\\PY{n}{args}\\PY{p}{)}\\PY{p}{:}\n",
       "  \\PY{c+c1}{\\PYZsh{} args: \\PYZdq{}Collection\\PYZdq{} }\n",
       "  \\PY{n}{t0}\\PY{p}{,} \\PYZbs{}\n",
       "  \\PY{n}{t1}\\PY{p}{,} \\PYZbs{}\n",
       "  \\PY{n}{t2}\\PY{p}{,} \\PYZbs{}\n",
       "  \\PY{o}{=} \\PY{n}{args}\n",
       "  \\PY{n}{t3} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{functional}\\PY{o}{.}\\PY{n}{linear}\\PY{p}{(}\\PY{n}{t0}\\PY{p}{,} \\PY{n}{t1}\\PY{p}{,} \\PY{k+kc}{None}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t3: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t3 = ltorch.linear(t0, t1, None)  \\PYZsh{} t3: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "      \\PY{c+c1}{\\PYZsh{} t3 = prims.linear(t0, t1, None)  \\PYZsh{} t3: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{p}{[}\\PY{n}{t4}\\PY{p}{]} \\PY{o}{=} \\PY{n}{nvFusion0}\\PY{p}{(}\\PY{n}{t3}\\PY{p}{)}\n",
       "    \\PY{c+c1}{\\PYZsh{} t4 = prims.tanh(t3)  \\PYZsh{} t4: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t5} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{functional}\\PY{o}{.}\\PY{n}{linear}\\PY{p}{(}\\PY{n}{t4}\\PY{p}{,} \\PY{n}{t2}\\PY{p}{,} \\PY{k+kc}{None}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t5: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t5 = ltorch.linear(t4, t2, None)  \\PYZsh{} t5: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "      \\PY{c+c1}{\\PYZsh{} t5 = prims.linear(t4, t2, None)  \\PYZsh{} t5: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{k}{return} \\PY{p}{\\PYZob{}}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{output}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{:} \\PY{n}{t5}\\PY{p}{,} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{flat\\PYZus{}args}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{:} \\PY{p}{[}\\PY{n}{t0}\\PY{p}{,} \\PY{n}{t1}\\PY{p}{,} \\PY{n}{t2}\\PY{p}{]}\\PY{p}{,} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{flat\\PYZus{}output}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{:} \\PY{p}{(}\\PY{n}{t5}\\PY{p}{,}\\PY{p}{)}\\PY{p}{\\PYZcb{}}\\PY{p}{,} \\PY{p}{(}\\PY{p}{(}\\PY{n}{t0}\\PY{p}{,} \\PY{n}{t2}\\PY{p}{,} \\PY{n}{t4}\\PY{p}{)}\\PY{p}{,} \\PY{p}{(}\\PY{p}{)}\\PY{p}{)}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "# Constructed by Dead Code Elimination (took 0 milliseconds)\n",
       "import torch\n",
       "import torch.nn.functional\n",
       "from thunder.executors.torchex import no_autocast\n",
       "\n",
       "@torch.no_grad()\n",
       "@no_autocast()\n",
       "def augmented_forward_fn(*args):\n",
       "  # args: \"Collection\" \n",
       "  t0, \\\n",
       "  t1, \\\n",
       "  t2, \\\n",
       "  = args\n",
       "  t3 = torch.nn.functional.linear(t0, t1, None)  # t3: \"cuda:0 f32[64, 64]\"\n",
       "    # t3 = ltorch.linear(t0, t1, None)  # t3: \"cuda:0 f32[64, 64]\"\n",
       "      # t3 = prims.linear(t0, t1, None)  # t3: \"cuda:0 f32[64, 64]\"\n",
       "  [t4] = nvFusion0(t3)\n",
       "    # t4 = prims.tanh(t3)  # t4: \"cuda:0 f32[64, 64]\"\n",
       "  t5 = torch.nn.functional.linear(t4, t2, None)  # t5: \"cuda:0 f32[64, 64]\"\n",
       "    # t5 = ltorch.linear(t4, t2, None)  # t5: \"cuda:0 f32[64, 64]\"\n",
       "      # t5 = prims.linear(t4, t2, None)  # t5: \"cuda:0 f32[64, 64]\"\n",
       "  return {'output': t5, 'flat_args': [t0, t1, t2], 'flat_output': (t5,)}, ((t0, t2, t4), ())"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrap_as_highlighted_code(computation_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 : Configuration\n",
    "\n",
    "For our implementation of FSDP, we will generate the trace where we are sharding our model over 2 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FSDP Config \n",
    "# Usually these values are set in the environment by `torchrun` but for this example\n",
    "# we will set them ourselves\n",
    "world_size = 2  # We have two processes.\n",
    "global_rank = 0  # Current process is the very first process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Function to shard parameters\n",
    "\n",
    "Next step is to write a function which will actually shard the parameters over 0-dim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: We shard over 0th dimension of the param.\n",
    "def shard_param(param: torch.Tensor, rank: int, world_size: int, name: str) -> None:\n",
    "    # We will keep it simple and error if param's 0th dim is not divisible by ``world_size``.\n",
    "    # Alternative is that we can pad our parameters so that they are divisible by `world_size`.\n",
    "    assert param.shape[0] % world_size == 0,(\n",
    "        f\"Current sharding requires the first dimension of the parameter {name!r} ({param.shape[0]})\"\n",
    "        f\" to be divisible by the world size ({world_size})\"\n",
    "    )\n",
    "    chunk_size = param.shape[0] // world_size\n",
    "\n",
    "    # rank helps us determine which chunk of the parameter we will hold.\n",
    "    shard = param.data.narrow(0, chunk_size * rank, chunk_size).clone()\n",
    "    param.data = shard\n",
    "\n",
    "# Shard each parameter of the model\n",
    "for param_name, param in model.named_parameters():\n",
    "    shard_param(param, global_rank, world_size, param_name)\n",
    "    # Mark the param to denote that it is sharded.\n",
    "    # This is required by the synchronization primitive we will use below.\n",
    "    param.ddp_type = thunder.core.proxies.DDPType.FULLY_SHARDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=64, out_features=64, bias=False)\n",
       "  (1): Tanh()\n",
       "  (2): Linear(in_features=64, out_features=64, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify our model looks as expected\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us verify that we have actually sharded the parameters.\n",
    "# Checking if the weight of 1st Linear layer is sharded over 0th dim.\n",
    "assert model[0].weight.shape == (dim / world_size, dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Add an operation to synchronize the parameters before calling the model.forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to create a process group. This is needed because the synchronization primitive `synchronize` that we will use to gather and scatter our weights in forward and backward requires a process group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a process group\n",
    "options = torch.distributed.distributed_c10d.ProcessGroup.Options(backend=\"nccl\")\n",
    "process_group = torch.distributed.distributed_c10d.ProcessGroup(torch.distributed.distributed_c10d.Store(),\n",
    "                                                     global_rank, world_size, options)\n",
    "torch.distributed.distributed_c10d.GroupMember.WORLD = process_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Because we are trying to play tricks with the traces and skip the part that inserts the synchronization automatically but also does the translation from PyTorch to thunder, we need to drop one layer of the trace to apply this manually.\n",
    "(This is really hacky, don't try it at home!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"c1\"># Constructed by Dead Code Elimination (took 0 milliseconds)</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">thunder</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">thunder.core.prims</span> <span class=\"k\">as</span> <span class=\"nn\">prims</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">thunder.torch</span> <span class=\"k\">as</span> <span class=\"nn\">ltorch</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">torch.nn.functional</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">thunder.executors.torchex</span> <span class=\"kn\">import</span> <span class=\"n\">no_autocast</span>\n",
       "\n",
       "<span class=\"nd\">@torch</span><span class=\"o\">.</span><span class=\"n\">no_grad</span><span class=\"p\">()</span>\n",
       "<span class=\"nd\">@no_autocast</span><span class=\"p\">()</span>\n",
       "<span class=\"k\">def</span> <span class=\"nf\">augmented_forward_fn</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">):</span>\n",
       "  <span class=\"c1\"># args: &quot;Collection&quot; </span>\n",
       "  <span class=\"n\">t0</span><span class=\"p\">,</span> \\\n",
       "  <span class=\"n\">t1</span><span class=\"p\">,</span> \\\n",
       "  <span class=\"n\">t2</span><span class=\"p\">,</span> \\\n",
       "  <span class=\"o\">=</span> <span class=\"n\">args</span>\n",
       "  <span class=\"n\">t3</span> <span class=\"o\">=</span> <span class=\"n\">ltorch</span><span class=\"o\">.</span><span class=\"n\">linear</span><span class=\"p\">(</span><span class=\"n\">t0</span><span class=\"p\">,</span> <span class=\"n\">t1</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">)</span>  <span class=\"c1\"># t3: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t3 = ltorch.linear(t0, t1, None)  # t3: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "      <span class=\"c1\"># t3 = prims.linear(t0, t1, None)  # t3: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t4</span> <span class=\"o\">=</span> <span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">tanh</span><span class=\"p\">(</span><span class=\"n\">t3</span><span class=\"p\">)</span>  <span class=\"c1\"># t4: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t5</span> <span class=\"o\">=</span> <span class=\"n\">ltorch</span><span class=\"o\">.</span><span class=\"n\">linear</span><span class=\"p\">(</span><span class=\"n\">t4</span><span class=\"p\">,</span> <span class=\"n\">t2</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">)</span>  <span class=\"c1\"># t5: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t5 = ltorch.linear(t4, t2, None)  # t5: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "      <span class=\"c1\"># t5 = prims.linear(t4, t2, None)  # t5: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"k\">return</span> <span class=\"p\">{</span><span class=\"s1\">&#39;output&#39;</span><span class=\"p\">:</span> <span class=\"n\">t5</span><span class=\"p\">,</span> <span class=\"s1\">&#39;flat_args&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"n\">t0</span><span class=\"p\">,</span> <span class=\"n\">t1</span><span class=\"p\">,</span> <span class=\"n\">t2</span><span class=\"p\">],</span> <span class=\"s1\">&#39;flat_output&#39;</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"n\">t5</span><span class=\"p\">,)},</span> <span class=\"p\">((</span><span class=\"n\">t0</span><span class=\"p\">,</span> <span class=\"n\">t2</span><span class=\"p\">,</span> <span class=\"n\">t4</span><span class=\"p\">),</span> <span class=\"p\">())</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{c+c1}{\\PYZsh{} Constructed by Dead Code Elimination (took 0 milliseconds)}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{thunder}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{thunder}\\PY{n+nn}{.}\\PY{n+nn}{core}\\PY{n+nn}{.}\\PY{n+nn}{prims} \\PY{k}{as} \\PY{n+nn}{prims}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{thunder}\\PY{n+nn}{.}\\PY{n+nn}{torch} \\PY{k}{as} \\PY{n+nn}{ltorch}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{torch}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{nn}\\PY{n+nn}{.}\\PY{n+nn}{functional}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{thunder}\\PY{n+nn}{.}\\PY{n+nn}{executors}\\PY{n+nn}{.}\\PY{n+nn}{torchex} \\PY{k+kn}{import} \\PY{n}{no\\PYZus{}autocast}\n",
       "\n",
       "\\PY{n+nd}{@torch}\\PY{o}{.}\\PY{n}{no\\PYZus{}grad}\\PY{p}{(}\\PY{p}{)}\n",
       "\\PY{n+nd}{@no\\PYZus{}autocast}\\PY{p}{(}\\PY{p}{)}\n",
       "\\PY{k}{def} \\PY{n+nf}{augmented\\PYZus{}forward\\PYZus{}fn}\\PY{p}{(}\\PY{o}{*}\\PY{n}{args}\\PY{p}{)}\\PY{p}{:}\n",
       "  \\PY{c+c1}{\\PYZsh{} args: \\PYZdq{}Collection\\PYZdq{} }\n",
       "  \\PY{n}{t0}\\PY{p}{,} \\PYZbs{}\n",
       "  \\PY{n}{t1}\\PY{p}{,} \\PYZbs{}\n",
       "  \\PY{n}{t2}\\PY{p}{,} \\PYZbs{}\n",
       "  \\PY{o}{=} \\PY{n}{args}\n",
       "  \\PY{n}{t3} \\PY{o}{=} \\PY{n}{ltorch}\\PY{o}{.}\\PY{n}{linear}\\PY{p}{(}\\PY{n}{t0}\\PY{p}{,} \\PY{n}{t1}\\PY{p}{,} \\PY{k+kc}{None}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t3: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t3 = ltorch.linear(t0, t1, None)  \\PYZsh{} t3: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "      \\PY{c+c1}{\\PYZsh{} t3 = prims.linear(t0, t1, None)  \\PYZsh{} t3: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t4} \\PY{o}{=} \\PY{n}{prims}\\PY{o}{.}\\PY{n}{tanh}\\PY{p}{(}\\PY{n}{t3}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t4: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t5} \\PY{o}{=} \\PY{n}{ltorch}\\PY{o}{.}\\PY{n}{linear}\\PY{p}{(}\\PY{n}{t4}\\PY{p}{,} \\PY{n}{t2}\\PY{p}{,} \\PY{k+kc}{None}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t5: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t5 = ltorch.linear(t4, t2, None)  \\PYZsh{} t5: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "      \\PY{c+c1}{\\PYZsh{} t5 = prims.linear(t4, t2, None)  \\PYZsh{} t5: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{k}{return} \\PY{p}{\\PYZob{}}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{output}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{:} \\PY{n}{t5}\\PY{p}{,} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{flat\\PYZus{}args}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{:} \\PY{p}{[}\\PY{n}{t0}\\PY{p}{,} \\PY{n}{t1}\\PY{p}{,} \\PY{n}{t2}\\PY{p}{]}\\PY{p}{,} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{flat\\PYZus{}output}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{:} \\PY{p}{(}\\PY{n}{t5}\\PY{p}{,}\\PY{p}{)}\\PY{p}{\\PYZcb{}}\\PY{p}{,} \\PY{p}{(}\\PY{p}{(}\\PY{n}{t0}\\PY{p}{,} \\PY{n}{t2}\\PY{p}{,} \\PY{n}{t4}\\PY{p}{)}\\PY{p}{,} \\PY{p}{(}\\PY{p}{)}\\PY{p}{)}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "# Constructed by Dead Code Elimination (took 0 milliseconds)\n",
       "import thunder\n",
       "import thunder.core.prims as prims\n",
       "import thunder.torch as ltorch\n",
       "import torch\n",
       "import torch.nn.functional\n",
       "from thunder.executors.torchex import no_autocast\n",
       "\n",
       "@torch.no_grad()\n",
       "@no_autocast()\n",
       "def augmented_forward_fn(*args):\n",
       "  # args: \"Collection\" \n",
       "  t0, \\\n",
       "  t1, \\\n",
       "  t2, \\\n",
       "  = args\n",
       "  t3 = ltorch.linear(t0, t1, None)  # t3: \"cuda:0 f32[64, 64]\"\n",
       "    # t3 = ltorch.linear(t0, t1, None)  # t3: \"cuda:0 f32[64, 64]\"\n",
       "      # t3 = prims.linear(t0, t1, None)  # t3: \"cuda:0 f32[64, 64]\"\n",
       "  t4 = prims.tanh(t3)  # t4: \"cuda:0 f32[64, 64]\"\n",
       "  t5 = ltorch.linear(t4, t2, None)  # t5: \"cuda:0 f32[64, 64]\"\n",
       "    # t5 = ltorch.linear(t4, t2, None)  # t5: \"cuda:0 f32[64, 64]\"\n",
       "      # t5 = prims.linear(t4, t2, None)  # t5: \"cuda:0 f32[64, 64]\"\n",
       "  return {'output': t5, 'flat_args': [t0, t1, t2], 'flat_output': (t5,)}, ((t0, t2, t4), ())"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### DON'T TRY THIS AT HOME\n",
    "computation_trace.bound_symbols[2].sym = cache_rec.computation_traces[0].bound_symbols[2].subsymbols[0].sym\n",
    "if cache_rec.computation_traces[0].bound_symbols[3].subsymbols:\n",
    "    computation_trace.bound_symbols[3] = cache_rec.computation_traces[0].bound_symbols[3].subsymbols[0]\n",
    "computation_trace.bound_symbols[4].sym = cache_rec.computation_traces[0].bound_symbols[4].subsymbols[0].sym\n",
    "\n",
    "wrap_as_highlighted_code(computation_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have a  functional version of the model which\n",
    "# takes as inputs the expected arguments and all the parameters.\n",
    "functional_forward = computation_trace.python_callable()\n",
    "\n",
    "# This function creates a model with synchronization\n",
    "# before calling the forward pass.\n",
    "def model_with_syncs(x, *params):\n",
    "    # We call `prims.synchronize` on all the parameters.\n",
    "    # This is essentially calling `all_gather` so that we have the complete\n",
    "    # parameter before we actually to the forward computation.\n",
    "    unsharded_params = []\n",
    "    for param in params:\n",
    "        unsharded_params.append(thunder.distributed.prims.synchronize(param, process_group))\n",
    "\n",
    "    return functional_forward(x, *unsharded_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now see what the trace of our model looks like with all the synchronization.\n",
    "\n",
    "Two main observations regarding the below trace \n",
    "1. We can observe the `prims.synchronize` that we inserted using `model_with_syncs`.\n",
    "2. Output of the `prims.synchronize` have the shape of unsharded (original) parameter.\n",
    "\n",
    "With this, we have implemented the FSDP for the forward pass of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"c1\"># Constructed by Dead Code Elimination (took 0 milliseconds)</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">thunder</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">thunder.core.prims</span> <span class=\"k\">as</span> <span class=\"nn\">prims</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">thunder.distributed.prims</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">thunder.torch</span> <span class=\"k\">as</span> <span class=\"nn\">ltorch</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">thunder.executors.torchex</span> <span class=\"kn\">import</span> <span class=\"n\">no_autocast</span>\n",
       "\n",
       "<span class=\"nd\">@torch</span><span class=\"o\">.</span><span class=\"n\">no_grad</span><span class=\"p\">()</span>\n",
       "<span class=\"nd\">@no_autocast</span><span class=\"p\">()</span>\n",
       "<span class=\"k\">def</span> <span class=\"nf\">model_with_syncs</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"o\">*</span><span class=\"n\">params</span><span class=\"p\">):</span>\n",
       "  <span class=\"c1\"># x: &quot;cuda:0 f32[64, 64]&quot; </span>\n",
       "  <span class=\"c1\"># params: &quot;Collection&quot; </span>\n",
       "  <span class=\"n\">t0</span><span class=\"p\">,</span> \\\n",
       "  <span class=\"n\">t1</span><span class=\"p\">,</span> \\\n",
       "  <span class=\"o\">=</span> <span class=\"n\">params</span>\n",
       "  <span class=\"n\">t2</span> <span class=\"o\">=</span> <span class=\"n\">thunder</span><span class=\"o\">.</span><span class=\"n\">distributed</span><span class=\"o\">.</span><span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">synchronize</span><span class=\"p\">(</span><span class=\"n\">t0</span><span class=\"p\">,</span> <span class=\"n\">_torch_distributed_distributed_c10d_ProcessGroup_0</span><span class=\"p\">)</span>  <span class=\"c1\"># t2: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t3</span> <span class=\"o\">=</span> <span class=\"n\">thunder</span><span class=\"o\">.</span><span class=\"n\">distributed</span><span class=\"o\">.</span><span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">synchronize</span><span class=\"p\">(</span><span class=\"n\">t1</span><span class=\"p\">,</span> <span class=\"n\">_torch_distributed_distributed_c10d_ProcessGroup_0</span><span class=\"p\">)</span>  <span class=\"c1\"># t3: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t4</span> <span class=\"o\">=</span> <span class=\"n\">ltorch</span><span class=\"o\">.</span><span class=\"n\">linear</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">t2</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">)</span>  <span class=\"c1\"># t4: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t4 = prims.linear(x, t2, None)  # t4: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t5</span> <span class=\"o\">=</span> <span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">tanh</span><span class=\"p\">(</span><span class=\"n\">t4</span><span class=\"p\">)</span>  <span class=\"c1\"># t5: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t6</span> <span class=\"o\">=</span> <span class=\"n\">ltorch</span><span class=\"o\">.</span><span class=\"n\">linear</span><span class=\"p\">(</span><span class=\"n\">t5</span><span class=\"p\">,</span> <span class=\"n\">t3</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">)</span>  <span class=\"c1\"># t6: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t6 = prims.linear(t5, t3, None)  # t6: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"k\">return</span> <span class=\"p\">({</span><span class=\"s1\">&#39;output&#39;</span><span class=\"p\">:</span> <span class=\"n\">t6</span><span class=\"p\">,</span> <span class=\"s1\">&#39;flat_args&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">t2</span><span class=\"p\">,</span> <span class=\"n\">t3</span><span class=\"p\">],</span> <span class=\"s1\">&#39;flat_output&#39;</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"n\">t6</span><span class=\"p\">,)},</span> <span class=\"p\">((</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">t3</span><span class=\"p\">,</span> <span class=\"n\">t5</span><span class=\"p\">),</span> <span class=\"p\">()))</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{c+c1}{\\PYZsh{} Constructed by Dead Code Elimination (took 0 milliseconds)}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{thunder}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{thunder}\\PY{n+nn}{.}\\PY{n+nn}{core}\\PY{n+nn}{.}\\PY{n+nn}{prims} \\PY{k}{as} \\PY{n+nn}{prims}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{thunder}\\PY{n+nn}{.}\\PY{n+nn}{distributed}\\PY{n+nn}{.}\\PY{n+nn}{prims}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{thunder}\\PY{n+nn}{.}\\PY{n+nn}{torch} \\PY{k}{as} \\PY{n+nn}{ltorch}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{torch}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{thunder}\\PY{n+nn}{.}\\PY{n+nn}{executors}\\PY{n+nn}{.}\\PY{n+nn}{torchex} \\PY{k+kn}{import} \\PY{n}{no\\PYZus{}autocast}\n",
       "\n",
       "\\PY{n+nd}{@torch}\\PY{o}{.}\\PY{n}{no\\PYZus{}grad}\\PY{p}{(}\\PY{p}{)}\n",
       "\\PY{n+nd}{@no\\PYZus{}autocast}\\PY{p}{(}\\PY{p}{)}\n",
       "\\PY{k}{def} \\PY{n+nf}{model\\PYZus{}with\\PYZus{}syncs}\\PY{p}{(}\\PY{n}{x}\\PY{p}{,} \\PY{o}{*}\\PY{n}{params}\\PY{p}{)}\\PY{p}{:}\n",
       "  \\PY{c+c1}{\\PYZsh{} x: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{} }\n",
       "  \\PY{c+c1}{\\PYZsh{} params: \\PYZdq{}Collection\\PYZdq{} }\n",
       "  \\PY{n}{t0}\\PY{p}{,} \\PYZbs{}\n",
       "  \\PY{n}{t1}\\PY{p}{,} \\PYZbs{}\n",
       "  \\PY{o}{=} \\PY{n}{params}\n",
       "  \\PY{n}{t2} \\PY{o}{=} \\PY{n}{thunder}\\PY{o}{.}\\PY{n}{distributed}\\PY{o}{.}\\PY{n}{prims}\\PY{o}{.}\\PY{n}{synchronize}\\PY{p}{(}\\PY{n}{t0}\\PY{p}{,} \\PY{n}{\\PYZus{}torch\\PYZus{}distributed\\PYZus{}distributed\\PYZus{}c10d\\PYZus{}ProcessGroup\\PYZus{}0}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t2: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t3} \\PY{o}{=} \\PY{n}{thunder}\\PY{o}{.}\\PY{n}{distributed}\\PY{o}{.}\\PY{n}{prims}\\PY{o}{.}\\PY{n}{synchronize}\\PY{p}{(}\\PY{n}{t1}\\PY{p}{,} \\PY{n}{\\PYZus{}torch\\PYZus{}distributed\\PYZus{}distributed\\PYZus{}c10d\\PYZus{}ProcessGroup\\PYZus{}0}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t3: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t4} \\PY{o}{=} \\PY{n}{ltorch}\\PY{o}{.}\\PY{n}{linear}\\PY{p}{(}\\PY{n}{x}\\PY{p}{,} \\PY{n}{t2}\\PY{p}{,} \\PY{k+kc}{None}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t4: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t4 = prims.linear(x, t2, None)  \\PYZsh{} t4: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t5} \\PY{o}{=} \\PY{n}{prims}\\PY{o}{.}\\PY{n}{tanh}\\PY{p}{(}\\PY{n}{t4}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t5: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t6} \\PY{o}{=} \\PY{n}{ltorch}\\PY{o}{.}\\PY{n}{linear}\\PY{p}{(}\\PY{n}{t5}\\PY{p}{,} \\PY{n}{t3}\\PY{p}{,} \\PY{k+kc}{None}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t6: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t6 = prims.linear(t5, t3, None)  \\PYZsh{} t6: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{k}{return} \\PY{p}{(}\\PY{p}{\\PYZob{}}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{output}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{:} \\PY{n}{t6}\\PY{p}{,} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{flat\\PYZus{}args}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{:} \\PY{p}{[}\\PY{n}{x}\\PY{p}{,} \\PY{n}{t2}\\PY{p}{,} \\PY{n}{t3}\\PY{p}{]}\\PY{p}{,} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{flat\\PYZus{}output}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{:} \\PY{p}{(}\\PY{n}{t6}\\PY{p}{,}\\PY{p}{)}\\PY{p}{\\PYZcb{}}\\PY{p}{,} \\PY{p}{(}\\PY{p}{(}\\PY{n}{x}\\PY{p}{,} \\PY{n}{t3}\\PY{p}{,} \\PY{n}{t5}\\PY{p}{)}\\PY{p}{,} \\PY{p}{(}\\PY{p}{)}\\PY{p}{)}\\PY{p}{)}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "# Constructed by Dead Code Elimination (took 0 milliseconds)\n",
       "import thunder\n",
       "import thunder.core.prims as prims\n",
       "import thunder.distributed.prims\n",
       "import thunder.torch as ltorch\n",
       "import torch\n",
       "from thunder.executors.torchex import no_autocast\n",
       "\n",
       "@torch.no_grad()\n",
       "@no_autocast()\n",
       "def model_with_syncs(x, *params):\n",
       "  # x: \"cuda:0 f32[64, 64]\" \n",
       "  # params: \"Collection\" \n",
       "  t0, \\\n",
       "  t1, \\\n",
       "  = params\n",
       "  t2 = thunder.distributed.prims.synchronize(t0, _torch_distributed_distributed_c10d_ProcessGroup_0)  # t2: \"cuda:0 f32[64, 64]\"\n",
       "  t3 = thunder.distributed.prims.synchronize(t1, _torch_distributed_distributed_c10d_ProcessGroup_0)  # t3: \"cuda:0 f32[64, 64]\"\n",
       "  t4 = ltorch.linear(x, t2, None)  # t4: \"cuda:0 f32[64, 64]\"\n",
       "    # t4 = prims.linear(x, t2, None)  # t4: \"cuda:0 f32[64, 64]\"\n",
       "  t5 = prims.tanh(t4)  # t5: \"cuda:0 f32[64, 64]\"\n",
       "  t6 = ltorch.linear(t5, t3, None)  # t6: \"cuda:0 f32[64, 64]\"\n",
       "    # t6 = prims.linear(t5, t3, None)  # t6: \"cuda:0 f32[64, 64]\"\n",
       "  return ({'output': t6, 'flat_args': [x, t2, t3], 'flat_output': (t6,)}, ((x, t3, t5), ()))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace = thunder.trace()(model_with_syncs, x, *model.parameters())\n",
    "\n",
    "wrap_as_highlighted_code(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For backward, we don't have to do anything because `thunder` already knows how to compute the backward of `prims.synchronize`. We can verify that by using the `value_and_grad` transform to generate the complete forward and backward trace together.\n",
    "\n",
    "Observations for the trace below:\n",
    "1. `prims.synchronize` from previous trace is now decomposed into `prims.all_gather` and `prims.wait`. So, we can clearly see that we make a communication call to gather the parameter (which is asynchronous) and wait till we have the complete parameter.\n",
    "2. At the end of the trace (after the forward and the backward computation), we see calls to `prims.reduce_scatter` and `prims.wait`. This takes care of reducing the gradients across all the GPUs and sharding them. One thing to note, for averaging gradients with low dynamic range dtype like `float16`, if we naively sum the gradients across GPUs before dividing by `world_size`, it can lead to overflows. So we scale the gradient tensor with `world_size`, before calling `reduce_scatter` with `sum` reduction to effectively average the gradients without overflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"c1\"># Constructed by Dead Code Elimination (took 1 milliseconds)</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">thunder</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">thunder.core.devices</span> <span class=\"k\">as</span> <span class=\"nn\">devices</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">thunder.core.dtypes</span> <span class=\"k\">as</span> <span class=\"nn\">dtypes</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">thunder.core.prims</span> <span class=\"k\">as</span> <span class=\"nn\">prims</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">thunder.distributed.prims</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">thunder.torch</span> <span class=\"k\">as</span> <span class=\"nn\">ltorch</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">thunder.executors.torchex</span> <span class=\"kn\">import</span> <span class=\"n\">no_autocast</span>\n",
       "\n",
       "<span class=\"nd\">@torch</span><span class=\"o\">.</span><span class=\"n\">no_grad</span><span class=\"p\">()</span>\n",
       "<span class=\"nd\">@no_autocast</span><span class=\"p\">()</span>\n",
       "<span class=\"k\">def</span> <span class=\"nf\">_value_and_grad</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">):</span>\n",
       "  <span class=\"c1\"># args: &quot;Collection&quot; </span>\n",
       "  <span class=\"n\">t0</span><span class=\"p\">,</span> \\\n",
       "  <span class=\"n\">t1</span><span class=\"p\">,</span> \\\n",
       "  <span class=\"n\">t2</span><span class=\"p\">,</span> \\\n",
       "  <span class=\"o\">=</span> <span class=\"n\">args</span>\n",
       "  <span class=\"n\">t3</span> <span class=\"o\">=</span> <span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">full</span><span class=\"p\">((</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">),</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">devices</span><span class=\"o\">.</span><span class=\"n\">Device</span><span class=\"p\">(</span><span class=\"s2\">&quot;cuda:0&quot;</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtypes</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>  <span class=\"c1\"># t3: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t4</span> <span class=\"o\">=</span> <span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">full</span><span class=\"p\">((</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">),</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">devices</span><span class=\"o\">.</span><span class=\"n\">Device</span><span class=\"p\">(</span><span class=\"s2\">&quot;cuda:0&quot;</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtypes</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>  <span class=\"c1\"># t4: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t5</span> <span class=\"o\">=</span> <span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">full</span><span class=\"p\">((</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">),</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">devices</span><span class=\"o\">.</span><span class=\"n\">Device</span><span class=\"p\">(</span><span class=\"s2\">&quot;cuda:0&quot;</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtypes</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>  <span class=\"c1\"># t5: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t6</span> <span class=\"o\">=</span> <span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">full</span><span class=\"p\">((</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">),</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">devices</span><span class=\"o\">.</span><span class=\"n\">Device</span><span class=\"p\">(</span><span class=\"s2\">&quot;cuda:0&quot;</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtypes</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>  <span class=\"c1\"># t6: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t7</span> <span class=\"o\">=</span> <span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">full</span><span class=\"p\">((</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">),</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">devices</span><span class=\"o\">.</span><span class=\"n\">Device</span><span class=\"p\">(</span><span class=\"s2\">&quot;cuda:0&quot;</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtypes</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>  <span class=\"c1\"># t7: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t8</span> <span class=\"o\">=</span> <span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">full</span><span class=\"p\">((</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">),</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">devices</span><span class=\"o\">.</span><span class=\"n\">Device</span><span class=\"p\">(</span><span class=\"s2\">&quot;cuda:0&quot;</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtypes</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>  <span class=\"c1\"># t8: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t9</span> <span class=\"o\">=</span> <span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">full</span><span class=\"p\">((</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">),</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">devices</span><span class=\"o\">.</span><span class=\"n\">Device</span><span class=\"p\">(</span><span class=\"s2\">&quot;cuda:0&quot;</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtypes</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>  <span class=\"c1\"># t9: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t10</span> <span class=\"o\">=</span> <span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">full</span><span class=\"p\">((</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">),</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">devices</span><span class=\"o\">.</span><span class=\"n\">Device</span><span class=\"p\">(</span><span class=\"s2\">&quot;cuda:0&quot;</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtypes</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>  <span class=\"c1\"># t10: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">p11</span> <span class=\"o\">=</span> <span class=\"n\">thunder</span><span class=\"o\">.</span><span class=\"n\">distributed</span><span class=\"o\">.</span><span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">all_gather</span><span class=\"p\">(</span><span class=\"n\">t1</span><span class=\"p\">,</span> <span class=\"n\">_torch_distributed_distributed_c10d_ProcessGroup_0</span><span class=\"p\">,</span> <span class=\"kc\">True</span><span class=\"p\">)</span>  <span class=\"c1\"># p11: &quot;FUTURE cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t12</span> <span class=\"o\">=</span> <span class=\"n\">thunder</span><span class=\"o\">.</span><span class=\"n\">distributed</span><span class=\"o\">.</span><span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">wait</span><span class=\"p\">(</span><span class=\"n\">p11</span><span class=\"p\">)</span>  <span class=\"c1\"># t12: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">p13</span> <span class=\"o\">=</span> <span class=\"n\">thunder</span><span class=\"o\">.</span><span class=\"n\">distributed</span><span class=\"o\">.</span><span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">all_gather</span><span class=\"p\">(</span><span class=\"n\">t2</span><span class=\"p\">,</span> <span class=\"n\">_torch_distributed_distributed_c10d_ProcessGroup_0</span><span class=\"p\">,</span> <span class=\"kc\">True</span><span class=\"p\">)</span>  <span class=\"c1\"># p13: &quot;FUTURE cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t14</span> <span class=\"o\">=</span> <span class=\"n\">thunder</span><span class=\"o\">.</span><span class=\"n\">distributed</span><span class=\"o\">.</span><span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">wait</span><span class=\"p\">(</span><span class=\"n\">p13</span><span class=\"p\">)</span>  <span class=\"c1\"># t14: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t15</span> <span class=\"o\">=</span> <span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">linear</span><span class=\"p\">(</span><span class=\"n\">t0</span><span class=\"p\">,</span> <span class=\"n\">t12</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">)</span>  <span class=\"c1\"># t15: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t16</span> <span class=\"o\">=</span> <span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">tanh</span><span class=\"p\">(</span><span class=\"n\">t15</span><span class=\"p\">)</span>  <span class=\"c1\"># t16: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t17</span> <span class=\"o\">=</span> <span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">linear</span><span class=\"p\">(</span><span class=\"n\">t16</span><span class=\"p\">,</span> <span class=\"n\">t14</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">)</span>  <span class=\"c1\"># t17: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t18</span> <span class=\"o\">=</span> <span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">add</span><span class=\"p\">(</span><span class=\"n\">t6</span><span class=\"p\">,</span> <span class=\"n\">t7</span><span class=\"p\">)</span>  <span class=\"c1\"># t18: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t19</span> <span class=\"o\">=</span> <span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">add</span><span class=\"p\">(</span><span class=\"n\">t3</span><span class=\"p\">,</span> <span class=\"n\">t8</span><span class=\"p\">)</span>  <span class=\"c1\"># t19: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t20</span> <span class=\"o\">=</span> <span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">add</span><span class=\"p\">(</span><span class=\"n\">t5</span><span class=\"p\">,</span> <span class=\"n\">t9</span><span class=\"p\">)</span>  <span class=\"c1\"># t20: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t21</span> <span class=\"o\">=</span> <span class=\"n\">ltorch</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">t18</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">)</span>  <span class=\"c1\"># t21: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t21 = prims.reshape(t18, (64, 64))  # t21: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t22</span> <span class=\"o\">=</span> <span class=\"n\">ltorch</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">t21</span><span class=\"p\">,</span> <span class=\"n\">t14</span><span class=\"p\">)</span>  <span class=\"c1\"># t22: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t22 = prims.matmul(t21, t14)  # t22: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t23</span> <span class=\"o\">=</span> <span class=\"n\">ltorch</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">t18</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">)</span>  <span class=\"c1\"># t23: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t23 = prims.reshape(t18, (64, 64))  # t23: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t24</span> <span class=\"o\">=</span> <span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">transpose</span><span class=\"p\">(</span><span class=\"n\">t23</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">))</span>  <span class=\"c1\"># t24: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t25</span> <span class=\"o\">=</span> <span class=\"n\">ltorch</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">t16</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">)</span>  <span class=\"c1\"># t25: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t25 = prims.reshape(t16, (64, 64))  # t25: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t26</span> <span class=\"o\">=</span> <span class=\"n\">ltorch</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">t24</span><span class=\"p\">,</span> <span class=\"n\">t25</span><span class=\"p\">)</span>  <span class=\"c1\"># t26: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t26 = prims.matmul(t24, t25)  # t26: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t27</span> <span class=\"o\">=</span> <span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">add</span><span class=\"p\">(</span><span class=\"n\">t10</span><span class=\"p\">,</span> <span class=\"n\">t22</span><span class=\"p\">)</span>  <span class=\"c1\"># t27: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t28</span> <span class=\"o\">=</span> <span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">add</span><span class=\"p\">(</span><span class=\"n\">t20</span><span class=\"p\">,</span> <span class=\"n\">t26</span><span class=\"p\">)</span>  <span class=\"c1\"># t28: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t29</span> <span class=\"o\">=</span> <span class=\"n\">ltorch</span><span class=\"o\">.</span><span class=\"n\">mul</span><span class=\"p\">(</span><span class=\"n\">t16</span><span class=\"p\">,</span> <span class=\"n\">t16</span><span class=\"p\">)</span>  <span class=\"c1\"># t29: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t29 = prims.mul(t16, t16)  # t29: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t30</span> <span class=\"o\">=</span> <span class=\"n\">ltorch</span><span class=\"o\">.</span><span class=\"n\">sub</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">t29</span><span class=\"p\">,</span> <span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">)</span>  <span class=\"c1\"># t30: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># _ = prims.convert_element_type(1, float)</span>\n",
       "    <span class=\"c1\"># t30 = prims.sub(1.0, t29)  # t30: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t31</span> <span class=\"o\">=</span> <span class=\"n\">ltorch</span><span class=\"o\">.</span><span class=\"n\">mul</span><span class=\"p\">(</span><span class=\"n\">t27</span><span class=\"p\">,</span> <span class=\"n\">t30</span><span class=\"p\">)</span>  <span class=\"c1\"># t31: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t31 = prims.mul(t27, t30)  # t31: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t32</span> <span class=\"o\">=</span> <span class=\"n\">ltorch</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">t31</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">)</span>  <span class=\"c1\"># t32: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t32 = prims.reshape(t31, (64, 64))  # t32: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t33</span> <span class=\"o\">=</span> <span class=\"n\">ltorch</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">t32</span><span class=\"p\">,</span> <span class=\"n\">t12</span><span class=\"p\">)</span>  <span class=\"c1\"># t33: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t33 = prims.matmul(t32, t12)  # t33: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t34</span> <span class=\"o\">=</span> <span class=\"n\">ltorch</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">t31</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">)</span>  <span class=\"c1\"># t34: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t34 = prims.reshape(t31, (64, 64))  # t34: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t35</span> <span class=\"o\">=</span> <span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">transpose</span><span class=\"p\">(</span><span class=\"n\">t34</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">))</span>  <span class=\"c1\"># t35: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t36</span> <span class=\"o\">=</span> <span class=\"n\">ltorch</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">t0</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">)</span>  <span class=\"c1\"># t36: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t36 = prims.reshape(t0, (64, 64))  # t36: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t37</span> <span class=\"o\">=</span> <span class=\"n\">ltorch</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">t35</span><span class=\"p\">,</span> <span class=\"n\">t36</span><span class=\"p\">)</span>  <span class=\"c1\"># t37: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t37 = prims.matmul(t35, t36)  # t37: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t38</span> <span class=\"o\">=</span> <span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">add</span><span class=\"p\">(</span><span class=\"n\">t19</span><span class=\"p\">,</span> <span class=\"n\">t33</span><span class=\"p\">)</span>  <span class=\"c1\"># t38: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t39</span> <span class=\"o\">=</span> <span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">add</span><span class=\"p\">(</span><span class=\"n\">t4</span><span class=\"p\">,</span> <span class=\"n\">t37</span><span class=\"p\">)</span>  <span class=\"c1\"># t39: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t40</span> <span class=\"o\">=</span> <span class=\"n\">ltorch</span><span class=\"o\">.</span><span class=\"n\">true_divide</span><span class=\"p\">(</span><span class=\"n\">t28</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)</span>  <span class=\"c1\"># t40: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># _ = prims.convert_element_type(2, float)</span>\n",
       "    <span class=\"c1\"># t40 = prims.div(t28, 2.0)  # t40: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">p41</span> <span class=\"o\">=</span> <span class=\"n\">thunder</span><span class=\"o\">.</span><span class=\"n\">distributed</span><span class=\"o\">.</span><span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">reduce_scatter</span><span class=\"p\">(</span><span class=\"n\">t40</span><span class=\"p\">,</span> <span class=\"n\">_DistributedReduceOps_1</span><span class=\"p\">,</span> <span class=\"n\">_torch_distributed_distributed_c10d_ProcessGroup_0</span><span class=\"p\">,</span> <span class=\"kc\">True</span><span class=\"p\">)</span>  <span class=\"c1\"># p41: &quot;FUTURE cuda:0 f32[32, 64]&quot;</span>\n",
       "  <span class=\"n\">t42</span> <span class=\"o\">=</span> <span class=\"n\">thunder</span><span class=\"o\">.</span><span class=\"n\">distributed</span><span class=\"o\">.</span><span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">wait</span><span class=\"p\">(</span><span class=\"n\">p41</span><span class=\"p\">)</span>  <span class=\"c1\"># t42: &quot;cuda:0 f32[32, 64]&quot;</span>\n",
       "  <span class=\"n\">t43</span> <span class=\"o\">=</span> <span class=\"n\">ltorch</span><span class=\"o\">.</span><span class=\"n\">true_divide</span><span class=\"p\">(</span><span class=\"n\">t39</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)</span>  <span class=\"c1\"># t43: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># _ = prims.convert_element_type(2, float)</span>\n",
       "    <span class=\"c1\"># t43 = prims.div(t39, 2.0)  # t43: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">p44</span> <span class=\"o\">=</span> <span class=\"n\">thunder</span><span class=\"o\">.</span><span class=\"n\">distributed</span><span class=\"o\">.</span><span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">reduce_scatter</span><span class=\"p\">(</span><span class=\"n\">t43</span><span class=\"p\">,</span> <span class=\"n\">_DistributedReduceOps_1</span><span class=\"p\">,</span> <span class=\"n\">_torch_distributed_distributed_c10d_ProcessGroup_0</span><span class=\"p\">,</span> <span class=\"kc\">True</span><span class=\"p\">)</span>  <span class=\"c1\"># p44: &quot;FUTURE cuda:0 f32[32, 64]&quot;</span>\n",
       "  <span class=\"n\">t45</span> <span class=\"o\">=</span> <span class=\"n\">thunder</span><span class=\"o\">.</span><span class=\"n\">distributed</span><span class=\"o\">.</span><span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">wait</span><span class=\"p\">(</span><span class=\"n\">p44</span><span class=\"p\">)</span>  <span class=\"c1\"># t45: &quot;cuda:0 f32[32, 64]&quot;</span>\n",
       "  <span class=\"k\">return</span> <span class=\"p\">(({</span><span class=\"s1\">&#39;output&#39;</span><span class=\"p\">:</span> <span class=\"n\">t17</span><span class=\"p\">,</span> <span class=\"s1\">&#39;flat_args&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"n\">t0</span><span class=\"p\">,</span> <span class=\"n\">t12</span><span class=\"p\">,</span> <span class=\"n\">t14</span><span class=\"p\">],</span> <span class=\"s1\">&#39;flat_output&#39;</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"n\">t17</span><span class=\"p\">,)},</span> <span class=\"p\">((</span><span class=\"n\">t0</span><span class=\"p\">,</span> <span class=\"n\">t14</span><span class=\"p\">,</span> <span class=\"n\">t16</span><span class=\"p\">),</span> <span class=\"p\">())),</span> <span class=\"p\">(</span><span class=\"n\">t38</span><span class=\"p\">,</span> <span class=\"n\">t45</span><span class=\"p\">,</span> <span class=\"n\">t42</span><span class=\"p\">))</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{c+c1}{\\PYZsh{} Constructed by Dead Code Elimination (took 1 milliseconds)}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{thunder}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{thunder}\\PY{n+nn}{.}\\PY{n+nn}{core}\\PY{n+nn}{.}\\PY{n+nn}{devices} \\PY{k}{as} \\PY{n+nn}{devices}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{thunder}\\PY{n+nn}{.}\\PY{n+nn}{core}\\PY{n+nn}{.}\\PY{n+nn}{dtypes} \\PY{k}{as} \\PY{n+nn}{dtypes}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{thunder}\\PY{n+nn}{.}\\PY{n+nn}{core}\\PY{n+nn}{.}\\PY{n+nn}{prims} \\PY{k}{as} \\PY{n+nn}{prims}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{thunder}\\PY{n+nn}{.}\\PY{n+nn}{distributed}\\PY{n+nn}{.}\\PY{n+nn}{prims}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{thunder}\\PY{n+nn}{.}\\PY{n+nn}{torch} \\PY{k}{as} \\PY{n+nn}{ltorch}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{torch}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{thunder}\\PY{n+nn}{.}\\PY{n+nn}{executors}\\PY{n+nn}{.}\\PY{n+nn}{torchex} \\PY{k+kn}{import} \\PY{n}{no\\PYZus{}autocast}\n",
       "\n",
       "\\PY{n+nd}{@torch}\\PY{o}{.}\\PY{n}{no\\PYZus{}grad}\\PY{p}{(}\\PY{p}{)}\n",
       "\\PY{n+nd}{@no\\PYZus{}autocast}\\PY{p}{(}\\PY{p}{)}\n",
       "\\PY{k}{def} \\PY{n+nf}{\\PYZus{}value\\PYZus{}and\\PYZus{}grad}\\PY{p}{(}\\PY{o}{*}\\PY{n}{args}\\PY{p}{)}\\PY{p}{:}\n",
       "  \\PY{c+c1}{\\PYZsh{} args: \\PYZdq{}Collection\\PYZdq{} }\n",
       "  \\PY{n}{t0}\\PY{p}{,} \\PYZbs{}\n",
       "  \\PY{n}{t1}\\PY{p}{,} \\PYZbs{}\n",
       "  \\PY{n}{t2}\\PY{p}{,} \\PYZbs{}\n",
       "  \\PY{o}{=} \\PY{n}{args}\n",
       "  \\PY{n}{t3} \\PY{o}{=} \\PY{n}{prims}\\PY{o}{.}\\PY{n}{full}\\PY{p}{(}\\PY{p}{(}\\PY{l+m+mi}{64}\\PY{p}{,} \\PY{l+m+mi}{64}\\PY{p}{)}\\PY{p}{,} \\PY{l+m+mi}{1}\\PY{p}{,} \\PY{n}{device}\\PY{o}{=}\\PY{n}{devices}\\PY{o}{.}\\PY{n}{Device}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{cuda:0}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\\PY{p}{,} \\PY{n}{dtype}\\PY{o}{=}\\PY{n}{dtypes}\\PY{o}{.}\\PY{n}{float32}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t3: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t4} \\PY{o}{=} \\PY{n}{prims}\\PY{o}{.}\\PY{n}{full}\\PY{p}{(}\\PY{p}{(}\\PY{l+m+mi}{64}\\PY{p}{,} \\PY{l+m+mi}{64}\\PY{p}{)}\\PY{p}{,} \\PY{l+m+mi}{1}\\PY{p}{,} \\PY{n}{device}\\PY{o}{=}\\PY{n}{devices}\\PY{o}{.}\\PY{n}{Device}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{cuda:0}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\\PY{p}{,} \\PY{n}{dtype}\\PY{o}{=}\\PY{n}{dtypes}\\PY{o}{.}\\PY{n}{float32}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t4: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t5} \\PY{o}{=} \\PY{n}{prims}\\PY{o}{.}\\PY{n}{full}\\PY{p}{(}\\PY{p}{(}\\PY{l+m+mi}{64}\\PY{p}{,} \\PY{l+m+mi}{64}\\PY{p}{)}\\PY{p}{,} \\PY{l+m+mi}{1}\\PY{p}{,} \\PY{n}{device}\\PY{o}{=}\\PY{n}{devices}\\PY{o}{.}\\PY{n}{Device}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{cuda:0}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\\PY{p}{,} \\PY{n}{dtype}\\PY{o}{=}\\PY{n}{dtypes}\\PY{o}{.}\\PY{n}{float32}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t5: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t6} \\PY{o}{=} \\PY{n}{prims}\\PY{o}{.}\\PY{n}{full}\\PY{p}{(}\\PY{p}{(}\\PY{l+m+mi}{64}\\PY{p}{,} \\PY{l+m+mi}{64}\\PY{p}{)}\\PY{p}{,} \\PY{l+m+mi}{1}\\PY{p}{,} \\PY{n}{device}\\PY{o}{=}\\PY{n}{devices}\\PY{o}{.}\\PY{n}{Device}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{cuda:0}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\\PY{p}{,} \\PY{n}{dtype}\\PY{o}{=}\\PY{n}{dtypes}\\PY{o}{.}\\PY{n}{float32}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t6: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t7} \\PY{o}{=} \\PY{n}{prims}\\PY{o}{.}\\PY{n}{full}\\PY{p}{(}\\PY{p}{(}\\PY{l+m+mi}{64}\\PY{p}{,} \\PY{l+m+mi}{64}\\PY{p}{)}\\PY{p}{,} \\PY{l+m+mi}{1}\\PY{p}{,} \\PY{n}{device}\\PY{o}{=}\\PY{n}{devices}\\PY{o}{.}\\PY{n}{Device}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{cuda:0}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\\PY{p}{,} \\PY{n}{dtype}\\PY{o}{=}\\PY{n}{dtypes}\\PY{o}{.}\\PY{n}{float32}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t7: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t8} \\PY{o}{=} \\PY{n}{prims}\\PY{o}{.}\\PY{n}{full}\\PY{p}{(}\\PY{p}{(}\\PY{l+m+mi}{64}\\PY{p}{,} \\PY{l+m+mi}{64}\\PY{p}{)}\\PY{p}{,} \\PY{l+m+mi}{1}\\PY{p}{,} \\PY{n}{device}\\PY{o}{=}\\PY{n}{devices}\\PY{o}{.}\\PY{n}{Device}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{cuda:0}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\\PY{p}{,} \\PY{n}{dtype}\\PY{o}{=}\\PY{n}{dtypes}\\PY{o}{.}\\PY{n}{float32}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t8: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t9} \\PY{o}{=} \\PY{n}{prims}\\PY{o}{.}\\PY{n}{full}\\PY{p}{(}\\PY{p}{(}\\PY{l+m+mi}{64}\\PY{p}{,} \\PY{l+m+mi}{64}\\PY{p}{)}\\PY{p}{,} \\PY{l+m+mi}{1}\\PY{p}{,} \\PY{n}{device}\\PY{o}{=}\\PY{n}{devices}\\PY{o}{.}\\PY{n}{Device}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{cuda:0}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\\PY{p}{,} \\PY{n}{dtype}\\PY{o}{=}\\PY{n}{dtypes}\\PY{o}{.}\\PY{n}{float32}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t9: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t10} \\PY{o}{=} \\PY{n}{prims}\\PY{o}{.}\\PY{n}{full}\\PY{p}{(}\\PY{p}{(}\\PY{l+m+mi}{64}\\PY{p}{,} \\PY{l+m+mi}{64}\\PY{p}{)}\\PY{p}{,} \\PY{l+m+mi}{1}\\PY{p}{,} \\PY{n}{device}\\PY{o}{=}\\PY{n}{devices}\\PY{o}{.}\\PY{n}{Device}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{cuda:0}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\\PY{p}{,} \\PY{n}{dtype}\\PY{o}{=}\\PY{n}{dtypes}\\PY{o}{.}\\PY{n}{float32}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t10: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{p11} \\PY{o}{=} \\PY{n}{thunder}\\PY{o}{.}\\PY{n}{distributed}\\PY{o}{.}\\PY{n}{prims}\\PY{o}{.}\\PY{n}{all\\PYZus{}gather}\\PY{p}{(}\\PY{n}{t1}\\PY{p}{,} \\PY{n}{\\PYZus{}torch\\PYZus{}distributed\\PYZus{}distributed\\PYZus{}c10d\\PYZus{}ProcessGroup\\PYZus{}0}\\PY{p}{,} \\PY{k+kc}{True}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} p11: \\PYZdq{}FUTURE cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t12} \\PY{o}{=} \\PY{n}{thunder}\\PY{o}{.}\\PY{n}{distributed}\\PY{o}{.}\\PY{n}{prims}\\PY{o}{.}\\PY{n}{wait}\\PY{p}{(}\\PY{n}{p11}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t12: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{p13} \\PY{o}{=} \\PY{n}{thunder}\\PY{o}{.}\\PY{n}{distributed}\\PY{o}{.}\\PY{n}{prims}\\PY{o}{.}\\PY{n}{all\\PYZus{}gather}\\PY{p}{(}\\PY{n}{t2}\\PY{p}{,} \\PY{n}{\\PYZus{}torch\\PYZus{}distributed\\PYZus{}distributed\\PYZus{}c10d\\PYZus{}ProcessGroup\\PYZus{}0}\\PY{p}{,} \\PY{k+kc}{True}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} p13: \\PYZdq{}FUTURE cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t14} \\PY{o}{=} \\PY{n}{thunder}\\PY{o}{.}\\PY{n}{distributed}\\PY{o}{.}\\PY{n}{prims}\\PY{o}{.}\\PY{n}{wait}\\PY{p}{(}\\PY{n}{p13}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t14: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t15} \\PY{o}{=} \\PY{n}{prims}\\PY{o}{.}\\PY{n}{linear}\\PY{p}{(}\\PY{n}{t0}\\PY{p}{,} \\PY{n}{t12}\\PY{p}{,} \\PY{k+kc}{None}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t15: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t16} \\PY{o}{=} \\PY{n}{prims}\\PY{o}{.}\\PY{n}{tanh}\\PY{p}{(}\\PY{n}{t15}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t16: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t17} \\PY{o}{=} \\PY{n}{prims}\\PY{o}{.}\\PY{n}{linear}\\PY{p}{(}\\PY{n}{t16}\\PY{p}{,} \\PY{n}{t14}\\PY{p}{,} \\PY{k+kc}{None}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t17: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t18} \\PY{o}{=} \\PY{n}{prims}\\PY{o}{.}\\PY{n}{add}\\PY{p}{(}\\PY{n}{t6}\\PY{p}{,} \\PY{n}{t7}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t18: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t19} \\PY{o}{=} \\PY{n}{prims}\\PY{o}{.}\\PY{n}{add}\\PY{p}{(}\\PY{n}{t3}\\PY{p}{,} \\PY{n}{t8}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t19: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t20} \\PY{o}{=} \\PY{n}{prims}\\PY{o}{.}\\PY{n}{add}\\PY{p}{(}\\PY{n}{t5}\\PY{p}{,} \\PY{n}{t9}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t20: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t21} \\PY{o}{=} \\PY{n}{ltorch}\\PY{o}{.}\\PY{n}{reshape}\\PY{p}{(}\\PY{n}{t18}\\PY{p}{,} \\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+m+mi}{64}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t21: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t21 = prims.reshape(t18, (64, 64))  \\PYZsh{} t21: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t22} \\PY{o}{=} \\PY{n}{ltorch}\\PY{o}{.}\\PY{n}{matmul}\\PY{p}{(}\\PY{n}{t21}\\PY{p}{,} \\PY{n}{t14}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t22: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t22 = prims.matmul(t21, t14)  \\PYZsh{} t22: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t23} \\PY{o}{=} \\PY{n}{ltorch}\\PY{o}{.}\\PY{n}{reshape}\\PY{p}{(}\\PY{n}{t18}\\PY{p}{,} \\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+m+mi}{64}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t23: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t23 = prims.reshape(t18, (64, 64))  \\PYZsh{} t23: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t24} \\PY{o}{=} \\PY{n}{prims}\\PY{o}{.}\\PY{n}{transpose}\\PY{p}{(}\\PY{n}{t23}\\PY{p}{,} \\PY{p}{(}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+m+mi}{0}\\PY{p}{)}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t24: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t25} \\PY{o}{=} \\PY{n}{ltorch}\\PY{o}{.}\\PY{n}{reshape}\\PY{p}{(}\\PY{n}{t16}\\PY{p}{,} \\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+m+mi}{64}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t25: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t25 = prims.reshape(t16, (64, 64))  \\PYZsh{} t25: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t26} \\PY{o}{=} \\PY{n}{ltorch}\\PY{o}{.}\\PY{n}{matmul}\\PY{p}{(}\\PY{n}{t24}\\PY{p}{,} \\PY{n}{t25}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t26: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t26 = prims.matmul(t24, t25)  \\PYZsh{} t26: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t27} \\PY{o}{=} \\PY{n}{prims}\\PY{o}{.}\\PY{n}{add}\\PY{p}{(}\\PY{n}{t10}\\PY{p}{,} \\PY{n}{t22}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t27: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t28} \\PY{o}{=} \\PY{n}{prims}\\PY{o}{.}\\PY{n}{add}\\PY{p}{(}\\PY{n}{t20}\\PY{p}{,} \\PY{n}{t26}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t28: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t29} \\PY{o}{=} \\PY{n}{ltorch}\\PY{o}{.}\\PY{n}{mul}\\PY{p}{(}\\PY{n}{t16}\\PY{p}{,} \\PY{n}{t16}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t29: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t29 = prims.mul(t16, t16)  \\PYZsh{} t29: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t30} \\PY{o}{=} \\PY{n}{ltorch}\\PY{o}{.}\\PY{n}{sub}\\PY{p}{(}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{n}{t29}\\PY{p}{,} \\PY{n}{alpha}\\PY{o}{=}\\PY{k+kc}{None}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t30: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} \\PYZus{} = prims.convert\\PYZus{}element\\PYZus{}type(1, float)}\n",
       "    \\PY{c+c1}{\\PYZsh{} t30 = prims.sub(1.0, t29)  \\PYZsh{} t30: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t31} \\PY{o}{=} \\PY{n}{ltorch}\\PY{o}{.}\\PY{n}{mul}\\PY{p}{(}\\PY{n}{t27}\\PY{p}{,} \\PY{n}{t30}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t31: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t31 = prims.mul(t27, t30)  \\PYZsh{} t31: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t32} \\PY{o}{=} \\PY{n}{ltorch}\\PY{o}{.}\\PY{n}{reshape}\\PY{p}{(}\\PY{n}{t31}\\PY{p}{,} \\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+m+mi}{64}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t32: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t32 = prims.reshape(t31, (64, 64))  \\PYZsh{} t32: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t33} \\PY{o}{=} \\PY{n}{ltorch}\\PY{o}{.}\\PY{n}{matmul}\\PY{p}{(}\\PY{n}{t32}\\PY{p}{,} \\PY{n}{t12}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t33: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t33 = prims.matmul(t32, t12)  \\PYZsh{} t33: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t34} \\PY{o}{=} \\PY{n}{ltorch}\\PY{o}{.}\\PY{n}{reshape}\\PY{p}{(}\\PY{n}{t31}\\PY{p}{,} \\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+m+mi}{64}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t34: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t34 = prims.reshape(t31, (64, 64))  \\PYZsh{} t34: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t35} \\PY{o}{=} \\PY{n}{prims}\\PY{o}{.}\\PY{n}{transpose}\\PY{p}{(}\\PY{n}{t34}\\PY{p}{,} \\PY{p}{(}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+m+mi}{0}\\PY{p}{)}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t35: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t36} \\PY{o}{=} \\PY{n}{ltorch}\\PY{o}{.}\\PY{n}{reshape}\\PY{p}{(}\\PY{n}{t0}\\PY{p}{,} \\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+m+mi}{64}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t36: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t36 = prims.reshape(t0, (64, 64))  \\PYZsh{} t36: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t37} \\PY{o}{=} \\PY{n}{ltorch}\\PY{o}{.}\\PY{n}{matmul}\\PY{p}{(}\\PY{n}{t35}\\PY{p}{,} \\PY{n}{t36}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t37: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t37 = prims.matmul(t35, t36)  \\PYZsh{} t37: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t38} \\PY{o}{=} \\PY{n}{prims}\\PY{o}{.}\\PY{n}{add}\\PY{p}{(}\\PY{n}{t19}\\PY{p}{,} \\PY{n}{t33}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t38: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t39} \\PY{o}{=} \\PY{n}{prims}\\PY{o}{.}\\PY{n}{add}\\PY{p}{(}\\PY{n}{t4}\\PY{p}{,} \\PY{n}{t37}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t39: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t40} \\PY{o}{=} \\PY{n}{ltorch}\\PY{o}{.}\\PY{n}{true\\PYZus{}divide}\\PY{p}{(}\\PY{n}{t28}\\PY{p}{,} \\PY{l+m+mi}{2}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t40: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} \\PYZus{} = prims.convert\\PYZus{}element\\PYZus{}type(2, float)}\n",
       "    \\PY{c+c1}{\\PYZsh{} t40 = prims.div(t28, 2.0)  \\PYZsh{} t40: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{p41} \\PY{o}{=} \\PY{n}{thunder}\\PY{o}{.}\\PY{n}{distributed}\\PY{o}{.}\\PY{n}{prims}\\PY{o}{.}\\PY{n}{reduce\\PYZus{}scatter}\\PY{p}{(}\\PY{n}{t40}\\PY{p}{,} \\PY{n}{\\PYZus{}DistributedReduceOps\\PYZus{}1}\\PY{p}{,} \\PY{n}{\\PYZus{}torch\\PYZus{}distributed\\PYZus{}distributed\\PYZus{}c10d\\PYZus{}ProcessGroup\\PYZus{}0}\\PY{p}{,} \\PY{k+kc}{True}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} p41: \\PYZdq{}FUTURE cuda:0 f32[32, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t42} \\PY{o}{=} \\PY{n}{thunder}\\PY{o}{.}\\PY{n}{distributed}\\PY{o}{.}\\PY{n}{prims}\\PY{o}{.}\\PY{n}{wait}\\PY{p}{(}\\PY{n}{p41}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t42: \\PYZdq{}cuda:0 f32[32, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t43} \\PY{o}{=} \\PY{n}{ltorch}\\PY{o}{.}\\PY{n}{true\\PYZus{}divide}\\PY{p}{(}\\PY{n}{t39}\\PY{p}{,} \\PY{l+m+mi}{2}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t43: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} \\PYZus{} = prims.convert\\PYZus{}element\\PYZus{}type(2, float)}\n",
       "    \\PY{c+c1}{\\PYZsh{} t43 = prims.div(t39, 2.0)  \\PYZsh{} t43: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{p44} \\PY{o}{=} \\PY{n}{thunder}\\PY{o}{.}\\PY{n}{distributed}\\PY{o}{.}\\PY{n}{prims}\\PY{o}{.}\\PY{n}{reduce\\PYZus{}scatter}\\PY{p}{(}\\PY{n}{t43}\\PY{p}{,} \\PY{n}{\\PYZus{}DistributedReduceOps\\PYZus{}1}\\PY{p}{,} \\PY{n}{\\PYZus{}torch\\PYZus{}distributed\\PYZus{}distributed\\PYZus{}c10d\\PYZus{}ProcessGroup\\PYZus{}0}\\PY{p}{,} \\PY{k+kc}{True}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} p44: \\PYZdq{}FUTURE cuda:0 f32[32, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t45} \\PY{o}{=} \\PY{n}{thunder}\\PY{o}{.}\\PY{n}{distributed}\\PY{o}{.}\\PY{n}{prims}\\PY{o}{.}\\PY{n}{wait}\\PY{p}{(}\\PY{n}{p44}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t45: \\PYZdq{}cuda:0 f32[32, 64]\\PYZdq{}}\n",
       "  \\PY{k}{return} \\PY{p}{(}\\PY{p}{(}\\PY{p}{\\PYZob{}}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{output}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{:} \\PY{n}{t17}\\PY{p}{,} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{flat\\PYZus{}args}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{:} \\PY{p}{[}\\PY{n}{t0}\\PY{p}{,} \\PY{n}{t12}\\PY{p}{,} \\PY{n}{t14}\\PY{p}{]}\\PY{p}{,} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{flat\\PYZus{}output}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{:} \\PY{p}{(}\\PY{n}{t17}\\PY{p}{,}\\PY{p}{)}\\PY{p}{\\PYZcb{}}\\PY{p}{,} \\PY{p}{(}\\PY{p}{(}\\PY{n}{t0}\\PY{p}{,} \\PY{n}{t14}\\PY{p}{,} \\PY{n}{t16}\\PY{p}{)}\\PY{p}{,} \\PY{p}{(}\\PY{p}{)}\\PY{p}{)}\\PY{p}{)}\\PY{p}{,} \\PY{p}{(}\\PY{n}{t38}\\PY{p}{,} \\PY{n}{t45}\\PY{p}{,} \\PY{n}{t42}\\PY{p}{)}\\PY{p}{)}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "# Constructed by Dead Code Elimination (took 1 milliseconds)\n",
       "import thunder\n",
       "import thunder.core.devices as devices\n",
       "import thunder.core.dtypes as dtypes\n",
       "import thunder.core.prims as prims\n",
       "import thunder.distributed.prims\n",
       "import thunder.torch as ltorch\n",
       "import torch\n",
       "from thunder.executors.torchex import no_autocast\n",
       "\n",
       "@torch.no_grad()\n",
       "@no_autocast()\n",
       "def _value_and_grad(*args):\n",
       "  # args: \"Collection\" \n",
       "  t0, \\\n",
       "  t1, \\\n",
       "  t2, \\\n",
       "  = args\n",
       "  t3 = prims.full((64, 64), 1, device=devices.Device(\"cuda:0\"), dtype=dtypes.float32)  # t3: \"cuda:0 f32[64, 64]\"\n",
       "  t4 = prims.full((64, 64), 1, device=devices.Device(\"cuda:0\"), dtype=dtypes.float32)  # t4: \"cuda:0 f32[64, 64]\"\n",
       "  t5 = prims.full((64, 64), 1, device=devices.Device(\"cuda:0\"), dtype=dtypes.float32)  # t5: \"cuda:0 f32[64, 64]\"\n",
       "  t6 = prims.full((64, 64), 1, device=devices.Device(\"cuda:0\"), dtype=dtypes.float32)  # t6: \"cuda:0 f32[64, 64]\"\n",
       "  t7 = prims.full((64, 64), 1, device=devices.Device(\"cuda:0\"), dtype=dtypes.float32)  # t7: \"cuda:0 f32[64, 64]\"\n",
       "  t8 = prims.full((64, 64), 1, device=devices.Device(\"cuda:0\"), dtype=dtypes.float32)  # t8: \"cuda:0 f32[64, 64]\"\n",
       "  t9 = prims.full((64, 64), 1, device=devices.Device(\"cuda:0\"), dtype=dtypes.float32)  # t9: \"cuda:0 f32[64, 64]\"\n",
       "  t10 = prims.full((64, 64), 1, device=devices.Device(\"cuda:0\"), dtype=dtypes.float32)  # t10: \"cuda:0 f32[64, 64]\"\n",
       "  p11 = thunder.distributed.prims.all_gather(t1, _torch_distributed_distributed_c10d_ProcessGroup_0, True)  # p11: \"FUTURE cuda:0 f32[64, 64]\"\n",
       "  t12 = thunder.distributed.prims.wait(p11)  # t12: \"cuda:0 f32[64, 64]\"\n",
       "  p13 = thunder.distributed.prims.all_gather(t2, _torch_distributed_distributed_c10d_ProcessGroup_0, True)  # p13: \"FUTURE cuda:0 f32[64, 64]\"\n",
       "  t14 = thunder.distributed.prims.wait(p13)  # t14: \"cuda:0 f32[64, 64]\"\n",
       "  t15 = prims.linear(t0, t12, None)  # t15: \"cuda:0 f32[64, 64]\"\n",
       "  t16 = prims.tanh(t15)  # t16: \"cuda:0 f32[64, 64]\"\n",
       "  t17 = prims.linear(t16, t14, None)  # t17: \"cuda:0 f32[64, 64]\"\n",
       "  t18 = prims.add(t6, t7)  # t18: \"cuda:0 f32[64, 64]\"\n",
       "  t19 = prims.add(t3, t8)  # t19: \"cuda:0 f32[64, 64]\"\n",
       "  t20 = prims.add(t5, t9)  # t20: \"cuda:0 f32[64, 64]\"\n",
       "  t21 = ltorch.reshape(t18, -1, 64)  # t21: \"cuda:0 f32[64, 64]\"\n",
       "    # t21 = prims.reshape(t18, (64, 64))  # t21: \"cuda:0 f32[64, 64]\"\n",
       "  t22 = ltorch.matmul(t21, t14)  # t22: \"cuda:0 f32[64, 64]\"\n",
       "    # t22 = prims.matmul(t21, t14)  # t22: \"cuda:0 f32[64, 64]\"\n",
       "  t23 = ltorch.reshape(t18, -1, 64)  # t23: \"cuda:0 f32[64, 64]\"\n",
       "    # t23 = prims.reshape(t18, (64, 64))  # t23: \"cuda:0 f32[64, 64]\"\n",
       "  t24 = prims.transpose(t23, (1, 0))  # t24: \"cuda:0 f32[64, 64]\"\n",
       "  t25 = ltorch.reshape(t16, -1, 64)  # t25: \"cuda:0 f32[64, 64]\"\n",
       "    # t25 = prims.reshape(t16, (64, 64))  # t25: \"cuda:0 f32[64, 64]\"\n",
       "  t26 = ltorch.matmul(t24, t25)  # t26: \"cuda:0 f32[64, 64]\"\n",
       "    # t26 = prims.matmul(t24, t25)  # t26: \"cuda:0 f32[64, 64]\"\n",
       "  t27 = prims.add(t10, t22)  # t27: \"cuda:0 f32[64, 64]\"\n",
       "  t28 = prims.add(t20, t26)  # t28: \"cuda:0 f32[64, 64]\"\n",
       "  t29 = ltorch.mul(t16, t16)  # t29: \"cuda:0 f32[64, 64]\"\n",
       "    # t29 = prims.mul(t16, t16)  # t29: \"cuda:0 f32[64, 64]\"\n",
       "  t30 = ltorch.sub(1, t29, alpha=None)  # t30: \"cuda:0 f32[64, 64]\"\n",
       "    # _ = prims.convert_element_type(1, float)\n",
       "    # t30 = prims.sub(1.0, t29)  # t30: \"cuda:0 f32[64, 64]\"\n",
       "  t31 = ltorch.mul(t27, t30)  # t31: \"cuda:0 f32[64, 64]\"\n",
       "    # t31 = prims.mul(t27, t30)  # t31: \"cuda:0 f32[64, 64]\"\n",
       "  t32 = ltorch.reshape(t31, -1, 64)  # t32: \"cuda:0 f32[64, 64]\"\n",
       "    # t32 = prims.reshape(t31, (64, 64))  # t32: \"cuda:0 f32[64, 64]\"\n",
       "  t33 = ltorch.matmul(t32, t12)  # t33: \"cuda:0 f32[64, 64]\"\n",
       "    # t33 = prims.matmul(t32, t12)  # t33: \"cuda:0 f32[64, 64]\"\n",
       "  t34 = ltorch.reshape(t31, -1, 64)  # t34: \"cuda:0 f32[64, 64]\"\n",
       "    # t34 = prims.reshape(t31, (64, 64))  # t34: \"cuda:0 f32[64, 64]\"\n",
       "  t35 = prims.transpose(t34, (1, 0))  # t35: \"cuda:0 f32[64, 64]\"\n",
       "  t36 = ltorch.reshape(t0, -1, 64)  # t36: \"cuda:0 f32[64, 64]\"\n",
       "    # t36 = prims.reshape(t0, (64, 64))  # t36: \"cuda:0 f32[64, 64]\"\n",
       "  t37 = ltorch.matmul(t35, t36)  # t37: \"cuda:0 f32[64, 64]\"\n",
       "    # t37 = prims.matmul(t35, t36)  # t37: \"cuda:0 f32[64, 64]\"\n",
       "  t38 = prims.add(t19, t33)  # t38: \"cuda:0 f32[64, 64]\"\n",
       "  t39 = prims.add(t4, t37)  # t39: \"cuda:0 f32[64, 64]\"\n",
       "  t40 = ltorch.true_divide(t28, 2)  # t40: \"cuda:0 f32[64, 64]\"\n",
       "    # _ = prims.convert_element_type(2, float)\n",
       "    # t40 = prims.div(t28, 2.0)  # t40: \"cuda:0 f32[64, 64]\"\n",
       "  p41 = thunder.distributed.prims.reduce_scatter(t40, _DistributedReduceOps_1, _torch_distributed_distributed_c10d_ProcessGroup_0, True)  # p41: \"FUTURE cuda:0 f32[32, 64]\"\n",
       "  t42 = thunder.distributed.prims.wait(p41)  # t42: \"cuda:0 f32[32, 64]\"\n",
       "  t43 = ltorch.true_divide(t39, 2)  # t43: \"cuda:0 f32[64, 64]\"\n",
       "    # _ = prims.convert_element_type(2, float)\n",
       "    # t43 = prims.div(t39, 2.0)  # t43: \"cuda:0 f32[64, 64]\"\n",
       "  p44 = thunder.distributed.prims.reduce_scatter(t43, _DistributedReduceOps_1, _torch_distributed_distributed_c10d_ProcessGroup_0, True)  # p44: \"FUTURE cuda:0 f32[32, 64]\"\n",
       "  t45 = thunder.distributed.prims.wait(p44)  # t45: \"cuda:0 f32[32, 64]\"\n",
       "  return (({'output': t17, 'flat_args': [t0, t12, t14], 'flat_output': (t17,)}, ((t0, t14, t16), ())), (t38, t45, t42))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from thunder.core.transforms import value_and_grad\n",
    "\n",
    "forward_and_backward_model = value_and_grad(model_with_syncs)\n",
    "\n",
    "forward_backward_trace = thunder.trace()(forward_and_backward_model, x, *model.parameters())\n",
    "\n",
    "wrap_as_highlighted_code(forward_backward_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above trace, only contains primitive which specifies the semantic of an operation abstractly but doesn't perform the actual computation.\n",
    "\n",
    "Now we will generate the execution trace which can actually perform the compute.\n",
    "\n",
    "In the execution trace generated below, we can see that all the primitives have been replaced with actually PyTorch operations. Also, our synchronization primitives have been replaced with PyTorch implementation provided by thunder i.e. `torch_all_gather_prim_impl`, `torch_reduce_scatter_prim_impl`, `torch_wait_prim_impl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"c1\"># Constructed by Delete Last Used (took 0 milliseconds)</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">torch.nn.functional</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">thunder.executors.torchex</span> <span class=\"kn\">import</span> <span class=\"n\">no_autocast</span>\n",
       "\n",
       "<span class=\"nd\">@torch</span><span class=\"o\">.</span><span class=\"n\">no_grad</span><span class=\"p\">()</span>\n",
       "<span class=\"nd\">@no_autocast</span><span class=\"p\">()</span>\n",
       "<span class=\"k\">def</span> <span class=\"nf\">_value_and_grad</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">):</span>\n",
       "  <span class=\"c1\"># args: &quot;Collection&quot; </span>\n",
       "  <span class=\"n\">t0</span><span class=\"p\">,</span> \\\n",
       "  <span class=\"n\">t1</span><span class=\"p\">,</span> \\\n",
       "  <span class=\"n\">t2</span><span class=\"p\">,</span> \\\n",
       "  <span class=\"o\">=</span> <span class=\"n\">args</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">args</span>\n",
       "  <span class=\"n\">t3</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">full</span><span class=\"p\">((</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">),</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">(</span><span class=\"s2\">&quot;cuda:0&quot;</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>  <span class=\"c1\"># t3: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t3 = ltorch.full((64, 64), 1, device=torch.device(&quot;cuda:0&quot;), dtype=torch.float32)  # t3: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "      <span class=\"c1\"># t3 = prims.full((64, 64), 1, device=devices.Device(&quot;cuda:0&quot;), dtype=dtypes.float32)  # t3: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t4</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">full</span><span class=\"p\">((</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">),</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">(</span><span class=\"s2\">&quot;cuda:0&quot;</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>  <span class=\"c1\"># t4: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t4 = ltorch.full((64, 64), 1, device=torch.device(&quot;cuda:0&quot;), dtype=torch.float32)  # t4: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "      <span class=\"c1\"># t4 = prims.full((64, 64), 1, device=devices.Device(&quot;cuda:0&quot;), dtype=dtypes.float32)  # t4: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t5</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">full</span><span class=\"p\">((</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">),</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">(</span><span class=\"s2\">&quot;cuda:0&quot;</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>  <span class=\"c1\"># t5: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t5 = ltorch.full((64, 64), 1, device=torch.device(&quot;cuda:0&quot;), dtype=torch.float32)  # t5: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "      <span class=\"c1\"># t5 = prims.full((64, 64), 1, device=devices.Device(&quot;cuda:0&quot;), dtype=dtypes.float32)  # t5: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t6</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">full</span><span class=\"p\">((</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">),</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">(</span><span class=\"s2\">&quot;cuda:0&quot;</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>  <span class=\"c1\"># t6: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t6 = ltorch.full((64, 64), 1, device=torch.device(&quot;cuda:0&quot;), dtype=torch.float32)  # t6: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "      <span class=\"c1\"># t6 = prims.full((64, 64), 1, device=devices.Device(&quot;cuda:0&quot;), dtype=dtypes.float32)  # t6: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t7</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">full</span><span class=\"p\">((</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">),</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">(</span><span class=\"s2\">&quot;cuda:0&quot;</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>  <span class=\"c1\"># t7: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t7 = ltorch.full((64, 64), 1, device=torch.device(&quot;cuda:0&quot;), dtype=torch.float32)  # t7: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "      <span class=\"c1\"># t7 = prims.full((64, 64), 1, device=devices.Device(&quot;cuda:0&quot;), dtype=dtypes.float32)  # t7: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t8</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">full</span><span class=\"p\">((</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">),</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">(</span><span class=\"s2\">&quot;cuda:0&quot;</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>  <span class=\"c1\"># t8: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t8 = ltorch.full((64, 64), 1, device=torch.device(&quot;cuda:0&quot;), dtype=torch.float32)  # t8: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "      <span class=\"c1\"># t8 = prims.full((64, 64), 1, device=devices.Device(&quot;cuda:0&quot;), dtype=dtypes.float32)  # t8: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t9</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">full</span><span class=\"p\">((</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">),</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">(</span><span class=\"s2\">&quot;cuda:0&quot;</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>  <span class=\"c1\"># t9: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t9 = ltorch.full((64, 64), 1, device=torch.device(&quot;cuda:0&quot;), dtype=torch.float32)  # t9: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "      <span class=\"c1\"># t9 = prims.full((64, 64), 1, device=devices.Device(&quot;cuda:0&quot;), dtype=dtypes.float32)  # t9: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t10</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">full</span><span class=\"p\">((</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">),</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">(</span><span class=\"s2\">&quot;cuda:0&quot;</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>  <span class=\"c1\"># t10: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t10 = ltorch.full((64, 64), 1, device=torch.device(&quot;cuda:0&quot;), dtype=torch.float32)  # t10: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "      <span class=\"c1\"># t10 = prims.full((64, 64), 1, device=devices.Device(&quot;cuda:0&quot;), dtype=dtypes.float32)  # t10: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">p11</span> <span class=\"o\">=</span> <span class=\"n\">torch_all_gather_prim_impl</span><span class=\"p\">(</span><span class=\"n\">t1</span><span class=\"p\">,</span> <span class=\"n\">_torch_distributed_distributed_c10d_ProcessGroup_2</span><span class=\"p\">,</span> <span class=\"kc\">True</span><span class=\"p\">)</span>  <span class=\"c1\"># p11: &quot;FUTURE cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t1</span>\n",
       "  <span class=\"n\">t12</span> <span class=\"o\">=</span> <span class=\"n\">torch_wait_prim_impl</span><span class=\"p\">(</span><span class=\"n\">p11</span><span class=\"p\">)</span>  <span class=\"c1\"># t12: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">p11</span>\n",
       "  <span class=\"n\">p13</span> <span class=\"o\">=</span> <span class=\"n\">torch_all_gather_prim_impl</span><span class=\"p\">(</span><span class=\"n\">t2</span><span class=\"p\">,</span> <span class=\"n\">_torch_distributed_distributed_c10d_ProcessGroup_2</span><span class=\"p\">,</span> <span class=\"kc\">True</span><span class=\"p\">)</span>  <span class=\"c1\"># p13: &quot;FUTURE cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t2</span>\n",
       "  <span class=\"n\">t14</span> <span class=\"o\">=</span> <span class=\"n\">torch_wait_prim_impl</span><span class=\"p\">(</span><span class=\"n\">p13</span><span class=\"p\">)</span>  <span class=\"c1\"># t14: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">p13</span>\n",
       "  <span class=\"n\">t15</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">functional</span><span class=\"o\">.</span><span class=\"n\">linear</span><span class=\"p\">(</span><span class=\"n\">t0</span><span class=\"p\">,</span> <span class=\"n\">t12</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">)</span>  <span class=\"c1\"># t15: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t15 = ltorch.linear(t0, t12, None)  # t15: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "      <span class=\"c1\"># t15 = prims.linear(t0, t12, None)  # t15: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t16</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">tanh</span><span class=\"p\">(</span><span class=\"n\">t15</span><span class=\"p\">)</span>  <span class=\"c1\"># t16: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t16 = ltorch.tanh(t15)  # t16: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "      <span class=\"c1\"># t16 = prims.tanh(t15)  # t16: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t15</span>\n",
       "  <span class=\"n\">t17</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">functional</span><span class=\"o\">.</span><span class=\"n\">linear</span><span class=\"p\">(</span><span class=\"n\">t16</span><span class=\"p\">,</span> <span class=\"n\">t14</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">)</span>  <span class=\"c1\"># t17: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t17 = ltorch.linear(t16, t14, None)  # t17: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "      <span class=\"c1\"># t17 = prims.linear(t16, t14, None)  # t17: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t18</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">add</span><span class=\"p\">(</span><span class=\"n\">t6</span><span class=\"p\">,</span> <span class=\"n\">t7</span><span class=\"p\">)</span>  <span class=\"c1\"># t18: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t18 = ltorch.add(t6, t7, alpha=None)  # t18: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "      <span class=\"c1\"># t18 = prims.add(t6, t7)  # t18: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t6</span><span class=\"p\">,</span> <span class=\"n\">t7</span>\n",
       "  <span class=\"n\">t19</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">add</span><span class=\"p\">(</span><span class=\"n\">t3</span><span class=\"p\">,</span> <span class=\"n\">t8</span><span class=\"p\">)</span>  <span class=\"c1\"># t19: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t19 = ltorch.add(t3, t8, alpha=None)  # t19: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "      <span class=\"c1\"># t19 = prims.add(t3, t8)  # t19: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t3</span><span class=\"p\">,</span> <span class=\"n\">t8</span>\n",
       "  <span class=\"n\">t20</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">add</span><span class=\"p\">(</span><span class=\"n\">t5</span><span class=\"p\">,</span> <span class=\"n\">t9</span><span class=\"p\">)</span>  <span class=\"c1\"># t20: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t20 = ltorch.add(t5, t9, alpha=None)  # t20: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "      <span class=\"c1\"># t20 = prims.add(t5, t9)  # t20: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t5</span><span class=\"p\">,</span> <span class=\"n\">t9</span>\n",
       "  <span class=\"n\">t21</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">t18</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">))</span>  <span class=\"c1\"># t21: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t21 = ltorch.reshape(t18, (-1, 64))  # t21: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "      <span class=\"c1\"># t21 = prims.reshape(t18, (64, 64))  # t21: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t22</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">t21</span><span class=\"p\">,</span> <span class=\"n\">t14</span><span class=\"p\">)</span>  <span class=\"c1\"># t22: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t22 = ltorch.matmul(t21, t14)  # t22: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "      <span class=\"c1\"># t22 = prims.matmul(t21, t14)  # t22: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t21</span>\n",
       "  <span class=\"n\">t23</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">t18</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">))</span>  <span class=\"c1\"># t23: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t23 = ltorch.reshape(t18, (-1, 64))  # t23: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "      <span class=\"c1\"># t23 = prims.reshape(t18, (64, 64))  # t23: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t18</span>\n",
       "  <span class=\"n\">t24</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">permute</span><span class=\"p\">(</span><span class=\"n\">t23</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">))</span>  <span class=\"c1\"># t24: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t24 = ltorch.permute(t23, (1, 0))  # t24: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "      <span class=\"c1\"># t24 = prims.transpose(t23, (1, 0))  # t24: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t23</span>\n",
       "  <span class=\"n\">t25</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">t16</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">))</span>  <span class=\"c1\"># t25: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t25 = ltorch.reshape(t16, (-1, 64))  # t25: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "      <span class=\"c1\"># t25 = prims.reshape(t16, (64, 64))  # t25: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t26</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">t24</span><span class=\"p\">,</span> <span class=\"n\">t25</span><span class=\"p\">)</span>  <span class=\"c1\"># t26: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t26 = ltorch.matmul(t24, t25)  # t26: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "      <span class=\"c1\"># t26 = prims.matmul(t24, t25)  # t26: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t24</span><span class=\"p\">,</span> <span class=\"n\">t25</span>\n",
       "  <span class=\"n\">t27</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">add</span><span class=\"p\">(</span><span class=\"n\">t10</span><span class=\"p\">,</span> <span class=\"n\">t22</span><span class=\"p\">)</span>  <span class=\"c1\"># t27: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t27 = ltorch.add(t10, t22, alpha=None)  # t27: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "      <span class=\"c1\"># t27 = prims.add(t10, t22)  # t27: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t10</span><span class=\"p\">,</span> <span class=\"n\">t22</span>\n",
       "  <span class=\"n\">t28</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">add</span><span class=\"p\">(</span><span class=\"n\">t20</span><span class=\"p\">,</span> <span class=\"n\">t26</span><span class=\"p\">)</span>  <span class=\"c1\"># t28: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t28 = ltorch.add(t20, t26, alpha=None)  # t28: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "      <span class=\"c1\"># t28 = prims.add(t20, t26)  # t28: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t20</span><span class=\"p\">,</span> <span class=\"n\">t26</span>\n",
       "  <span class=\"n\">t29</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">mul</span><span class=\"p\">(</span><span class=\"n\">t16</span><span class=\"p\">,</span> <span class=\"n\">t16</span><span class=\"p\">)</span>  <span class=\"c1\"># t29: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t29 = ltorch.mul(t16, t16)  # t29: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "      <span class=\"c1\"># t29 = prims.mul(t16, t16)  # t29: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t30</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">sub</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">t29</span><span class=\"p\">)</span>  <span class=\"c1\"># t30: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t30 = ltorch.sub(1, t29, alpha=None)  # t30: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "      <span class=\"c1\"># _ = prims.convert_element_type(1, float)</span>\n",
       "      <span class=\"c1\"># t30 = prims.sub(1.0, t29)  # t30: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t29</span>\n",
       "  <span class=\"n\">t31</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">mul</span><span class=\"p\">(</span><span class=\"n\">t27</span><span class=\"p\">,</span> <span class=\"n\">t30</span><span class=\"p\">)</span>  <span class=\"c1\"># t31: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t31 = ltorch.mul(t27, t30)  # t31: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "      <span class=\"c1\"># t31 = prims.mul(t27, t30)  # t31: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t27</span><span class=\"p\">,</span> <span class=\"n\">t30</span>\n",
       "  <span class=\"n\">t32</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">t31</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">))</span>  <span class=\"c1\"># t32: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t32 = ltorch.reshape(t31, (-1, 64))  # t32: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "      <span class=\"c1\"># t32 = prims.reshape(t31, (64, 64))  # t32: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t33</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">t32</span><span class=\"p\">,</span> <span class=\"n\">t12</span><span class=\"p\">)</span>  <span class=\"c1\"># t33: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t33 = ltorch.matmul(t32, t12)  # t33: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "      <span class=\"c1\"># t33 = prims.matmul(t32, t12)  # t33: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t32</span>\n",
       "  <span class=\"n\">t34</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">t31</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">))</span>  <span class=\"c1\"># t34: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t34 = ltorch.reshape(t31, (-1, 64))  # t34: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "      <span class=\"c1\"># t34 = prims.reshape(t31, (64, 64))  # t34: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t31</span>\n",
       "  <span class=\"n\">t35</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">permute</span><span class=\"p\">(</span><span class=\"n\">t34</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">))</span>  <span class=\"c1\"># t35: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t35 = ltorch.permute(t34, (1, 0))  # t35: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "      <span class=\"c1\"># t35 = prims.transpose(t34, (1, 0))  # t35: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t34</span>\n",
       "  <span class=\"n\">t36</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">t0</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">))</span>  <span class=\"c1\"># t36: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t36 = ltorch.reshape(t0, (-1, 64))  # t36: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "      <span class=\"c1\"># t36 = prims.reshape(t0, (64, 64))  # t36: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"n\">t37</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">t35</span><span class=\"p\">,</span> <span class=\"n\">t36</span><span class=\"p\">)</span>  <span class=\"c1\"># t37: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t37 = ltorch.matmul(t35, t36)  # t37: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "      <span class=\"c1\"># t37 = prims.matmul(t35, t36)  # t37: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t35</span><span class=\"p\">,</span> <span class=\"n\">t36</span>\n",
       "  <span class=\"n\">t38</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">add</span><span class=\"p\">(</span><span class=\"n\">t19</span><span class=\"p\">,</span> <span class=\"n\">t33</span><span class=\"p\">)</span>  <span class=\"c1\"># t38: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t38 = ltorch.add(t19, t33, alpha=None)  # t38: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "      <span class=\"c1\"># t38 = prims.add(t19, t33)  # t38: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t19</span><span class=\"p\">,</span> <span class=\"n\">t33</span>\n",
       "  <span class=\"n\">t39</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">add</span><span class=\"p\">(</span><span class=\"n\">t4</span><span class=\"p\">,</span> <span class=\"n\">t37</span><span class=\"p\">)</span>  <span class=\"c1\"># t39: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t39 = ltorch.add(t4, t37, alpha=None)  # t39: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "      <span class=\"c1\"># t39 = prims.add(t4, t37)  # t39: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t4</span><span class=\"p\">,</span> <span class=\"n\">t37</span>\n",
       "  <span class=\"n\">t40</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">true_divide</span><span class=\"p\">(</span><span class=\"n\">t28</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)</span>  <span class=\"c1\"># t40: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t40 = ltorch.true_divide(t28, 2)  # t40: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "      <span class=\"c1\"># _ = prims.convert_element_type(2, float)</span>\n",
       "      <span class=\"c1\"># t40 = prims.div(t28, 2.0)  # t40: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t28</span>\n",
       "  <span class=\"n\">p41</span> <span class=\"o\">=</span> <span class=\"n\">torch_reduce_scatter_prim_impl</span><span class=\"p\">(</span><span class=\"n\">t40</span><span class=\"p\">,</span> <span class=\"n\">_DistributedReduceOps_3</span><span class=\"p\">,</span> <span class=\"n\">_torch_distributed_distributed_c10d_ProcessGroup_2</span><span class=\"p\">,</span> <span class=\"kc\">True</span><span class=\"p\">)</span>  <span class=\"c1\"># p41: &quot;FUTURE cuda:0 f32[32, 64]&quot;</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t40</span>\n",
       "  <span class=\"n\">t42</span> <span class=\"o\">=</span> <span class=\"n\">torch_wait_prim_impl</span><span class=\"p\">(</span><span class=\"n\">p41</span><span class=\"p\">)</span>  <span class=\"c1\"># t42: &quot;cuda:0 f32[32, 64]&quot;</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">p41</span>\n",
       "  <span class=\"n\">t43</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">true_divide</span><span class=\"p\">(</span><span class=\"n\">t39</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)</span>  <span class=\"c1\"># t43: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "    <span class=\"c1\"># t43 = ltorch.true_divide(t39, 2)  # t43: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "      <span class=\"c1\"># _ = prims.convert_element_type(2, float)</span>\n",
       "      <span class=\"c1\"># t43 = prims.div(t39, 2.0)  # t43: &quot;cuda:0 f32[64, 64]&quot;</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t39</span>\n",
       "  <span class=\"n\">p44</span> <span class=\"o\">=</span> <span class=\"n\">torch_reduce_scatter_prim_impl</span><span class=\"p\">(</span><span class=\"n\">t43</span><span class=\"p\">,</span> <span class=\"n\">_DistributedReduceOps_3</span><span class=\"p\">,</span> <span class=\"n\">_torch_distributed_distributed_c10d_ProcessGroup_2</span><span class=\"p\">,</span> <span class=\"kc\">True</span><span class=\"p\">)</span>  <span class=\"c1\"># p44: &quot;FUTURE cuda:0 f32[32, 64]&quot;</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t43</span>\n",
       "  <span class=\"n\">t45</span> <span class=\"o\">=</span> <span class=\"n\">torch_wait_prim_impl</span><span class=\"p\">(</span><span class=\"n\">p44</span><span class=\"p\">)</span>  <span class=\"c1\"># t45: &quot;cuda:0 f32[32, 64]&quot;</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">p44</span>\n",
       "  <span class=\"k\">return</span> <span class=\"p\">(({</span><span class=\"s1\">&#39;output&#39;</span><span class=\"p\">:</span> <span class=\"n\">t17</span><span class=\"p\">,</span> <span class=\"s1\">&#39;flat_args&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"n\">t0</span><span class=\"p\">,</span> <span class=\"n\">t12</span><span class=\"p\">,</span> <span class=\"n\">t14</span><span class=\"p\">],</span> <span class=\"s1\">&#39;flat_output&#39;</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"n\">t17</span><span class=\"p\">,)},</span> <span class=\"p\">((</span><span class=\"n\">t0</span><span class=\"p\">,</span> <span class=\"n\">t14</span><span class=\"p\">,</span> <span class=\"n\">t16</span><span class=\"p\">),</span> <span class=\"p\">())),</span> <span class=\"p\">(</span><span class=\"n\">t38</span><span class=\"p\">,</span> <span class=\"n\">t45</span><span class=\"p\">,</span> <span class=\"n\">t42</span><span class=\"p\">))</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{c+c1}{\\PYZsh{} Constructed by Delete Last Used (took 0 milliseconds)}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{torch}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{nn}\\PY{n+nn}{.}\\PY{n+nn}{functional}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{thunder}\\PY{n+nn}{.}\\PY{n+nn}{executors}\\PY{n+nn}{.}\\PY{n+nn}{torchex} \\PY{k+kn}{import} \\PY{n}{no\\PYZus{}autocast}\n",
       "\n",
       "\\PY{n+nd}{@torch}\\PY{o}{.}\\PY{n}{no\\PYZus{}grad}\\PY{p}{(}\\PY{p}{)}\n",
       "\\PY{n+nd}{@no\\PYZus{}autocast}\\PY{p}{(}\\PY{p}{)}\n",
       "\\PY{k}{def} \\PY{n+nf}{\\PYZus{}value\\PYZus{}and\\PYZus{}grad}\\PY{p}{(}\\PY{o}{*}\\PY{n}{args}\\PY{p}{)}\\PY{p}{:}\n",
       "  \\PY{c+c1}{\\PYZsh{} args: \\PYZdq{}Collection\\PYZdq{} }\n",
       "  \\PY{n}{t0}\\PY{p}{,} \\PYZbs{}\n",
       "  \\PY{n}{t1}\\PY{p}{,} \\PYZbs{}\n",
       "  \\PY{n}{t2}\\PY{p}{,} \\PYZbs{}\n",
       "  \\PY{o}{=} \\PY{n}{args}\n",
       "  \\PY{k}{del} \\PY{n}{args}\n",
       "  \\PY{n}{t3} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{full}\\PY{p}{(}\\PY{p}{(}\\PY{l+m+mi}{64}\\PY{p}{,} \\PY{l+m+mi}{64}\\PY{p}{)}\\PY{p}{,} \\PY{l+m+mi}{1}\\PY{p}{,} \\PY{n}{device}\\PY{o}{=}\\PY{n}{torch}\\PY{o}{.}\\PY{n}{device}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{cuda:0}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\\PY{p}{,} \\PY{n}{dtype}\\PY{o}{=}\\PY{n}{torch}\\PY{o}{.}\\PY{n}{float32}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t3: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t3 = ltorch.full((64, 64), 1, device=torch.device(\\PYZdq{}cuda:0\\PYZdq{}), dtype=torch.float32)  \\PYZsh{} t3: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "      \\PY{c+c1}{\\PYZsh{} t3 = prims.full((64, 64), 1, device=devices.Device(\\PYZdq{}cuda:0\\PYZdq{}), dtype=dtypes.float32)  \\PYZsh{} t3: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t4} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{full}\\PY{p}{(}\\PY{p}{(}\\PY{l+m+mi}{64}\\PY{p}{,} \\PY{l+m+mi}{64}\\PY{p}{)}\\PY{p}{,} \\PY{l+m+mi}{1}\\PY{p}{,} \\PY{n}{device}\\PY{o}{=}\\PY{n}{torch}\\PY{o}{.}\\PY{n}{device}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{cuda:0}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\\PY{p}{,} \\PY{n}{dtype}\\PY{o}{=}\\PY{n}{torch}\\PY{o}{.}\\PY{n}{float32}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t4: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t4 = ltorch.full((64, 64), 1, device=torch.device(\\PYZdq{}cuda:0\\PYZdq{}), dtype=torch.float32)  \\PYZsh{} t4: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "      \\PY{c+c1}{\\PYZsh{} t4 = prims.full((64, 64), 1, device=devices.Device(\\PYZdq{}cuda:0\\PYZdq{}), dtype=dtypes.float32)  \\PYZsh{} t4: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t5} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{full}\\PY{p}{(}\\PY{p}{(}\\PY{l+m+mi}{64}\\PY{p}{,} \\PY{l+m+mi}{64}\\PY{p}{)}\\PY{p}{,} \\PY{l+m+mi}{1}\\PY{p}{,} \\PY{n}{device}\\PY{o}{=}\\PY{n}{torch}\\PY{o}{.}\\PY{n}{device}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{cuda:0}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\\PY{p}{,} \\PY{n}{dtype}\\PY{o}{=}\\PY{n}{torch}\\PY{o}{.}\\PY{n}{float32}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t5: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t5 = ltorch.full((64, 64), 1, device=torch.device(\\PYZdq{}cuda:0\\PYZdq{}), dtype=torch.float32)  \\PYZsh{} t5: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "      \\PY{c+c1}{\\PYZsh{} t5 = prims.full((64, 64), 1, device=devices.Device(\\PYZdq{}cuda:0\\PYZdq{}), dtype=dtypes.float32)  \\PYZsh{} t5: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t6} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{full}\\PY{p}{(}\\PY{p}{(}\\PY{l+m+mi}{64}\\PY{p}{,} \\PY{l+m+mi}{64}\\PY{p}{)}\\PY{p}{,} \\PY{l+m+mi}{1}\\PY{p}{,} \\PY{n}{device}\\PY{o}{=}\\PY{n}{torch}\\PY{o}{.}\\PY{n}{device}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{cuda:0}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\\PY{p}{,} \\PY{n}{dtype}\\PY{o}{=}\\PY{n}{torch}\\PY{o}{.}\\PY{n}{float32}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t6: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t6 = ltorch.full((64, 64), 1, device=torch.device(\\PYZdq{}cuda:0\\PYZdq{}), dtype=torch.float32)  \\PYZsh{} t6: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "      \\PY{c+c1}{\\PYZsh{} t6 = prims.full((64, 64), 1, device=devices.Device(\\PYZdq{}cuda:0\\PYZdq{}), dtype=dtypes.float32)  \\PYZsh{} t6: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t7} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{full}\\PY{p}{(}\\PY{p}{(}\\PY{l+m+mi}{64}\\PY{p}{,} \\PY{l+m+mi}{64}\\PY{p}{)}\\PY{p}{,} \\PY{l+m+mi}{1}\\PY{p}{,} \\PY{n}{device}\\PY{o}{=}\\PY{n}{torch}\\PY{o}{.}\\PY{n}{device}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{cuda:0}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\\PY{p}{,} \\PY{n}{dtype}\\PY{o}{=}\\PY{n}{torch}\\PY{o}{.}\\PY{n}{float32}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t7: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t7 = ltorch.full((64, 64), 1, device=torch.device(\\PYZdq{}cuda:0\\PYZdq{}), dtype=torch.float32)  \\PYZsh{} t7: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "      \\PY{c+c1}{\\PYZsh{} t7 = prims.full((64, 64), 1, device=devices.Device(\\PYZdq{}cuda:0\\PYZdq{}), dtype=dtypes.float32)  \\PYZsh{} t7: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t8} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{full}\\PY{p}{(}\\PY{p}{(}\\PY{l+m+mi}{64}\\PY{p}{,} \\PY{l+m+mi}{64}\\PY{p}{)}\\PY{p}{,} \\PY{l+m+mi}{1}\\PY{p}{,} \\PY{n}{device}\\PY{o}{=}\\PY{n}{torch}\\PY{o}{.}\\PY{n}{device}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{cuda:0}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\\PY{p}{,} \\PY{n}{dtype}\\PY{o}{=}\\PY{n}{torch}\\PY{o}{.}\\PY{n}{float32}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t8: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t8 = ltorch.full((64, 64), 1, device=torch.device(\\PYZdq{}cuda:0\\PYZdq{}), dtype=torch.float32)  \\PYZsh{} t8: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "      \\PY{c+c1}{\\PYZsh{} t8 = prims.full((64, 64), 1, device=devices.Device(\\PYZdq{}cuda:0\\PYZdq{}), dtype=dtypes.float32)  \\PYZsh{} t8: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t9} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{full}\\PY{p}{(}\\PY{p}{(}\\PY{l+m+mi}{64}\\PY{p}{,} \\PY{l+m+mi}{64}\\PY{p}{)}\\PY{p}{,} \\PY{l+m+mi}{1}\\PY{p}{,} \\PY{n}{device}\\PY{o}{=}\\PY{n}{torch}\\PY{o}{.}\\PY{n}{device}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{cuda:0}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\\PY{p}{,} \\PY{n}{dtype}\\PY{o}{=}\\PY{n}{torch}\\PY{o}{.}\\PY{n}{float32}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t9: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t9 = ltorch.full((64, 64), 1, device=torch.device(\\PYZdq{}cuda:0\\PYZdq{}), dtype=torch.float32)  \\PYZsh{} t9: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "      \\PY{c+c1}{\\PYZsh{} t9 = prims.full((64, 64), 1, device=devices.Device(\\PYZdq{}cuda:0\\PYZdq{}), dtype=dtypes.float32)  \\PYZsh{} t9: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t10} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{full}\\PY{p}{(}\\PY{p}{(}\\PY{l+m+mi}{64}\\PY{p}{,} \\PY{l+m+mi}{64}\\PY{p}{)}\\PY{p}{,} \\PY{l+m+mi}{1}\\PY{p}{,} \\PY{n}{device}\\PY{o}{=}\\PY{n}{torch}\\PY{o}{.}\\PY{n}{device}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{cuda:0}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\\PY{p}{,} \\PY{n}{dtype}\\PY{o}{=}\\PY{n}{torch}\\PY{o}{.}\\PY{n}{float32}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t10: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t10 = ltorch.full((64, 64), 1, device=torch.device(\\PYZdq{}cuda:0\\PYZdq{}), dtype=torch.float32)  \\PYZsh{} t10: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "      \\PY{c+c1}{\\PYZsh{} t10 = prims.full((64, 64), 1, device=devices.Device(\\PYZdq{}cuda:0\\PYZdq{}), dtype=dtypes.float32)  \\PYZsh{} t10: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{p11} \\PY{o}{=} \\PY{n}{torch\\PYZus{}all\\PYZus{}gather\\PYZus{}prim\\PYZus{}impl}\\PY{p}{(}\\PY{n}{t1}\\PY{p}{,} \\PY{n}{\\PYZus{}torch\\PYZus{}distributed\\PYZus{}distributed\\PYZus{}c10d\\PYZus{}ProcessGroup\\PYZus{}2}\\PY{p}{,} \\PY{k+kc}{True}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} p11: \\PYZdq{}FUTURE cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{k}{del} \\PY{n}{t1}\n",
       "  \\PY{n}{t12} \\PY{o}{=} \\PY{n}{torch\\PYZus{}wait\\PYZus{}prim\\PYZus{}impl}\\PY{p}{(}\\PY{n}{p11}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t12: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{k}{del} \\PY{n}{p11}\n",
       "  \\PY{n}{p13} \\PY{o}{=} \\PY{n}{torch\\PYZus{}all\\PYZus{}gather\\PYZus{}prim\\PYZus{}impl}\\PY{p}{(}\\PY{n}{t2}\\PY{p}{,} \\PY{n}{\\PYZus{}torch\\PYZus{}distributed\\PYZus{}distributed\\PYZus{}c10d\\PYZus{}ProcessGroup\\PYZus{}2}\\PY{p}{,} \\PY{k+kc}{True}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} p13: \\PYZdq{}FUTURE cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{k}{del} \\PY{n}{t2}\n",
       "  \\PY{n}{t14} \\PY{o}{=} \\PY{n}{torch\\PYZus{}wait\\PYZus{}prim\\PYZus{}impl}\\PY{p}{(}\\PY{n}{p13}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t14: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{k}{del} \\PY{n}{p13}\n",
       "  \\PY{n}{t15} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{functional}\\PY{o}{.}\\PY{n}{linear}\\PY{p}{(}\\PY{n}{t0}\\PY{p}{,} \\PY{n}{t12}\\PY{p}{,} \\PY{k+kc}{None}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t15: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t15 = ltorch.linear(t0, t12, None)  \\PYZsh{} t15: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "      \\PY{c+c1}{\\PYZsh{} t15 = prims.linear(t0, t12, None)  \\PYZsh{} t15: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t16} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{tanh}\\PY{p}{(}\\PY{n}{t15}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t16: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t16 = ltorch.tanh(t15)  \\PYZsh{} t16: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "      \\PY{c+c1}{\\PYZsh{} t16 = prims.tanh(t15)  \\PYZsh{} t16: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{k}{del} \\PY{n}{t15}\n",
       "  \\PY{n}{t17} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{functional}\\PY{o}{.}\\PY{n}{linear}\\PY{p}{(}\\PY{n}{t16}\\PY{p}{,} \\PY{n}{t14}\\PY{p}{,} \\PY{k+kc}{None}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t17: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t17 = ltorch.linear(t16, t14, None)  \\PYZsh{} t17: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "      \\PY{c+c1}{\\PYZsh{} t17 = prims.linear(t16, t14, None)  \\PYZsh{} t17: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t18} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{add}\\PY{p}{(}\\PY{n}{t6}\\PY{p}{,} \\PY{n}{t7}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t18: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t18 = ltorch.add(t6, t7, alpha=None)  \\PYZsh{} t18: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "      \\PY{c+c1}{\\PYZsh{} t18 = prims.add(t6, t7)  \\PYZsh{} t18: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{k}{del} \\PY{n}{t6}\\PY{p}{,} \\PY{n}{t7}\n",
       "  \\PY{n}{t19} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{add}\\PY{p}{(}\\PY{n}{t3}\\PY{p}{,} \\PY{n}{t8}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t19: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t19 = ltorch.add(t3, t8, alpha=None)  \\PYZsh{} t19: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "      \\PY{c+c1}{\\PYZsh{} t19 = prims.add(t3, t8)  \\PYZsh{} t19: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{k}{del} \\PY{n}{t3}\\PY{p}{,} \\PY{n}{t8}\n",
       "  \\PY{n}{t20} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{add}\\PY{p}{(}\\PY{n}{t5}\\PY{p}{,} \\PY{n}{t9}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t20: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t20 = ltorch.add(t5, t9, alpha=None)  \\PYZsh{} t20: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "      \\PY{c+c1}{\\PYZsh{} t20 = prims.add(t5, t9)  \\PYZsh{} t20: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{k}{del} \\PY{n}{t5}\\PY{p}{,} \\PY{n}{t9}\n",
       "  \\PY{n}{t21} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{reshape}\\PY{p}{(}\\PY{n}{t18}\\PY{p}{,} \\PY{p}{(}\\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+m+mi}{64}\\PY{p}{)}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t21: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t21 = ltorch.reshape(t18, (\\PYZhy{}1, 64))  \\PYZsh{} t21: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "      \\PY{c+c1}{\\PYZsh{} t21 = prims.reshape(t18, (64, 64))  \\PYZsh{} t21: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t22} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{matmul}\\PY{p}{(}\\PY{n}{t21}\\PY{p}{,} \\PY{n}{t14}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t22: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t22 = ltorch.matmul(t21, t14)  \\PYZsh{} t22: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "      \\PY{c+c1}{\\PYZsh{} t22 = prims.matmul(t21, t14)  \\PYZsh{} t22: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{k}{del} \\PY{n}{t21}\n",
       "  \\PY{n}{t23} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{reshape}\\PY{p}{(}\\PY{n}{t18}\\PY{p}{,} \\PY{p}{(}\\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+m+mi}{64}\\PY{p}{)}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t23: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t23 = ltorch.reshape(t18, (\\PYZhy{}1, 64))  \\PYZsh{} t23: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "      \\PY{c+c1}{\\PYZsh{} t23 = prims.reshape(t18, (64, 64))  \\PYZsh{} t23: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{k}{del} \\PY{n}{t18}\n",
       "  \\PY{n}{t24} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{permute}\\PY{p}{(}\\PY{n}{t23}\\PY{p}{,} \\PY{p}{(}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+m+mi}{0}\\PY{p}{)}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t24: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t24 = ltorch.permute(t23, (1, 0))  \\PYZsh{} t24: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "      \\PY{c+c1}{\\PYZsh{} t24 = prims.transpose(t23, (1, 0))  \\PYZsh{} t24: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{k}{del} \\PY{n}{t23}\n",
       "  \\PY{n}{t25} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{reshape}\\PY{p}{(}\\PY{n}{t16}\\PY{p}{,} \\PY{p}{(}\\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+m+mi}{64}\\PY{p}{)}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t25: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t25 = ltorch.reshape(t16, (\\PYZhy{}1, 64))  \\PYZsh{} t25: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "      \\PY{c+c1}{\\PYZsh{} t25 = prims.reshape(t16, (64, 64))  \\PYZsh{} t25: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t26} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{matmul}\\PY{p}{(}\\PY{n}{t24}\\PY{p}{,} \\PY{n}{t25}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t26: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t26 = ltorch.matmul(t24, t25)  \\PYZsh{} t26: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "      \\PY{c+c1}{\\PYZsh{} t26 = prims.matmul(t24, t25)  \\PYZsh{} t26: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{k}{del} \\PY{n}{t24}\\PY{p}{,} \\PY{n}{t25}\n",
       "  \\PY{n}{t27} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{add}\\PY{p}{(}\\PY{n}{t10}\\PY{p}{,} \\PY{n}{t22}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t27: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t27 = ltorch.add(t10, t22, alpha=None)  \\PYZsh{} t27: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "      \\PY{c+c1}{\\PYZsh{} t27 = prims.add(t10, t22)  \\PYZsh{} t27: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{k}{del} \\PY{n}{t10}\\PY{p}{,} \\PY{n}{t22}\n",
       "  \\PY{n}{t28} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{add}\\PY{p}{(}\\PY{n}{t20}\\PY{p}{,} \\PY{n}{t26}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t28: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t28 = ltorch.add(t20, t26, alpha=None)  \\PYZsh{} t28: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "      \\PY{c+c1}{\\PYZsh{} t28 = prims.add(t20, t26)  \\PYZsh{} t28: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{k}{del} \\PY{n}{t20}\\PY{p}{,} \\PY{n}{t26}\n",
       "  \\PY{n}{t29} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{mul}\\PY{p}{(}\\PY{n}{t16}\\PY{p}{,} \\PY{n}{t16}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t29: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t29 = ltorch.mul(t16, t16)  \\PYZsh{} t29: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "      \\PY{c+c1}{\\PYZsh{} t29 = prims.mul(t16, t16)  \\PYZsh{} t29: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t30} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{sub}\\PY{p}{(}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{n}{t29}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t30: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t30 = ltorch.sub(1, t29, alpha=None)  \\PYZsh{} t30: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "      \\PY{c+c1}{\\PYZsh{} \\PYZus{} = prims.convert\\PYZus{}element\\PYZus{}type(1, float)}\n",
       "      \\PY{c+c1}{\\PYZsh{} t30 = prims.sub(1.0, t29)  \\PYZsh{} t30: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{k}{del} \\PY{n}{t29}\n",
       "  \\PY{n}{t31} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{mul}\\PY{p}{(}\\PY{n}{t27}\\PY{p}{,} \\PY{n}{t30}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t31: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t31 = ltorch.mul(t27, t30)  \\PYZsh{} t31: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "      \\PY{c+c1}{\\PYZsh{} t31 = prims.mul(t27, t30)  \\PYZsh{} t31: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{k}{del} \\PY{n}{t27}\\PY{p}{,} \\PY{n}{t30}\n",
       "  \\PY{n}{t32} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{reshape}\\PY{p}{(}\\PY{n}{t31}\\PY{p}{,} \\PY{p}{(}\\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+m+mi}{64}\\PY{p}{)}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t32: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t32 = ltorch.reshape(t31, (\\PYZhy{}1, 64))  \\PYZsh{} t32: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "      \\PY{c+c1}{\\PYZsh{} t32 = prims.reshape(t31, (64, 64))  \\PYZsh{} t32: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t33} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{matmul}\\PY{p}{(}\\PY{n}{t32}\\PY{p}{,} \\PY{n}{t12}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t33: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t33 = ltorch.matmul(t32, t12)  \\PYZsh{} t33: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "      \\PY{c+c1}{\\PYZsh{} t33 = prims.matmul(t32, t12)  \\PYZsh{} t33: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{k}{del} \\PY{n}{t32}\n",
       "  \\PY{n}{t34} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{reshape}\\PY{p}{(}\\PY{n}{t31}\\PY{p}{,} \\PY{p}{(}\\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+m+mi}{64}\\PY{p}{)}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t34: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t34 = ltorch.reshape(t31, (\\PYZhy{}1, 64))  \\PYZsh{} t34: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "      \\PY{c+c1}{\\PYZsh{} t34 = prims.reshape(t31, (64, 64))  \\PYZsh{} t34: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{k}{del} \\PY{n}{t31}\n",
       "  \\PY{n}{t35} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{permute}\\PY{p}{(}\\PY{n}{t34}\\PY{p}{,} \\PY{p}{(}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+m+mi}{0}\\PY{p}{)}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t35: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t35 = ltorch.permute(t34, (1, 0))  \\PYZsh{} t35: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "      \\PY{c+c1}{\\PYZsh{} t35 = prims.transpose(t34, (1, 0))  \\PYZsh{} t35: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{k}{del} \\PY{n}{t34}\n",
       "  \\PY{n}{t36} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{reshape}\\PY{p}{(}\\PY{n}{t0}\\PY{p}{,} \\PY{p}{(}\\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+m+mi}{64}\\PY{p}{)}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t36: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t36 = ltorch.reshape(t0, (\\PYZhy{}1, 64))  \\PYZsh{} t36: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "      \\PY{c+c1}{\\PYZsh{} t36 = prims.reshape(t0, (64, 64))  \\PYZsh{} t36: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{n}{t37} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{matmul}\\PY{p}{(}\\PY{n}{t35}\\PY{p}{,} \\PY{n}{t36}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t37: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t37 = ltorch.matmul(t35, t36)  \\PYZsh{} t37: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "      \\PY{c+c1}{\\PYZsh{} t37 = prims.matmul(t35, t36)  \\PYZsh{} t37: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{k}{del} \\PY{n}{t35}\\PY{p}{,} \\PY{n}{t36}\n",
       "  \\PY{n}{t38} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{add}\\PY{p}{(}\\PY{n}{t19}\\PY{p}{,} \\PY{n}{t33}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t38: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t38 = ltorch.add(t19, t33, alpha=None)  \\PYZsh{} t38: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "      \\PY{c+c1}{\\PYZsh{} t38 = prims.add(t19, t33)  \\PYZsh{} t38: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{k}{del} \\PY{n}{t19}\\PY{p}{,} \\PY{n}{t33}\n",
       "  \\PY{n}{t39} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{add}\\PY{p}{(}\\PY{n}{t4}\\PY{p}{,} \\PY{n}{t37}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t39: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t39 = ltorch.add(t4, t37, alpha=None)  \\PYZsh{} t39: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "      \\PY{c+c1}{\\PYZsh{} t39 = prims.add(t4, t37)  \\PYZsh{} t39: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{k}{del} \\PY{n}{t4}\\PY{p}{,} \\PY{n}{t37}\n",
       "  \\PY{n}{t40} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{true\\PYZus{}divide}\\PY{p}{(}\\PY{n}{t28}\\PY{p}{,} \\PY{l+m+mi}{2}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t40: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t40 = ltorch.true\\PYZus{}divide(t28, 2)  \\PYZsh{} t40: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "      \\PY{c+c1}{\\PYZsh{} \\PYZus{} = prims.convert\\PYZus{}element\\PYZus{}type(2, float)}\n",
       "      \\PY{c+c1}{\\PYZsh{} t40 = prims.div(t28, 2.0)  \\PYZsh{} t40: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{k}{del} \\PY{n}{t28}\n",
       "  \\PY{n}{p41} \\PY{o}{=} \\PY{n}{torch\\PYZus{}reduce\\PYZus{}scatter\\PYZus{}prim\\PYZus{}impl}\\PY{p}{(}\\PY{n}{t40}\\PY{p}{,} \\PY{n}{\\PYZus{}DistributedReduceOps\\PYZus{}3}\\PY{p}{,} \\PY{n}{\\PYZus{}torch\\PYZus{}distributed\\PYZus{}distributed\\PYZus{}c10d\\PYZus{}ProcessGroup\\PYZus{}2}\\PY{p}{,} \\PY{k+kc}{True}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} p41: \\PYZdq{}FUTURE cuda:0 f32[32, 64]\\PYZdq{}}\n",
       "  \\PY{k}{del} \\PY{n}{t40}\n",
       "  \\PY{n}{t42} \\PY{o}{=} \\PY{n}{torch\\PYZus{}wait\\PYZus{}prim\\PYZus{}impl}\\PY{p}{(}\\PY{n}{p41}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t42: \\PYZdq{}cuda:0 f32[32, 64]\\PYZdq{}}\n",
       "  \\PY{k}{del} \\PY{n}{p41}\n",
       "  \\PY{n}{t43} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{true\\PYZus{}divide}\\PY{p}{(}\\PY{n}{t39}\\PY{p}{,} \\PY{l+m+mi}{2}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t43: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} t43 = ltorch.true\\PYZus{}divide(t39, 2)  \\PYZsh{} t43: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "      \\PY{c+c1}{\\PYZsh{} \\PYZus{} = prims.convert\\PYZus{}element\\PYZus{}type(2, float)}\n",
       "      \\PY{c+c1}{\\PYZsh{} t43 = prims.div(t39, 2.0)  \\PYZsh{} t43: \\PYZdq{}cuda:0 f32[64, 64]\\PYZdq{}}\n",
       "  \\PY{k}{del} \\PY{n}{t39}\n",
       "  \\PY{n}{p44} \\PY{o}{=} \\PY{n}{torch\\PYZus{}reduce\\PYZus{}scatter\\PYZus{}prim\\PYZus{}impl}\\PY{p}{(}\\PY{n}{t43}\\PY{p}{,} \\PY{n}{\\PYZus{}DistributedReduceOps\\PYZus{}3}\\PY{p}{,} \\PY{n}{\\PYZus{}torch\\PYZus{}distributed\\PYZus{}distributed\\PYZus{}c10d\\PYZus{}ProcessGroup\\PYZus{}2}\\PY{p}{,} \\PY{k+kc}{True}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} p44: \\PYZdq{}FUTURE cuda:0 f32[32, 64]\\PYZdq{}}\n",
       "  \\PY{k}{del} \\PY{n}{t43}\n",
       "  \\PY{n}{t45} \\PY{o}{=} \\PY{n}{torch\\PYZus{}wait\\PYZus{}prim\\PYZus{}impl}\\PY{p}{(}\\PY{n}{p44}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t45: \\PYZdq{}cuda:0 f32[32, 64]\\PYZdq{}}\n",
       "  \\PY{k}{del} \\PY{n}{p44}\n",
       "  \\PY{k}{return} \\PY{p}{(}\\PY{p}{(}\\PY{p}{\\PYZob{}}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{output}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{:} \\PY{n}{t17}\\PY{p}{,} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{flat\\PYZus{}args}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{:} \\PY{p}{[}\\PY{n}{t0}\\PY{p}{,} \\PY{n}{t12}\\PY{p}{,} \\PY{n}{t14}\\PY{p}{]}\\PY{p}{,} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{flat\\PYZus{}output}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{:} \\PY{p}{(}\\PY{n}{t17}\\PY{p}{,}\\PY{p}{)}\\PY{p}{\\PYZcb{}}\\PY{p}{,} \\PY{p}{(}\\PY{p}{(}\\PY{n}{t0}\\PY{p}{,} \\PY{n}{t14}\\PY{p}{,} \\PY{n}{t16}\\PY{p}{)}\\PY{p}{,} \\PY{p}{(}\\PY{p}{)}\\PY{p}{)}\\PY{p}{)}\\PY{p}{,} \\PY{p}{(}\\PY{n}{t38}\\PY{p}{,} \\PY{n}{t45}\\PY{p}{,} \\PY{n}{t42}\\PY{p}{)}\\PY{p}{)}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "# Constructed by Delete Last Used (took 0 milliseconds)\n",
       "import torch\n",
       "import torch.nn.functional\n",
       "from thunder.executors.torchex import no_autocast\n",
       "\n",
       "@torch.no_grad()\n",
       "@no_autocast()\n",
       "def _value_and_grad(*args):\n",
       "  # args: \"Collection\" \n",
       "  t0, \\\n",
       "  t1, \\\n",
       "  t2, \\\n",
       "  = args\n",
       "  del args\n",
       "  t3 = torch.full((64, 64), 1, device=torch.device(\"cuda:0\"), dtype=torch.float32)  # t3: \"cuda:0 f32[64, 64]\"\n",
       "    # t3 = ltorch.full((64, 64), 1, device=torch.device(\"cuda:0\"), dtype=torch.float32)  # t3: \"cuda:0 f32[64, 64]\"\n",
       "      # t3 = prims.full((64, 64), 1, device=devices.Device(\"cuda:0\"), dtype=dtypes.float32)  # t3: \"cuda:0 f32[64, 64]\"\n",
       "  t4 = torch.full((64, 64), 1, device=torch.device(\"cuda:0\"), dtype=torch.float32)  # t4: \"cuda:0 f32[64, 64]\"\n",
       "    # t4 = ltorch.full((64, 64), 1, device=torch.device(\"cuda:0\"), dtype=torch.float32)  # t4: \"cuda:0 f32[64, 64]\"\n",
       "      # t4 = prims.full((64, 64), 1, device=devices.Device(\"cuda:0\"), dtype=dtypes.float32)  # t4: \"cuda:0 f32[64, 64]\"\n",
       "  t5 = torch.full((64, 64), 1, device=torch.device(\"cuda:0\"), dtype=torch.float32)  # t5: \"cuda:0 f32[64, 64]\"\n",
       "    # t5 = ltorch.full((64, 64), 1, device=torch.device(\"cuda:0\"), dtype=torch.float32)  # t5: \"cuda:0 f32[64, 64]\"\n",
       "      # t5 = prims.full((64, 64), 1, device=devices.Device(\"cuda:0\"), dtype=dtypes.float32)  # t5: \"cuda:0 f32[64, 64]\"\n",
       "  t6 = torch.full((64, 64), 1, device=torch.device(\"cuda:0\"), dtype=torch.float32)  # t6: \"cuda:0 f32[64, 64]\"\n",
       "    # t6 = ltorch.full((64, 64), 1, device=torch.device(\"cuda:0\"), dtype=torch.float32)  # t6: \"cuda:0 f32[64, 64]\"\n",
       "      # t6 = prims.full((64, 64), 1, device=devices.Device(\"cuda:0\"), dtype=dtypes.float32)  # t6: \"cuda:0 f32[64, 64]\"\n",
       "  t7 = torch.full((64, 64), 1, device=torch.device(\"cuda:0\"), dtype=torch.float32)  # t7: \"cuda:0 f32[64, 64]\"\n",
       "    # t7 = ltorch.full((64, 64), 1, device=torch.device(\"cuda:0\"), dtype=torch.float32)  # t7: \"cuda:0 f32[64, 64]\"\n",
       "      # t7 = prims.full((64, 64), 1, device=devices.Device(\"cuda:0\"), dtype=dtypes.float32)  # t7: \"cuda:0 f32[64, 64]\"\n",
       "  t8 = torch.full((64, 64), 1, device=torch.device(\"cuda:0\"), dtype=torch.float32)  # t8: \"cuda:0 f32[64, 64]\"\n",
       "    # t8 = ltorch.full((64, 64), 1, device=torch.device(\"cuda:0\"), dtype=torch.float32)  # t8: \"cuda:0 f32[64, 64]\"\n",
       "      # t8 = prims.full((64, 64), 1, device=devices.Device(\"cuda:0\"), dtype=dtypes.float32)  # t8: \"cuda:0 f32[64, 64]\"\n",
       "  t9 = torch.full((64, 64), 1, device=torch.device(\"cuda:0\"), dtype=torch.float32)  # t9: \"cuda:0 f32[64, 64]\"\n",
       "    # t9 = ltorch.full((64, 64), 1, device=torch.device(\"cuda:0\"), dtype=torch.float32)  # t9: \"cuda:0 f32[64, 64]\"\n",
       "      # t9 = prims.full((64, 64), 1, device=devices.Device(\"cuda:0\"), dtype=dtypes.float32)  # t9: \"cuda:0 f32[64, 64]\"\n",
       "  t10 = torch.full((64, 64), 1, device=torch.device(\"cuda:0\"), dtype=torch.float32)  # t10: \"cuda:0 f32[64, 64]\"\n",
       "    # t10 = ltorch.full((64, 64), 1, device=torch.device(\"cuda:0\"), dtype=torch.float32)  # t10: \"cuda:0 f32[64, 64]\"\n",
       "      # t10 = prims.full((64, 64), 1, device=devices.Device(\"cuda:0\"), dtype=dtypes.float32)  # t10: \"cuda:0 f32[64, 64]\"\n",
       "  p11 = torch_all_gather_prim_impl(t1, _torch_distributed_distributed_c10d_ProcessGroup_2, True)  # p11: \"FUTURE cuda:0 f32[64, 64]\"\n",
       "  del t1\n",
       "  t12 = torch_wait_prim_impl(p11)  # t12: \"cuda:0 f32[64, 64]\"\n",
       "  del p11\n",
       "  p13 = torch_all_gather_prim_impl(t2, _torch_distributed_distributed_c10d_ProcessGroup_2, True)  # p13: \"FUTURE cuda:0 f32[64, 64]\"\n",
       "  del t2\n",
       "  t14 = torch_wait_prim_impl(p13)  # t14: \"cuda:0 f32[64, 64]\"\n",
       "  del p13\n",
       "  t15 = torch.nn.functional.linear(t0, t12, None)  # t15: \"cuda:0 f32[64, 64]\"\n",
       "    # t15 = ltorch.linear(t0, t12, None)  # t15: \"cuda:0 f32[64, 64]\"\n",
       "      # t15 = prims.linear(t0, t12, None)  # t15: \"cuda:0 f32[64, 64]\"\n",
       "  t16 = torch.tanh(t15)  # t16: \"cuda:0 f32[64, 64]\"\n",
       "    # t16 = ltorch.tanh(t15)  # t16: \"cuda:0 f32[64, 64]\"\n",
       "      # t16 = prims.tanh(t15)  # t16: \"cuda:0 f32[64, 64]\"\n",
       "  del t15\n",
       "  t17 = torch.nn.functional.linear(t16, t14, None)  # t17: \"cuda:0 f32[64, 64]\"\n",
       "    # t17 = ltorch.linear(t16, t14, None)  # t17: \"cuda:0 f32[64, 64]\"\n",
       "      # t17 = prims.linear(t16, t14, None)  # t17: \"cuda:0 f32[64, 64]\"\n",
       "  t18 = torch.add(t6, t7)  # t18: \"cuda:0 f32[64, 64]\"\n",
       "    # t18 = ltorch.add(t6, t7, alpha=None)  # t18: \"cuda:0 f32[64, 64]\"\n",
       "      # t18 = prims.add(t6, t7)  # t18: \"cuda:0 f32[64, 64]\"\n",
       "  del t6, t7\n",
       "  t19 = torch.add(t3, t8)  # t19: \"cuda:0 f32[64, 64]\"\n",
       "    # t19 = ltorch.add(t3, t8, alpha=None)  # t19: \"cuda:0 f32[64, 64]\"\n",
       "      # t19 = prims.add(t3, t8)  # t19: \"cuda:0 f32[64, 64]\"\n",
       "  del t3, t8\n",
       "  t20 = torch.add(t5, t9)  # t20: \"cuda:0 f32[64, 64]\"\n",
       "    # t20 = ltorch.add(t5, t9, alpha=None)  # t20: \"cuda:0 f32[64, 64]\"\n",
       "      # t20 = prims.add(t5, t9)  # t20: \"cuda:0 f32[64, 64]\"\n",
       "  del t5, t9\n",
       "  t21 = torch.reshape(t18, (-1, 64))  # t21: \"cuda:0 f32[64, 64]\"\n",
       "    # t21 = ltorch.reshape(t18, (-1, 64))  # t21: \"cuda:0 f32[64, 64]\"\n",
       "      # t21 = prims.reshape(t18, (64, 64))  # t21: \"cuda:0 f32[64, 64]\"\n",
       "  t22 = torch.matmul(t21, t14)  # t22: \"cuda:0 f32[64, 64]\"\n",
       "    # t22 = ltorch.matmul(t21, t14)  # t22: \"cuda:0 f32[64, 64]\"\n",
       "      # t22 = prims.matmul(t21, t14)  # t22: \"cuda:0 f32[64, 64]\"\n",
       "  del t21\n",
       "  t23 = torch.reshape(t18, (-1, 64))  # t23: \"cuda:0 f32[64, 64]\"\n",
       "    # t23 = ltorch.reshape(t18, (-1, 64))  # t23: \"cuda:0 f32[64, 64]\"\n",
       "      # t23 = prims.reshape(t18, (64, 64))  # t23: \"cuda:0 f32[64, 64]\"\n",
       "  del t18\n",
       "  t24 = torch.permute(t23, (1, 0))  # t24: \"cuda:0 f32[64, 64]\"\n",
       "    # t24 = ltorch.permute(t23, (1, 0))  # t24: \"cuda:0 f32[64, 64]\"\n",
       "      # t24 = prims.transpose(t23, (1, 0))  # t24: \"cuda:0 f32[64, 64]\"\n",
       "  del t23\n",
       "  t25 = torch.reshape(t16, (-1, 64))  # t25: \"cuda:0 f32[64, 64]\"\n",
       "    # t25 = ltorch.reshape(t16, (-1, 64))  # t25: \"cuda:0 f32[64, 64]\"\n",
       "      # t25 = prims.reshape(t16, (64, 64))  # t25: \"cuda:0 f32[64, 64]\"\n",
       "  t26 = torch.matmul(t24, t25)  # t26: \"cuda:0 f32[64, 64]\"\n",
       "    # t26 = ltorch.matmul(t24, t25)  # t26: \"cuda:0 f32[64, 64]\"\n",
       "      # t26 = prims.matmul(t24, t25)  # t26: \"cuda:0 f32[64, 64]\"\n",
       "  del t24, t25\n",
       "  t27 = torch.add(t10, t22)  # t27: \"cuda:0 f32[64, 64]\"\n",
       "    # t27 = ltorch.add(t10, t22, alpha=None)  # t27: \"cuda:0 f32[64, 64]\"\n",
       "      # t27 = prims.add(t10, t22)  # t27: \"cuda:0 f32[64, 64]\"\n",
       "  del t10, t22\n",
       "  t28 = torch.add(t20, t26)  # t28: \"cuda:0 f32[64, 64]\"\n",
       "    # t28 = ltorch.add(t20, t26, alpha=None)  # t28: \"cuda:0 f32[64, 64]\"\n",
       "      # t28 = prims.add(t20, t26)  # t28: \"cuda:0 f32[64, 64]\"\n",
       "  del t20, t26\n",
       "  t29 = torch.mul(t16, t16)  # t29: \"cuda:0 f32[64, 64]\"\n",
       "    # t29 = ltorch.mul(t16, t16)  # t29: \"cuda:0 f32[64, 64]\"\n",
       "      # t29 = prims.mul(t16, t16)  # t29: \"cuda:0 f32[64, 64]\"\n",
       "  t30 = torch.sub(1, t29)  # t30: \"cuda:0 f32[64, 64]\"\n",
       "    # t30 = ltorch.sub(1, t29, alpha=None)  # t30: \"cuda:0 f32[64, 64]\"\n",
       "      # _ = prims.convert_element_type(1, float)\n",
       "      # t30 = prims.sub(1.0, t29)  # t30: \"cuda:0 f32[64, 64]\"\n",
       "  del t29\n",
       "  t31 = torch.mul(t27, t30)  # t31: \"cuda:0 f32[64, 64]\"\n",
       "    # t31 = ltorch.mul(t27, t30)  # t31: \"cuda:0 f32[64, 64]\"\n",
       "      # t31 = prims.mul(t27, t30)  # t31: \"cuda:0 f32[64, 64]\"\n",
       "  del t27, t30\n",
       "  t32 = torch.reshape(t31, (-1, 64))  # t32: \"cuda:0 f32[64, 64]\"\n",
       "    # t32 = ltorch.reshape(t31, (-1, 64))  # t32: \"cuda:0 f32[64, 64]\"\n",
       "      # t32 = prims.reshape(t31, (64, 64))  # t32: \"cuda:0 f32[64, 64]\"\n",
       "  t33 = torch.matmul(t32, t12)  # t33: \"cuda:0 f32[64, 64]\"\n",
       "    # t33 = ltorch.matmul(t32, t12)  # t33: \"cuda:0 f32[64, 64]\"\n",
       "      # t33 = prims.matmul(t32, t12)  # t33: \"cuda:0 f32[64, 64]\"\n",
       "  del t32\n",
       "  t34 = torch.reshape(t31, (-1, 64))  # t34: \"cuda:0 f32[64, 64]\"\n",
       "    # t34 = ltorch.reshape(t31, (-1, 64))  # t34: \"cuda:0 f32[64, 64]\"\n",
       "      # t34 = prims.reshape(t31, (64, 64))  # t34: \"cuda:0 f32[64, 64]\"\n",
       "  del t31\n",
       "  t35 = torch.permute(t34, (1, 0))  # t35: \"cuda:0 f32[64, 64]\"\n",
       "    # t35 = ltorch.permute(t34, (1, 0))  # t35: \"cuda:0 f32[64, 64]\"\n",
       "      # t35 = prims.transpose(t34, (1, 0))  # t35: \"cuda:0 f32[64, 64]\"\n",
       "  del t34\n",
       "  t36 = torch.reshape(t0, (-1, 64))  # t36: \"cuda:0 f32[64, 64]\"\n",
       "    # t36 = ltorch.reshape(t0, (-1, 64))  # t36: \"cuda:0 f32[64, 64]\"\n",
       "      # t36 = prims.reshape(t0, (64, 64))  # t36: \"cuda:0 f32[64, 64]\"\n",
       "  t37 = torch.matmul(t35, t36)  # t37: \"cuda:0 f32[64, 64]\"\n",
       "    # t37 = ltorch.matmul(t35, t36)  # t37: \"cuda:0 f32[64, 64]\"\n",
       "      # t37 = prims.matmul(t35, t36)  # t37: \"cuda:0 f32[64, 64]\"\n",
       "  del t35, t36\n",
       "  t38 = torch.add(t19, t33)  # t38: \"cuda:0 f32[64, 64]\"\n",
       "    # t38 = ltorch.add(t19, t33, alpha=None)  # t38: \"cuda:0 f32[64, 64]\"\n",
       "      # t38 = prims.add(t19, t33)  # t38: \"cuda:0 f32[64, 64]\"\n",
       "  del t19, t33\n",
       "  t39 = torch.add(t4, t37)  # t39: \"cuda:0 f32[64, 64]\"\n",
       "    # t39 = ltorch.add(t4, t37, alpha=None)  # t39: \"cuda:0 f32[64, 64]\"\n",
       "      # t39 = prims.add(t4, t37)  # t39: \"cuda:0 f32[64, 64]\"\n",
       "  del t4, t37\n",
       "  t40 = torch.true_divide(t28, 2)  # t40: \"cuda:0 f32[64, 64]\"\n",
       "    # t40 = ltorch.true_divide(t28, 2)  # t40: \"cuda:0 f32[64, 64]\"\n",
       "      # _ = prims.convert_element_type(2, float)\n",
       "      # t40 = prims.div(t28, 2.0)  # t40: \"cuda:0 f32[64, 64]\"\n",
       "  del t28\n",
       "  p41 = torch_reduce_scatter_prim_impl(t40, _DistributedReduceOps_3, _torch_distributed_distributed_c10d_ProcessGroup_2, True)  # p41: \"FUTURE cuda:0 f32[32, 64]\"\n",
       "  del t40\n",
       "  t42 = torch_wait_prim_impl(p41)  # t42: \"cuda:0 f32[32, 64]\"\n",
       "  del p41\n",
       "  t43 = torch.true_divide(t39, 2)  # t43: \"cuda:0 f32[64, 64]\"\n",
       "    # t43 = ltorch.true_divide(t39, 2)  # t43: \"cuda:0 f32[64, 64]\"\n",
       "      # _ = prims.convert_element_type(2, float)\n",
       "      # t43 = prims.div(t39, 2.0)  # t43: \"cuda:0 f32[64, 64]\"\n",
       "  del t39\n",
       "  p44 = torch_reduce_scatter_prim_impl(t43, _DistributedReduceOps_3, _torch_distributed_distributed_c10d_ProcessGroup_2, True)  # p44: \"FUTURE cuda:0 f32[32, 64]\"\n",
       "  del t43\n",
       "  t45 = torch_wait_prim_impl(p44)  # t45: \"cuda:0 f32[32, 64]\"\n",
       "  del p44\n",
       "  return (({'output': t17, 'flat_args': [t0, t12, t14], 'flat_output': (t17,)}, ((t0, t14, t16), ())), (t38, t45, t42))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_trace = thunder.transform_for_execution(forward_backward_trace, executors_list=thunder.get_always_executors())\n",
    "\n",
    "# Grab the final trace\n",
    "exec_trace = optimized_trace[-1]\n",
    "wrap_as_highlighted_code(exec_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 : Running the actual computation\n",
    "\n",
    "Running the actual computation will require setting up 2 processes and running our above code in both those processes (which can be tricky with Jupyter Notebook). Instead, we will write a small script and run it with `torchrun` which takes care of setting up the processes and relevant state.\n",
    "\n",
    "**NOTE**: This requires device running this notebook to have at least 2-GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below, we will use `thunder.distributed.fsdp` which does the same as what we did above (with some extra checks). The code below should look familiar as it is roughly all the above pieces in a single script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting thunder_fsdp_simple_example.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile thunder_fsdp_simple_example.py\n",
    "\n",
    "# imports\n",
    "from thunder.tests.litgpt_model import GPT, Config\n",
    "import torch\n",
    "import torch.distributed\n",
    "import thunder\n",
    "import thunder.distributed\n",
    "import os\n",
    "\n",
    "# # # # # # # #\n",
    "# Create Model\n",
    "# # # # # # # #\n",
    "\n",
    "# NOTE: We create the model on CPU.\n",
    "device='cpu'\n",
    "dim = 64\n",
    "def create_model():\n",
    "    layers = []\n",
    "    layers.append(torch.nn.Linear(dim, dim))\n",
    "    layers.append(torch.nn.ReLU())\n",
    "    layers.append(torch.nn.Linear(dim, dim))\n",
    "    return torch.nn.Sequential(*layers).to(device)\n",
    "\n",
    "# Model\n",
    "model = create_model()\n",
    "# Input\n",
    "x = torch.randn(dim, dim, device=device)\n",
    "\n",
    "# # # # # # # #\n",
    "# Setup for distributed\n",
    "# # # # # # # #\n",
    "torch.distributed.init_process_group(backend='nccl')\n",
    "\n",
    "rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "\n",
    "device = f\"cuda:{rank}\"\n",
    "\n",
    "# # # # # # # #\n",
    "# Move inputs to correct device\n",
    "# # # # # # # #\n",
    "x = x.to(device)\n",
    "\n",
    "# # # # # # # #\n",
    "# Wrap the model in thunder.distributed.fsdp\n",
    "# # # # # # # #\n",
    "\n",
    "# thunder.distributed.fsdp takes care of moving the parameter\n",
    "# shard to the correct GPU for the current process.\n",
    "cmodel = thunder.jit(thunder.distributed.fsdp(model))\n",
    "\n",
    "# Run the forward pass.\n",
    "cmodel(x)\n",
    "\n",
    "# # # # # # # #\n",
    "# Check the traces\n",
    "# # # # # # # #\n",
    "fwd_traces = thunder.last_traces(cmodel)\n",
    "bwd_traces = thunder.last_backward_traces(cmodel)\n",
    "\n",
    "# # # # # # # #\n",
    "# Print and check to see if they match ours\n",
    "# # # # # # # #\n",
    "if rank == 0:\n",
    "    print(fwd_traces[-1])\n",
    "    print(\"*******\"* 8)\n",
    "    print(bwd_traces[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us run the above script and check what the trace looks like.\n",
    "\n",
    "We can observe that forward trace has `torch_all_gather_prim_impl` to gather the parameter before forward pass and the backward trace has `torch_reduce_scatter_prim_impl` to reduce and scatter the gradients back to different GPUs. This is similar to our implementation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0314 08:26:39.130000 140292199276608 torch/distributed/run.py:757] \n",
      "W0314 08:26:39.130000 140292199276608 torch/distributed/run.py:757] *****************************************\n",
      "W0314 08:26:39.130000 140292199276608 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0314 08:26:39.130000 140292199276608 torch/distributed/run.py:757] *****************************************\n",
      "# Constructed by Delete Last Used (took 0 milliseconds)\n",
      "import torch\n",
      "import torch.nn.functional\n",
      "from thunder.executors.torchex import no_autocast\n",
      "\n",
      "@torch.no_grad()\n",
      "@no_autocast()\n",
      "def augmented_forward_fn(input, t_0_bias, t_2_bias, t_0_weight, t_2_weight):\n",
      "  # input: \"cuda:0 f32[64, 64]\" \n",
      "  # t_0_bias: \"cuda:0 f32[32]\" \n",
      "  p0 = torch_all_gather_prim_impl(t_0_bias, _torch_distributed_distributed_c10d_ProcessGroup_1, True)  # p0: \"FUTURE cuda:0 f32[64]\"\n",
      "  # t_2_bias: \"cuda:0 f32[32]\" \n",
      "  p2 = torch_all_gather_prim_impl(t_2_bias, _torch_distributed_distributed_c10d_ProcessGroup_1, True)  # p2: \"FUTURE cuda:0 f32[64]\"\n",
      "  # t_0_weight: \"cuda:0 f32[32, 64]\" \n",
      "  p4 = torch_all_gather_prim_impl(t_0_weight, _torch_distributed_distributed_c10d_ProcessGroup_1, True)  # p4: \"FUTURE cuda:0 f32[64, 64]\"\n",
      "  # t_2_weight: \"cuda:0 f32[32, 64]\" \n",
      "  p9 = torch_all_gather_prim_impl(t_2_weight, _torch_distributed_distributed_c10d_ProcessGroup_1, True)  # p9: \"FUTURE cuda:0 f32[64, 64]\"\n",
      "  t1 = torch_wait_prim_impl(p0)  # t1: \"cuda:0 f32[64]\"\n",
      "  del p0\n",
      "  t3 = torch_wait_prim_impl(p2)  # t3: \"cuda:0 f32[64]\"\n",
      "  del p2\n",
      "  t5 = torch_wait_prim_impl(p4)  # t5: \"cuda:0 f32[64, 64]\"\n",
      "  del p4\n",
      "  t6 = torch.nn.functional.linear(input, t5, t1)  # t6: \"cuda:0 f32[64, 64]\"\n",
      "    # t6 = ltorch.linear(input, t5, t1)  # t6: \"cuda:0 f32[64, 64]\"\n",
      "      # t6 = prims.linear(input, t5, t1)  # t6: \"cuda:0 f32[64, 64]\"\n",
      "  del t5, t1\n",
      "  [t7, t8] = nvFusion0(t6)\n",
      "    # t7 = prims.gt(t6, 0.0)  # t7: \"cuda:0 b8[64, 64]\"\n",
      "    # t8 = prims.where(t7, t6, 0.0)  # t8: \"cuda:0 f32[64, 64]\"\n",
      "  del t6\n",
      "  t10 = torch_wait_prim_impl(p9)  # t10: \"cuda:0 f32[64, 64]\"\n",
      "  del p9\n",
      "  t11 = torch.nn.functional.linear(t8, t10, t3)  # t11: \"cuda:0 f32[64, 64]\"\n",
      "    # t11 = ltorch.linear(t8, t10, t3)  # t11: \"cuda:0 f32[64, 64]\"\n",
      "      # t11 = prims.linear(t8, t10, t3)  # t11: \"cuda:0 f32[64, 64]\"\n",
      "  del t3\n",
      "  return {'output': t11, 'flat_args': [input, t_0_bias, t_2_bias, t_0_weight, t_2_weight], 'flat_output': (t11,)}, ((input, t10, t7, t8), ())\n",
      "********************************************************\n",
      "# Constructed by Delete Last Used (took 0 milliseconds)\n",
      "import torch\n",
      "from thunder.executors.torchex import no_autocast\n",
      "\n",
      "@torch.no_grad()\n",
      "@no_autocast()\n",
      "def backward_fn(saved_for_backward, cotangents):\n",
      "  # saved_for_backward: \"Collection\" \n",
      "  # cotangents: \"Collection\" \n",
      "  C0, \\\n",
      "  _, \\\n",
      "  = saved_for_backward\n",
      "  clear_collection(saved_for_backward)\n",
      "  del saved_for_backward\n",
      "  t0, \\\n",
      "  = cotangents\n",
      "  clear_collection(cotangents)\n",
      "  del cotangents\n",
      "  input, \\\n",
      "  t10, \\\n",
      "  t7, \\\n",
      "  t8, \\\n",
      "  = C0\n",
      "  clear_collection(C0)\n",
      "  del C0\n",
      "  t31 = torch.reshape(t0, (-1, 64))  # t31: \"cuda:0 f32[64, 64]\"\n",
      "    # t31 = ltorch.reshape(t0, (-1, 64))  # t31: \"cuda:0 f32[64, 64]\"\n",
      "      # t31 = prims.reshape(t0, (64, 64))  # t31: \"cuda:0 f32[64, 64]\"\n",
      "  t32 = torch.permute(t31, (1, 0))  # t32: \"cuda:0 f32[64, 64]\"\n",
      "    # t32 = ltorch.permute(t31, (1, 0))  # t32: \"cuda:0 f32[64, 64]\"\n",
      "      # t32 = prims.transpose(t31, (1, 0))  # t32: \"cuda:0 f32[64, 64]\"\n",
      "  t33 = torch.reshape(t8, (-1, 64))  # t33: \"cuda:0 f32[64, 64]\"\n",
      "    # t33 = ltorch.reshape(t8, (-1, 64))  # t33: \"cuda:0 f32[64, 64]\"\n",
      "      # t33 = prims.reshape(t8, (64, 64))  # t33: \"cuda:0 f32[64, 64]\"\n",
      "  del t8\n",
      "  t45 = torch.reshape(input, (-1, 64))  # t45: \"cuda:0 f32[64, 64]\"\n",
      "    # t45 = ltorch.reshape(input, (-1, 64))  # t45: \"cuda:0 f32[64, 64]\"\n",
      "      # t45 = prims.reshape(input, (64, 64))  # t45: \"cuda:0 f32[64, 64]\"\n",
      "  del input\n",
      "  [t51] = nvFusion0(t0)\n",
      "    # t35 = prims.sum(t0, (0,))  # t35: \"cuda:0 f32[64]\"\n",
      "    # t51 = prims.div(t35, 2.0)  # t51: \"cuda:0 f32[64]\"\n",
      "  del t0\n",
      "  p52 = torch_reduce_scatter_prim_impl(t51, _DistributedReduceOps_0, _torch_distributed_distributed_c10d_ProcessGroup_1, True)  # p52: \"FUTURE cuda:0 f32[32]\"\n",
      "  del t51\n",
      "  t30 = torch.matmul(t31, t10)  # t30: \"cuda:0 f32[64, 64]\"\n",
      "    # t30 = ltorch.matmul(t29, t10)  # t30: \"cuda:0 f32[64, 64]\"\n",
      "      # t30 = prims.matmul(t29, t10)  # t30: \"cuda:0 f32[64, 64]\"\n",
      "  del t31, t10\n",
      "  t34 = torch.matmul(t32, t33)  # t34: \"cuda:0 f32[64, 64]\"\n",
      "    # t34 = ltorch.matmul(t32, t33)  # t34: \"cuda:0 f32[64, 64]\"\n",
      "      # t34 = prims.matmul(t32, t33)  # t34: \"cuda:0 f32[64, 64]\"\n",
      "  del t32, t33\n",
      "  [t36, t39, t54] = nvFusion1(t30, t34, t7)\n",
      "    # t39 = prims.where(t7, t30, 0.0)  # t39: \"cuda:0 f32[64, 64]\"\n",
      "    # t47 = prims.sum(t39, (0,))  # t47: \"cuda:0 f32[64]\"\n",
      "    # t54 = prims.div(t47, 2.0)  # t54: \"cuda:0 f32[64]\"\n",
      "    # t36 = prims.div(t34, 2.0)  # t36: \"cuda:0 f32[64, 64]\"\n",
      "  del t30, t34, t7\n",
      "  p37 = torch_reduce_scatter_prim_impl(t36, _DistributedReduceOps_0, _torch_distributed_distributed_c10d_ProcessGroup_1, True)  # p37: \"FUTURE cuda:0 f32[32, 64]\"\n",
      "  del t36\n",
      "  p55 = torch_reduce_scatter_prim_impl(t54, _DistributedReduceOps_0, _torch_distributed_distributed_c10d_ProcessGroup_1, True)  # p55: \"FUTURE cuda:0 f32[32]\"\n",
      "  del t54\n",
      "  t43 = torch.reshape(t39, (-1, 64))  # t43: \"cuda:0 f32[64, 64]\"\n",
      "    # t43 = ltorch.reshape(t39, (-1, 64))  # t43: \"cuda:0 f32[64, 64]\"\n",
      "      # t43 = prims.reshape(t39, (64, 64))  # t43: \"cuda:0 f32[64, 64]\"\n",
      "  del t39\n",
      "  t44 = torch.permute(t43, (1, 0))  # t44: \"cuda:0 f32[64, 64]\"\n",
      "    # t44 = ltorch.permute(t43, (1, 0))  # t44: \"cuda:0 f32[64, 64]\"\n",
      "      # t44 = prims.transpose(t43, (1, 0))  # t44: \"cuda:0 f32[64, 64]\"\n",
      "  del t43\n",
      "  t46 = torch.matmul(t44, t45)  # t46: \"cuda:0 f32[64, 64]\"\n",
      "    # t46 = ltorch.matmul(t44, t45)  # t46: \"cuda:0 f32[64, 64]\"\n",
      "      # t46 = prims.matmul(t44, t45)  # t46: \"cuda:0 f32[64, 64]\"\n",
      "  del t44, t45\n",
      "  [t48] = nvFusion2(t46)\n",
      "    # t48 = prims.div(t46, 2.0)  # t48: \"cuda:0 f32[64, 64]\"\n",
      "  del t46\n",
      "  p49 = torch_reduce_scatter_prim_impl(t48, _DistributedReduceOps_0, _torch_distributed_distributed_c10d_ProcessGroup_1, True)  # p49: \"FUTURE cuda:0 f32[32, 64]\"\n",
      "  del t48\n",
      "  t53 = torch_wait_prim_impl(p52)  # t53: \"cuda:0 f32[32]\"\n",
      "  del p52\n",
      "  t38 = torch_wait_prim_impl(p37)  # t38: \"cuda:0 f32[32, 64]\"\n",
      "  del p37\n",
      "  t56 = torch_wait_prim_impl(p55)  # t56: \"cuda:0 f32[32]\"\n",
      "  del p55\n",
      "  t50 = torch_wait_prim_impl(p49)  # t50: \"cuda:0 f32[32, 64]\"\n",
      "  del p49\n",
      "  return (None, t56, t53, t50, t38)\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=2 thunder_fsdp_simple_example.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "We have created our implementation of FSDP to shard our model across multiple GPUs. In the process, we also learned that:\n",
    "\n",
    "1. `thunder` provides us with primitives for synchronization across mutiple GPUs.\n",
    "2. `thunder` also takes care of implementing the backward support for the synchronization primitives, so we don't have to explicitly do anything to get the backward working.\n",
    "3. We can just easily apply `thunder.distributed.fsdp` to our model and it will take care of sharding the parameters and also adding synchronizations to our model. Also, we can easily check the modifications by inspecting the traces."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
