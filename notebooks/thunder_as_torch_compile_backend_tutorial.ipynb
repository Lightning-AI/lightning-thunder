{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thunder as torch.compile backend (ThunderFX) Tutorial\n",
    "\n",
    "In this tutorial, we’ll explore how to use `thunder.jit` as a backend for `torch.compile`, and demonstrate the tools to inspect the compiling process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    "Starting with PyTorch2.0, the `torch.compile` feature introduces a powerful way to optimize and accelerate the PyTorch models. As its core, `torch.compile` relies on the following key components:\n",
    "1. TorchDynamo - A Python-level tracing tool that transforms Python function calls into an intermediate representation(IR)\n",
    "2. Backends - Systems that further process the IR, optimizing and executing the computational graph for better performance.\n",
    "\n",
    "While PyTorch provides several built-in backends such as \"inductor\" and \"cudagraphs\", it also supports custom backends that allow users to define their own optimization strategies. `thunder.jit` as a deep learning compiler can either be used on its own to accelerate model performance (see the [Thunder overview](https://lightning-thunder.readthedocs.io/en/latest/basic/overview.html) and other tutorials for more details) or also integrate with `torch.compile` as a backend. This is possible because TorchDynamo transforms the original Python code into new, optimized Python code that represents the same computation, which `thunder.jit` can directly process.\n",
    "\n",
    "For more information on `torch.compile`, we recommend reading PyTorch documentation and tutorials:\n",
    "\n",
    "1. Introduction to torch.compile - [Link](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)\n",
    "2. Docs of torch.compile - [Link](https://pytorch.org/docs/stable/generated/torch.compile.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Usage\n",
    "\n",
    "By simply specifying the `backend` argument as `ThunderCompiler`, we can seamlessly use `torch.compile` with `thunder.jit` as the backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4813, -0.1173,  1.8416,  1.7156],\n",
      "        [ 1.3569,  0.0439,  1.8695,  1.1239],\n",
      "        [-0.0412,  1.7470,  1.8008,  2.0433],\n",
      "        [ 1.3816,  2.0845,  0.0250,  1.9432]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from thunder.dynamo import ThunderCompiler\n",
    "\n",
    "def foo(x, y):\n",
    "    a = torch.sin(x)\n",
    "    return a + torch.sinc(a) + torch.cos(y)\n",
    "\n",
    "# Create the ThunderCompiler backend\n",
    "backend = ThunderCompiler()\n",
    "# Pass the ThunderCompiler backend to torch.compile by using the backend argument.\n",
    "opt_foo1 = torch.compile(foo, backend=backend)\n",
    "# Run the compiled model as you normally would\n",
    "print(opt_foo1(torch.randn(4, 4), torch.randn(4, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation Details and Debugging\n",
    "\n",
    "Now Let’s dive into the [FX graphs](https://pytorch.org/docs/stable/fx.html#torch.fx.Graph) generated by TorchDynamo and explore how Thunder processes them.\n",
    "\n",
    "##### Exploring FX Graphs Generated by TorchDynamo\n",
    "\n",
    "TorchDynamo transforms Python functions into FX graphs. It can segment computations into smaller subgraphs to handle dynamic behavior or unsupported operations, allowing parts of the code to fall back to native execution while optimizing supported segments. \n",
    "\n",
    "In our example, all operators in the `foo` function are supported, resulting in a single FX graph.\n",
    "\n",
    "**NOTE**: For more information about TorchDynamo, refer to the official [Dynamo overview](https://pytorch.org/docs/stable/torch.compiler_dynamo_overview.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchDynamo extracts 1 FX graphs\n",
      "graph 0:\n",
      "GraphModule()\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, L_x_ : torch.Tensor, L_y_ : torch.Tensor):\n",
      "    l_x_ = L_x_\n",
      "    l_y_ = L_y_\n",
      "    a = torch.sin(l_x_);  l_x_ = None\n",
      "    sinc = torch.sinc(a)\n",
      "    add = a + sinc;  a = sinc = None\n",
      "    cos = torch.cos(l_y_);  l_y_ = None\n",
      "    add_1 = add + cos;  add = cos = None\n",
      "    return (add_1,)\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subgraph_infos = backend.subgraph_infos\n",
    "print(f\"TorchDynamo extracts {len(subgraph_infos)} FX graphs\")\n",
    "for graph_id, subgraph_info in enumerate(subgraph_infos):\n",
    "    print(f\"graph {graph_id}:\\n{subgraph_info.original_graph_module}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How `ThunderCompiler` Handles FX Graphs\n",
    "\n",
    "The `ThunderCompiler` serves as the backend for torch.compile, processing the FX graph generated by TorchDynamo. If the graph contains regions unsupported by `thunder.jit`, ThunderCompiler splits the FX graph into smaller subgraphs. To achieve this, it leverages the [split module pass](https://github.com/pytorch/pytorch/blob/main/torch/fx/passes/split_module.py) provided by `torch.fx` to customize the rules of how to split the FX graph. ThunderCompiler implements its own [callback function]() to:\n",
    "1. Split the FX graph into supported subgraph that is executed by `thunder.jit`\n",
    "2. Send unsupported subgraphs to alternative execution path -- PyTorch’s Inductor.\n",
    "\n",
    "Some common causes for graph splitting include:\n",
    "1. Unsupported operators: when encounter operators that are not supported by Thunder\n",
    "2. Compilation Errors: when exceptions occur while attempting to compile operators using Thunder. \n",
    "\n",
    "You can inspect the split reasons and review how the FX graph was split by accessing the `TorchCompiler.subgraph_infos` attribute.\n",
    "\n",
    "Note that ThunderCompiler accepts `thunder.jit` options as keyword arguments to customize the compilation of subgraphs executed by Thunder. Similarly, `torch_inductor_options` options can be specified for subgraphs executed by Inductor.\n",
    "\n",
    "In this example, the `sinc` operator is not yet supported by Thunder. As a result, the original FX graph is split into three parts. The first and third part is executed by Thunder and the second part contains the unsupported `sinc` operator and is executed by Inductor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thunder spliter splits the graph into 3 subgraphs, in which 2 subgraphs are run by Thunder\n",
      "The structure of the split graph:\n",
      "\n",
      "GraphModule(\n",
      "  (thunder_0): ThunderModule(\n",
      "    (_model): GraphModule()\n",
      "  )\n",
      "  (inductor_1): OptimizedModule(\n",
      "    (_orig_mod): GraphModule()\n",
      "  )\n",
      "  (thunder_2): ThunderModule(\n",
      "    (_model): GraphModule()\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, l_x_ : torch.Tensor, l_y_ : torch.Tensor):\n",
      "    thunder_0 = self.thunder_0(l_x_);  l_x_ = None\n",
      "    inductor_1 = self.inductor_1(thunder_0)\n",
      "    thunder_2 = self.thunder_2(thunder_0, inductor_1, l_y_);  thunder_0 = inductor_1 = l_y_ = None\n",
      "    return (thunder_2,)\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "graph 0:\n",
      "GraphModule()\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, l_x_ : torch.Tensor):\n",
      "    a = torch.sin(l_x_);  l_x_ = None\n",
      "    return a\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\n",
      "graph 1:\n",
      "GraphModule()\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, a):\n",
      "    sinc = torch.sinc(a);  a = None\n",
      "    return sinc\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\n",
      "graph 2:\n",
      "GraphModule()\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, a, sinc, l_y_ : torch.Tensor):\n",
      "    add = a + sinc;  a = sinc = None\n",
      "    cos = torch.cos(l_y_);  l_y_ = None\n",
      "    add_1 = add + cos;  add = cos = None\n",
      "    return add_1\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subgraph_info = subgraph_infos[0]\n",
    "num_of_submodules = len(subgraph_info.submodule_to_compiled_functions)\n",
    "num_of_thunder_modules = len(subgraph_info.thunder_compiled_fns)\n",
    "print(f\"Thunder spliter splits the graph into {num_of_submodules} subgraphs, in which {num_of_thunder_modules} subgraphs are run by Thunder\")\n",
    "print(\"The structure of the split graph:\\n\")\n",
    "print(subgraph_info.split_graph_module)\n",
    "\n",
    "for subgraph_id, (original_graph, compiled_graph)  in enumerate(subgraph_info.submodule_to_compiled_functions.items()):\n",
    "    print(f\"graph {subgraph_id}:\\n{original_graph}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To inspect why the original graph is split, we can print the split reasons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split reason 0:\n",
      "SplitReason(reason_type=<SplitReasonType.MISSING_OP_SUPPORT: 2>, info='node with name: sinc and target: <built-in method sinc of type object at 0x78d989d84380> only has an automatic torch fallback in thunder.', exception=None)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for reason_id, split_reason in enumerate(subgraph_info.split_reasons):\n",
    "    print(f\"Split reason {reason_id}:\\n{split_reason}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
