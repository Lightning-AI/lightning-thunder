{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2055f09e-ea78-4726-9a48-65115000b140",
   "metadata": {},
   "source": [
    "# Thunder bindings for Liger operators\n",
    "\n",
    "In this notebook we explore Thunder Bindings for Liger Operators.\n",
    "\n",
    "It is based on [Episode 10 of the Thunder Sessions podcast](https://www.youtube.com/watch?v=3H_aw6o-d9c&list=PLaMu-SDt_RB7ImARcTT_Wjypwx2vBIBen&index=10).\n",
    "\n",
    "Let's import things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f4102a8-f68b-4012-bd5b-a1d3daeab367",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Sequence\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.testing import assert_close\n",
    "import litgpt\n",
    "import thunder\n",
    "from thunder.core.proxies import TensorProxy, AnyProxy\n",
    "from thunder.core.transforms import get_grad, put_grads\n",
    "from thunder.torch import TensorLike\n",
    "import thunder.extend\n",
    "\n",
    "import liger_kernel.ops.rms_norm\n",
    "import liger_kernel.ops.rope\n",
    "import liger_kernel.ops.swiglu\n",
    "import liger_kernel.ops.geglu  # TODO\n",
    "import liger_kernel.ops.cross_entropy  # TODO\n",
    "import liger_kernel.ops.fused_linear_cross_entropy\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b44643e-a92c-4398-861f-793cec2e7414",
   "metadata": {},
   "source": [
    "We define and register an executor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5232472-a67c-4650-abf9-370e4692e93d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "thunder.extend.OperatorExecutor('liger')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liger_ex = thunder.extend.OperatorExecutor(\"liger\", version=\"0.1\")\n",
    "thunder.extend.register_executor(liger_ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b207657e-a40c-4cda-a2d6-3f0e11ae4949",
   "metadata": {},
   "source": [
    "## RMS Norm\n",
    "\n",
    "The first thing to fuse is RMS Norm.\n",
    "\n",
    "After that, Liger's implementation is a drop-in replacement. We define operators for forward and backward and then a gradient and execution rule.\n",
    "\n",
    "We register these as an implementation for the rms_norm operand that we divert the PyTorch function to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4411cc5f-5535-48e2-ba7c-00b984f15ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tiny detail here is that PyTorch gained a `rms_norm` function somewhat\n",
    "# recently and we need to tell LitGPT to use it.\n",
    "\n",
    "\n",
    "def RMSNorm_forward(self, x):\n",
    "    return torch.nn.functional.rms_norm(x, self.weight.shape, self.weight, self.eps)\n",
    "\n",
    "\n",
    "litgpt.model.RMSNorm.forward = RMSNorm_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77757535-b292-4a96-a6a3-c0e7f05d70ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "prod = lambda *args: functools.reduce(lambda x, y: x * y, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f542954c-aba1-4523-9a7d-436348a6af96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************************* RMS NORM *******************************\n",
    "import functools\n",
    "\n",
    "\n",
    "def liger_rms_norm_forward_meta(X, W, eps, offset, casting_mode):\n",
    "    *n_rows, n_cols = X.shape\n",
    "    n_rows = prod(*n_rows)\n",
    "    # RSTD is always computed/stored in fp32 if we are using Llama or Gemma casting mode\n",
    "    rstd_dtype = (\n",
    "        thunder.dtypes.float32\n",
    "        if casting_mode\n",
    "        in (liger_kernel.ops.rms_norm._CASTING_MODE_LLAMA.value, liger_kernel.ops.rms_norm._CASTING_MODE_GEMMA.value)\n",
    "        else X.dtype\n",
    "    )\n",
    "    Y = TensorProxy(like=X)\n",
    "    RSTD = TensorProxy(like=X, shape=(n_rows,), dtype=rstd_dtype)\n",
    "    BLOCK_SIZE, num_warps = liger_kernel.ops.rms_norm.calculate_settings(n_cols)\n",
    "    return Y, TensorProxy(like=X, shape=(n_rows, n_cols)), RSTD, BLOCK_SIZE, num_warps, casting_mode\n",
    "\n",
    "\n",
    "liger_rms_norm_forward = liger_ex.register_operator(\n",
    "    \"liger_rms_norm_forward\", meta=liger_rms_norm_forward_meta, fn=liger_kernel.ops.rms_norm.rms_norm_forward\n",
    ")\n",
    "\n",
    "\n",
    "def liger_rms_norm_backward_meta(dY, X, W, RSTD, offset, casting_mode, BLOCK_SIZE, num_warps):\n",
    "    return TensorProxy(like=X), TensorProxy(like=W)\n",
    "\n",
    "\n",
    "liger_rms_norm_backward = liger_ex.register_operator(\n",
    "    \"liger_rms_norm_backward\", meta=liger_rms_norm_backward_meta, fn=liger_kernel.ops.rms_norm.rms_norm_backward\n",
    ")\n",
    "\n",
    "\n",
    "def rms_norm_meta(x, shape, w, eps):\n",
    "    return thunder.TensorProxy(like=x)\n",
    "\n",
    "\n",
    "rms_norm = liger_ex.register_operator(\n",
    "    \"rms_norm\", meta=rms_norm_meta, fn=torch.nn.functional.rms_norm, replaces=torch.nn.functional.rms_norm\n",
    ")\n",
    "\n",
    "\n",
    "def rms_norm_grad_transform(x, shape, weight, eps):\n",
    "    Y, X, RSTD, BLOCK_SIZE, num_warps, casting_mode = liger_rms_norm_forward(\n",
    "        x, weight, eps, offset=0.0, casting_mode=\"llama\"\n",
    "    )\n",
    "    dY = get_grad(Y)\n",
    "    dX, dW = liger_rms_norm_backward(\n",
    "        dY, X, weight, RSTD, offset=0.0, casting_mode=\"llama\", BLOCK_SIZE=BLOCK_SIZE, num_warps=num_warps\n",
    "    )\n",
    "    dX = dX.view(*x.shape)\n",
    "    put_grads((x, weight), (dX, dW))\n",
    "    return Y\n",
    "\n",
    "\n",
    "def rms_norm_execution_transform(x, weight, eps):\n",
    "    Y, *_ = liger_rms_norm_forward(x, weight, eps, offset=0.0, casting_mode=\"llama\")\n",
    "    return Y\n",
    "\n",
    "\n",
    "liger_ex.register_implementation(\n",
    "    rms_norm, execution_transform=rms_norm_execution_transform, grad_transform=rms_norm_grad_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ace1ad2-25f4-4a20-ad39-1f030bca0f38",
   "metadata": {},
   "source": [
    "### Testing RMS Norm\n",
    "\n",
    "Let's test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56f1d6ee-a4ac-42f1-9d65-2c774cda4d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 64\n",
    "\n",
    "example_input = torch.randn(32, 10, hidden_size, device=device, requires_grad=True)\n",
    "\n",
    "with device:\n",
    "    model = litgpt.model.RMSNorm(hidden_size)\n",
    "thunder_model = thunder.jit(model, executors=[liger_ex])\n",
    "ref = model(example_input.clone())\n",
    "res = thunder_model(example_input.clone())\n",
    "go = torch.randn_like(ref)\n",
    "grad_ref, grad_ref_weight = torch.autograd.grad(ref, (example_input, model.weight), go)\n",
    "grad_res, grad_res_weight = torch.autograd.grad(res, (example_input, model.weight), go)\n",
    "\n",
    "\n",
    "assert liger_rms_norm_forward in {bsym.sym for bsym in thunder.last_traces(thunder_model)[-1].bound_symbols}\n",
    "assert liger_rms_norm_backward in {bsym.sym for bsym in thunder.last_backward_traces(thunder_model)[-1].bound_symbols}\n",
    "\n",
    "assert_close(ref, res)\n",
    "assert_close(grad_ref, grad_res)\n",
    "assert_close(grad_ref_weight, grad_res_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dcb262-2255-4c17-b64f-f38e8ebd8e33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71c49c38-ce84-4727-9f57-8b42b908349f",
   "metadata": {},
   "source": [
    "# RoPE\n",
    "\n",
    "Next is the RoPE implementation. Liger does both rope applications to query and key in one kernel whereas\n",
    "LitGPT uses two. So we define not only forward and backward and a symbol to capture the litgpt version,\n",
    "but also a small transform fusing the two `apply_rope` calls to one `liger_rope`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32cd98f0-a36f-4e01-8ae4-6e36adf2699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def liger_rope_forward_meta(q, k, cos, sin):\n",
    "    return TensorProxy(like=q), TensorProxy(like=k), cos, sin\n",
    "\n",
    "\n",
    "liger_rope_forward = liger_ex.register_operator(\n",
    "    \"liger_rope_forward\",\n",
    "    meta=liger_rope_forward_meta,\n",
    "    fn=liger_kernel.ops.rope.rope_forward,\n",
    ")\n",
    "\n",
    "\n",
    "def liger_rope_backward_meta(dq, dk, cos, sin):\n",
    "    return TensorLike(like=dq), TensorLike(like=dk)\n",
    "\n",
    "\n",
    "liger_rope_backward = liger_ex.register_operator(\n",
    "    \"liger_rope_backward\",\n",
    "    meta=liger_rope_backward_meta,\n",
    "    fn=liger_kernel.ops.rope.rope_backward,\n",
    ")\n",
    "\n",
    "\n",
    "def liger_rope_grad_transform(q, k, cos, sin):\n",
    "    q_out, k_out, _, _ = liger_rope_forward(q, k, cos, sin)\n",
    "    q_out_grad = get_grad(q_out)\n",
    "    k_out_grad = get_grad(k_out)\n",
    "    dq, dk = liger_rope_backward(q_out_grad, k_out_grad, cos, sin)\n",
    "    put_grads((q, k), (dq, dk))\n",
    "    return q_out, k_out\n",
    "\n",
    "\n",
    "def liger_rope_execution_transform(q, k, cos, sin):\n",
    "    q_out, k_out, _, _ = liger_rope_forward(q, k, cos, sin)\n",
    "    return q_out, k_out\n",
    "\n",
    "\n",
    "def liger_rope_impl(q, k, cos, sin):\n",
    "    qr, kr, _, _ = liger_rope_forward(q, k, cos, sin)\n",
    "    return qr, kr\n",
    "\n",
    "\n",
    "liger_rope = liger_ex.register_operator(\"liger_rope\", fn=liger_rope_impl, like=liger_rope_impl)\n",
    "\n",
    "liger_ex.register_implementation(\n",
    "    liger_rope,\n",
    "    execution_transform=liger_rope_execution_transform,\n",
    "    grad_transform=liger_rope_grad_transform,\n",
    ")\n",
    "\n",
    "\n",
    "def litgpt_apply_rope_meta(x, cos, sin):\n",
    "    return TensorProxy(like=x)\n",
    "\n",
    "\n",
    "litgpt_apply_rope = liger_ex.register_operator(\n",
    "    \"litgpt_apply_rope\", fn=litgpt.model.apply_rope, meta=litgpt_apply_rope_meta, replaces=litgpt.model.apply_rope\n",
    ")\n",
    "\n",
    "\n",
    "class MergeRopeTransform(thunder.core.transform_common.Transform):\n",
    "    def transform_traces_pre_prologue(self, prologue_trace, compute_trace, epilogue_trace, **kwargs):\n",
    "        new_compute_trace = thunder.core.trace.from_trace(compute_trace)\n",
    "        bound_symbols = compute_trace.bound_symbols[:]\n",
    "        while bound_symbols:\n",
    "            bsym = bound_symbols.pop(0)\n",
    "            if bsym.sym == litgpt_apply_rope:\n",
    "                for i, bsym2 in enumerate(bound_symbols):\n",
    "                    assert not any(o is bsym.output for o in bsym2.flat_outs)\n",
    "                    if bsym2.sym == litgpt_apply_rope:\n",
    "                        break\n",
    "                bsym2 = bound_symbols.pop(i)\n",
    "                assert bsym2.sym == litgpt_apply_rope\n",
    "\n",
    "                output = (bsym.output, bsym2.output)\n",
    "                args = (bsym.args[0], bsym2.args[0], *bsym.args[1:])\n",
    "\n",
    "                new_compute_trace.bound_symbols.append(bsym.from_bsym(args=args, output=output, sym=liger_rope))\n",
    "            else:\n",
    "                new_compute_trace.bound_symbols.append(bsym.from_bsym())\n",
    "        new_compute_trace.set_provenance(thunder.core.trace.TraceProvenance(self.__class__))\n",
    "        return prologue_trace, new_compute_trace, epilogue_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44187b29-c101-41f0-a811-4c9f29757c81",
   "metadata": {},
   "source": [
    "# Test\n",
    "\n",
    "We test with a scaled-down Llama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8fd2563-7b89-487d-8fdc-21661380e2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = litgpt.Config.from_name(\"Llama-3.2-1B\", n_layer=1)\n",
    "with device:\n",
    "    m = litgpt.GPT(cfg)\n",
    "    m.max_seq_length = 1024\n",
    "    m.set_kv_cache(1)\n",
    "    inp = torch.arange(1, 6, dtype=torch.int64)[None]\n",
    "    inp_pos = torch.arange(1, 6, dtype=torch.int64)\n",
    "\n",
    "\n",
    "jm = thunder.jit(m, executors=(liger_ex,), transforms=(MergeRopeTransform(),))\n",
    "res = jm(inp, inp_pos)\n",
    "ref = m(inp, inp_pos)\n",
    "\n",
    "go = torch.randn_like(res)\n",
    "(grad_res,) = torch.autograd.grad(res, jm.get_parameter(\"transformer.wte.weight\"), go)\n",
    "(grad_ref,) = torch.autograd.grad(ref, m.get_parameter(\"transformer.wte.weight\"), go)\n",
    "\n",
    "assert_close(res, ref)\n",
    "assert_close(grad_res, grad_ref)\n",
    "\n",
    "assert any(bsym.sym is liger_rope_forward for bsym in thunder.last_traces(jm)[-1].bound_symbols)\n",
    "assert any(bsym.sym is liger_rope_backward for bsym in thunder.last_backward_traces(jm)[-1].bound_symbols)\n",
    "assert any(bsym.sym is liger_rms_norm_forward for bsym in thunder.last_traces(jm)[-1].bound_symbols)\n",
    "assert any(bsym.sym is liger_rms_norm_backward for bsym in thunder.last_backward_traces(jm)[-1].bound_symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e341460b-71d4-4e83-b67c-14bdec7d8026",
   "metadata": {},
   "source": [
    "## SwiGLU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e26e9ec-3eb0-46e8-90d1-8cd84d5dd1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def liger_swiglu_forward_meta(a, b):\n",
    "    return TensorProxy(like=a)\n",
    "\n",
    "\n",
    "def liger_swiglu_forward_impl(a, b):\n",
    "    _, _, res = liger_kernel.ops.swiglu.swiglu_forward(a, b)\n",
    "    return res\n",
    "\n",
    "\n",
    "liger_swiglu_forward = liger_ex.register_operator(\n",
    "    \"liger_swiglu_forward\",\n",
    "    meta=liger_swiglu_forward_meta,\n",
    "    fn=liger_swiglu_forward_impl,\n",
    ")\n",
    "\n",
    "\n",
    "def liger_swiglu_backward_meta(a, b, grad_res):\n",
    "    return TensorProxy(like=a), TensorProxy(like=b)\n",
    "\n",
    "\n",
    "liger_swiglu_backward = liger_ex.register_operator(\n",
    "    \"liger_swiglu_backward\",\n",
    "    meta=liger_swiglu_backward_meta,\n",
    "    fn=liger_kernel.ops.swiglu.swiglu_backward,\n",
    ")\n",
    "\n",
    "\n",
    "def liger_swiglu_gradient_transform(a, b):\n",
    "    res = liger_swiglu_forward(a, b)\n",
    "    grad_res = get_grad(res)\n",
    "    grad_a, grad_b = liger_swiglu_backward(a, b, grad_res)\n",
    "    put_grads((a, b), (grad_a, grad_b))\n",
    "    return res\n",
    "\n",
    "\n",
    "liger_ex.register_implementation(\n",
    "    liger_swiglu_forward, grad_transform=liger_swiglu_gradient_transform, execution_transform=liger_swiglu_forward\n",
    ")\n",
    "\n",
    "\n",
    "class FuseSwigLUTransform(thunder.core.transform_common.Transform):\n",
    "    def transform_traces_pre_prologue(self, prologue_trace, computation_trace, epilogue_trace, **kwargs):\n",
    "        _, consumers = thunder.core.utils.producers_and_consumers(computation_trace)\n",
    "        new_computation_trace = thunder.core.trace.from_trace(computation_trace)\n",
    "        bsyms_to_skip = set()\n",
    "        for b in computation_trace.bound_symbols:\n",
    "            if b in bsyms_to_skip:\n",
    "                continue\n",
    "            new_bsym = b\n",
    "            if b.sym == thunder.torch.silu:\n",
    "                c = consumers[b.output]\n",
    "                if len(c) == 1 and c[0].sym == thunder.torch.mul:\n",
    "                    (mul,) = c\n",
    "                    mul_l, mul_r = mul.args\n",
    "                    if mul_l is b.output:\n",
    "                        other = mul_r\n",
    "                    else:\n",
    "                        other = mul_l\n",
    "                    new_bsym = b.from_bsym(\n",
    "                        sym=liger_swiglu_forward, output=mul.output, args=(b.args[0], other), subsymbols=[]\n",
    "                    )\n",
    "                    bsyms_to_skip.add(mul)\n",
    "            new_computation_trace.bound_symbols.append(new_bsym)\n",
    "        new_computation_trace.set_provenance(thunder.core.trace.TraceProvenance(\"constructed by FuseSwigLU\"))\n",
    "        return prologue_trace, new_computation_trace, epilogue_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c004b1f6-9756-44ae-88f3-d088f0c838c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efb5dcb5-a411-4832-9cac-a868dc3142b0",
   "metadata": {},
   "source": [
    "## Fused Linear and Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55ff0b33-99ec-4de1-a3c0-7a78ebdf83c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def liger_fused_linear_cross_entropy_forward_meta(\n",
    "    _input, weight, target, bias=None, ignore_index=-100, label_smoothing=0.0, reduction=\"mean\"\n",
    "):\n",
    "    logits = thunder.torch.linear(_input, weight, bias)\n",
    "    loss = thunder.torch.cross_entropy(\n",
    "        logits, target, ignore_index=ignore_index, label_smoothing=label_smoothing, reduction=reduction\n",
    "    )\n",
    "    grad_input = TensorProxy(like=_input)\n",
    "    grad_weight = TensorProxy(like=weight)\n",
    "    grad_bias = None if bias is None else TensorProxy(like=bias)\n",
    "    return loss, grad_input, grad_weight, grad_bias\n",
    "\n",
    "\n",
    "liger_fused_linear_cross_entropy_forward = liger_ex.register_operator(\n",
    "    \"liger_fused_linear_cross_entropy_forward\",\n",
    "    fn=liger_kernel.ops.fused_linear_cross_entropy.fused_linear_cross_entropy_forward,\n",
    "    like=liger_fused_linear_cross_entropy_forward_meta,\n",
    ")\n",
    "\n",
    "\n",
    "def liger_fused_linear_cross_entropy_backward_meta(grad_output, grad_input, grad_weight, grad_bias):\n",
    "    return (\n",
    "        TensorProxy(like=grad_input),\n",
    "        TensorProxy(like=grad_weight),\n",
    "        (TensorProxy(like=grad_bias) if grad_bias is not None else None),\n",
    "    )\n",
    "\n",
    "\n",
    "liger_fused_linear_cross_entropy_backward = liger_ex.register_operator(\n",
    "    \"liger_fused_linear_cross_entropy_backward\",\n",
    "    fn=liger_kernel.ops.fused_linear_cross_entropy.fused_linear_cross_entropy_backward,\n",
    "    meta=liger_fused_linear_cross_entropy_backward_meta,\n",
    ")\n",
    "\n",
    "\n",
    "def liger_fused_linear_cross_entropy_grad_transform(\n",
    "    _input, weight, target, bias=None, ignore_index=-100, label_smoothing=0.0, reduction=\"mean\"\n",
    "):\n",
    "    loss, grad_input_1, grad_weight_1, grad_bias_1 = liger_fused_linear_cross_entropy_forward(\n",
    "        _input,\n",
    "        weight,\n",
    "        target,\n",
    "        bias=bias,\n",
    "        ignore_index=ignore_index,\n",
    "        label_smoothing=label_smoothing,\n",
    "        reduction=reduction,\n",
    "    )\n",
    "    grad_loss = get_grad(loss)\n",
    "    grad_input, grad_weight, grad_bias = liger_fused_linear_cross_entropy_backward(\n",
    "        grad_loss, grad_input_1, grad_weight_1, grad_bias_1\n",
    "    )\n",
    "    put_grads((_input, weight, target), (grad_input, grad_weight, grad_bias))\n",
    "    return loss\n",
    "\n",
    "\n",
    "liger_ex.register_implementation(\n",
    "    liger_fused_linear_cross_entropy_forward,\n",
    "    grad_transform=liger_fused_linear_cross_entropy_grad_transform,\n",
    "    execution_transform=liger_fused_linear_cross_entropy_forward,\n",
    ")\n",
    "\n",
    "\n",
    "class FuseLinearCrossEntropyTransform(thunder.core.transform_common.Transform):\n",
    "    def transform_traces_pre_prologue(self, prologue_trace, computation_trace, epilogue_trace, **kwargs):\n",
    "        _, consumers = thunder.core.utils.producers_and_consumers(computation_trace)\n",
    "        new_computation_trace = thunder.core.trace.from_trace(computation_trace)\n",
    "        bsyms_to_skip = set()\n",
    "        for b in computation_trace.bound_symbols:\n",
    "            if b in bsyms_to_skip:\n",
    "                continue\n",
    "            new_bsym = b\n",
    "            if b.sym == thunder.torch.linear:\n",
    "                c = consumers[b.output]\n",
    "                if len(c) == 1 and c[0].sym == thunder.torch.cross_entropy:\n",
    "                    (ce,) = c\n",
    "                    assert not ce.kwargs\n",
    "                    assert not b.kwargs\n",
    "                    assert ce.args[0] is b.output\n",
    "                    inp, weight, bias = b.args\n",
    "                    _, targets, ce_weight, size_average, ignore_index, reduce, reduction, label_smoothing = ce.args\n",
    "                    assert ce_weight is None\n",
    "                    assert size_average is None\n",
    "                    assert reduce is None\n",
    "                    new_bsym = b.from_bsym(\n",
    "                        sym=liger_fused_linear_cross_entropy_forward,\n",
    "                        output=ce.output,\n",
    "                        args=(inp, weight, targets, bias, ignore_index, label_smoothing, reduction),\n",
    "                        subsymbols=[],\n",
    "                    )\n",
    "                    bsyms_to_skip.add(ce)\n",
    "            new_computation_trace.bound_symbols.append(new_bsym)\n",
    "        new_computation_trace.set_provenance(\n",
    "            thunder.core.trace.TraceProvenance(\"constructed by FuseLinearCrossEntropy\")\n",
    "        )\n",
    "        return prologue_trace, new_computation_trace, epilogue_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89431922-f074-4825-a6b6-7365abe5b0b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def apply_eye_meta(x):\n",
    "    return thunder.TensorProxy(like=x)\n",
    "\n",
    "\n",
    "def apply_eye(mask):\n",
    "    mask = mask | torch.eye(mask.shape[-1], dtype=torch.bool, device=mask.device)[None, None]\n",
    "    return mask\n",
    "\n",
    "\n",
    "t_apply_eye = liger_ex.register_operator(\"t_apply_eye\", fn=apply_eye, meta=apply_eye_meta, replaces=apply_eye)\n",
    "\n",
    "\n",
    "def apply_eye_grad_transform(x):\n",
    "    return t_apply_eye(x)\n",
    "\n",
    "\n",
    "liger_ex.register_implementation(\n",
    "    t_apply_eye, execution_transform=apply_eye_grad_transform, grad_transform=apply_eye_grad_transform\n",
    ")\n",
    "\n",
    "\n",
    "class GPTForFineTuningLastToken(litgpt.model.GPT):\n",
    "    def forward(self, idx: torch.Tensor, *, mask: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
    "        mask = mask.bool()\n",
    "        T = idx.size(1)\n",
    "        if self.max_seq_length < T:\n",
    "            raise ValueError(f\"Cannot forward sequence of length {T}, max seq length is only {self.max_seq_length}.\")\n",
    "\n",
    "        attn_mask = (\n",
    "            litgpt.model.build_mask_cache(mask.shape[-1], mask.device).expand(4, -1, -1, -1) * mask[:, None, None, :]\n",
    "        )\n",
    "        attn_mask = apply_eye(attn_mask)\n",
    "\n",
    "        cos = self.cos[:T]\n",
    "        sin = self.sin[:T]\n",
    "        x = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)\n",
    "        if self.config.scale_embeddings:\n",
    "            x = x * torch.tensor(self.config.n_embd**0.5, dtype=x.dtype)\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x, cos, sin, attn_mask, None)\n",
    "\n",
    "        # second to last prediction is the output\n",
    "        x = x[:, -2]\n",
    "        x = self.transformer.ln_f(x)\n",
    "        x = self.lm_head(x)  # (b, t, vocab_size)\n",
    "        if self.config.final_logit_softcapping is not None:\n",
    "            x = torch.tanh(x / self.config.final_logit_softcapping) * self.config.final_logit_softcapping\n",
    "        loss = torch.nn.functional.cross_entropy(x, labels)\n",
    "        return loss\n",
    "\n",
    "\n",
    "cfg = litgpt.Config.from_name(\"Llama-3.2-1B\", n_layer=1)\n",
    "with device:\n",
    "    m = GPTForFineTuningLastToken(cfg)\n",
    "    m.max_seq_length = 1024\n",
    "    inp = torch.ones(4, 32, dtype=torch.int64)\n",
    "    mask = torch.ones(4, 32, dtype=torch.int64)\n",
    "    labels = torch.ones(4, dtype=torch.int64)\n",
    "\n",
    "\n",
    "jm = thunder.jit(\n",
    "    m,\n",
    "    executors=(liger_ex,),\n",
    "    transforms=(\n",
    "        MergeRopeTransform(),\n",
    "        FuseSwigLUTransform(),\n",
    "        FuseLinearCrossEntropyTransform(),\n",
    "    ),\n",
    ")\n",
    "res = jm(inp, mask=mask, labels=labels)\n",
    "ref = m(inp, mask=mask, labels=labels)\n",
    "\n",
    "go = torch.randn_like(res)\n",
    "(grad_res,) = torch.autograd.grad(res, jm.get_parameter(\"transformer.wte.weight\"), go)\n",
    "(grad_ref,) = torch.autograd.grad(ref, m.get_parameter(\"transformer.wte.weight\"), go)\n",
    "\n",
    "assert_close(res, ref)\n",
    "assert_close(grad_res, grad_ref)\n",
    "\n",
    "assert any(bsym.sym is liger_rope_forward for bsym in thunder.last_traces(jm)[-1].bound_symbols)\n",
    "assert any(bsym.sym is liger_rope_backward for bsym in thunder.last_backward_traces(jm)[-1].bound_symbols)\n",
    "assert any(bsym.sym is liger_rms_norm_forward for bsym in thunder.last_traces(jm)[-1].bound_symbols)\n",
    "assert any(bsym.sym is liger_rms_norm_backward for bsym in thunder.last_backward_traces(jm)[-1].bound_symbols)\n",
    "assert any(bsym.sym is liger_swiglu_forward for bsym in thunder.last_traces(jm)[-1].bound_symbols)\n",
    "assert any(bsym.sym is liger_swiglu_backward for bsym in thunder.last_backward_traces(jm)[-1].bound_symbols)\n",
    "assert any(bsym.sym is liger_fused_linear_cross_entropy_forward for bsym in thunder.last_traces(jm)[-1].bound_symbols)\n",
    "assert any(\n",
    "    bsym.sym is liger_fused_linear_cross_entropy_backward for bsym in thunder.last_backward_traces(jm)[-1].bound_symbols\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2812d18c-c6a6-4d02-ba12-7a8002efc0e5",
   "metadata": {},
   "source": [
    "# End to end example\n",
    "\n",
    "adapted from a [Liger-Kernel example](https://github.com/linkedin/Liger-Kernel/blob/de12602d858a6e83aaacc56e5cb64ab218c75a0a/examples/lightning/training.py).\n",
    "\n",
    "Code below is\n",
    "\n",
    "Copyright 2024 LinkedIn Corporation ([BSD 2-CLAUSE LICENSE](https://github.com/linkedin/Liger-Kernel/blob/de12602d858a6e83aaacc56e5cb64ab218c75a0a/LICENSE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "420850af-878a-415a-bacd-a0d3258a0cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import math\n",
    "import os\n",
    "from dataclasses import _MISSING_TYPE, dataclass\n",
    "import litgpt\n",
    "\n",
    "import datasets\n",
    "import lightning.pytorch as pl\n",
    "import torch\n",
    "import transformers\n",
    "from lightning.pytorch.strategies import DeepSpeedStrategy, FSDPStrategy\n",
    "from torch.distributed.fsdp import BackwardPrefetch, MixedPrecision\n",
    "from torch.utils.data import DataLoader\n",
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "_RETAIN_COLUMNS = {\"input_ids\", \"attention_mask\", \"labels\"}\n",
    "QUESTION = \"<Question>\"\n",
    "CHOICES = \"<Choices>\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    model: str = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "    data: str = \"cais/mmlu\"\n",
    "    output_dir: str = \"mmlu_finetuning\"\n",
    "    max_length: int = 2048\n",
    "    # for llam3 8B model, deepspeed will OOM with 16 on 8XA100 80G and 8 will OOM with 8XA100 40G\n",
    "    batch_size: int = 4\n",
    "    lr: float = 6e-6\n",
    "    weight_decay: float = 0.05\n",
    "    warmup_ratio: float = 0.1\n",
    "    seed: int = 42\n",
    "    strategy: str = \"auto\"\n",
    "    num_gpu: int = 1\n",
    "\n",
    "\n",
    "def warmup_cosine_schedule(warmup_steps, total_steps, min_lr=0):\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < warmup_steps:\n",
    "            # Linear warmup\n",
    "            return float(current_step) / float(max(1, warmup_steps))\n",
    "        else:\n",
    "            # Cosine annealing\n",
    "            progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "            return max(min_lr, 0.5 * (1 + math.cos(math.pi * progress)))\n",
    "\n",
    "    return lr_lambda\n",
    "\n",
    "\n",
    "def parse_args() -> Args:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    for k, v in Args.__dataclass_fields__.items():\n",
    "        parser.add_argument(f\"--{k}\", type=v.type, default=v.default)\n",
    "    parsed = parser.parse_args([])\n",
    "    return Args(**{k: v for k, v in vars(parsed).items() if not isinstance(v, _MISSING_TYPE)})\n",
    "\n",
    "\n",
    "class LanguageModel(pl.LightningModule):\n",
    "    def __init__(self, args: Args, tokenizer):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = None\n",
    "\n",
    "    def configure_model(self):\n",
    "        # https://lightning.ai/docs/pytorch/stable/advanced/model_parallel/fsdp.html#speed-up-model-initialization\n",
    "        if self.model is not None:\n",
    "            return\n",
    "        self.model = GPTForFineTuningLastToken.from_name(self.args.model.rsplit(\"/\", 1)[-1]).to(torch.bfloat16)\n",
    "        self.model.load_state_dict(litgpt.utils.lazy_load(f\"checkpoints/{self.args.model}/lit_model.pth\"))\n",
    "        self.model = thunder.jit(\n",
    "            self.model,\n",
    "            executors=(liger_ex, *thunder.get_default_executors()),\n",
    "            transforms=(MergeRopeTransform(), FuseSwigLUTransform(), FuseLinearCrossEntropyTransform()),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None, **kwargs):\n",
    "        return self.model(idx=input_ids, mask=attention_mask, labels=labels, **kwargs)\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        outputs = self.model(\n",
    "            idx=batch[\"input_ids\"],\n",
    "            mask=batch[\"attention_mask\"],\n",
    "            labels=batch[\"labels\"][:, -1],\n",
    "        )\n",
    "        loss = outputs\n",
    "        self.log_dict(\n",
    "            {\"train_loss\": loss},\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "            rank_zero_only=True,\n",
    "            sync_dist=False,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        outputs = self.model(\n",
    "            idx=batch[\"input_ids\"],\n",
    "            mask=batch[\"attention_mask\"],\n",
    "            labels=batch[\"labels\"][:, -1],\n",
    "        )\n",
    "        loss = outputs\n",
    "        self.log_dict(\n",
    "            {\"val_loss\": loss},\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "            rank_zero_only=True,\n",
    "            sync_dist=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.args.lr,\n",
    "            weight_decay=self.args.weight_decay,\n",
    "            fused=True,\n",
    "        )\n",
    "        lr_lambda = warmup_cosine_schedule(\n",
    "            warmup_steps=self.trainer.estimated_stepping_batches * self.args.warmup_ratio,\n",
    "            total_steps=self.trainer.estimated_stepping_batches,\n",
    "            min_lr=0,\n",
    "        )\n",
    "        lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\"scheduler\": lr_scheduler, \"interval\": \"step\"},\n",
    "        }\n",
    "\n",
    "\n",
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, tokenizer, args: Args):\n",
    "        super().__init__()\n",
    "        self.train_dataset = None\n",
    "        self.args = args\n",
    "        self.tokenizer = tokenizer\n",
    "        self.response_template_str = \" <Answer>\"\n",
    "        response_prompt = tokenizer.encode(f\"{self.response_template_str}\", add_special_tokens=False)\n",
    "        self.collator = DataCollatorForCompletionOnlyLM(\n",
    "            tokenizer=tokenizer,\n",
    "            response_template=response_prompt,\n",
    "            pad_to_multiple_of=16,\n",
    "        )\n",
    "\n",
    "    def formatting_func(self, example):\n",
    "        output_texts = []\n",
    "        for i in range(len(example[\"question\"])):\n",
    "            choices = \"\"\n",
    "            for j in range(len(example[\"choices\"][i])):\n",
    "                choices += f\"{j+1}. {example['choices'][i][j]}; \"\n",
    "            s = \"Below is a question and multiple choice answers, choices separated by a semicolon. Please select the best answer for the question. \"\n",
    "            s += f\"{QUESTION}{example['question'][i]} \"\n",
    "            s += f\"{CHOICES}{choices} \"\n",
    "            s += f\"{self.response_template_str}{example['answer'][i]}\"\n",
    "            output_texts.append(s)\n",
    "        return output_texts\n",
    "\n",
    "    def tokenize(self, example):\n",
    "        outputs = self.tokenizer(\n",
    "            self.formatting_func(example),\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            max_length=self.args.max_length,\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": outputs[\"input_ids\"],\n",
    "            \"attention_mask\": outputs[\"attention_mask\"],\n",
    "        }\n",
    "\n",
    "    def setup(self, stage) -> None:\n",
    "        if self.train_dataset is not None:\n",
    "            return\n",
    "        dataset = datasets.load_dataset(self.args.data, \"auxiliary_train\")\n",
    "        flattened_data = [\n",
    "            {\n",
    "                \"answer\": x[\"train\"][\"answer\"],\n",
    "                \"choices\": x[\"train\"][\"choices\"],\n",
    "                \"question\": x[\"train\"][\"question\"],\n",
    "                \"subject\": x[\"train\"][\"subject\"],\n",
    "            }\n",
    "            for x in dataset[\"train\"]\n",
    "        ][:32]\n",
    "        dataset = datasets.Dataset.from_list(flattened_data)\n",
    "        dataset = dataset.train_test_split(test_size=4, seed=self.args.seed)\n",
    "        train_dataset, val_dataset = dataset[\"train\"], dataset[\"test\"]\n",
    "        self.train_dataset = train_dataset.map(\n",
    "            self.tokenize,\n",
    "            remove_columns=list(set(train_dataset.column_names) - _RETAIN_COLUMNS),\n",
    "            batched=True,\n",
    "            batch_size=1,\n",
    "            num_proc=4,\n",
    "        )\n",
    "        self.val_dataset = val_dataset.map(\n",
    "            self.tokenize,\n",
    "            remove_columns=list(set(val_dataset.column_names) - _RETAIN_COLUMNS),\n",
    "            batched=True,\n",
    "            batch_size=1,\n",
    "            num_proc=4,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.args.batch_size,\n",
    "            collate_fn=self.collator,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.args.batch_size,\n",
    "            collate_fn=self.collator,\n",
    "        )\n",
    "\n",
    "\n",
    "args = parse_args()\n",
    "pl.seed_everything(args.seed)\n",
    "os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "if args.strategy == \"fsdp\":\n",
    "    strategy = FSDPStrategy(\n",
    "        auto_wrap_policy=layers,\n",
    "        sharding_strategy=\"FULL_SHARD\",\n",
    "        backward_prefetch=BackwardPrefetch.BACKWARD_PRE,\n",
    "        sync_module_states=True,\n",
    "        activation_checkpointing_policy=layers,\n",
    "        mixed_precision=MixedPrecision(param_dtype=torch.bfloat16, reduce_dtype=torch.bfloat16),\n",
    "        forward_prefetch=True,\n",
    "    )\n",
    "    precision = None\n",
    "elif args.strategy == \"deepspeed\":\n",
    "    strategy = DeepSpeedStrategy(stage=3)\n",
    "    precision = \"bf16-mixed\"\n",
    "elif args.strategy == \"ddp\":\n",
    "    strategy = \"ddp\"\n",
    "    precision = \"bf16-true\"\n",
    "else:\n",
    "    strategy = \"auto\"\n",
    "    precision = \"bf16-true\"\n",
    "\n",
    "if False:\n",
    "    # This only works if you have a snapshot to work from.\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=\"cuda\",\n",
    "        strategy=strategy,\n",
    "        devices=torch.cuda.device_count() if args.num_gpu is None else args.num_gpu,\n",
    "        default_root_dir=args.output_dir,\n",
    "        log_every_n_steps=1,\n",
    "        max_epochs=1,\n",
    "        precision=precision,\n",
    "    )\n",
    "\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(args.model, padding_side=\"left\", truncation_side=\"left\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    data_module = DataModule(\n",
    "        tokenizer=tokenizer,\n",
    "        args=args,\n",
    "    )\n",
    "\n",
    "    model = LanguageModel(args=args, tokenizer=tokenizer)\n",
    "    trainer.fit(model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434dbcf7-1ed3-4669-a90b-12044909be44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
