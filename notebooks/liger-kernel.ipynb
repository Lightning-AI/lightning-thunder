{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2055f09e-ea78-4726-9a48-65115000b140",
   "metadata": {},
   "source": [
    "# Thunder bindings for Liger operators\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "162c9a69-e19b-47fd-a0f1-ef475ac5d631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "thunder.extend.OperatorExecutor('liger')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections.abc import Sequence\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.testing import assert_close\n",
    "import litgpt\n",
    "import thunder\n",
    "from thunder.core.proxies import TensorProxy, AnyProxy\n",
    "from thunder.core.transforms import get_grad, put_grads\n",
    "from thunder.torch import TensorLike\n",
    "from thunder.executors.utils import Context, set_saved_tensors\n",
    "from thunder.core.compile_data import get_compile_option\n",
    "import thunder.extend\n",
    "\n",
    "import liger_kernel.ops.rms_norm\n",
    "import liger_kernel.ops.rope\n",
    "import liger_kernel.ops.swiglu\n",
    "import liger_kernel.ops.geglu\n",
    "import liger_kernel.ops.cross_entropy\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "liger_ex = thunder.extend.OperatorExecutor(\"liger\", version=\"0.1\")\n",
    "thunder.extend.register_executor(liger_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7956941-d539-462d-8b4b-ee18c24dc719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b207657e-a40c-4cda-a2d6-3f0e11ae4949",
   "metadata": {},
   "source": [
    "## RMS Norm\n",
    "\n",
    "The first thing to fuse is RMS Norm.\n",
    "\n",
    "After that, Liger's implementation is a drop-in replacement. We define operators for forward and backward and then a gradient and execution rule.\n",
    "\n",
    "We register these as an implementation for the rms_norm operand that we divert the PyTorch function to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4411cc5f-5535-48e2-ba7c-00b984f15ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tiny detail here is that PyTorch gained a `rms_norm` function somewhat\n",
    "# recently and we need to tell LitGPT to use it.\n",
    "\n",
    "def RMSNorm_forward(self, x):\n",
    "    return torch.nn.functional.rms_norm(x, self.weight.shape, self.weight, self.eps)\n",
    "litgpt.model.RMSNorm.forward = RMSNorm_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77757535-b292-4a96-a6a3-c0e7f05d70ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "prod = lambda *args: functools.reduce(lambda x, y: x * y, args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f542954c-aba1-4523-9a7d-436348a6af96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************************* RMS NORM *******************************   \n",
    "import functools\n",
    "def liger_rms_norm_forward_meta(X, W, eps, offset, casting_mode):\n",
    "    *n_rows, n_cols = X.shape\n",
    "    n_rows = prod(*n_rows)\n",
    "    # RSTD is always computed/stored in fp32 if we are using Llama or Gemma casting mode\n",
    "    rstd_dtype = (\n",
    "        thunder.dtypes.float32\n",
    "        if casting_mode in (liger_kernel.ops.rms_norm._CASTING_MODE_LLAMA.value,\n",
    "                            liger_kernel.ops.rms_norm._CASTING_MODE_GEMMA.value)\n",
    "        else X.dtype\n",
    "    )\n",
    "    Y = TensorProxy(like=X)\n",
    "    RSTD = TensorProxy(like=X, shape=(n_rows,), dtype=rstd_dtype)\n",
    "    BLOCK_SIZE, num_warps = liger_kernel.ops.rms_norm.calculate_settings(n_cols)\n",
    "    return Y, TensorProxy(like=X, shape=(n_rows, n_cols)), RSTD, BLOCK_SIZE, num_warps, casting_mode \n",
    "\n",
    "liger_rms_norm_forward = liger_ex.register_operator(\n",
    "    \"liger_rms_norm_forward\",\n",
    "    meta=liger_rms_norm_forward_meta,\n",
    "    fn=liger_kernel.ops.rms_norm.rms_norm_forward\n",
    ")\n",
    "\n",
    "def liger_rms_norm_backward_meta(dY, X, W, RSTD, offset, casting_mode, BLOCK_SIZE, num_warps):\n",
    "    return TensorProxy(like=X), TensorProxy(like=W)\n",
    "\n",
    "liger_rms_norm_backward = liger_ex.register_operator(\n",
    "    \"liger_rms_norm_backward\",\n",
    "    meta=liger_rms_norm_backward_meta,\n",
    "    fn=liger_kernel.ops.rms_norm.rms_norm_backward\n",
    ")\n",
    "\n",
    "def rms_norm_meta(x, shape, w, eps):\n",
    "    return thunder.TensorProxy(like=x)\n",
    "\n",
    "rms_norm = liger_ex.register_operator('rms_norm', meta=rms_norm_meta, fn=torch.nn.functional.rms_norm, replaces=torch.nn.functional.rms_norm)\n",
    "\n",
    "def rms_norm_grad_transform(x, shape, weight, eps):\n",
    "    Y, X, RSTD, BLOCK_SIZE, num_warps, casting_mode = liger_rms_norm_forward(x, weight, eps, offset=0.0, casting_mode=\"llama\")\n",
    "    dY = get_grad(Y)\n",
    "    dX, dW = liger_rms_norm_backward(dY, X, weight, RSTD, offset=0.0, casting_mode=\"llama\", BLOCK_SIZE=BLOCK_SIZE, num_warps=num_warps)\n",
    "    dX = dX.view(*x.shape)\n",
    "    put_grads((x, weight), (dX, dW))\n",
    "    return Y\n",
    "\n",
    "def rms_norm_execution_transform(x, weight, eps):\n",
    "    Y, *_ = liger_rms_norm_forward(x, weight, eps, offset=0.0, casting_mode=\"llama\")\n",
    "    return Y\n",
    "\n",
    "liger_ex.register_implementation(\n",
    "    rms_norm,\n",
    "    execution_transform=rms_norm_execution_transform,\n",
    "    grad_transform=rms_norm_grad_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ace1ad2-25f4-4a20-ad39-1f030bca0f38",
   "metadata": {},
   "source": [
    "### Testing RMS Norm\n",
    "\n",
    "Let's test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c85dfdb0-b90a-4ee3-948d-2392eb6b010d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 64\n",
    "\n",
    "example_input = torch.randn(32, 10, hidden_size, device=device, requires_grad=True)\n",
    "\n",
    "with device:\n",
    "    model = litgpt.model.RMSNorm(hidden_size)\n",
    "thunder_model = thunder.jit(model, executors=[liger_ex])\n",
    "ref = model(example_input.clone())\n",
    "res = thunder_model(example_input.clone())\n",
    "go = torch.randn_like(ref)\n",
    "grad_ref, grad_ref_weight = torch.autograd.grad(ref, (example_input, model.weight), go)\n",
    "grad_res, grad_res_weight = torch.autograd.grad(res, (example_input, model.weight), go)\n",
    "\n",
    "\n",
    "assert liger_rms_norm_forward in {bsym.sym for bsym in thunder.last_traces(thunder_model)[-1].bound_symbols}\n",
    "assert liger_rms_norm_backward in {bsym.sym for bsym in thunder.last_backward_traces(thunder_model)[-1].bound_symbols}\n",
    "\n",
    "assert_close(ref, res)\n",
    "assert_close(grad_ref, grad_res)\n",
    "assert_close(grad_ref_weight, grad_res_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c49c38-ce84-4727-9f57-8b42b908349f",
   "metadata": {},
   "source": [
    "# RoPE\n",
    "\n",
    "Next is the RoPE implementation. Liger does both rope applications to query and key in one kernel whereas\n",
    "LitGPT uses two. So we define not only forward and backward and a symbol to capture the litgpt version,\n",
    "but also a small transform fusing the two `apply_rope` calls to one `liger_rope`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32cd98f0-a36f-4e01-8ae4-6e36adf2699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def liger_rope_forward_meta(q, k, cos, sin):\n",
    "    return TensorProxy(like=q), TensorProxy(like=k), cos, sin\n",
    "\n",
    "liger_rope_forward = liger_ex.register_operator(\n",
    "    \"liger_rope_forward\",\n",
    "    meta=liger_rope_forward_meta,\n",
    "    fn=liger_kernel.ops.rope.rope_forward,\n",
    ")\n",
    "\n",
    "def liger_rope_backward_meta(dq, dk, cos, sin):\n",
    "    return TensorLike(like=dq), TensorLike(like=dk)\n",
    "\n",
    "liger_rope_backward = liger_ex.register_operator(\n",
    "    \"liger_rope_backward\",\n",
    "    meta=liger_rope_backward_meta,\n",
    "    fn=liger_kernel.ops.rope.rope_backward,\n",
    ")\n",
    "\n",
    "def liger_rope_grad_transform(q, k, cos, sin):\n",
    "    q_out, k_out, _, _ = liger_rope_forward(q, k, cos, sin)\n",
    "    q_out_grad = get_grad(q_out) \n",
    "    k_out_grad = get_grad(k_out)\n",
    "    dq, dk = liger_rope_backward(q_out_grad, k_out_grad, cos, sin)\n",
    "    put_grads((q, k), (dq, dk))\n",
    "    return q_out, k_out\n",
    "\n",
    "def liger_rope_execution_transform(q, k, cos, sin):\n",
    "    q_out, k_out, _, _ = liger_rope_forward(q, k, cos, sin)\n",
    "    return q_out, k_out\n",
    "\n",
    "def liger_rope_impl(q, k, cos, sin):\n",
    "    qr, kr, _, _ = liger_rope_forward(q, k, cos, sin)\n",
    "    return qr, kr\n",
    "\n",
    "liger_rope = liger_ex.register_operator('liger_rope', fn=liger_rope_impl, like=liger_rope_impl)\n",
    "\n",
    "liger_ex.register_implementation(\n",
    "    liger_rope,\n",
    "    execution_transform=liger_rope_execution_transform,\n",
    "    grad_transform=liger_rope_grad_transform,\n",
    ")\n",
    "\n",
    "def litgpt_apply_rope_meta(x, cos, sin):\n",
    "    return TensorProxy(like=x)\n",
    "\n",
    "litgpt_apply_rope = liger_ex.register_operator(\n",
    "    'litgpt_apply_rope', fn=litgpt.model.apply_rope, meta=litgpt_apply_rope_meta, replaces=litgpt.model.apply_rope\n",
    ")\n",
    "\n",
    "class MergeRopeTransform(thunder.core.transform_common.Transform):\n",
    "    def transform_traces_pre_prologue(self, prologue_trace, compute_trace, epilogue_trace, **kwargs):\n",
    "        new_compute_trace = thunder.core.trace.from_trace(compute_trace)\n",
    "        bound_symbols = compute_trace.bound_symbols[:]\n",
    "        while bound_symbols:\n",
    "            bsym = bound_symbols.pop(0)\n",
    "            if bsym.sym == litgpt_apply_rope:\n",
    "                for i, bsym2 in enumerate(bound_symbols):\n",
    "                    assert not any(o is bsym.output for o in bsym2.flat_outs)\n",
    "                    if bsym2.sym == litgpt_apply_rope:\n",
    "                        break\n",
    "                bsym2 = bound_symbols.pop(i)\n",
    "                assert bsym2.sym == litgpt_apply_rope\n",
    "\n",
    "                output = (bsym.output, bsym2.output)\n",
    "                args = (bsym.args[0], bsym2.args[0], *bsym.args[1:])\n",
    "\n",
    "                new_compute_trace.bound_symbols.append(bsym.from_bsym(args=args, output=output, sym=liger_rope))\n",
    "            else:\n",
    "                new_compute_trace.bound_symbols.append(bsym.from_bsym())\n",
    "        new_compute_trace.set_provenance(thunder.core.trace.TraceProvenance(self.__class__))\n",
    "        return prologue_trace, new_compute_trace, epilogue_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44187b29-c101-41f0-a811-4c9f29757c81",
   "metadata": {},
   "source": [
    "# Test\n",
    "\n",
    "We test with a scaled-down Llama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ca6f1a5-864b-4d20-bc9e-f942e9a90d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = litgpt.Config.from_name('Llama-3.1-8B', n_layer=1)\n",
    "\n",
    "with device:\n",
    "    m = litgpt.GPT(cfg)\n",
    "    m.max_seq_length = 1024\n",
    "    m.set_kv_cache(1)\n",
    "    inp = torch.arange(1, 6, dtype=torch.int64)[None]\n",
    "    inp_pos = torch.arange(5)\n",
    "\n",
    "jm = thunder.jit(m, executors=(liger_ex,), transforms=(MergeRopeTransform(),))\n",
    "res = jm(inp, inp_pos)\n",
    "ref = m(inp, inp_pos)\n",
    "\n",
    "go = torch.randn_like(res)\n",
    "grad_res, = torch.autograd.grad(res, jm.get_parameter('transformer.wte.weight'), go)\n",
    "grad_ref, = torch.autograd.grad(ref, m.get_parameter('transformer.wte.weight'), go)\n",
    "\n",
    "assert_close(res, ref)\n",
    "assert_close(grad_res, grad_ref)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
