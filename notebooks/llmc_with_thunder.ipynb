{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "809df460-f70a-44e9-8af6-f1cfdf073300",
   "metadata": {},
   "source": [
    "# From prototyping in PyTorch to running with custom CUDA kernels\n",
    "In this demo we will show how one can go about prototyping models in PyTorch, and then iteratively replacing some parts with custom code using Thunder. Here, we will be dealing with custom CUDA kernels, although Thunder is quite flexible and can be adapted to work with other compute environments. We highly recommend checking the [Zero to Thunder tutorial](./zero_to_thunder.ipynb) for a general overview of the Thunder's capabilities.\n",
    "\n",
    "As an example, we will use a GPT-2 implementation from [llm.c](https://github.com/karpathy/llm.c). We will start with the PyTorch reference implementation and then replace many of its parts with the native [llm.c](https://github.com/karpathy/llm.c) CUDA kernels using Thunder. This serves to demonstrate the following iterative strategy for model runtime performance optimization:\n",
    "* One starts with a prototype model implemented in PyTorch.\n",
    "* Once critical sections of PyTorch program are identified, they are to be replaced with custom implementations for better performance and/or control. It is quite convenient to do so in Python, and Thunder is especially well-suited for this task.\n",
    "* Once these critical sections are performing as expected, one can go further and re-implement the whole model for the native environment to reduce any additional overhead coming from Python, PyTorch, among others. For example, [llm.c](https://github.com/karpathy/llm.c) implements a very lean C/CUDA GPT-2 model that does not depend on PyTorch nor cPython."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766c8699-8859-4dbc-a3b9-14e0534a08d7",
   "metadata": {},
   "source": [
    "## [llm.c](https://github.com/karpathy/llm.c)\n",
    "[llm.c](https://github.com/karpathy/llm.c) showcases that training LLMs is quite simple, and provides a very lean C/CUDA implementation of GPT-2 without additional overhead that comes from using PyTorch and cPython. It has a PyTorch reference implementation that we will be actively working with.\n",
    "\n",
    "Before we dive deeper, let us do some preparatory work with [llm.c](https://github.com/karpathy/llm.c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5a51b9c-c15d-4f9f-b346-c82eb6292aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'llm.c' already exists and is not an empty directory.\n",
      "Previous HEAD position was 50acc12 Merge branch 'ngc92-split-file' Separates out common error-checking wrapper utils, that are broadly useful across all file\n",
      "HEAD is now at 954077f TRAINING WORKSgit add train_gpt2.cu! ITS SLOW BUT IT WORKS WOOT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/nik/miniconda3/envs/thunder_dev/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (4.66.2)\n",
      "Requirement already satisfied: numpy in /home/nik/miniconda3/envs/thunder_dev/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (1.24.3)\n",
      "Requirement already satisfied: torch in /home/nik/miniconda3/envs/thunder_dev/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (2.4.0.dev20240430+cu121)\n",
      "Requirement already satisfied: tiktoken in /home/nik/miniconda3/envs/thunder_dev/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.6.0)\n",
      "Requirement already satisfied: transformers in /home/nik/miniconda3/envs/thunder_dev/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (4.38.2)\n",
      "Requirement already satisfied: filelock in /home/nik/miniconda3/envs/thunder_dev/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/nik/miniconda3/envs/thunder_dev/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (4.8.0)\n",
      "Requirement already satisfied: sympy in /home/nik/miniconda3/envs/thunder_dev/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (1.11.1)\n",
      "Requirement already satisfied: networkx in /home/nik/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/nik/miniconda3/envs/thunder_dev/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/nik/miniconda3/envs/thunder_dev/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/nik/miniconda3/envs/thunder_dev/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/nik/miniconda3/envs/thunder_dev/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/nik/miniconda3/envs/thunder_dev/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/nik/miniconda3/envs/thunder_dev/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/nik/miniconda3/envs/thunder_dev/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/nik/miniconda3/envs/thunder_dev/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/nik/miniconda3/envs/thunder_dev/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/nik/miniconda3/envs/thunder_dev/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/nik/miniconda3/envs/thunder_dev/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/nik/miniconda3/envs/thunder_dev/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/nik/miniconda3/envs/thunder_dev/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (12.1.105)\n",
      "Requirement already satisfied: pytorch-triton==3.0.0+45fff310c8 in /home/nik/miniconda3/envs/thunder_dev/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (3.0.0+45fff310c8)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/nik/miniconda3/envs/thunder_dev/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->-r requirements.txt (line 3)) (12.1.105)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/nik/miniconda3/envs/thunder_dev/lib/python3.10/site-packages (from tiktoken->-r requirements.txt (line 4)) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/nik/miniconda3/envs/thunder_dev/lib/python3.10/site-packages (from tiktoken->-r requirements.txt (line 4)) (2.31.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/nik/miniconda3/envs/thunder_dev/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 5)) (0.21.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/nik/miniconda3/envs/thunder_dev/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 5)) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/nik/miniconda3/envs/thunder_dev/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 5)) (6.0.1)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/nik/miniconda3/envs/thunder_dev/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 5)) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/nik/miniconda3/envs/thunder_dev/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 5)) (0.4.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nik/miniconda3/envs/thunder_dev/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->-r requirements.txt (line 4)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nik/miniconda3/envs/thunder_dev/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->-r requirements.txt (line 4)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nik/miniconda3/envs/thunder_dev/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->-r requirements.txt (line 4)) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nik/miniconda3/envs/thunder_dev/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->-r requirements.txt (line 4)) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/nik/miniconda3/envs/thunder_dev/lib/python3.10/site-packages (from jinja2->torch->-r requirements.txt (line 3)) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/nik/miniconda3/envs/thunder_dev/lib/python3.10/site-packages (from sympy->torch->-r requirements.txt (line 3)) (1.2.1)\n",
      "data/tiny_shakespeare.txt already exists, skipping download...\n",
      "Saved 32768 tokens to data/tiny_shakespeare_val.bin\n",
      "Saved 305260 tokens to data/tiny_shakespeare_train.bin\n",
      "using device: cuda\n",
      "wrote gpt2_tokenizer.bin\n",
      "loading weights from pretrained gpt: gpt2\n",
      "loading cached tokens in data/tiny_shakespeare_val.bin\n",
      "wrote gpt2_124M.bin\n",
      "wrote gpt2_124M_debug_state.bin\n",
      "iteration 0, loss: 5.270008563995361, time: 3038.967ms\n",
      "iteration 1, loss: 4.059720993041992, time: 33.077ms\n",
      "iteration 2, loss: 3.3751838207244873, time: 35.995ms\n",
      "iteration 3, loss: 2.800813913345337, time: 36.473ms\n",
      "iteration 4, loss: 2.315413475036621, time: 36.510ms\n",
      "iteration 5, loss: 1.8490413427352905, time: 36.740ms\n",
      "iteration 6, loss: 1.3946460485458374, time: 42.617ms\n",
      "iteration 7, loss: 0.9992104768753052, time: 42.732ms\n",
      "iteration 8, loss: 0.6240706443786621, time: 42.645ms\n",
      "iteration 9, loss: 0.3764864206314087, time: 42.529ms\n",
      "final 20 iters avg: 338.828ms\n",
      "<|endoftext|>One year ago today:\n",
      "This is the first week since we last spoke.\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "git clone https://github.com/karpathy/llm.c.git\n",
    "cd llm.c\n",
    "git checkout 954077fb887d2770e4d537bafea056473d4bb4ce\n",
    "pip install -r requirements.txt\n",
    "python prepro_tinyshakespeare.py\n",
    "python train_gpt2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0be8be-6b63-40bb-a131-15e941d2ac9c",
   "metadata": {},
   "source": [
    "The lines above clone the repository, check it out at `954077fb887d2770e4d537bafea056473d4bb4ce` (yes, we need this as it is being developed very rapidly) and then install all the necessary dependencies. `python prepro_tinyshakespeare.py` downloads the [tinyshakespeare](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt) dataset, tokenezes it with the GPT-2 Tokenizer and saves the GPT-2 (124M) weights. `python train_gpt2.py` loads the weights and runs the reference PyTorch model for a dozen of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2f516a-203c-4ab4-aa6c-ab3bfb514ebd",
   "metadata": {},
   "source": [
    "## Reference PyTorch implementation\n",
    "This is where it all begins for us! The code below is a modified version of the original [train_gpt2.py](./llm.c/train_gpt2.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85db81f9-255b-496c-b18d-0409625be9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based off train_gpt2.py from https://github.com/karpathy/llm.c/tree/954077fb887d2770e4d537bafea056473d4bb4ce\n",
    "\n",
    "\"\"\"\n",
    "Reference code for GPT-2 training and inference.\n",
    "Will save the model weights into files, to be read from C as initialization.\n",
    "\n",
    "References:\n",
    "1) the official GPT-2 TensorFlow implementation released by OpenAI:\n",
    "https://github.com/openai/gpt-2/blob/master/src/model.py\n",
    "2) huggingface/transformers PyTorch implementation:\n",
    "https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import struct\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Attention-related {\n",
    "def _permute(qkv, n_embd, n_head, B, T, C):\n",
    "    q, k, v = qkv.split(n_embd, dim=2)\n",
    "    q = q.view(B, T, n_head, C // n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "    k = k.view(B, T, n_head, C // n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "    v = v.view(B, T, n_head, C // n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "    return q, k, v\n",
    "\n",
    "\n",
    "def _unpermute(y, B, T, C):\n",
    "    return y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "\n",
    "def _manual_attention(qkv, bias, n_embd, n_head, B, T, C):\n",
    "    q, k, v = _permute(qkv, n_embd, n_head, B, T, C)\n",
    "\n",
    "    # manual implementation of attention\n",
    "    att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "    att = att.masked_fill(bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "    att = F.softmax(att, dim=-1)\n",
    "    y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "\n",
    "    y = _unpermute(y, B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "    return y\n",
    "# }\n",
    "\n",
    "class NewGELU(nn.Module):\n",
    "    \"\"\"Careful there are a few versions of GeLU, this one is the exact one used by OpenAI\"\"\"\n",
    "    def forward(self, input):\n",
    "        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        # not really a 'bias', more of a mask, but following the OpenAI/HF naming though\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        qkv = self.c_attn(x)\n",
    "\n",
    "        y = _manual_attention(qkv, self.bias, self.n_embd, self.n_head, B, T, C)\n",
    "\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu    = NewGELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50257\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "# a few utilities for saving params/grads/activations to files for loading in C\n",
    "def write_fp32(tensor, file):\n",
    "    file.write(tensor.detach().numpy().astype(\"float32\").tobytes())\n",
    "\n",
    "def write_tensors(model_tensors, L, file):\n",
    "    write_fp32(model_tensors[\"transformer.wte.weight\"], file) # (V, C)\n",
    "    write_fp32(model_tensors[\"transformer.wpe.weight\"], file) # (T, C)\n",
    "    for i in range(L): # (L, C)\n",
    "        write_fp32(model_tensors[f\"transformer.h.{i}.ln_1.weight\"], file)\n",
    "    for i in range(L): # (L, C)\n",
    "        write_fp32(model_tensors[f\"transformer.h.{i}.ln_1.bias\"], file)\n",
    "    for i in range(L): # (L, 3C, C)\n",
    "        write_fp32(model_tensors[f\"transformer.h.{i}.attn.c_attn.weight\"], file)\n",
    "    for i in range(L): # (L, 3C)\n",
    "        write_fp32(model_tensors[f\"transformer.h.{i}.attn.c_attn.bias\"], file)\n",
    "    for i in range(L): # (L, C, C)\n",
    "        write_fp32(model_tensors[f\"transformer.h.{i}.attn.c_proj.weight\"], file)\n",
    "    for i in range(L): # (L, C)\n",
    "        write_fp32(model_tensors[f\"transformer.h.{i}.attn.c_proj.bias\"], file)\n",
    "    for i in range(L): # (L, C)\n",
    "        write_fp32(model_tensors[f\"transformer.h.{i}.ln_2.weight\"], file)\n",
    "    for i in range(L): # (L, C)\n",
    "        write_fp32(model_tensors[f\"transformer.h.{i}.ln_2.bias\"], file)\n",
    "    for i in range(L): # (L, 4C, C)\n",
    "        write_fp32(model_tensors[f\"transformer.h.{i}.mlp.c_fc.weight\"], file)\n",
    "    for i in range(L): # (L, 4C)\n",
    "        write_fp32(model_tensors[f\"transformer.h.{i}.mlp.c_fc.bias\"], file)\n",
    "    for i in range(L): # (L, C, 4C)\n",
    "        write_fp32(model_tensors[f\"transformer.h.{i}.mlp.c_proj.weight\"], file)\n",
    "    for i in range(L): # (L, C)\n",
    "        write_fp32(model_tensors[f\"transformer.h.{i}.mlp.c_proj.bias\"], file)\n",
    "    write_fp32(model_tensors[\"transformer.ln_f.weight\"], file) # (C, )\n",
    "    write_fp32(model_tensors[\"transformer.ln_f.bias\"], file) # (C, )\n",
    "\n",
    "def write_model(model, filename):\n",
    "    # everything we need to instantiate the model\n",
    "    # 1) header is: version int, GPTConfig ints, padding to 1024 bytes\n",
    "    header = torch.zeros(256, dtype=torch.int32)\n",
    "    header[0] = 20240326 # magic\n",
    "    header[1] = 1 # checkpoint version = 1\n",
    "    header[2] = model.config.block_size\n",
    "    header[3] = model.config.vocab_size\n",
    "    header[4] = model.config.n_layer\n",
    "    header[5] = model.config.n_head\n",
    "    header[6] = model.config.n_embd\n",
    "    # 2) the parameters on CPU are next\n",
    "    params = {name: param.cpu() for name, param in model.named_parameters()}\n",
    "    # now write\n",
    "    with open(filename, \"wb\") as file:\n",
    "        # header\n",
    "        file.write(header.numpy().tobytes())\n",
    "        # model parameters\n",
    "        write_tensors(params, model.config.n_layer, file)\n",
    "    print(f\"wrote {filename}\")\n",
    "\n",
    "def write_state(model, x, y, logits, loss, filename):\n",
    "    # the state is used for debugging.\n",
    "    # it contains information about the input, logits, loss, and the parameter gradients\n",
    "    # this can be used for checking the computation correctness in C\n",
    "    header = torch.zeros(256, dtype=torch.int32)\n",
    "    header[0] = 20240327 # magic\n",
    "    header[1] = 1 # run state version = 1\n",
    "    header[2] = x.size(0) # batch size of the batch, B\n",
    "    header[3] = x.size(1) # temporal extent of the batch, T\n",
    "    grads = {name: param.grad.cpu() for name, param in model.named_parameters()}\n",
    "    with open(filename, \"wb\") as file:\n",
    "        # header\n",
    "        file.write(header.numpy().tobytes())\n",
    "        # input x\n",
    "        file.write(x.cpu().numpy().astype(\"int32\").tobytes()) # (B, T)\n",
    "        # targets y\n",
    "        file.write(y.cpu().numpy().astype(\"int32\").tobytes()) # (B, T)\n",
    "        # logits (result of the model forward pass)\n",
    "        write_fp32(logits.cpu(), file)\n",
    "        # loss (single float, result of the cross entropy loss)\n",
    "        write_fp32(loss.cpu(), file)\n",
    "        # gradients\n",
    "        write_tensors(grads, model.config.n_layer, file)\n",
    "    print(f\"wrote {filename}\")\n",
    "\n",
    "def write_tokenizer(enc, filename):\n",
    "    n = enc.max_token_value + 1\n",
    "    header = torch.zeros(256, dtype=torch.int32)\n",
    "    header[0] = 20240328 # magic\n",
    "    header[1] = 1 # tokenizer version = 1\n",
    "    header[2] = n # number of tokens\n",
    "    with open(filename, \"wb\") as file:\n",
    "        file.write(header.numpy().tobytes())\n",
    "        for i in range(n):\n",
    "            b = enc.decode_bytes([i])\n",
    "            length = len(b)\n",
    "            assert length < 256, f\"Token length exceeds 255: {length}\"\n",
    "            file.write(struct.pack(\"<B\", length))  # Write the length as a 1-byte unsigned integer\n",
    "            file.write(b)  # Write the actual bytes\n",
    "    print(f\"wrote {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1241a39d-0ad2-4c4e-b71f-4f2e3ddee224",
   "metadata": {},
   "source": [
    "Methods such as `_permute`, `_unpermute` and `_manual_attention` right at the very top group some parts of the original code. These we will map to CUDA kernels that [llm.c](https://github.com/karpathy/llm.c) provides using Thunder later on!\n",
    "\n",
    "The code below is a simplification of the `__main__` method from [llm.c/train_gpt2.py](./llm.c/train_gpt2.py) that we wrap into a callable for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ad65b35-ae58-4c12-9769-9fdcddf61c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based of train_gpt2.py from https://github.com/karpathy/llm.c/tree/954077fb887d2770e4d537bafea056473d4bb4ce\n",
    "import time\n",
    "import argparse\n",
    "import tiktoken\n",
    "import thunder\n",
    "\n",
    "\n",
    "def get_data_iter(B, T, device):\n",
    "    # load the tokens\n",
    "    # prefer to use tiny_shakespeare if it's available, otherwise use tiny_stories\n",
    "    # we're using val instead of train split just because it is smaller/faster\n",
    "    shake_tokens_bin = \"./llm.c/data/tiny_shakespeare_val.bin\"\n",
    "    story_tokens_bin = \"./llm.c/data/TinyStories_val.bin\"\n",
    "    assert os.path.isfile(shake_tokens_bin) or os.path.isfile(story_tokens_bin), \"you must run prepro on some dataset\"\n",
    "    tokens_bin = shake_tokens_bin if os.path.isfile(shake_tokens_bin) else story_tokens_bin\n",
    "    assert os.path.isfile(tokens_bin)\n",
    "    #print(f\"loading cached tokens in {tokens_bin}\")\n",
    "    with open(tokens_bin, \"rb\") as f:\n",
    "        tokens = np.frombuffer(f.read(), dtype=np.int32)\n",
    "\n",
    "    # np -> tensor, long, on device\n",
    "    tokens = torch.tensor(tokens)\n",
    "    tokens = tokens.to(torch.long)\n",
    "    tokens = tokens.to(device)\n",
    "\n",
    "    # lightweight dataloader\n",
    "    def get_batch():\n",
    "        assert B*T+1 <= len(tokens), \"not enough tokens\"\n",
    "        # for 338,025 tokens. E.g. with B=8 T=1024, this will yield 41 batches before looping\n",
    "        i = 0\n",
    "        while True:\n",
    "            x = tokens[i:i+B*T].view(B, T)\n",
    "            y = tokens[i+1:i+B*T+1].view(B, T)\n",
    "            yield x, y\n",
    "            i += B*T\n",
    "            if i + B*T + 1 >= len(tokens):\n",
    "                i = 0 # in prod we'd want to randomize the start point a bit\n",
    "\n",
    "    data_iter = iter(get_batch())\n",
    "    return data_iter\n",
    "\n",
    "\n",
    "def demo_model(\n",
    "    *,\n",
    "    write_tensors: int = 0,  # write tensors to disk\n",
    "    inference_only: int = 0,  # only run inference\n",
    "    compile: int = 0,  # torch.compile the model\n",
    "    tensorcores: int = 0,  # use tensorcores\n",
    "    num_iterations: int = 10,  # number of iterations to run\n",
    "    batch_size: int = 4,  # batch size\n",
    "    sequence_length: int = 64,  # sequence length\n",
    "    thunder_jit: int = 1,  # thunder.jit the model\n",
    "    thunder_executors = None,\n",
    "):\n",
    "    # default settings will overfit a tiny batch of data\n",
    "    # and save model weights and debug state to disk on the first iteration\n",
    "    # if you'd like to e.g. time the forward pass only, call this function with:\n",
    "    # inference_only=1, write_tensors=0, sequence_length=1024\n",
    "\n",
    "    B, T = batch_size, sequence_length\n",
    "    assert 1 <= T <= 1024\n",
    "\n",
    "    # we use CUDA globally\n",
    "    device = \"cuda\"\n",
    "    \n",
    "    # seed the random number generators\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "    # init the tokenizer\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "    decode = lambda l: enc.decode(l)\n",
    "    write_tokenizer(enc, \"./llm.c/gpt2_tokenizer.bin\")\n",
    "\n",
    "    if tensorcores:\n",
    "        torch.set_float32_matmul_precision('high')\n",
    "\n",
    "    # load the GPT-2 model weights\n",
    "    model = GPT.from_pretrained(\"gpt2\")\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    if compile:\n",
    "        print(\"compiling the model with torch.compile...\")\n",
    "        model = torch.compile(model)\n",
    "    if thunder_jit:\n",
    "        print(\"compiling the model with thunder.jit...\")\n",
    "        if thunder_executors == None:\n",
    "            thunder_executors = ()\n",
    "        model = thunder.jit(model, executors=tuple(thunder_executors) + thunder.get_default_executors())\n",
    "\n",
    "    # forward backward for a few iterations\n",
    "    data_iter = get_data_iter(B, T, device)\n",
    "    x, y = next(data_iter) # we'll overfit this batch below\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    timings = []\n",
    "    # Warm-up runs!\n",
    "    for _ in range(5):\n",
    "        model(x, y)\n",
    "        \n",
    "    for i in range(num_iterations):\n",
    "        # Now measure the runtime!\n",
    "        t0 = time.time()\n",
    "        logits, loss = model(x, y)\n",
    "        if not inference_only:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            if i == 0:\n",
    "                #print(thunder.last_backward_traces(model)[-1])\n",
    "                pass\n",
    "            print(f\"{loss=}\")\n",
    "            # TODO: investigate missing keys\n",
    "            # on the first iteration only, save the state dict to file for later reference\n",
    "            #if i == 0 and args.write_tensors:\n",
    "            #    write_model(model, \"gpt2_124M.bin\")\n",
    "            #    write_state(model, x, y, logits, loss, \"gpt2_124M_debug_state.bin\")\n",
    "            optimizer.step()\n",
    "            torch.cuda.synchronize()\n",
    "        t1 = time.time()\n",
    "        if i > num_iterations - 20:\n",
    "            timings.append(t1-t0)\n",
    "        print(f\"iteration {i}, loss: {loss.item()}, time: {(t1-t0)*1000:.3f}ms\")\n",
    "    if len(timings) > 0:\n",
    "        print(f\"final 20 iters avg: {np.mean(timings)*1000:.3f}ms\")\n",
    "\n",
    "    # before we end, let's also do one round of inference\n",
    "    # we'll kick off the generation with \"<|endoftext|>\", which designates the start of a new sequence\n",
    "    start = \"<|endoftext|>\"\n",
    "    start_ids = encode(start)\n",
    "    x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "    # run generation for 16 time steps (tokens)\n",
    "    max_new_tokens = 16\n",
    "    temperature = 1.0\n",
    "    top_k = 40\n",
    "    model.eval()\n",
    "    y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "    print(decode(y[0].tolist()))\n",
    "    print('---------------')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa059b39-46c9-42a8-bd1d-e98dc26ca3ca",
   "metadata": {},
   "source": [
    "Let's see whether our modifications actually work.\n",
    "We first try to reproduce `python train_gpt2.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42db269f-907a-4579-a068-034f06501147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote ./llm.c/gpt2_tokenizer.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nik/miniconda3/envs/thunder_dev/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n",
      "loss=tensor(5.2700, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iteration 0, loss: 5.270008563995361, time: 204.454ms\n",
      "loss=tensor(4.0597, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iteration 1, loss: 4.059720993041992, time: 35.370ms\n",
      "loss=tensor(3.3752, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iteration 2, loss: 3.3751838207244873, time: 36.551ms\n",
      "loss=tensor(2.8008, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iteration 3, loss: 2.800813913345337, time: 37.395ms\n",
      "loss=tensor(2.3154, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iteration 4, loss: 2.315413475036621, time: 38.213ms\n",
      "loss=tensor(1.8490, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iteration 5, loss: 1.8490413427352905, time: 38.244ms\n",
      "loss=tensor(1.3946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iteration 6, loss: 1.3946460485458374, time: 44.990ms\n",
      "loss=tensor(0.9992, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iteration 7, loss: 0.9992104768753052, time: 50.108ms\n",
      "loss=tensor(0.6241, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iteration 8, loss: 0.6240706443786621, time: 50.126ms\n",
      "loss=tensor(0.3765, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iteration 9, loss: 0.3764864206314087, time: 51.362ms\n",
      "final 20 iters avg: 58.681ms\n",
      "<|endoftext|>One year ago today:\n",
      "This is the first week since we last spoke.\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "default_model = demo_model(thunder_jit=0)\n",
    "del default_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85aad7d-2ae3-46ee-905a-6e14104f222f",
   "metadata": {},
   "source": [
    "Great! The numbers match. Let's see whether we can directly run the model through Thunder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78a4d690-3199-441c-9fe1-c2d80ddf03a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote ./llm.c/gpt2_tokenizer.bin\n",
      "loading weights from pretrained gpt: gpt2\n",
      "compiling the model with thunder.jit...\n",
      "loss=tensor(5.2700, device='cuda:0', grad_fn=<ThunderFunctionBackward>)\n",
      "iteration 0, loss: 5.270007610321045, time: 8706.774ms\n",
      "loss=tensor(4.0597, device='cuda:0', grad_fn=<ThunderFunctionBackward>)\n",
      "iteration 1, loss: 4.059719562530518, time: 54.336ms\n",
      "loss=tensor(3.3752, device='cuda:0', grad_fn=<ThunderFunctionBackward>)\n",
      "iteration 2, loss: 3.375183582305908, time: 58.817ms\n",
      "loss=tensor(2.8008, device='cuda:0', grad_fn=<ThunderFunctionBackward>)\n",
      "iteration 3, loss: 2.8008131980895996, time: 58.689ms\n",
      "loss=tensor(2.3154, device='cuda:0', grad_fn=<ThunderFunctionBackward>)\n",
      "iteration 4, loss: 2.315413475036621, time: 66.542ms\n",
      "loss=tensor(1.8490, device='cuda:0', grad_fn=<ThunderFunctionBackward>)\n",
      "iteration 5, loss: 1.8490394353866577, time: 65.207ms\n",
      "loss=tensor(1.3946, device='cuda:0', grad_fn=<ThunderFunctionBackward>)\n",
      "iteration 6, loss: 1.3946452140808105, time: 65.406ms\n",
      "loss=tensor(0.9992, device='cuda:0', grad_fn=<ThunderFunctionBackward>)\n",
      "iteration 7, loss: 0.9992080926895142, time: 64.580ms\n",
      "loss=tensor(0.6241, device='cuda:0', grad_fn=<ThunderFunctionBackward>)\n",
      "iteration 8, loss: 0.6240673065185547, time: 66.025ms\n",
      "loss=tensor(0.3765, device='cuda:0', grad_fn=<ThunderFunctionBackward>)\n",
      "iteration 9, loss: 0.3764849901199341, time: 65.069ms\n",
      "final 20 iters avg: 927.145ms\n",
      "<|endoftext|>One year ago today:\n",
      "This is the first week since we last spoke.\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "thunder_model = demo_model(thunder_jit=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414fcfe8-c137-41f7-9f0f-6dffc63f34c0",
   "metadata": {},
   "source": [
    "We can inspect the forward trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "624e7cbe-ee36-4b5f-860f-c6ac1f0edd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Constructed by Delete Last Used (took 8 milliseconds)\n",
      "import torch\n",
      "from torch import Tensor\n",
      "import torch.nn.functional\n",
      "from thunder.executors.torchex import no_autocast\n",
      "\n",
      "@torch.no_grad()\n",
      "@no_autocast()\n",
      "def augmented_forward_fn(idx, targets, t_transformer_h_0_ln_1_bias, t_transformer_h_0_attn_c_attn_bias, t_transformer_h_0_attn_c_proj_bias, t_transformer_h_0_ln_2_bias, t_transformer_h_0_mlp_c_fc_bias, t_transformer_h_0_mlp_c_proj_bias, t_transformer_h_1_ln_1_bias, t_transformer_h_1_attn_c_attn_bias, t_transformer_h_1_attn_c_proj_bias, t_transformer_h_1_ln_2_bias, t_transformer_h_1_mlp_c_fc_bias, t_transformer_h_1_mlp_c_proj_bias, t_transformer_h_2_ln_1_bias, t_transformer_h_2_attn_c_attn_bias, t_transformer_h_2_attn_c_proj_bias, t_transformer_h_2_ln_2_bias, t_transformer_h_2_mlp_c_fc_bias, t_transformer_h_2_mlp_c_proj_bias, t_transformer_h_3_ln_1_bias, t_transformer_h_3_attn_c_attn_bias, t_transformer_h_3_attn_c_proj_bias, t_transformer_h_3_ln_2_bias, t_transformer_h_3_mlp_c_fc_bias, t_transformer_h_3_mlp_c_proj_bias, t_transformer_h_4_ln_1_bias, t_transformer_h_4_attn_c_attn_bias, t_transformer_h_4_attn_c_proj_bias, t_transformer_h_4_ln_2_bias, t_transformer_h_4_mlp_c_fc_bias, t_transformer_h_4_mlp_c_proj_bias, t_transformer_h_5_ln_1_bias, t_transformer_h_5_attn_c_attn_bias, t_transformer_h_5_attn_c_proj_bias, t_transformer_h_5_ln_2_bias, t_transformer_h_5_mlp_c_fc_bias, t_transformer_h_5_mlp_c_proj_bias, t_transformer_h_6_ln_1_bias, t_transformer_h_6_attn_c_attn_bias, t_transformer_h_6_attn_c_proj_bias, t_transformer_h_6_ln_2_bias, t_transformer_h_6_mlp_c_fc_bias, t_transformer_h_6_mlp_c_proj_bias, t_transformer_h_7_ln_1_bias, t_transformer_h_7_attn_c_attn_bias, t_transformer_h_7_attn_c_proj_bias, t_transformer_h_7_ln_2_bias, t_transformer_h_7_mlp_c_fc_bias, t_transformer_h_7_mlp_c_proj_bias, t_transformer_h_8_ln_1_bias, t_transformer_h_8_attn_c_attn_bias, t_transformer_h_8_attn_c_proj_bias, t_transformer_h_8_ln_2_bias, t_transformer_h_8_mlp_c_fc_bias, t_transformer_h_8_mlp_c_proj_bias, t_transformer_h_9_ln_1_bias, t_transformer_h_9_attn_c_attn_bias, t_transformer_h_9_attn_c_proj_bias, t_transformer_h_9_ln_2_bias, t_transformer_h_9_mlp_c_fc_bias, t_transformer_h_9_mlp_c_proj_bias, t_transformer_h_10_ln_1_bias, t_transformer_h_10_attn_c_attn_bias, t_transformer_h_10_attn_c_proj_bias, t_transformer_h_10_ln_2_bias, t_transformer_h_10_mlp_c_fc_bias, t_transformer_h_10_mlp_c_proj_bias, t_transformer_h_11_ln_1_bias, t_transformer_h_11_attn_c_attn_bias, t_transformer_h_11_attn_c_proj_bias, t_transformer_h_11_ln_2_bias, t_transformer_h_11_mlp_c_fc_bias, t_transformer_h_11_mlp_c_proj_bias, t_transformer_ln_f_bias, bias, tos1, t_transformer_h_2_attn_bias, t_transformer_h_3_attn_bias, t_transformer_h_4_attn_bias, t_transformer_h_5_attn_bias, t_transformer_h_6_attn_bias, t_transformer_h_7_attn_bias, t_transformer_h_8_attn_bias, t_transformer_h_9_attn_bias, t_transformer_h_10_attn_bias, t_transformer_h_11_attn_bias, t_transformer_h_0_attn_c_attn_weight, t_transformer_h_1_attn_c_attn_weight, t_transformer_h_2_attn_c_attn_weight, t_transformer_h_3_attn_c_attn_weight, t_transformer_h_4_attn_c_attn_weight, t_transformer_h_5_attn_c_attn_weight, t_transformer_h_6_attn_c_attn_weight, t_transformer_h_7_attn_c_attn_weight, t_transformer_h_8_attn_c_attn_weight, t_transformer_h_9_attn_c_attn_weight, t_transformer_h_10_attn_c_attn_weight, t_transformer_h_11_attn_c_attn_weight, t_transformer_h_0_mlp_c_fc_weight, t_transformer_h_1_mlp_c_fc_weight, t_transformer_h_2_mlp_c_fc_weight, t_transformer_h_3_mlp_c_fc_weight, t_transformer_h_4_mlp_c_fc_weight, t_transformer_h_5_mlp_c_fc_weight, t_transformer_h_6_mlp_c_fc_weight, t_transformer_h_7_mlp_c_fc_weight, t_transformer_h_8_mlp_c_fc_weight, t_transformer_h_9_mlp_c_fc_weight, t_transformer_h_10_mlp_c_fc_weight, t_transformer_h_11_mlp_c_fc_weight, t_transformer_h_0_attn_c_proj_weight, t_transformer_h_0_mlp_c_proj_weight, t_transformer_h_1_attn_c_proj_weight, t_transformer_h_1_mlp_c_proj_weight, t_transformer_h_2_attn_c_proj_weight, t_transformer_h_2_mlp_c_proj_weight, t_transformer_h_3_attn_c_proj_weight, t_transformer_h_3_mlp_c_proj_weight, t_transformer_h_4_attn_c_proj_weight, t_transformer_h_4_mlp_c_proj_weight, t_transformer_h_5_attn_c_proj_weight, t_transformer_h_5_mlp_c_proj_weight, t_transformer_h_6_attn_c_proj_weight, t_transformer_h_6_mlp_c_proj_weight, t_transformer_h_7_attn_c_proj_weight, t_transformer_h_7_mlp_c_proj_weight, t_transformer_h_8_attn_c_proj_weight, t_transformer_h_8_mlp_c_proj_weight, t_transformer_h_9_attn_c_proj_weight, t_transformer_h_9_mlp_c_proj_weight, t_transformer_h_10_attn_c_proj_weight, t_transformer_h_10_mlp_c_proj_weight, t_transformer_h_11_attn_c_proj_weight, t_transformer_h_11_mlp_c_proj_weight, t_lm_head_weight, t_transformer_h_0_ln_1_weight, t_transformer_h_10_ln_1_weight, t_transformer_h_11_ln_1_weight, t_transformer_h_1_ln_1_weight, t_transformer_h_2_ln_1_weight, t_transformer_h_3_ln_1_weight, t_transformer_h_4_ln_1_weight, t_transformer_h_5_ln_1_weight, t_transformer_h_6_ln_1_weight, t_transformer_h_7_ln_1_weight, t_transformer_h_8_ln_1_weight, t_transformer_h_9_ln_1_weight, t_transformer_h_0_ln_2_weight, t_transformer_h_1_ln_2_weight, t_transformer_h_2_ln_2_weight, t_transformer_h_3_ln_2_weight, t_transformer_h_4_ln_2_weight, t_transformer_h_5_ln_2_weight, t_transformer_h_6_ln_2_weight, t_transformer_h_7_ln_2_weight, t_transformer_h_8_ln_2_weight, t_transformer_h_9_ln_2_weight, t_transformer_h_10_ln_2_weight, t_transformer_h_11_ln_2_weight, t_transformer_ln_f_weight, t_transformer_wpe_weight, t_transformer_wte_weight):\n",
      "  # idx: \"cuda:0 i64[4, 64]\"\n",
      "  # targets: \"cuda:0 i64[4, 64]\"\n",
      "  # t_transformer_h_0_ln_1_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_0_attn_c_attn_bias: \"cuda:0 f32[2304]\"\n",
      "  # t_transformer_h_0_attn_c_proj_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_0_ln_2_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_0_mlp_c_fc_bias: \"cuda:0 f32[3072]\"\n",
      "  # t_transformer_h_0_mlp_c_proj_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_1_ln_1_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_1_attn_c_attn_bias: \"cuda:0 f32[2304]\"\n",
      "  # t_transformer_h_1_attn_c_proj_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_1_ln_2_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_1_mlp_c_fc_bias: \"cuda:0 f32[3072]\"\n",
      "  # t_transformer_h_1_mlp_c_proj_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_2_ln_1_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_2_attn_c_attn_bias: \"cuda:0 f32[2304]\"\n",
      "  # t_transformer_h_2_attn_c_proj_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_2_ln_2_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_2_mlp_c_fc_bias: \"cuda:0 f32[3072]\"\n",
      "  # t_transformer_h_2_mlp_c_proj_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_3_ln_1_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_3_attn_c_attn_bias: \"cuda:0 f32[2304]\"\n",
      "  # t_transformer_h_3_attn_c_proj_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_3_ln_2_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_3_mlp_c_fc_bias: \"cuda:0 f32[3072]\"\n",
      "  # t_transformer_h_3_mlp_c_proj_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_4_ln_1_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_4_attn_c_attn_bias: \"cuda:0 f32[2304]\"\n",
      "  # t_transformer_h_4_attn_c_proj_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_4_ln_2_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_4_mlp_c_fc_bias: \"cuda:0 f32[3072]\"\n",
      "  # t_transformer_h_4_mlp_c_proj_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_5_ln_1_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_5_attn_c_attn_bias: \"cuda:0 f32[2304]\"\n",
      "  # t_transformer_h_5_attn_c_proj_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_5_ln_2_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_5_mlp_c_fc_bias: \"cuda:0 f32[3072]\"\n",
      "  # t_transformer_h_5_mlp_c_proj_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_6_ln_1_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_6_attn_c_attn_bias: \"cuda:0 f32[2304]\"\n",
      "  # t_transformer_h_6_attn_c_proj_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_6_ln_2_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_6_mlp_c_fc_bias: \"cuda:0 f32[3072]\"\n",
      "  # t_transformer_h_6_mlp_c_proj_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_7_ln_1_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_7_attn_c_attn_bias: \"cuda:0 f32[2304]\"\n",
      "  # t_transformer_h_7_attn_c_proj_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_7_ln_2_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_7_mlp_c_fc_bias: \"cuda:0 f32[3072]\"\n",
      "  # t_transformer_h_7_mlp_c_proj_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_8_ln_1_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_8_attn_c_attn_bias: \"cuda:0 f32[2304]\"\n",
      "  # t_transformer_h_8_attn_c_proj_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_8_ln_2_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_8_mlp_c_fc_bias: \"cuda:0 f32[3072]\"\n",
      "  # t_transformer_h_8_mlp_c_proj_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_9_ln_1_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_9_attn_c_attn_bias: \"cuda:0 f32[2304]\"\n",
      "  # t_transformer_h_9_attn_c_proj_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_9_ln_2_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_9_mlp_c_fc_bias: \"cuda:0 f32[3072]\"\n",
      "  # t_transformer_h_9_mlp_c_proj_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_10_ln_1_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_10_attn_c_attn_bias: \"cuda:0 f32[2304]\"\n",
      "  # t_transformer_h_10_attn_c_proj_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_10_ln_2_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_10_mlp_c_fc_bias: \"cuda:0 f32[3072]\"\n",
      "  # t_transformer_h_10_mlp_c_proj_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_11_ln_1_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_11_attn_c_attn_bias: \"cuda:0 f32[2304]\"\n",
      "  # t_transformer_h_11_attn_c_proj_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_11_ln_2_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_11_mlp_c_fc_bias: \"cuda:0 f32[3072]\"\n",
      "  # t_transformer_h_11_mlp_c_proj_bias: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_ln_f_bias: \"cuda:0 f32[768]\"\n",
      "  # bias: \"cuda:0 f32[1, 1, 1024, 1024]\"\n",
      "  # tos1: \"cuda:0 f32[1, 1, 1024, 1024]\"\n",
      "  # t_transformer_h_2_attn_bias: \"cuda:0 f32[1, 1, 1024, 1024]\"\n",
      "  # t_transformer_h_3_attn_bias: \"cuda:0 f32[1, 1, 1024, 1024]\"\n",
      "  # t_transformer_h_4_attn_bias: \"cuda:0 f32[1, 1, 1024, 1024]\"\n",
      "  # t_transformer_h_5_attn_bias: \"cuda:0 f32[1, 1, 1024, 1024]\"\n",
      "  # t_transformer_h_6_attn_bias: \"cuda:0 f32[1, 1, 1024, 1024]\"\n",
      "  # t_transformer_h_7_attn_bias: \"cuda:0 f32[1, 1, 1024, 1024]\"\n",
      "  # t_transformer_h_8_attn_bias: \"cuda:0 f32[1, 1, 1024, 1024]\"\n",
      "  # t_transformer_h_9_attn_bias: \"cuda:0 f32[1, 1, 1024, 1024]\"\n",
      "  # t_transformer_h_10_attn_bias: \"cuda:0 f32[1, 1, 1024, 1024]\"\n",
      "  # t_transformer_h_11_attn_bias: \"cuda:0 f32[1, 1, 1024, 1024]\"\n",
      "  # t_transformer_h_0_attn_c_attn_weight: \"cuda:0 f32[2304, 768]\"\n",
      "  # t_transformer_h_1_attn_c_attn_weight: \"cuda:0 f32[2304, 768]\"\n",
      "  # t_transformer_h_2_attn_c_attn_weight: \"cuda:0 f32[2304, 768]\"\n",
      "  # t_transformer_h_3_attn_c_attn_weight: \"cuda:0 f32[2304, 768]\"\n",
      "  # t_transformer_h_4_attn_c_attn_weight: \"cuda:0 f32[2304, 768]\"\n",
      "  # t_transformer_h_5_attn_c_attn_weight: \"cuda:0 f32[2304, 768]\"\n",
      "  # t_transformer_h_6_attn_c_attn_weight: \"cuda:0 f32[2304, 768]\"\n",
      "  # t_transformer_h_7_attn_c_attn_weight: \"cuda:0 f32[2304, 768]\"\n",
      "  # t_transformer_h_8_attn_c_attn_weight: \"cuda:0 f32[2304, 768]\"\n",
      "  # t_transformer_h_9_attn_c_attn_weight: \"cuda:0 f32[2304, 768]\"\n",
      "  # t_transformer_h_10_attn_c_attn_weight: \"cuda:0 f32[2304, 768]\"\n",
      "  # t_transformer_h_11_attn_c_attn_weight: \"cuda:0 f32[2304, 768]\"\n",
      "  # t_transformer_h_0_mlp_c_fc_weight: \"cuda:0 f32[3072, 768]\"\n",
      "  # t_transformer_h_1_mlp_c_fc_weight: \"cuda:0 f32[3072, 768]\"\n",
      "  # t_transformer_h_2_mlp_c_fc_weight: \"cuda:0 f32[3072, 768]\"\n",
      "  # t_transformer_h_3_mlp_c_fc_weight: \"cuda:0 f32[3072, 768]\"\n",
      "  # t_transformer_h_4_mlp_c_fc_weight: \"cuda:0 f32[3072, 768]\"\n",
      "  # t_transformer_h_5_mlp_c_fc_weight: \"cuda:0 f32[3072, 768]\"\n",
      "  # t_transformer_h_6_mlp_c_fc_weight: \"cuda:0 f32[3072, 768]\"\n",
      "  # t_transformer_h_7_mlp_c_fc_weight: \"cuda:0 f32[3072, 768]\"\n",
      "  # t_transformer_h_8_mlp_c_fc_weight: \"cuda:0 f32[3072, 768]\"\n",
      "  # t_transformer_h_9_mlp_c_fc_weight: \"cuda:0 f32[3072, 768]\"\n",
      "  # t_transformer_h_10_mlp_c_fc_weight: \"cuda:0 f32[3072, 768]\"\n",
      "  # t_transformer_h_11_mlp_c_fc_weight: \"cuda:0 f32[3072, 768]\"\n",
      "  # t_transformer_h_0_attn_c_proj_weight: \"cuda:0 f32[768, 768]\"\n",
      "  # t_transformer_h_0_mlp_c_proj_weight: \"cuda:0 f32[768, 3072]\"\n",
      "  # t_transformer_h_1_attn_c_proj_weight: \"cuda:0 f32[768, 768]\"\n",
      "  # t_transformer_h_1_mlp_c_proj_weight: \"cuda:0 f32[768, 3072]\"\n",
      "  # t_transformer_h_2_attn_c_proj_weight: \"cuda:0 f32[768, 768]\"\n",
      "  # t_transformer_h_2_mlp_c_proj_weight: \"cuda:0 f32[768, 3072]\"\n",
      "  # t_transformer_h_3_attn_c_proj_weight: \"cuda:0 f32[768, 768]\"\n",
      "  # t_transformer_h_3_mlp_c_proj_weight: \"cuda:0 f32[768, 3072]\"\n",
      "  # t_transformer_h_4_attn_c_proj_weight: \"cuda:0 f32[768, 768]\"\n",
      "  # t_transformer_h_4_mlp_c_proj_weight: \"cuda:0 f32[768, 3072]\"\n",
      "  # t_transformer_h_5_attn_c_proj_weight: \"cuda:0 f32[768, 768]\"\n",
      "  # t_transformer_h_5_mlp_c_proj_weight: \"cuda:0 f32[768, 3072]\"\n",
      "  # t_transformer_h_6_attn_c_proj_weight: \"cuda:0 f32[768, 768]\"\n",
      "  # t_transformer_h_6_mlp_c_proj_weight: \"cuda:0 f32[768, 3072]\"\n",
      "  # t_transformer_h_7_attn_c_proj_weight: \"cuda:0 f32[768, 768]\"\n",
      "  # t_transformer_h_7_mlp_c_proj_weight: \"cuda:0 f32[768, 3072]\"\n",
      "  # t_transformer_h_8_attn_c_proj_weight: \"cuda:0 f32[768, 768]\"\n",
      "  # t_transformer_h_8_mlp_c_proj_weight: \"cuda:0 f32[768, 3072]\"\n",
      "  # t_transformer_h_9_attn_c_proj_weight: \"cuda:0 f32[768, 768]\"\n",
      "  # t_transformer_h_9_mlp_c_proj_weight: \"cuda:0 f32[768, 3072]\"\n",
      "  # t_transformer_h_10_attn_c_proj_weight: \"cuda:0 f32[768, 768]\"\n",
      "  # t_transformer_h_10_mlp_c_proj_weight: \"cuda:0 f32[768, 3072]\"\n",
      "  # t_transformer_h_11_attn_c_proj_weight: \"cuda:0 f32[768, 768]\"\n",
      "  # t_transformer_h_11_mlp_c_proj_weight: \"cuda:0 f32[768, 3072]\"\n",
      "  # t_lm_head_weight: \"cuda:0 f32[50257, 768]\"\n",
      "  # t_transformer_h_0_ln_1_weight: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_10_ln_1_weight: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_11_ln_1_weight: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_1_ln_1_weight: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_2_ln_1_weight: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_3_ln_1_weight: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_4_ln_1_weight: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_5_ln_1_weight: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_6_ln_1_weight: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_7_ln_1_weight: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_8_ln_1_weight: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_9_ln_1_weight: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_0_ln_2_weight: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_1_ln_2_weight: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_2_ln_2_weight: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_3_ln_2_weight: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_4_ln_2_weight: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_5_ln_2_weight: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_6_ln_2_weight: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_7_ln_2_weight: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_8_ln_2_weight: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_9_ln_2_weight: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_10_ln_2_weight: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_h_11_ln_2_weight: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_ln_f_weight: \"cuda:0 f32[768]\"\n",
      "  # t_transformer_wpe_weight: \"cuda:0 f32[1024, 768]\"\n",
      "  # t_transformer_wte_weight: \"cuda:0 f32[50257, 768]\"\n",
      "  t925 = torch.reshape(targets, (256,))  # t925: \"cuda:0 i64[256]\"\n",
      "    # t925 = ltorch.reshape(targets, (256,))  # t925: \"cuda:0 i64[256]\"\n",
      "      # t925 = prims.reshape(targets, (256,))  # t925: \"cuda:0 i64[256]\"\n",
      "  [t0] = nvFusion0()\n",
      "    # t0 = prims.iota(64, start=0, step=1, device=devices.Device(\"cuda:0\"), dtype=dtypes.int64)  # t0: \"cuda:0 i64[64]\"\n",
      "  t3 = torch.nn.functional.embedding(idx, t_transformer_wte_weight, None, None, 2.0, False, False)  # t3: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3 = ltorch.embedding(idx, t_transformer_wte_weight, None, None, 2.0, False, False)  # t3: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t945 = ltorch.reshape(idx, [256])  # t945: \"cuda:0 i64[256]\"\n",
      "        # t945 = prims.reshape(idx, (256,))  # t945: \"cuda:0 i64[256]\"\n",
      "      # t946 = prims.take(t_transformer_wte_weight, t945, 0)  # t946: \"cuda:0 f32[256, 768]\"\n",
      "      # t3 = ltorch.reshape(t946, [4, 64, 768])  # t3: \"cuda:0 f32[4, 64, 768]\"\n",
      "        # t3 = prims.reshape(t946, (4, 64, 768))  # t3: \"cuda:0 f32[4, 64, 768]\"\n",
      "  t4 = torch.nn.functional.embedding(t0, t_transformer_wpe_weight, None, None, 2.0, False, False)  # t4: \"cuda:0 f32[64, 768]\"\n",
      "    # t4 = ltorch.embedding(t0, t_transformer_wpe_weight, None, None, 2.0, False, False)  # t4: \"cuda:0 f32[64, 768]\"\n",
      "      # t4 = prims.take(t_transformer_wpe_weight, t0, 0)  # t4: \"cuda:0 f32[64, 768]\"\n",
      "  t1042 = torch.unsqueeze(t4, 0)  # t1042: \"cuda:0 f32[1, 64, 768]\"\n",
      "    # t1042 = ltorch.unsqueeze(t4, 0)  # t1042: \"cuda:0 f32[1, 64, 768]\"\n",
      "      # t1042 = prims.broadcast_in_dim(t4, [1, 64, 768], [1, 2])  # t1042: \"cuda:0 f32[1, 64, 768]\"\n",
      "  del t4\n",
      "  t5 = Tensor.expand(t1042, (4, 64, 768))  # t5: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5 = ltorch.expand(t1042, (4, 64, 768))  # t5: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t5 = prims.broadcast_in_dim(t1042, (4, 64, 768), (0, 1, 2))  # t5: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1042\n",
      "  t1044 = torch.unsqueeze(t_transformer_h_0_ln_1_weight, 0)  # t1044: \"cuda:0 f32[1, 768]\"\n",
      "    # t1044 = ltorch.unsqueeze(t_transformer_h_0_ln_1_weight, 0)  # t1044: \"cuda:0 f32[1, 768]\"\n",
      "      # t1044 = prims.broadcast_in_dim(t_transformer_h_0_ln_1_weight, [1, 768], [1])  # t1044: \"cuda:0 f32[1, 768]\"\n",
      "  t1045 = torch.unsqueeze(t1044, 1)  # t1045: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1045 = ltorch.unsqueeze(t1044, 1)  # t1045: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1045 = prims.broadcast_in_dim(t1044, [1, 1, 768], [0, 2])  # t1045: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1044\n",
      "  t19 = Tensor.expand(t1045, (4, 64, 768))  # t19: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t19 = ltorch.expand(t1045, (4, 64, 768))  # t19: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t19 = prims.broadcast_in_dim(t1045, (4, 64, 768), (0, 1, 2))  # t19: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1045\n",
      "  t1047 = torch.unsqueeze(t_transformer_h_0_ln_1_bias, 0)  # t1047: \"cuda:0 f32[1, 768]\"\n",
      "    # t1047 = ltorch.unsqueeze(t_transformer_h_0_ln_1_bias, 0)  # t1047: \"cuda:0 f32[1, 768]\"\n",
      "      # t1047 = prims.broadcast_in_dim(t_transformer_h_0_ln_1_bias, [1, 768], [1])  # t1047: \"cuda:0 f32[1, 768]\"\n",
      "  t1048 = torch.unsqueeze(t1047, 1)  # t1048: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1048 = ltorch.unsqueeze(t1047, 1)  # t1048: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1048 = prims.broadcast_in_dim(t1047, [1, 1, 768], [0, 2])  # t1048: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1047\n",
      "  t21 = Tensor.expand(t1048, (4, 64, 768))  # t21: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t21 = ltorch.expand(t1048, (4, 64, 768))  # t21: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t21 = prims.broadcast_in_dim(t1048, (4, 64, 768), (0, 1, 2))  # t21: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1048\n",
      "  [t10, t14, t16, t22, t6] = nvFusion1(t19, t21, t3, t5)\n",
      "    # t6 = prims.add(t3, t5)  # t6: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # (t9, t10) = prims.var_mean(t6, (2,), correction=0)\n",
      "    # t11 = prims.broadcast_in_dim(t9, [4, 64, 1], [0, 1])  # t11: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t12 = prims.broadcast_in_dim(t10, [4, 64, 1], [0, 1])  # t12: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t13 = prims.add(t11, 1e-05)  # t13: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t14 = prims.rsqrt(t13)  # t14: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t15 = prims.broadcast_in_dim(t12, (4, 64, 768), (0, 1, 2))  # t15: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t16 = prims.sub(t6, t15)  # t16: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t17 = prims.broadcast_in_dim(t14, (4, 64, 768), (0, 1, 2))  # t17: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t18 = prims.mul(t16, t17)  # t18: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t20 = prims.mul(t18, t19)  # t20: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t22 = prims.add(t20, t21)  # t22: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t21, t3, t5\n",
      "  t23 = torch.nn.functional.linear(t22, t_transformer_h_0_attn_c_attn_weight, t_transformer_h_0_attn_c_attn_bias)  # t23: \"cuda:0 f32[4, 64, 2304]\"\n",
      "    # t23 = ltorch.linear(t22, t_transformer_h_0_attn_c_attn_weight, t_transformer_h_0_attn_c_attn_bias)  # t23: \"cuda:0 f32[4, 64, 2304]\"\n",
      "      # t23 = prims.linear(t22, t_transformer_h_0_attn_c_attn_weight, t_transformer_h_0_attn_c_attn_bias)  # t23: \"cuda:0 f32[4, 64, 2304]\"\n",
      "  [t24, t25, t26] = torch.split(t23, 768, 2)\n",
      "    # [t24, t25, t26] = ltorch.split(t23, 768, 2)\n",
      "      # t24 = prims.slice_prim(t23, [0, 0, 0], [4, 64, 768], [1, 1, 1])  # t24: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t25 = prims.slice_prim(t23, [0, 0, 768], [4, 64, 1536], [1, 1, 1])  # t25: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t26 = prims.slice_prim(t23, [0, 0, 1536], [4, 64, 2304], [1, 1, 1])  # t26: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t23\n",
      "  t27 = torch.reshape(t24, (4, 64, 12, 64))  # t27: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t27 = ltorch.reshape(t24, (4, 64, 12, 64))  # t27: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t27 = prims.reshape(t24, (4, 64, 12, 64))  # t27: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t24\n",
      "  t28 = torch.permute(t27, (0, 2, 1, 3))  # t28: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t28 = ltorch.permute(t27, (0, 2, 1, 3))  # t28: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t28 = prims.transpose(t27, (0, 2, 1, 3))  # t28: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t27\n",
      "  t29 = torch.reshape(t25, (4, 64, 12, 64))  # t29: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t29 = ltorch.reshape(t25, (4, 64, 12, 64))  # t29: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t29 = prims.reshape(t25, (4, 64, 12, 64))  # t29: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t25\n",
      "  t30 = torch.permute(t29, (0, 2, 1, 3))  # t30: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t30 = ltorch.permute(t29, (0, 2, 1, 3))  # t30: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t30 = prims.transpose(t29, (0, 2, 1, 3))  # t30: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t29\n",
      "  t31 = torch.reshape(t26, (4, 64, 12, 64))  # t31: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t31 = ltorch.reshape(t26, (4, 64, 12, 64))  # t31: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t31 = prims.reshape(t26, (4, 64, 12, 64))  # t31: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t26\n",
      "  t32 = torch.permute(t31, (0, 2, 1, 3))  # t32: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t32 = ltorch.permute(t31, (0, 2, 1, 3))  # t32: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t32 = prims.transpose(t31, (0, 2, 1, 3))  # t32: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t31\n",
      "  t33 = torch.permute(t30, (0, 1, 3, 2))  # t33: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t33 = ltorch.permute(t30, (0, 1, 3, 2))  # t33: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t33 = prims.transpose(t30, (0, 1, 3, 2))  # t33: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t30\n",
      "  t34 = torch.matmul(t28, t33)  # t34: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t34 = ltorch.matmul(t28, t33)  # t34: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t34 = prims.matmul(t28, t33)  # t34: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  t36 = torch_slice_prim_impl(bias, [0, 0, 0, 0], [1, 1, 64, 64], [1, 1, 1, 1])  # t36: \"cuda:0 f32[1, 1, 64, 64]\"\n",
      "  [t38, t48] = nvFusion2(t34, t36)\n",
      "    # t35 = prims.mul(t34, 0.125)  # t35: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t37 = prims.eq(t36, 0.0)  # t37: \"cuda:0 b8[1, 1, 64, 64]\"\n",
      "    # t38 = prims.broadcast_in_dim(t37, (4, 12, 64, 64), (0, 1, 2, 3))  # t38: \"cuda:0 b8[4, 12, 64, 64]\"\n",
      "    # t39 = prims.where(t38, -float('inf'), t35)  # t39: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t40 = prims.amax(t39, (3,))  # t40: \"cuda:0 f32[4, 12, 64]\"\n",
      "    # t41 = prims.broadcast_in_dim(t40, [4, 12, 64, 1], [0, 1, 2])  # t41: \"cuda:0 f32[4, 12, 64, 1]\"\n",
      "    # t42 = prims.broadcast_in_dim(t41, (4, 12, 64, 64), (0, 1, 2, 3))  # t42: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t43 = prims.sub(t39, t42)  # t43: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t44 = prims.exp(t43)  # t44: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t45 = prims.sum(t44, (3,))  # t45: \"cuda:0 f32[4, 12, 64]\"\n",
      "    # t46 = prims.broadcast_in_dim(t45, [4, 12, 64, 1], [0, 1, 2])  # t46: \"cuda:0 f32[4, 12, 64, 1]\"\n",
      "    # t47 = prims.broadcast_in_dim(t46, (4, 12, 64, 64), (0, 1, 2, 3))  # t47: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t48 = prims.div(t44, t47)  # t48: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t34, t36\n",
      "  t49 = torch.matmul(t48, t32)  # t49: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t49 = ltorch.matmul(t48, t32)  # t49: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t49 = prims.matmul(t48, t32)  # t49: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  t50 = torch.permute(t49, (0, 2, 1, 3))  # t50: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t50 = ltorch.permute(t49, (0, 2, 1, 3))  # t50: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t50 = prims.transpose(t49, (0, 2, 1, 3))  # t50: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t49\n",
      "  [t51] = nvFusion3(t50)\n",
      "    # t51 = prims.stride_order(t50, (3, 2, 1, 0))  # t51: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t50\n",
      "  t52 = torch.reshape(t51, (4, 64, 768))  # t52: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t52 = ltorch.reshape(t51, (4, 64, 768))  # t52: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t52 = prims.reshape(t51, (4, 64, 768))  # t52: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t51\n",
      "  t53 = torch.nn.functional.linear(t52, t_transformer_h_0_attn_c_proj_weight, t_transformer_h_0_attn_c_proj_bias)  # t53: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t53 = ltorch.linear(t52, t_transformer_h_0_attn_c_proj_weight, t_transformer_h_0_attn_c_proj_bias)  # t53: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t53 = prims.linear(t52, t_transformer_h_0_attn_c_proj_weight, t_transformer_h_0_attn_c_proj_bias)  # t53: \"cuda:0 f32[4, 64, 768]\"\n",
      "  t1063 = torch.unsqueeze(t_transformer_h_0_ln_2_weight, 0)  # t1063: \"cuda:0 f32[1, 768]\"\n",
      "    # t1063 = ltorch.unsqueeze(t_transformer_h_0_ln_2_weight, 0)  # t1063: \"cuda:0 f32[1, 768]\"\n",
      "      # t1063 = prims.broadcast_in_dim(t_transformer_h_0_ln_2_weight, [1, 768], [1])  # t1063: \"cuda:0 f32[1, 768]\"\n",
      "  t1064 = torch.unsqueeze(t1063, 1)  # t1064: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1064 = ltorch.unsqueeze(t1063, 1)  # t1064: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1064 = prims.broadcast_in_dim(t1063, [1, 1, 768], [0, 2])  # t1064: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1063\n",
      "  t67 = Tensor.expand(t1064, (4, 64, 768))  # t67: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t67 = ltorch.expand(t1064, (4, 64, 768))  # t67: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t67 = prims.broadcast_in_dim(t1064, (4, 64, 768), (0, 1, 2))  # t67: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1064\n",
      "  t1066 = torch.unsqueeze(t_transformer_h_0_ln_2_bias, 0)  # t1066: \"cuda:0 f32[1, 768]\"\n",
      "    # t1066 = ltorch.unsqueeze(t_transformer_h_0_ln_2_bias, 0)  # t1066: \"cuda:0 f32[1, 768]\"\n",
      "      # t1066 = prims.broadcast_in_dim(t_transformer_h_0_ln_2_bias, [1, 768], [1])  # t1066: \"cuda:0 f32[1, 768]\"\n",
      "  t1067 = torch.unsqueeze(t1066, 1)  # t1067: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1067 = ltorch.unsqueeze(t1066, 1)  # t1067: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1067 = prims.broadcast_in_dim(t1066, [1, 1, 768], [0, 2])  # t1067: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1066\n",
      "  t69 = Tensor.expand(t1067, (4, 64, 768))  # t69: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t69 = ltorch.expand(t1067, (4, 64, 768))  # t69: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t69 = prims.broadcast_in_dim(t1067, (4, 64, 768), (0, 1, 2))  # t69: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1067\n",
      "  [t54, t58, t62, t64, t70] = nvFusion4(t53, t6, t67, t69)\n",
      "    # t54 = prims.add(t6, t53)  # t54: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # (t57, t58) = prims.var_mean(t54, (2,), correction=0)\n",
      "    # t59 = prims.broadcast_in_dim(t57, [4, 64, 1], [0, 1])  # t59: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t60 = prims.broadcast_in_dim(t58, [4, 64, 1], [0, 1])  # t60: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t61 = prims.add(t59, 1e-05)  # t61: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t62 = prims.rsqrt(t61)  # t62: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t63 = prims.broadcast_in_dim(t60, (4, 64, 768), (0, 1, 2))  # t63: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t64 = prims.sub(t54, t63)  # t64: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t65 = prims.broadcast_in_dim(t62, (4, 64, 768), (0, 1, 2))  # t65: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t66 = prims.mul(t64, t65)  # t66: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t68 = prims.mul(t66, t67)  # t68: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t70 = prims.add(t68, t69)  # t70: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t53, t69\n",
      "  t71 = torch.nn.functional.linear(t70, t_transformer_h_0_mlp_c_fc_weight, t_transformer_h_0_mlp_c_fc_bias)  # t71: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t71 = ltorch.linear(t70, t_transformer_h_0_mlp_c_fc_weight, t_transformer_h_0_mlp_c_fc_bias)  # t71: \"cuda:0 f32[4, 64, 3072]\"\n",
      "      # t71 = prims.linear(t70, t_transformer_h_0_mlp_c_fc_weight, t_transformer_h_0_mlp_c_fc_bias)  # t71: \"cuda:0 f32[4, 64, 3072]\"\n",
      "  [t79] = nvFusion5(t71)\n",
      "    # t72 = prims.mul(0.5, t71)  # t72: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t73 = prims.pow(t71, 3.0)  # t73: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t74 = prims.mul(0.044715, t73)  # t74: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t75 = prims.add(t71, t74)  # t75: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t76 = prims.mul(0.7978845608028654, t75)  # t76: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t77 = prims.tanh(t76)  # t77: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t78 = prims.add(1.0, t77)  # t78: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t79 = prims.mul(t72, t78)  # t79: \"cuda:0 f32[4, 64, 3072]\"\n",
      "  t80 = torch.nn.functional.linear(t79, t_transformer_h_0_mlp_c_proj_weight, t_transformer_h_0_mlp_c_proj_bias)  # t80: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t80 = ltorch.linear(t79, t_transformer_h_0_mlp_c_proj_weight, t_transformer_h_0_mlp_c_proj_bias)  # t80: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t80 = prims.linear(t79, t_transformer_h_0_mlp_c_proj_weight, t_transformer_h_0_mlp_c_proj_bias)  # t80: \"cuda:0 f32[4, 64, 768]\"\n",
      "  t1069 = torch.unsqueeze(t_transformer_h_1_ln_1_weight, 0)  # t1069: \"cuda:0 f32[1, 768]\"\n",
      "    # t1069 = ltorch.unsqueeze(t_transformer_h_1_ln_1_weight, 0)  # t1069: \"cuda:0 f32[1, 768]\"\n",
      "      # t1069 = prims.broadcast_in_dim(t_transformer_h_1_ln_1_weight, [1, 768], [1])  # t1069: \"cuda:0 f32[1, 768]\"\n",
      "  t1070 = torch.unsqueeze(t1069, 1)  # t1070: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1070 = ltorch.unsqueeze(t1069, 1)  # t1070: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1070 = prims.broadcast_in_dim(t1069, [1, 1, 768], [0, 2])  # t1070: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1069\n",
      "  t94 = Tensor.expand(t1070, (4, 64, 768))  # t94: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t94 = ltorch.expand(t1070, (4, 64, 768))  # t94: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t94 = prims.broadcast_in_dim(t1070, (4, 64, 768), (0, 1, 2))  # t94: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1070\n",
      "  t1072 = torch.unsqueeze(t_transformer_h_1_ln_1_bias, 0)  # t1072: \"cuda:0 f32[1, 768]\"\n",
      "    # t1072 = ltorch.unsqueeze(t_transformer_h_1_ln_1_bias, 0)  # t1072: \"cuda:0 f32[1, 768]\"\n",
      "      # t1072 = prims.broadcast_in_dim(t_transformer_h_1_ln_1_bias, [1, 768], [1])  # t1072: \"cuda:0 f32[1, 768]\"\n",
      "  t1073 = torch.unsqueeze(t1072, 1)  # t1073: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1073 = ltorch.unsqueeze(t1072, 1)  # t1073: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1073 = prims.broadcast_in_dim(t1072, [1, 1, 768], [0, 2])  # t1073: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1072\n",
      "  t96 = Tensor.expand(t1073, (4, 64, 768))  # t96: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t96 = ltorch.expand(t1073, (4, 64, 768))  # t96: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t96 = prims.broadcast_in_dim(t1073, (4, 64, 768), (0, 1, 2))  # t96: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1073\n",
      "  [t81, t85, t89, t91, t97] = nvFusion6(t54, t80, t94, t96)\n",
      "    # t81 = prims.add(t54, t80)  # t81: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # (t84, t85) = prims.var_mean(t81, (2,), correction=0)\n",
      "    # t86 = prims.broadcast_in_dim(t84, [4, 64, 1], [0, 1])  # t86: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t87 = prims.broadcast_in_dim(t85, [4, 64, 1], [0, 1])  # t87: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t88 = prims.add(t86, 1e-05)  # t88: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t89 = prims.rsqrt(t88)  # t89: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t90 = prims.broadcast_in_dim(t87, (4, 64, 768), (0, 1, 2))  # t90: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t91 = prims.sub(t81, t90)  # t91: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t92 = prims.broadcast_in_dim(t89, (4, 64, 768), (0, 1, 2))  # t92: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t93 = prims.mul(t91, t92)  # t93: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t95 = prims.mul(t93, t94)  # t95: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t97 = prims.add(t95, t96)  # t97: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t80, t96\n",
      "  t98 = torch.nn.functional.linear(t97, t_transformer_h_1_attn_c_attn_weight, t_transformer_h_1_attn_c_attn_bias)  # t98: \"cuda:0 f32[4, 64, 2304]\"\n",
      "    # t98 = ltorch.linear(t97, t_transformer_h_1_attn_c_attn_weight, t_transformer_h_1_attn_c_attn_bias)  # t98: \"cuda:0 f32[4, 64, 2304]\"\n",
      "      # t98 = prims.linear(t97, t_transformer_h_1_attn_c_attn_weight, t_transformer_h_1_attn_c_attn_bias)  # t98: \"cuda:0 f32[4, 64, 2304]\"\n",
      "  [t99, t100, t101] = torch.split(t98, 768, 2)\n",
      "    # [t99, t100, t101] = ltorch.split(t98, 768, 2)\n",
      "      # t99 = prims.slice_prim(t98, [0, 0, 0], [4, 64, 768], [1, 1, 1])  # t99: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t100 = prims.slice_prim(t98, [0, 0, 768], [4, 64, 1536], [1, 1, 1])  # t100: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t101 = prims.slice_prim(t98, [0, 0, 1536], [4, 64, 2304], [1, 1, 1])  # t101: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t98\n",
      "  t102 = torch.reshape(t99, (4, 64, 12, 64))  # t102: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t102 = ltorch.reshape(t99, (4, 64, 12, 64))  # t102: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t102 = prims.reshape(t99, (4, 64, 12, 64))  # t102: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t99\n",
      "  t103 = torch.permute(t102, (0, 2, 1, 3))  # t103: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t103 = ltorch.permute(t102, (0, 2, 1, 3))  # t103: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t103 = prims.transpose(t102, (0, 2, 1, 3))  # t103: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t102\n",
      "  t104 = torch.reshape(t100, (4, 64, 12, 64))  # t104: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t104 = ltorch.reshape(t100, (4, 64, 12, 64))  # t104: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t104 = prims.reshape(t100, (4, 64, 12, 64))  # t104: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t100\n",
      "  t105 = torch.permute(t104, (0, 2, 1, 3))  # t105: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t105 = ltorch.permute(t104, (0, 2, 1, 3))  # t105: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t105 = prims.transpose(t104, (0, 2, 1, 3))  # t105: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t104\n",
      "  t106 = torch.reshape(t101, (4, 64, 12, 64))  # t106: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t106 = ltorch.reshape(t101, (4, 64, 12, 64))  # t106: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t106 = prims.reshape(t101, (4, 64, 12, 64))  # t106: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t101\n",
      "  t107 = torch.permute(t106, (0, 2, 1, 3))  # t107: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t107 = ltorch.permute(t106, (0, 2, 1, 3))  # t107: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t107 = prims.transpose(t106, (0, 2, 1, 3))  # t107: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t106\n",
      "  t108 = torch.permute(t105, (0, 1, 3, 2))  # t108: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t108 = ltorch.permute(t105, (0, 1, 3, 2))  # t108: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t108 = prims.transpose(t105, (0, 1, 3, 2))  # t108: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t105\n",
      "  t109 = torch.matmul(t103, t108)  # t109: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t109 = ltorch.matmul(t103, t108)  # t109: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t109 = prims.matmul(t103, t108)  # t109: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  t111 = torch_slice_prim_impl(tos1, [0, 0, 0, 0], [1, 1, 64, 64], [1, 1, 1, 1])  # t111: \"cuda:0 f32[1, 1, 64, 64]\"\n",
      "  [t113, t123] = nvFusion7(t109, t111)\n",
      "    # t110 = prims.mul(t109, 0.125)  # t110: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t112 = prims.eq(t111, 0.0)  # t112: \"cuda:0 b8[1, 1, 64, 64]\"\n",
      "    # t113 = prims.broadcast_in_dim(t112, (4, 12, 64, 64), (0, 1, 2, 3))  # t113: \"cuda:0 b8[4, 12, 64, 64]\"\n",
      "    # t114 = prims.where(t113, -float('inf'), t110)  # t114: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t115 = prims.amax(t114, (3,))  # t115: \"cuda:0 f32[4, 12, 64]\"\n",
      "    # t116 = prims.broadcast_in_dim(t115, [4, 12, 64, 1], [0, 1, 2])  # t116: \"cuda:0 f32[4, 12, 64, 1]\"\n",
      "    # t117 = prims.broadcast_in_dim(t116, (4, 12, 64, 64), (0, 1, 2, 3))  # t117: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t118 = prims.sub(t114, t117)  # t118: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t119 = prims.exp(t118)  # t119: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t120 = prims.sum(t119, (3,))  # t120: \"cuda:0 f32[4, 12, 64]\"\n",
      "    # t121 = prims.broadcast_in_dim(t120, [4, 12, 64, 1], [0, 1, 2])  # t121: \"cuda:0 f32[4, 12, 64, 1]\"\n",
      "    # t122 = prims.broadcast_in_dim(t121, (4, 12, 64, 64), (0, 1, 2, 3))  # t122: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t123 = prims.div(t119, t122)  # t123: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t109, t111\n",
      "  t124 = torch.matmul(t123, t107)  # t124: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t124 = ltorch.matmul(t123, t107)  # t124: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t124 = prims.matmul(t123, t107)  # t124: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  t125 = torch.permute(t124, (0, 2, 1, 3))  # t125: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t125 = ltorch.permute(t124, (0, 2, 1, 3))  # t125: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t125 = prims.transpose(t124, (0, 2, 1, 3))  # t125: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t124\n",
      "  [t126] = nvFusion8(t125)\n",
      "    # t126 = prims.stride_order(t125, (3, 2, 1, 0))  # t126: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t125\n",
      "  t127 = torch.reshape(t126, (4, 64, 768))  # t127: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t127 = ltorch.reshape(t126, (4, 64, 768))  # t127: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t127 = prims.reshape(t126, (4, 64, 768))  # t127: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t126\n",
      "  t128 = torch.nn.functional.linear(t127, t_transformer_h_1_attn_c_proj_weight, t_transformer_h_1_attn_c_proj_bias)  # t128: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t128 = ltorch.linear(t127, t_transformer_h_1_attn_c_proj_weight, t_transformer_h_1_attn_c_proj_bias)  # t128: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t128 = prims.linear(t127, t_transformer_h_1_attn_c_proj_weight, t_transformer_h_1_attn_c_proj_bias)  # t128: \"cuda:0 f32[4, 64, 768]\"\n",
      "  t1088 = torch.unsqueeze(t_transformer_h_1_ln_2_weight, 0)  # t1088: \"cuda:0 f32[1, 768]\"\n",
      "    # t1088 = ltorch.unsqueeze(t_transformer_h_1_ln_2_weight, 0)  # t1088: \"cuda:0 f32[1, 768]\"\n",
      "      # t1088 = prims.broadcast_in_dim(t_transformer_h_1_ln_2_weight, [1, 768], [1])  # t1088: \"cuda:0 f32[1, 768]\"\n",
      "  t1089 = torch.unsqueeze(t1088, 1)  # t1089: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1089 = ltorch.unsqueeze(t1088, 1)  # t1089: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1089 = prims.broadcast_in_dim(t1088, [1, 1, 768], [0, 2])  # t1089: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1088\n",
      "  t142 = Tensor.expand(t1089, (4, 64, 768))  # t142: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t142 = ltorch.expand(t1089, (4, 64, 768))  # t142: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t142 = prims.broadcast_in_dim(t1089, (4, 64, 768), (0, 1, 2))  # t142: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1089\n",
      "  t1091 = torch.unsqueeze(t_transformer_h_1_ln_2_bias, 0)  # t1091: \"cuda:0 f32[1, 768]\"\n",
      "    # t1091 = ltorch.unsqueeze(t_transformer_h_1_ln_2_bias, 0)  # t1091: \"cuda:0 f32[1, 768]\"\n",
      "      # t1091 = prims.broadcast_in_dim(t_transformer_h_1_ln_2_bias, [1, 768], [1])  # t1091: \"cuda:0 f32[1, 768]\"\n",
      "  t1092 = torch.unsqueeze(t1091, 1)  # t1092: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1092 = ltorch.unsqueeze(t1091, 1)  # t1092: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1092 = prims.broadcast_in_dim(t1091, [1, 1, 768], [0, 2])  # t1092: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1091\n",
      "  t144 = Tensor.expand(t1092, (4, 64, 768))  # t144: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t144 = ltorch.expand(t1092, (4, 64, 768))  # t144: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t144 = prims.broadcast_in_dim(t1092, (4, 64, 768), (0, 1, 2))  # t144: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1092\n",
      "  [t129, t133, t137, t139, t145] = nvFusion9(t128, t142, t144, t81)\n",
      "    # t129 = prims.add(t81, t128)  # t129: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # (t132, t133) = prims.var_mean(t129, (2,), correction=0)\n",
      "    # t134 = prims.broadcast_in_dim(t132, [4, 64, 1], [0, 1])  # t134: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t135 = prims.broadcast_in_dim(t133, [4, 64, 1], [0, 1])  # t135: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t136 = prims.add(t134, 1e-05)  # t136: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t137 = prims.rsqrt(t136)  # t137: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t138 = prims.broadcast_in_dim(t135, (4, 64, 768), (0, 1, 2))  # t138: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t139 = prims.sub(t129, t138)  # t139: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t140 = prims.broadcast_in_dim(t137, (4, 64, 768), (0, 1, 2))  # t140: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t141 = prims.mul(t139, t140)  # t141: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t143 = prims.mul(t141, t142)  # t143: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t145 = prims.add(t143, t144)  # t145: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t128, t144\n",
      "  t146 = torch.nn.functional.linear(t145, t_transformer_h_1_mlp_c_fc_weight, t_transformer_h_1_mlp_c_fc_bias)  # t146: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t146 = ltorch.linear(t145, t_transformer_h_1_mlp_c_fc_weight, t_transformer_h_1_mlp_c_fc_bias)  # t146: \"cuda:0 f32[4, 64, 3072]\"\n",
      "      # t146 = prims.linear(t145, t_transformer_h_1_mlp_c_fc_weight, t_transformer_h_1_mlp_c_fc_bias)  # t146: \"cuda:0 f32[4, 64, 3072]\"\n",
      "  [t154] = nvFusion10(t146)\n",
      "    # t147 = prims.mul(0.5, t146)  # t147: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t148 = prims.pow(t146, 3.0)  # t148: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t149 = prims.mul(0.044715, t148)  # t149: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t150 = prims.add(t146, t149)  # t150: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t151 = prims.mul(0.7978845608028654, t150)  # t151: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t152 = prims.tanh(t151)  # t152: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t153 = prims.add(1.0, t152)  # t153: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t154 = prims.mul(t147, t153)  # t154: \"cuda:0 f32[4, 64, 3072]\"\n",
      "  t155 = torch.nn.functional.linear(t154, t_transformer_h_1_mlp_c_proj_weight, t_transformer_h_1_mlp_c_proj_bias)  # t155: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t155 = ltorch.linear(t154, t_transformer_h_1_mlp_c_proj_weight, t_transformer_h_1_mlp_c_proj_bias)  # t155: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t155 = prims.linear(t154, t_transformer_h_1_mlp_c_proj_weight, t_transformer_h_1_mlp_c_proj_bias)  # t155: \"cuda:0 f32[4, 64, 768]\"\n",
      "  t1094 = torch.unsqueeze(t_transformer_h_2_ln_1_weight, 0)  # t1094: \"cuda:0 f32[1, 768]\"\n",
      "    # t1094 = ltorch.unsqueeze(t_transformer_h_2_ln_1_weight, 0)  # t1094: \"cuda:0 f32[1, 768]\"\n",
      "      # t1094 = prims.broadcast_in_dim(t_transformer_h_2_ln_1_weight, [1, 768], [1])  # t1094: \"cuda:0 f32[1, 768]\"\n",
      "  t1095 = torch.unsqueeze(t1094, 1)  # t1095: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1095 = ltorch.unsqueeze(t1094, 1)  # t1095: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1095 = prims.broadcast_in_dim(t1094, [1, 1, 768], [0, 2])  # t1095: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1094\n",
      "  t169 = Tensor.expand(t1095, (4, 64, 768))  # t169: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t169 = ltorch.expand(t1095, (4, 64, 768))  # t169: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t169 = prims.broadcast_in_dim(t1095, (4, 64, 768), (0, 1, 2))  # t169: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1095\n",
      "  t1097 = torch.unsqueeze(t_transformer_h_2_ln_1_bias, 0)  # t1097: \"cuda:0 f32[1, 768]\"\n",
      "    # t1097 = ltorch.unsqueeze(t_transformer_h_2_ln_1_bias, 0)  # t1097: \"cuda:0 f32[1, 768]\"\n",
      "      # t1097 = prims.broadcast_in_dim(t_transformer_h_2_ln_1_bias, [1, 768], [1])  # t1097: \"cuda:0 f32[1, 768]\"\n",
      "  t1098 = torch.unsqueeze(t1097, 1)  # t1098: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1098 = ltorch.unsqueeze(t1097, 1)  # t1098: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1098 = prims.broadcast_in_dim(t1097, [1, 1, 768], [0, 2])  # t1098: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1097\n",
      "  t171 = Tensor.expand(t1098, (4, 64, 768))  # t171: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t171 = ltorch.expand(t1098, (4, 64, 768))  # t171: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t171 = prims.broadcast_in_dim(t1098, (4, 64, 768), (0, 1, 2))  # t171: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1098\n",
      "  [t156, t160, t164, t166, t172] = nvFusion11(t129, t155, t169, t171)\n",
      "    # t156 = prims.add(t129, t155)  # t156: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # (t159, t160) = prims.var_mean(t156, (2,), correction=0)\n",
      "    # t161 = prims.broadcast_in_dim(t159, [4, 64, 1], [0, 1])  # t161: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t162 = prims.broadcast_in_dim(t160, [4, 64, 1], [0, 1])  # t162: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t163 = prims.add(t161, 1e-05)  # t163: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t164 = prims.rsqrt(t163)  # t164: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t165 = prims.broadcast_in_dim(t162, (4, 64, 768), (0, 1, 2))  # t165: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t166 = prims.sub(t156, t165)  # t166: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t167 = prims.broadcast_in_dim(t164, (4, 64, 768), (0, 1, 2))  # t167: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t168 = prims.mul(t166, t167)  # t168: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t170 = prims.mul(t168, t169)  # t170: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t172 = prims.add(t170, t171)  # t172: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t155, t171\n",
      "  t173 = torch.nn.functional.linear(t172, t_transformer_h_2_attn_c_attn_weight, t_transformer_h_2_attn_c_attn_bias)  # t173: \"cuda:0 f32[4, 64, 2304]\"\n",
      "    # t173 = ltorch.linear(t172, t_transformer_h_2_attn_c_attn_weight, t_transformer_h_2_attn_c_attn_bias)  # t173: \"cuda:0 f32[4, 64, 2304]\"\n",
      "      # t173 = prims.linear(t172, t_transformer_h_2_attn_c_attn_weight, t_transformer_h_2_attn_c_attn_bias)  # t173: \"cuda:0 f32[4, 64, 2304]\"\n",
      "  [t174, t175, t176] = torch.split(t173, 768, 2)\n",
      "    # [t174, t175, t176] = ltorch.split(t173, 768, 2)\n",
      "      # t174 = prims.slice_prim(t173, [0, 0, 0], [4, 64, 768], [1, 1, 1])  # t174: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t175 = prims.slice_prim(t173, [0, 0, 768], [4, 64, 1536], [1, 1, 1])  # t175: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t176 = prims.slice_prim(t173, [0, 0, 1536], [4, 64, 2304], [1, 1, 1])  # t176: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t173\n",
      "  t177 = torch.reshape(t174, (4, 64, 12, 64))  # t177: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t177 = ltorch.reshape(t174, (4, 64, 12, 64))  # t177: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t177 = prims.reshape(t174, (4, 64, 12, 64))  # t177: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t174\n",
      "  t178 = torch.permute(t177, (0, 2, 1, 3))  # t178: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t178 = ltorch.permute(t177, (0, 2, 1, 3))  # t178: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t178 = prims.transpose(t177, (0, 2, 1, 3))  # t178: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t177\n",
      "  t179 = torch.reshape(t175, (4, 64, 12, 64))  # t179: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t179 = ltorch.reshape(t175, (4, 64, 12, 64))  # t179: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t179 = prims.reshape(t175, (4, 64, 12, 64))  # t179: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t175\n",
      "  t180 = torch.permute(t179, (0, 2, 1, 3))  # t180: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t180 = ltorch.permute(t179, (0, 2, 1, 3))  # t180: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t180 = prims.transpose(t179, (0, 2, 1, 3))  # t180: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t179\n",
      "  t181 = torch.reshape(t176, (4, 64, 12, 64))  # t181: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t181 = ltorch.reshape(t176, (4, 64, 12, 64))  # t181: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t181 = prims.reshape(t176, (4, 64, 12, 64))  # t181: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t176\n",
      "  t182 = torch.permute(t181, (0, 2, 1, 3))  # t182: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t182 = ltorch.permute(t181, (0, 2, 1, 3))  # t182: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t182 = prims.transpose(t181, (0, 2, 1, 3))  # t182: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t181\n",
      "  t183 = torch.permute(t180, (0, 1, 3, 2))  # t183: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t183 = ltorch.permute(t180, (0, 1, 3, 2))  # t183: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t183 = prims.transpose(t180, (0, 1, 3, 2))  # t183: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t180\n",
      "  t184 = torch.matmul(t178, t183)  # t184: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t184 = ltorch.matmul(t178, t183)  # t184: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t184 = prims.matmul(t178, t183)  # t184: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  t186 = torch_slice_prim_impl(t_transformer_h_2_attn_bias, [0, 0, 0, 0], [1, 1, 64, 64], [1, 1, 1, 1])  # t186: \"cuda:0 f32[1, 1, 64, 64]\"\n",
      "  [t188, t198] = nvFusion12(t184, t186)\n",
      "    # t185 = prims.mul(t184, 0.125)  # t185: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t187 = prims.eq(t186, 0.0)  # t187: \"cuda:0 b8[1, 1, 64, 64]\"\n",
      "    # t188 = prims.broadcast_in_dim(t187, (4, 12, 64, 64), (0, 1, 2, 3))  # t188: \"cuda:0 b8[4, 12, 64, 64]\"\n",
      "    # t189 = prims.where(t188, -float('inf'), t185)  # t189: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t190 = prims.amax(t189, (3,))  # t190: \"cuda:0 f32[4, 12, 64]\"\n",
      "    # t191 = prims.broadcast_in_dim(t190, [4, 12, 64, 1], [0, 1, 2])  # t191: \"cuda:0 f32[4, 12, 64, 1]\"\n",
      "    # t192 = prims.broadcast_in_dim(t191, (4, 12, 64, 64), (0, 1, 2, 3))  # t192: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t193 = prims.sub(t189, t192)  # t193: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t194 = prims.exp(t193)  # t194: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t195 = prims.sum(t194, (3,))  # t195: \"cuda:0 f32[4, 12, 64]\"\n",
      "    # t196 = prims.broadcast_in_dim(t195, [4, 12, 64, 1], [0, 1, 2])  # t196: \"cuda:0 f32[4, 12, 64, 1]\"\n",
      "    # t197 = prims.broadcast_in_dim(t196, (4, 12, 64, 64), (0, 1, 2, 3))  # t197: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t198 = prims.div(t194, t197)  # t198: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t184, t186\n",
      "  t199 = torch.matmul(t198, t182)  # t199: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t199 = ltorch.matmul(t198, t182)  # t199: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t199 = prims.matmul(t198, t182)  # t199: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  t200 = torch.permute(t199, (0, 2, 1, 3))  # t200: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t200 = ltorch.permute(t199, (0, 2, 1, 3))  # t200: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t200 = prims.transpose(t199, (0, 2, 1, 3))  # t200: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t199\n",
      "  [t201] = nvFusion13(t200)\n",
      "    # t201 = prims.stride_order(t200, (3, 2, 1, 0))  # t201: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t200\n",
      "  t202 = torch.reshape(t201, (4, 64, 768))  # t202: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t202 = ltorch.reshape(t201, (4, 64, 768))  # t202: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t202 = prims.reshape(t201, (4, 64, 768))  # t202: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t201\n",
      "  t203 = torch.nn.functional.linear(t202, t_transformer_h_2_attn_c_proj_weight, t_transformer_h_2_attn_c_proj_bias)  # t203: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t203 = ltorch.linear(t202, t_transformer_h_2_attn_c_proj_weight, t_transformer_h_2_attn_c_proj_bias)  # t203: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t203 = prims.linear(t202, t_transformer_h_2_attn_c_proj_weight, t_transformer_h_2_attn_c_proj_bias)  # t203: \"cuda:0 f32[4, 64, 768]\"\n",
      "  t1113 = torch.unsqueeze(t_transformer_h_2_ln_2_weight, 0)  # t1113: \"cuda:0 f32[1, 768]\"\n",
      "    # t1113 = ltorch.unsqueeze(t_transformer_h_2_ln_2_weight, 0)  # t1113: \"cuda:0 f32[1, 768]\"\n",
      "      # t1113 = prims.broadcast_in_dim(t_transformer_h_2_ln_2_weight, [1, 768], [1])  # t1113: \"cuda:0 f32[1, 768]\"\n",
      "  t1114 = torch.unsqueeze(t1113, 1)  # t1114: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1114 = ltorch.unsqueeze(t1113, 1)  # t1114: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1114 = prims.broadcast_in_dim(t1113, [1, 1, 768], [0, 2])  # t1114: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1113\n",
      "  t217 = Tensor.expand(t1114, (4, 64, 768))  # t217: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t217 = ltorch.expand(t1114, (4, 64, 768))  # t217: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t217 = prims.broadcast_in_dim(t1114, (4, 64, 768), (0, 1, 2))  # t217: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1114\n",
      "  t1116 = torch.unsqueeze(t_transformer_h_2_ln_2_bias, 0)  # t1116: \"cuda:0 f32[1, 768]\"\n",
      "    # t1116 = ltorch.unsqueeze(t_transformer_h_2_ln_2_bias, 0)  # t1116: \"cuda:0 f32[1, 768]\"\n",
      "      # t1116 = prims.broadcast_in_dim(t_transformer_h_2_ln_2_bias, [1, 768], [1])  # t1116: \"cuda:0 f32[1, 768]\"\n",
      "  t1117 = torch.unsqueeze(t1116, 1)  # t1117: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1117 = ltorch.unsqueeze(t1116, 1)  # t1117: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1117 = prims.broadcast_in_dim(t1116, [1, 1, 768], [0, 2])  # t1117: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1116\n",
      "  t219 = Tensor.expand(t1117, (4, 64, 768))  # t219: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t219 = ltorch.expand(t1117, (4, 64, 768))  # t219: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t219 = prims.broadcast_in_dim(t1117, (4, 64, 768), (0, 1, 2))  # t219: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1117\n",
      "  [t204, t208, t212, t214, t220] = nvFusion14(t156, t203, t217, t219)\n",
      "    # t204 = prims.add(t156, t203)  # t204: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # (t207, t208) = prims.var_mean(t204, (2,), correction=0)\n",
      "    # t209 = prims.broadcast_in_dim(t207, [4, 64, 1], [0, 1])  # t209: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t210 = prims.broadcast_in_dim(t208, [4, 64, 1], [0, 1])  # t210: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t211 = prims.add(t209, 1e-05)  # t211: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t212 = prims.rsqrt(t211)  # t212: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t213 = prims.broadcast_in_dim(t210, (4, 64, 768), (0, 1, 2))  # t213: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t214 = prims.sub(t204, t213)  # t214: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t215 = prims.broadcast_in_dim(t212, (4, 64, 768), (0, 1, 2))  # t215: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t216 = prims.mul(t214, t215)  # t216: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t218 = prims.mul(t216, t217)  # t218: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t220 = prims.add(t218, t219)  # t220: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t203, t219\n",
      "  t221 = torch.nn.functional.linear(t220, t_transformer_h_2_mlp_c_fc_weight, t_transformer_h_2_mlp_c_fc_bias)  # t221: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t221 = ltorch.linear(t220, t_transformer_h_2_mlp_c_fc_weight, t_transformer_h_2_mlp_c_fc_bias)  # t221: \"cuda:0 f32[4, 64, 3072]\"\n",
      "      # t221 = prims.linear(t220, t_transformer_h_2_mlp_c_fc_weight, t_transformer_h_2_mlp_c_fc_bias)  # t221: \"cuda:0 f32[4, 64, 3072]\"\n",
      "  [t229] = nvFusion15(t221)\n",
      "    # t222 = prims.mul(0.5, t221)  # t222: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t223 = prims.pow(t221, 3.0)  # t223: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t224 = prims.mul(0.044715, t223)  # t224: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t225 = prims.add(t221, t224)  # t225: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t226 = prims.mul(0.7978845608028654, t225)  # t226: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t227 = prims.tanh(t226)  # t227: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t228 = prims.add(1.0, t227)  # t228: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t229 = prims.mul(t222, t228)  # t229: \"cuda:0 f32[4, 64, 3072]\"\n",
      "  t230 = torch.nn.functional.linear(t229, t_transformer_h_2_mlp_c_proj_weight, t_transformer_h_2_mlp_c_proj_bias)  # t230: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t230 = ltorch.linear(t229, t_transformer_h_2_mlp_c_proj_weight, t_transformer_h_2_mlp_c_proj_bias)  # t230: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t230 = prims.linear(t229, t_transformer_h_2_mlp_c_proj_weight, t_transformer_h_2_mlp_c_proj_bias)  # t230: \"cuda:0 f32[4, 64, 768]\"\n",
      "  t1119 = torch.unsqueeze(t_transformer_h_3_ln_1_weight, 0)  # t1119: \"cuda:0 f32[1, 768]\"\n",
      "    # t1119 = ltorch.unsqueeze(t_transformer_h_3_ln_1_weight, 0)  # t1119: \"cuda:0 f32[1, 768]\"\n",
      "      # t1119 = prims.broadcast_in_dim(t_transformer_h_3_ln_1_weight, [1, 768], [1])  # t1119: \"cuda:0 f32[1, 768]\"\n",
      "  t1120 = torch.unsqueeze(t1119, 1)  # t1120: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1120 = ltorch.unsqueeze(t1119, 1)  # t1120: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1120 = prims.broadcast_in_dim(t1119, [1, 1, 768], [0, 2])  # t1120: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1119\n",
      "  t244 = Tensor.expand(t1120, (4, 64, 768))  # t244: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t244 = ltorch.expand(t1120, (4, 64, 768))  # t244: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t244 = prims.broadcast_in_dim(t1120, (4, 64, 768), (0, 1, 2))  # t244: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1120\n",
      "  t1122 = torch.unsqueeze(t_transformer_h_3_ln_1_bias, 0)  # t1122: \"cuda:0 f32[1, 768]\"\n",
      "    # t1122 = ltorch.unsqueeze(t_transformer_h_3_ln_1_bias, 0)  # t1122: \"cuda:0 f32[1, 768]\"\n",
      "      # t1122 = prims.broadcast_in_dim(t_transformer_h_3_ln_1_bias, [1, 768], [1])  # t1122: \"cuda:0 f32[1, 768]\"\n",
      "  t1123 = torch.unsqueeze(t1122, 1)  # t1123: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1123 = ltorch.unsqueeze(t1122, 1)  # t1123: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1123 = prims.broadcast_in_dim(t1122, [1, 1, 768], [0, 2])  # t1123: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1122\n",
      "  t246 = Tensor.expand(t1123, (4, 64, 768))  # t246: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t246 = ltorch.expand(t1123, (4, 64, 768))  # t246: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t246 = prims.broadcast_in_dim(t1123, (4, 64, 768), (0, 1, 2))  # t246: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1123\n",
      "  [t231, t235, t239, t241, t247] = nvFusion16(t204, t230, t244, t246)\n",
      "    # t231 = prims.add(t204, t230)  # t231: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # (t234, t235) = prims.var_mean(t231, (2,), correction=0)\n",
      "    # t236 = prims.broadcast_in_dim(t234, [4, 64, 1], [0, 1])  # t236: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t237 = prims.broadcast_in_dim(t235, [4, 64, 1], [0, 1])  # t237: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t238 = prims.add(t236, 1e-05)  # t238: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t239 = prims.rsqrt(t238)  # t239: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t240 = prims.broadcast_in_dim(t237, (4, 64, 768), (0, 1, 2))  # t240: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t241 = prims.sub(t231, t240)  # t241: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t242 = prims.broadcast_in_dim(t239, (4, 64, 768), (0, 1, 2))  # t242: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t243 = prims.mul(t241, t242)  # t243: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t245 = prims.mul(t243, t244)  # t245: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t247 = prims.add(t245, t246)  # t247: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t230, t246\n",
      "  t248 = torch.nn.functional.linear(t247, t_transformer_h_3_attn_c_attn_weight, t_transformer_h_3_attn_c_attn_bias)  # t248: \"cuda:0 f32[4, 64, 2304]\"\n",
      "    # t248 = ltorch.linear(t247, t_transformer_h_3_attn_c_attn_weight, t_transformer_h_3_attn_c_attn_bias)  # t248: \"cuda:0 f32[4, 64, 2304]\"\n",
      "      # t248 = prims.linear(t247, t_transformer_h_3_attn_c_attn_weight, t_transformer_h_3_attn_c_attn_bias)  # t248: \"cuda:0 f32[4, 64, 2304]\"\n",
      "  [t249, t250, t251] = torch.split(t248, 768, 2)\n",
      "    # [t249, t250, t251] = ltorch.split(t248, 768, 2)\n",
      "      # t249 = prims.slice_prim(t248, [0, 0, 0], [4, 64, 768], [1, 1, 1])  # t249: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t250 = prims.slice_prim(t248, [0, 0, 768], [4, 64, 1536], [1, 1, 1])  # t250: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t251 = prims.slice_prim(t248, [0, 0, 1536], [4, 64, 2304], [1, 1, 1])  # t251: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t248\n",
      "  t252 = torch.reshape(t249, (4, 64, 12, 64))  # t252: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t252 = ltorch.reshape(t249, (4, 64, 12, 64))  # t252: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t252 = prims.reshape(t249, (4, 64, 12, 64))  # t252: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t249\n",
      "  t253 = torch.permute(t252, (0, 2, 1, 3))  # t253: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t253 = ltorch.permute(t252, (0, 2, 1, 3))  # t253: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t253 = prims.transpose(t252, (0, 2, 1, 3))  # t253: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t252\n",
      "  t254 = torch.reshape(t250, (4, 64, 12, 64))  # t254: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t254 = ltorch.reshape(t250, (4, 64, 12, 64))  # t254: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t254 = prims.reshape(t250, (4, 64, 12, 64))  # t254: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t250\n",
      "  t255 = torch.permute(t254, (0, 2, 1, 3))  # t255: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t255 = ltorch.permute(t254, (0, 2, 1, 3))  # t255: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t255 = prims.transpose(t254, (0, 2, 1, 3))  # t255: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t254\n",
      "  t256 = torch.reshape(t251, (4, 64, 12, 64))  # t256: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t256 = ltorch.reshape(t251, (4, 64, 12, 64))  # t256: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t256 = prims.reshape(t251, (4, 64, 12, 64))  # t256: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t251\n",
      "  t257 = torch.permute(t256, (0, 2, 1, 3))  # t257: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t257 = ltorch.permute(t256, (0, 2, 1, 3))  # t257: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t257 = prims.transpose(t256, (0, 2, 1, 3))  # t257: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t256\n",
      "  t258 = torch.permute(t255, (0, 1, 3, 2))  # t258: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t258 = ltorch.permute(t255, (0, 1, 3, 2))  # t258: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t258 = prims.transpose(t255, (0, 1, 3, 2))  # t258: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t255\n",
      "  t259 = torch.matmul(t253, t258)  # t259: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t259 = ltorch.matmul(t253, t258)  # t259: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t259 = prims.matmul(t253, t258)  # t259: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  t261 = torch_slice_prim_impl(t_transformer_h_3_attn_bias, [0, 0, 0, 0], [1, 1, 64, 64], [1, 1, 1, 1])  # t261: \"cuda:0 f32[1, 1, 64, 64]\"\n",
      "  [t263, t273] = nvFusion17(t259, t261)\n",
      "    # t260 = prims.mul(t259, 0.125)  # t260: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t262 = prims.eq(t261, 0.0)  # t262: \"cuda:0 b8[1, 1, 64, 64]\"\n",
      "    # t263 = prims.broadcast_in_dim(t262, (4, 12, 64, 64), (0, 1, 2, 3))  # t263: \"cuda:0 b8[4, 12, 64, 64]\"\n",
      "    # t264 = prims.where(t263, -float('inf'), t260)  # t264: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t265 = prims.amax(t264, (3,))  # t265: \"cuda:0 f32[4, 12, 64]\"\n",
      "    # t266 = prims.broadcast_in_dim(t265, [4, 12, 64, 1], [0, 1, 2])  # t266: \"cuda:0 f32[4, 12, 64, 1]\"\n",
      "    # t267 = prims.broadcast_in_dim(t266, (4, 12, 64, 64), (0, 1, 2, 3))  # t267: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t268 = prims.sub(t264, t267)  # t268: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t269 = prims.exp(t268)  # t269: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t270 = prims.sum(t269, (3,))  # t270: \"cuda:0 f32[4, 12, 64]\"\n",
      "    # t271 = prims.broadcast_in_dim(t270, [4, 12, 64, 1], [0, 1, 2])  # t271: \"cuda:0 f32[4, 12, 64, 1]\"\n",
      "    # t272 = prims.broadcast_in_dim(t271, (4, 12, 64, 64), (0, 1, 2, 3))  # t272: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t273 = prims.div(t269, t272)  # t273: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t259, t261\n",
      "  t274 = torch.matmul(t273, t257)  # t274: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t274 = ltorch.matmul(t273, t257)  # t274: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t274 = prims.matmul(t273, t257)  # t274: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  t275 = torch.permute(t274, (0, 2, 1, 3))  # t275: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t275 = ltorch.permute(t274, (0, 2, 1, 3))  # t275: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t275 = prims.transpose(t274, (0, 2, 1, 3))  # t275: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t274\n",
      "  [t276] = nvFusion18(t275)\n",
      "    # t276 = prims.stride_order(t275, (3, 2, 1, 0))  # t276: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t275\n",
      "  t277 = torch.reshape(t276, (4, 64, 768))  # t277: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t277 = ltorch.reshape(t276, (4, 64, 768))  # t277: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t277 = prims.reshape(t276, (4, 64, 768))  # t277: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t276\n",
      "  t278 = torch.nn.functional.linear(t277, t_transformer_h_3_attn_c_proj_weight, t_transformer_h_3_attn_c_proj_bias)  # t278: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t278 = ltorch.linear(t277, t_transformer_h_3_attn_c_proj_weight, t_transformer_h_3_attn_c_proj_bias)  # t278: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t278 = prims.linear(t277, t_transformer_h_3_attn_c_proj_weight, t_transformer_h_3_attn_c_proj_bias)  # t278: \"cuda:0 f32[4, 64, 768]\"\n",
      "  t1138 = torch.unsqueeze(t_transformer_h_3_ln_2_weight, 0)  # t1138: \"cuda:0 f32[1, 768]\"\n",
      "    # t1138 = ltorch.unsqueeze(t_transformer_h_3_ln_2_weight, 0)  # t1138: \"cuda:0 f32[1, 768]\"\n",
      "      # t1138 = prims.broadcast_in_dim(t_transformer_h_3_ln_2_weight, [1, 768], [1])  # t1138: \"cuda:0 f32[1, 768]\"\n",
      "  t1139 = torch.unsqueeze(t1138, 1)  # t1139: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1139 = ltorch.unsqueeze(t1138, 1)  # t1139: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1139 = prims.broadcast_in_dim(t1138, [1, 1, 768], [0, 2])  # t1139: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1138\n",
      "  t292 = Tensor.expand(t1139, (4, 64, 768))  # t292: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t292 = ltorch.expand(t1139, (4, 64, 768))  # t292: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t292 = prims.broadcast_in_dim(t1139, (4, 64, 768), (0, 1, 2))  # t292: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1139\n",
      "  t1141 = torch.unsqueeze(t_transformer_h_3_ln_2_bias, 0)  # t1141: \"cuda:0 f32[1, 768]\"\n",
      "    # t1141 = ltorch.unsqueeze(t_transformer_h_3_ln_2_bias, 0)  # t1141: \"cuda:0 f32[1, 768]\"\n",
      "      # t1141 = prims.broadcast_in_dim(t_transformer_h_3_ln_2_bias, [1, 768], [1])  # t1141: \"cuda:0 f32[1, 768]\"\n",
      "  t1142 = torch.unsqueeze(t1141, 1)  # t1142: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1142 = ltorch.unsqueeze(t1141, 1)  # t1142: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1142 = prims.broadcast_in_dim(t1141, [1, 1, 768], [0, 2])  # t1142: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1141\n",
      "  t294 = Tensor.expand(t1142, (4, 64, 768))  # t294: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t294 = ltorch.expand(t1142, (4, 64, 768))  # t294: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t294 = prims.broadcast_in_dim(t1142, (4, 64, 768), (0, 1, 2))  # t294: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1142\n",
      "  [t279, t283, t287, t289, t295] = nvFusion19(t231, t278, t292, t294)\n",
      "    # t279 = prims.add(t231, t278)  # t279: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # (t282, t283) = prims.var_mean(t279, (2,), correction=0)\n",
      "    # t284 = prims.broadcast_in_dim(t282, [4, 64, 1], [0, 1])  # t284: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t285 = prims.broadcast_in_dim(t283, [4, 64, 1], [0, 1])  # t285: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t286 = prims.add(t284, 1e-05)  # t286: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t287 = prims.rsqrt(t286)  # t287: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t288 = prims.broadcast_in_dim(t285, (4, 64, 768), (0, 1, 2))  # t288: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t289 = prims.sub(t279, t288)  # t289: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t290 = prims.broadcast_in_dim(t287, (4, 64, 768), (0, 1, 2))  # t290: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t291 = prims.mul(t289, t290)  # t291: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t293 = prims.mul(t291, t292)  # t293: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t295 = prims.add(t293, t294)  # t295: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t278, t294\n",
      "  t296 = torch.nn.functional.linear(t295, t_transformer_h_3_mlp_c_fc_weight, t_transformer_h_3_mlp_c_fc_bias)  # t296: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t296 = ltorch.linear(t295, t_transformer_h_3_mlp_c_fc_weight, t_transformer_h_3_mlp_c_fc_bias)  # t296: \"cuda:0 f32[4, 64, 3072]\"\n",
      "      # t296 = prims.linear(t295, t_transformer_h_3_mlp_c_fc_weight, t_transformer_h_3_mlp_c_fc_bias)  # t296: \"cuda:0 f32[4, 64, 3072]\"\n",
      "  [t304] = nvFusion20(t296)\n",
      "    # t297 = prims.mul(0.5, t296)  # t297: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t298 = prims.pow(t296, 3.0)  # t298: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t299 = prims.mul(0.044715, t298)  # t299: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t300 = prims.add(t296, t299)  # t300: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t301 = prims.mul(0.7978845608028654, t300)  # t301: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t302 = prims.tanh(t301)  # t302: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t303 = prims.add(1.0, t302)  # t303: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t304 = prims.mul(t297, t303)  # t304: \"cuda:0 f32[4, 64, 3072]\"\n",
      "  t305 = torch.nn.functional.linear(t304, t_transformer_h_3_mlp_c_proj_weight, t_transformer_h_3_mlp_c_proj_bias)  # t305: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t305 = ltorch.linear(t304, t_transformer_h_3_mlp_c_proj_weight, t_transformer_h_3_mlp_c_proj_bias)  # t305: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t305 = prims.linear(t304, t_transformer_h_3_mlp_c_proj_weight, t_transformer_h_3_mlp_c_proj_bias)  # t305: \"cuda:0 f32[4, 64, 768]\"\n",
      "  t1144 = torch.unsqueeze(t_transformer_h_4_ln_1_weight, 0)  # t1144: \"cuda:0 f32[1, 768]\"\n",
      "    # t1144 = ltorch.unsqueeze(t_transformer_h_4_ln_1_weight, 0)  # t1144: \"cuda:0 f32[1, 768]\"\n",
      "      # t1144 = prims.broadcast_in_dim(t_transformer_h_4_ln_1_weight, [1, 768], [1])  # t1144: \"cuda:0 f32[1, 768]\"\n",
      "  t1145 = torch.unsqueeze(t1144, 1)  # t1145: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1145 = ltorch.unsqueeze(t1144, 1)  # t1145: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1145 = prims.broadcast_in_dim(t1144, [1, 1, 768], [0, 2])  # t1145: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1144\n",
      "  t319 = Tensor.expand(t1145, (4, 64, 768))  # t319: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t319 = ltorch.expand(t1145, (4, 64, 768))  # t319: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t319 = prims.broadcast_in_dim(t1145, (4, 64, 768), (0, 1, 2))  # t319: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1145\n",
      "  t1147 = torch.unsqueeze(t_transformer_h_4_ln_1_bias, 0)  # t1147: \"cuda:0 f32[1, 768]\"\n",
      "    # t1147 = ltorch.unsqueeze(t_transformer_h_4_ln_1_bias, 0)  # t1147: \"cuda:0 f32[1, 768]\"\n",
      "      # t1147 = prims.broadcast_in_dim(t_transformer_h_4_ln_1_bias, [1, 768], [1])  # t1147: \"cuda:0 f32[1, 768]\"\n",
      "  t1148 = torch.unsqueeze(t1147, 1)  # t1148: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1148 = ltorch.unsqueeze(t1147, 1)  # t1148: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1148 = prims.broadcast_in_dim(t1147, [1, 1, 768], [0, 2])  # t1148: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1147\n",
      "  t321 = Tensor.expand(t1148, (4, 64, 768))  # t321: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t321 = ltorch.expand(t1148, (4, 64, 768))  # t321: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t321 = prims.broadcast_in_dim(t1148, (4, 64, 768), (0, 1, 2))  # t321: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1148\n",
      "  [t306, t310, t314, t316, t322] = nvFusion21(t279, t305, t319, t321)\n",
      "    # t306 = prims.add(t279, t305)  # t306: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # (t309, t310) = prims.var_mean(t306, (2,), correction=0)\n",
      "    # t311 = prims.broadcast_in_dim(t309, [4, 64, 1], [0, 1])  # t311: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t312 = prims.broadcast_in_dim(t310, [4, 64, 1], [0, 1])  # t312: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t313 = prims.add(t311, 1e-05)  # t313: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t314 = prims.rsqrt(t313)  # t314: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t315 = prims.broadcast_in_dim(t312, (4, 64, 768), (0, 1, 2))  # t315: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t316 = prims.sub(t306, t315)  # t316: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t317 = prims.broadcast_in_dim(t314, (4, 64, 768), (0, 1, 2))  # t317: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t318 = prims.mul(t316, t317)  # t318: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t320 = prims.mul(t318, t319)  # t320: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t322 = prims.add(t320, t321)  # t322: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t305, t321\n",
      "  t323 = torch.nn.functional.linear(t322, t_transformer_h_4_attn_c_attn_weight, t_transformer_h_4_attn_c_attn_bias)  # t323: \"cuda:0 f32[4, 64, 2304]\"\n",
      "    # t323 = ltorch.linear(t322, t_transformer_h_4_attn_c_attn_weight, t_transformer_h_4_attn_c_attn_bias)  # t323: \"cuda:0 f32[4, 64, 2304]\"\n",
      "      # t323 = prims.linear(t322, t_transformer_h_4_attn_c_attn_weight, t_transformer_h_4_attn_c_attn_bias)  # t323: \"cuda:0 f32[4, 64, 2304]\"\n",
      "  [t324, t325, t326] = torch.split(t323, 768, 2)\n",
      "    # [t324, t325, t326] = ltorch.split(t323, 768, 2)\n",
      "      # t324 = prims.slice_prim(t323, [0, 0, 0], [4, 64, 768], [1, 1, 1])  # t324: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t325 = prims.slice_prim(t323, [0, 0, 768], [4, 64, 1536], [1, 1, 1])  # t325: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t326 = prims.slice_prim(t323, [0, 0, 1536], [4, 64, 2304], [1, 1, 1])  # t326: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t323\n",
      "  t327 = torch.reshape(t324, (4, 64, 12, 64))  # t327: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t327 = ltorch.reshape(t324, (4, 64, 12, 64))  # t327: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t327 = prims.reshape(t324, (4, 64, 12, 64))  # t327: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t324\n",
      "  t328 = torch.permute(t327, (0, 2, 1, 3))  # t328: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t328 = ltorch.permute(t327, (0, 2, 1, 3))  # t328: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t328 = prims.transpose(t327, (0, 2, 1, 3))  # t328: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t327\n",
      "  t329 = torch.reshape(t325, (4, 64, 12, 64))  # t329: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t329 = ltorch.reshape(t325, (4, 64, 12, 64))  # t329: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t329 = prims.reshape(t325, (4, 64, 12, 64))  # t329: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t325\n",
      "  t330 = torch.permute(t329, (0, 2, 1, 3))  # t330: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t330 = ltorch.permute(t329, (0, 2, 1, 3))  # t330: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t330 = prims.transpose(t329, (0, 2, 1, 3))  # t330: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t329\n",
      "  t331 = torch.reshape(t326, (4, 64, 12, 64))  # t331: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t331 = ltorch.reshape(t326, (4, 64, 12, 64))  # t331: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t331 = prims.reshape(t326, (4, 64, 12, 64))  # t331: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t326\n",
      "  t332 = torch.permute(t331, (0, 2, 1, 3))  # t332: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t332 = ltorch.permute(t331, (0, 2, 1, 3))  # t332: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t332 = prims.transpose(t331, (0, 2, 1, 3))  # t332: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t331\n",
      "  t333 = torch.permute(t330, (0, 1, 3, 2))  # t333: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t333 = ltorch.permute(t330, (0, 1, 3, 2))  # t333: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t333 = prims.transpose(t330, (0, 1, 3, 2))  # t333: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t330\n",
      "  t334 = torch.matmul(t328, t333)  # t334: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t334 = ltorch.matmul(t328, t333)  # t334: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t334 = prims.matmul(t328, t333)  # t334: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  t336 = torch_slice_prim_impl(t_transformer_h_4_attn_bias, [0, 0, 0, 0], [1, 1, 64, 64], [1, 1, 1, 1])  # t336: \"cuda:0 f32[1, 1, 64, 64]\"\n",
      "  [t338, t348] = nvFusion22(t334, t336)\n",
      "    # t335 = prims.mul(t334, 0.125)  # t335: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t337 = prims.eq(t336, 0.0)  # t337: \"cuda:0 b8[1, 1, 64, 64]\"\n",
      "    # t338 = prims.broadcast_in_dim(t337, (4, 12, 64, 64), (0, 1, 2, 3))  # t338: \"cuda:0 b8[4, 12, 64, 64]\"\n",
      "    # t339 = prims.where(t338, -float('inf'), t335)  # t339: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t340 = prims.amax(t339, (3,))  # t340: \"cuda:0 f32[4, 12, 64]\"\n",
      "    # t341 = prims.broadcast_in_dim(t340, [4, 12, 64, 1], [0, 1, 2])  # t341: \"cuda:0 f32[4, 12, 64, 1]\"\n",
      "    # t342 = prims.broadcast_in_dim(t341, (4, 12, 64, 64), (0, 1, 2, 3))  # t342: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t343 = prims.sub(t339, t342)  # t343: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t344 = prims.exp(t343)  # t344: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t345 = prims.sum(t344, (3,))  # t345: \"cuda:0 f32[4, 12, 64]\"\n",
      "    # t346 = prims.broadcast_in_dim(t345, [4, 12, 64, 1], [0, 1, 2])  # t346: \"cuda:0 f32[4, 12, 64, 1]\"\n",
      "    # t347 = prims.broadcast_in_dim(t346, (4, 12, 64, 64), (0, 1, 2, 3))  # t347: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t348 = prims.div(t344, t347)  # t348: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t334, t336\n",
      "  t349 = torch.matmul(t348, t332)  # t349: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t349 = ltorch.matmul(t348, t332)  # t349: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t349 = prims.matmul(t348, t332)  # t349: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  t350 = torch.permute(t349, (0, 2, 1, 3))  # t350: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t350 = ltorch.permute(t349, (0, 2, 1, 3))  # t350: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t350 = prims.transpose(t349, (0, 2, 1, 3))  # t350: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t349\n",
      "  [t351] = nvFusion23(t350)\n",
      "    # t351 = prims.stride_order(t350, (3, 2, 1, 0))  # t351: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t350\n",
      "  t352 = torch.reshape(t351, (4, 64, 768))  # t352: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t352 = ltorch.reshape(t351, (4, 64, 768))  # t352: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t352 = prims.reshape(t351, (4, 64, 768))  # t352: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t351\n",
      "  t353 = torch.nn.functional.linear(t352, t_transformer_h_4_attn_c_proj_weight, t_transformer_h_4_attn_c_proj_bias)  # t353: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t353 = ltorch.linear(t352, t_transformer_h_4_attn_c_proj_weight, t_transformer_h_4_attn_c_proj_bias)  # t353: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t353 = prims.linear(t352, t_transformer_h_4_attn_c_proj_weight, t_transformer_h_4_attn_c_proj_bias)  # t353: \"cuda:0 f32[4, 64, 768]\"\n",
      "  t1163 = torch.unsqueeze(t_transformer_h_4_ln_2_weight, 0)  # t1163: \"cuda:0 f32[1, 768]\"\n",
      "    # t1163 = ltorch.unsqueeze(t_transformer_h_4_ln_2_weight, 0)  # t1163: \"cuda:0 f32[1, 768]\"\n",
      "      # t1163 = prims.broadcast_in_dim(t_transformer_h_4_ln_2_weight, [1, 768], [1])  # t1163: \"cuda:0 f32[1, 768]\"\n",
      "  t1164 = torch.unsqueeze(t1163, 1)  # t1164: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1164 = ltorch.unsqueeze(t1163, 1)  # t1164: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1164 = prims.broadcast_in_dim(t1163, [1, 1, 768], [0, 2])  # t1164: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1163\n",
      "  t367 = Tensor.expand(t1164, (4, 64, 768))  # t367: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t367 = ltorch.expand(t1164, (4, 64, 768))  # t367: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t367 = prims.broadcast_in_dim(t1164, (4, 64, 768), (0, 1, 2))  # t367: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1164\n",
      "  t1166 = torch.unsqueeze(t_transformer_h_4_ln_2_bias, 0)  # t1166: \"cuda:0 f32[1, 768]\"\n",
      "    # t1166 = ltorch.unsqueeze(t_transformer_h_4_ln_2_bias, 0)  # t1166: \"cuda:0 f32[1, 768]\"\n",
      "      # t1166 = prims.broadcast_in_dim(t_transformer_h_4_ln_2_bias, [1, 768], [1])  # t1166: \"cuda:0 f32[1, 768]\"\n",
      "  t1167 = torch.unsqueeze(t1166, 1)  # t1167: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1167 = ltorch.unsqueeze(t1166, 1)  # t1167: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1167 = prims.broadcast_in_dim(t1166, [1, 1, 768], [0, 2])  # t1167: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1166\n",
      "  t369 = Tensor.expand(t1167, (4, 64, 768))  # t369: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t369 = ltorch.expand(t1167, (4, 64, 768))  # t369: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t369 = prims.broadcast_in_dim(t1167, (4, 64, 768), (0, 1, 2))  # t369: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1167\n",
      "  [t354, t358, t362, t364, t370] = nvFusion24(t306, t353, t367, t369)\n",
      "    # t354 = prims.add(t306, t353)  # t354: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # (t357, t358) = prims.var_mean(t354, (2,), correction=0)\n",
      "    # t359 = prims.broadcast_in_dim(t357, [4, 64, 1], [0, 1])  # t359: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t360 = prims.broadcast_in_dim(t358, [4, 64, 1], [0, 1])  # t360: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t361 = prims.add(t359, 1e-05)  # t361: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t362 = prims.rsqrt(t361)  # t362: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t363 = prims.broadcast_in_dim(t360, (4, 64, 768), (0, 1, 2))  # t363: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t364 = prims.sub(t354, t363)  # t364: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t365 = prims.broadcast_in_dim(t362, (4, 64, 768), (0, 1, 2))  # t365: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t366 = prims.mul(t364, t365)  # t366: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t368 = prims.mul(t366, t367)  # t368: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t370 = prims.add(t368, t369)  # t370: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t353, t369\n",
      "  t371 = torch.nn.functional.linear(t370, t_transformer_h_4_mlp_c_fc_weight, t_transformer_h_4_mlp_c_fc_bias)  # t371: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t371 = ltorch.linear(t370, t_transformer_h_4_mlp_c_fc_weight, t_transformer_h_4_mlp_c_fc_bias)  # t371: \"cuda:0 f32[4, 64, 3072]\"\n",
      "      # t371 = prims.linear(t370, t_transformer_h_4_mlp_c_fc_weight, t_transformer_h_4_mlp_c_fc_bias)  # t371: \"cuda:0 f32[4, 64, 3072]\"\n",
      "  [t379] = nvFusion25(t371)\n",
      "    # t372 = prims.mul(0.5, t371)  # t372: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t373 = prims.pow(t371, 3.0)  # t373: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t374 = prims.mul(0.044715, t373)  # t374: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t375 = prims.add(t371, t374)  # t375: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t376 = prims.mul(0.7978845608028654, t375)  # t376: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t377 = prims.tanh(t376)  # t377: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t378 = prims.add(1.0, t377)  # t378: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t379 = prims.mul(t372, t378)  # t379: \"cuda:0 f32[4, 64, 3072]\"\n",
      "  t380 = torch.nn.functional.linear(t379, t_transformer_h_4_mlp_c_proj_weight, t_transformer_h_4_mlp_c_proj_bias)  # t380: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t380 = ltorch.linear(t379, t_transformer_h_4_mlp_c_proj_weight, t_transformer_h_4_mlp_c_proj_bias)  # t380: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t380 = prims.linear(t379, t_transformer_h_4_mlp_c_proj_weight, t_transformer_h_4_mlp_c_proj_bias)  # t380: \"cuda:0 f32[4, 64, 768]\"\n",
      "  t1169 = torch.unsqueeze(t_transformer_h_5_ln_1_weight, 0)  # t1169: \"cuda:0 f32[1, 768]\"\n",
      "    # t1169 = ltorch.unsqueeze(t_transformer_h_5_ln_1_weight, 0)  # t1169: \"cuda:0 f32[1, 768]\"\n",
      "      # t1169 = prims.broadcast_in_dim(t_transformer_h_5_ln_1_weight, [1, 768], [1])  # t1169: \"cuda:0 f32[1, 768]\"\n",
      "  t1170 = torch.unsqueeze(t1169, 1)  # t1170: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1170 = ltorch.unsqueeze(t1169, 1)  # t1170: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1170 = prims.broadcast_in_dim(t1169, [1, 1, 768], [0, 2])  # t1170: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1169\n",
      "  t394 = Tensor.expand(t1170, (4, 64, 768))  # t394: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t394 = ltorch.expand(t1170, (4, 64, 768))  # t394: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t394 = prims.broadcast_in_dim(t1170, (4, 64, 768), (0, 1, 2))  # t394: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1170\n",
      "  t1172 = torch.unsqueeze(t_transformer_h_5_ln_1_bias, 0)  # t1172: \"cuda:0 f32[1, 768]\"\n",
      "    # t1172 = ltorch.unsqueeze(t_transformer_h_5_ln_1_bias, 0)  # t1172: \"cuda:0 f32[1, 768]\"\n",
      "      # t1172 = prims.broadcast_in_dim(t_transformer_h_5_ln_1_bias, [1, 768], [1])  # t1172: \"cuda:0 f32[1, 768]\"\n",
      "  t1173 = torch.unsqueeze(t1172, 1)  # t1173: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1173 = ltorch.unsqueeze(t1172, 1)  # t1173: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1173 = prims.broadcast_in_dim(t1172, [1, 1, 768], [0, 2])  # t1173: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1172\n",
      "  t396 = Tensor.expand(t1173, (4, 64, 768))  # t396: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t396 = ltorch.expand(t1173, (4, 64, 768))  # t396: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t396 = prims.broadcast_in_dim(t1173, (4, 64, 768), (0, 1, 2))  # t396: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1173\n",
      "  [t381, t385, t389, t391, t397] = nvFusion26(t354, t380, t394, t396)\n",
      "    # t381 = prims.add(t354, t380)  # t381: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # (t384, t385) = prims.var_mean(t381, (2,), correction=0)\n",
      "    # t386 = prims.broadcast_in_dim(t384, [4, 64, 1], [0, 1])  # t386: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t387 = prims.broadcast_in_dim(t385, [4, 64, 1], [0, 1])  # t387: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t388 = prims.add(t386, 1e-05)  # t388: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t389 = prims.rsqrt(t388)  # t389: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t390 = prims.broadcast_in_dim(t387, (4, 64, 768), (0, 1, 2))  # t390: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t391 = prims.sub(t381, t390)  # t391: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t392 = prims.broadcast_in_dim(t389, (4, 64, 768), (0, 1, 2))  # t392: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t393 = prims.mul(t391, t392)  # t393: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t395 = prims.mul(t393, t394)  # t395: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t397 = prims.add(t395, t396)  # t397: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t380, t396\n",
      "  t398 = torch.nn.functional.linear(t397, t_transformer_h_5_attn_c_attn_weight, t_transformer_h_5_attn_c_attn_bias)  # t398: \"cuda:0 f32[4, 64, 2304]\"\n",
      "    # t398 = ltorch.linear(t397, t_transformer_h_5_attn_c_attn_weight, t_transformer_h_5_attn_c_attn_bias)  # t398: \"cuda:0 f32[4, 64, 2304]\"\n",
      "      # t398 = prims.linear(t397, t_transformer_h_5_attn_c_attn_weight, t_transformer_h_5_attn_c_attn_bias)  # t398: \"cuda:0 f32[4, 64, 2304]\"\n",
      "  [t399, t400, t401] = torch.split(t398, 768, 2)\n",
      "    # [t399, t400, t401] = ltorch.split(t398, 768, 2)\n",
      "      # t399 = prims.slice_prim(t398, [0, 0, 0], [4, 64, 768], [1, 1, 1])  # t399: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t400 = prims.slice_prim(t398, [0, 0, 768], [4, 64, 1536], [1, 1, 1])  # t400: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t401 = prims.slice_prim(t398, [0, 0, 1536], [4, 64, 2304], [1, 1, 1])  # t401: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t398\n",
      "  t402 = torch.reshape(t399, (4, 64, 12, 64))  # t402: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t402 = ltorch.reshape(t399, (4, 64, 12, 64))  # t402: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t402 = prims.reshape(t399, (4, 64, 12, 64))  # t402: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t399\n",
      "  t403 = torch.permute(t402, (0, 2, 1, 3))  # t403: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t403 = ltorch.permute(t402, (0, 2, 1, 3))  # t403: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t403 = prims.transpose(t402, (0, 2, 1, 3))  # t403: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t402\n",
      "  t404 = torch.reshape(t400, (4, 64, 12, 64))  # t404: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t404 = ltorch.reshape(t400, (4, 64, 12, 64))  # t404: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t404 = prims.reshape(t400, (4, 64, 12, 64))  # t404: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t400\n",
      "  t405 = torch.permute(t404, (0, 2, 1, 3))  # t405: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t405 = ltorch.permute(t404, (0, 2, 1, 3))  # t405: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t405 = prims.transpose(t404, (0, 2, 1, 3))  # t405: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t404\n",
      "  t406 = torch.reshape(t401, (4, 64, 12, 64))  # t406: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t406 = ltorch.reshape(t401, (4, 64, 12, 64))  # t406: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t406 = prims.reshape(t401, (4, 64, 12, 64))  # t406: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t401\n",
      "  t407 = torch.permute(t406, (0, 2, 1, 3))  # t407: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t407 = ltorch.permute(t406, (0, 2, 1, 3))  # t407: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t407 = prims.transpose(t406, (0, 2, 1, 3))  # t407: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t406\n",
      "  t408 = torch.permute(t405, (0, 1, 3, 2))  # t408: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t408 = ltorch.permute(t405, (0, 1, 3, 2))  # t408: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t408 = prims.transpose(t405, (0, 1, 3, 2))  # t408: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t405\n",
      "  t409 = torch.matmul(t403, t408)  # t409: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t409 = ltorch.matmul(t403, t408)  # t409: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t409 = prims.matmul(t403, t408)  # t409: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  t411 = torch_slice_prim_impl(t_transformer_h_5_attn_bias, [0, 0, 0, 0], [1, 1, 64, 64], [1, 1, 1, 1])  # t411: \"cuda:0 f32[1, 1, 64, 64]\"\n",
      "  [t413, t423] = nvFusion27(t409, t411)\n",
      "    # t410 = prims.mul(t409, 0.125)  # t410: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t412 = prims.eq(t411, 0.0)  # t412: \"cuda:0 b8[1, 1, 64, 64]\"\n",
      "    # t413 = prims.broadcast_in_dim(t412, (4, 12, 64, 64), (0, 1, 2, 3))  # t413: \"cuda:0 b8[4, 12, 64, 64]\"\n",
      "    # t414 = prims.where(t413, -float('inf'), t410)  # t414: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t415 = prims.amax(t414, (3,))  # t415: \"cuda:0 f32[4, 12, 64]\"\n",
      "    # t416 = prims.broadcast_in_dim(t415, [4, 12, 64, 1], [0, 1, 2])  # t416: \"cuda:0 f32[4, 12, 64, 1]\"\n",
      "    # t417 = prims.broadcast_in_dim(t416, (4, 12, 64, 64), (0, 1, 2, 3))  # t417: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t418 = prims.sub(t414, t417)  # t418: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t419 = prims.exp(t418)  # t419: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t420 = prims.sum(t419, (3,))  # t420: \"cuda:0 f32[4, 12, 64]\"\n",
      "    # t421 = prims.broadcast_in_dim(t420, [4, 12, 64, 1], [0, 1, 2])  # t421: \"cuda:0 f32[4, 12, 64, 1]\"\n",
      "    # t422 = prims.broadcast_in_dim(t421, (4, 12, 64, 64), (0, 1, 2, 3))  # t422: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t423 = prims.div(t419, t422)  # t423: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t409, t411\n",
      "  t424 = torch.matmul(t423, t407)  # t424: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t424 = ltorch.matmul(t423, t407)  # t424: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t424 = prims.matmul(t423, t407)  # t424: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  t425 = torch.permute(t424, (0, 2, 1, 3))  # t425: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t425 = ltorch.permute(t424, (0, 2, 1, 3))  # t425: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t425 = prims.transpose(t424, (0, 2, 1, 3))  # t425: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t424\n",
      "  [t426] = nvFusion28(t425)\n",
      "    # t426 = prims.stride_order(t425, (3, 2, 1, 0))  # t426: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t425\n",
      "  t427 = torch.reshape(t426, (4, 64, 768))  # t427: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t427 = ltorch.reshape(t426, (4, 64, 768))  # t427: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t427 = prims.reshape(t426, (4, 64, 768))  # t427: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t426\n",
      "  t428 = torch.nn.functional.linear(t427, t_transformer_h_5_attn_c_proj_weight, t_transformer_h_5_attn_c_proj_bias)  # t428: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t428 = ltorch.linear(t427, t_transformer_h_5_attn_c_proj_weight, t_transformer_h_5_attn_c_proj_bias)  # t428: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t428 = prims.linear(t427, t_transformer_h_5_attn_c_proj_weight, t_transformer_h_5_attn_c_proj_bias)  # t428: \"cuda:0 f32[4, 64, 768]\"\n",
      "  t1188 = torch.unsqueeze(t_transformer_h_5_ln_2_weight, 0)  # t1188: \"cuda:0 f32[1, 768]\"\n",
      "    # t1188 = ltorch.unsqueeze(t_transformer_h_5_ln_2_weight, 0)  # t1188: \"cuda:0 f32[1, 768]\"\n",
      "      # t1188 = prims.broadcast_in_dim(t_transformer_h_5_ln_2_weight, [1, 768], [1])  # t1188: \"cuda:0 f32[1, 768]\"\n",
      "  t1189 = torch.unsqueeze(t1188, 1)  # t1189: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1189 = ltorch.unsqueeze(t1188, 1)  # t1189: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1189 = prims.broadcast_in_dim(t1188, [1, 1, 768], [0, 2])  # t1189: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1188\n",
      "  t442 = Tensor.expand(t1189, (4, 64, 768))  # t442: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t442 = ltorch.expand(t1189, (4, 64, 768))  # t442: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t442 = prims.broadcast_in_dim(t1189, (4, 64, 768), (0, 1, 2))  # t442: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1189\n",
      "  t1191 = torch.unsqueeze(t_transformer_h_5_ln_2_bias, 0)  # t1191: \"cuda:0 f32[1, 768]\"\n",
      "    # t1191 = ltorch.unsqueeze(t_transformer_h_5_ln_2_bias, 0)  # t1191: \"cuda:0 f32[1, 768]\"\n",
      "      # t1191 = prims.broadcast_in_dim(t_transformer_h_5_ln_2_bias, [1, 768], [1])  # t1191: \"cuda:0 f32[1, 768]\"\n",
      "  t1192 = torch.unsqueeze(t1191, 1)  # t1192: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1192 = ltorch.unsqueeze(t1191, 1)  # t1192: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1192 = prims.broadcast_in_dim(t1191, [1, 1, 768], [0, 2])  # t1192: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1191\n",
      "  t444 = Tensor.expand(t1192, (4, 64, 768))  # t444: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t444 = ltorch.expand(t1192, (4, 64, 768))  # t444: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t444 = prims.broadcast_in_dim(t1192, (4, 64, 768), (0, 1, 2))  # t444: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1192\n",
      "  [t429, t433, t437, t439, t445] = nvFusion29(t381, t428, t442, t444)\n",
      "    # t429 = prims.add(t381, t428)  # t429: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # (t432, t433) = prims.var_mean(t429, (2,), correction=0)\n",
      "    # t434 = prims.broadcast_in_dim(t432, [4, 64, 1], [0, 1])  # t434: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t435 = prims.broadcast_in_dim(t433, [4, 64, 1], [0, 1])  # t435: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t436 = prims.add(t434, 1e-05)  # t436: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t437 = prims.rsqrt(t436)  # t437: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t438 = prims.broadcast_in_dim(t435, (4, 64, 768), (0, 1, 2))  # t438: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t439 = prims.sub(t429, t438)  # t439: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t440 = prims.broadcast_in_dim(t437, (4, 64, 768), (0, 1, 2))  # t440: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t441 = prims.mul(t439, t440)  # t441: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t443 = prims.mul(t441, t442)  # t443: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t445 = prims.add(t443, t444)  # t445: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t428, t444\n",
      "  t446 = torch.nn.functional.linear(t445, t_transformer_h_5_mlp_c_fc_weight, t_transformer_h_5_mlp_c_fc_bias)  # t446: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t446 = ltorch.linear(t445, t_transformer_h_5_mlp_c_fc_weight, t_transformer_h_5_mlp_c_fc_bias)  # t446: \"cuda:0 f32[4, 64, 3072]\"\n",
      "      # t446 = prims.linear(t445, t_transformer_h_5_mlp_c_fc_weight, t_transformer_h_5_mlp_c_fc_bias)  # t446: \"cuda:0 f32[4, 64, 3072]\"\n",
      "  [t454] = nvFusion30(t446)\n",
      "    # t447 = prims.mul(0.5, t446)  # t447: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t448 = prims.pow(t446, 3.0)  # t448: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t449 = prims.mul(0.044715, t448)  # t449: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t450 = prims.add(t446, t449)  # t450: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t451 = prims.mul(0.7978845608028654, t450)  # t451: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t452 = prims.tanh(t451)  # t452: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t453 = prims.add(1.0, t452)  # t453: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t454 = prims.mul(t447, t453)  # t454: \"cuda:0 f32[4, 64, 3072]\"\n",
      "  t455 = torch.nn.functional.linear(t454, t_transformer_h_5_mlp_c_proj_weight, t_transformer_h_5_mlp_c_proj_bias)  # t455: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t455 = ltorch.linear(t454, t_transformer_h_5_mlp_c_proj_weight, t_transformer_h_5_mlp_c_proj_bias)  # t455: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t455 = prims.linear(t454, t_transformer_h_5_mlp_c_proj_weight, t_transformer_h_5_mlp_c_proj_bias)  # t455: \"cuda:0 f32[4, 64, 768]\"\n",
      "  t1194 = torch.unsqueeze(t_transformer_h_6_ln_1_weight, 0)  # t1194: \"cuda:0 f32[1, 768]\"\n",
      "    # t1194 = ltorch.unsqueeze(t_transformer_h_6_ln_1_weight, 0)  # t1194: \"cuda:0 f32[1, 768]\"\n",
      "      # t1194 = prims.broadcast_in_dim(t_transformer_h_6_ln_1_weight, [1, 768], [1])  # t1194: \"cuda:0 f32[1, 768]\"\n",
      "  t1195 = torch.unsqueeze(t1194, 1)  # t1195: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1195 = ltorch.unsqueeze(t1194, 1)  # t1195: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1195 = prims.broadcast_in_dim(t1194, [1, 1, 768], [0, 2])  # t1195: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1194\n",
      "  t469 = Tensor.expand(t1195, (4, 64, 768))  # t469: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t469 = ltorch.expand(t1195, (4, 64, 768))  # t469: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t469 = prims.broadcast_in_dim(t1195, (4, 64, 768), (0, 1, 2))  # t469: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1195\n",
      "  t1197 = torch.unsqueeze(t_transformer_h_6_ln_1_bias, 0)  # t1197: \"cuda:0 f32[1, 768]\"\n",
      "    # t1197 = ltorch.unsqueeze(t_transformer_h_6_ln_1_bias, 0)  # t1197: \"cuda:0 f32[1, 768]\"\n",
      "      # t1197 = prims.broadcast_in_dim(t_transformer_h_6_ln_1_bias, [1, 768], [1])  # t1197: \"cuda:0 f32[1, 768]\"\n",
      "  t1198 = torch.unsqueeze(t1197, 1)  # t1198: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1198 = ltorch.unsqueeze(t1197, 1)  # t1198: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1198 = prims.broadcast_in_dim(t1197, [1, 1, 768], [0, 2])  # t1198: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1197\n",
      "  t471 = Tensor.expand(t1198, (4, 64, 768))  # t471: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t471 = ltorch.expand(t1198, (4, 64, 768))  # t471: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t471 = prims.broadcast_in_dim(t1198, (4, 64, 768), (0, 1, 2))  # t471: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1198\n",
      "  [t456, t460, t464, t466, t472] = nvFusion31(t429, t455, t469, t471)\n",
      "    # t456 = prims.add(t429, t455)  # t456: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # (t459, t460) = prims.var_mean(t456, (2,), correction=0)\n",
      "    # t461 = prims.broadcast_in_dim(t459, [4, 64, 1], [0, 1])  # t461: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t462 = prims.broadcast_in_dim(t460, [4, 64, 1], [0, 1])  # t462: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t463 = prims.add(t461, 1e-05)  # t463: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t464 = prims.rsqrt(t463)  # t464: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t465 = prims.broadcast_in_dim(t462, (4, 64, 768), (0, 1, 2))  # t465: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t466 = prims.sub(t456, t465)  # t466: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t467 = prims.broadcast_in_dim(t464, (4, 64, 768), (0, 1, 2))  # t467: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t468 = prims.mul(t466, t467)  # t468: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t470 = prims.mul(t468, t469)  # t470: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t472 = prims.add(t470, t471)  # t472: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t455, t471\n",
      "  t473 = torch.nn.functional.linear(t472, t_transformer_h_6_attn_c_attn_weight, t_transformer_h_6_attn_c_attn_bias)  # t473: \"cuda:0 f32[4, 64, 2304]\"\n",
      "    # t473 = ltorch.linear(t472, t_transformer_h_6_attn_c_attn_weight, t_transformer_h_6_attn_c_attn_bias)  # t473: \"cuda:0 f32[4, 64, 2304]\"\n",
      "      # t473 = prims.linear(t472, t_transformer_h_6_attn_c_attn_weight, t_transformer_h_6_attn_c_attn_bias)  # t473: \"cuda:0 f32[4, 64, 2304]\"\n",
      "  [t474, t475, t476] = torch.split(t473, 768, 2)\n",
      "    # [t474, t475, t476] = ltorch.split(t473, 768, 2)\n",
      "      # t474 = prims.slice_prim(t473, [0, 0, 0], [4, 64, 768], [1, 1, 1])  # t474: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t475 = prims.slice_prim(t473, [0, 0, 768], [4, 64, 1536], [1, 1, 1])  # t475: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t476 = prims.slice_prim(t473, [0, 0, 1536], [4, 64, 2304], [1, 1, 1])  # t476: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t473\n",
      "  t477 = torch.reshape(t474, (4, 64, 12, 64))  # t477: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t477 = ltorch.reshape(t474, (4, 64, 12, 64))  # t477: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t477 = prims.reshape(t474, (4, 64, 12, 64))  # t477: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t474\n",
      "  t478 = torch.permute(t477, (0, 2, 1, 3))  # t478: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t478 = ltorch.permute(t477, (0, 2, 1, 3))  # t478: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t478 = prims.transpose(t477, (0, 2, 1, 3))  # t478: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t477\n",
      "  t479 = torch.reshape(t475, (4, 64, 12, 64))  # t479: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t479 = ltorch.reshape(t475, (4, 64, 12, 64))  # t479: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t479 = prims.reshape(t475, (4, 64, 12, 64))  # t479: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t475\n",
      "  t480 = torch.permute(t479, (0, 2, 1, 3))  # t480: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t480 = ltorch.permute(t479, (0, 2, 1, 3))  # t480: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t480 = prims.transpose(t479, (0, 2, 1, 3))  # t480: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t479\n",
      "  t481 = torch.reshape(t476, (4, 64, 12, 64))  # t481: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t481 = ltorch.reshape(t476, (4, 64, 12, 64))  # t481: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t481 = prims.reshape(t476, (4, 64, 12, 64))  # t481: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t476\n",
      "  t482 = torch.permute(t481, (0, 2, 1, 3))  # t482: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t482 = ltorch.permute(t481, (0, 2, 1, 3))  # t482: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t482 = prims.transpose(t481, (0, 2, 1, 3))  # t482: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t481\n",
      "  t483 = torch.permute(t480, (0, 1, 3, 2))  # t483: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t483 = ltorch.permute(t480, (0, 1, 3, 2))  # t483: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t483 = prims.transpose(t480, (0, 1, 3, 2))  # t483: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t480\n",
      "  t484 = torch.matmul(t478, t483)  # t484: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t484 = ltorch.matmul(t478, t483)  # t484: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t484 = prims.matmul(t478, t483)  # t484: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  t486 = torch_slice_prim_impl(t_transformer_h_6_attn_bias, [0, 0, 0, 0], [1, 1, 64, 64], [1, 1, 1, 1])  # t486: \"cuda:0 f32[1, 1, 64, 64]\"\n",
      "  [t488, t498] = nvFusion32(t484, t486)\n",
      "    # t485 = prims.mul(t484, 0.125)  # t485: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t487 = prims.eq(t486, 0.0)  # t487: \"cuda:0 b8[1, 1, 64, 64]\"\n",
      "    # t488 = prims.broadcast_in_dim(t487, (4, 12, 64, 64), (0, 1, 2, 3))  # t488: \"cuda:0 b8[4, 12, 64, 64]\"\n",
      "    # t489 = prims.where(t488, -float('inf'), t485)  # t489: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t490 = prims.amax(t489, (3,))  # t490: \"cuda:0 f32[4, 12, 64]\"\n",
      "    # t491 = prims.broadcast_in_dim(t490, [4, 12, 64, 1], [0, 1, 2])  # t491: \"cuda:0 f32[4, 12, 64, 1]\"\n",
      "    # t492 = prims.broadcast_in_dim(t491, (4, 12, 64, 64), (0, 1, 2, 3))  # t492: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t493 = prims.sub(t489, t492)  # t493: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t494 = prims.exp(t493)  # t494: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t495 = prims.sum(t494, (3,))  # t495: \"cuda:0 f32[4, 12, 64]\"\n",
      "    # t496 = prims.broadcast_in_dim(t495, [4, 12, 64, 1], [0, 1, 2])  # t496: \"cuda:0 f32[4, 12, 64, 1]\"\n",
      "    # t497 = prims.broadcast_in_dim(t496, (4, 12, 64, 64), (0, 1, 2, 3))  # t497: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t498 = prims.div(t494, t497)  # t498: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t484, t486\n",
      "  t499 = torch.matmul(t498, t482)  # t499: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t499 = ltorch.matmul(t498, t482)  # t499: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t499 = prims.matmul(t498, t482)  # t499: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  t500 = torch.permute(t499, (0, 2, 1, 3))  # t500: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t500 = ltorch.permute(t499, (0, 2, 1, 3))  # t500: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t500 = prims.transpose(t499, (0, 2, 1, 3))  # t500: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t499\n",
      "  [t501] = nvFusion33(t500)\n",
      "    # t501 = prims.stride_order(t500, (3, 2, 1, 0))  # t501: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t500\n",
      "  t502 = torch.reshape(t501, (4, 64, 768))  # t502: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t502 = ltorch.reshape(t501, (4, 64, 768))  # t502: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t502 = prims.reshape(t501, (4, 64, 768))  # t502: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t501\n",
      "  t503 = torch.nn.functional.linear(t502, t_transformer_h_6_attn_c_proj_weight, t_transformer_h_6_attn_c_proj_bias)  # t503: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t503 = ltorch.linear(t502, t_transformer_h_6_attn_c_proj_weight, t_transformer_h_6_attn_c_proj_bias)  # t503: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t503 = prims.linear(t502, t_transformer_h_6_attn_c_proj_weight, t_transformer_h_6_attn_c_proj_bias)  # t503: \"cuda:0 f32[4, 64, 768]\"\n",
      "  t1213 = torch.unsqueeze(t_transformer_h_6_ln_2_weight, 0)  # t1213: \"cuda:0 f32[1, 768]\"\n",
      "    # t1213 = ltorch.unsqueeze(t_transformer_h_6_ln_2_weight, 0)  # t1213: \"cuda:0 f32[1, 768]\"\n",
      "      # t1213 = prims.broadcast_in_dim(t_transformer_h_6_ln_2_weight, [1, 768], [1])  # t1213: \"cuda:0 f32[1, 768]\"\n",
      "  t1214 = torch.unsqueeze(t1213, 1)  # t1214: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1214 = ltorch.unsqueeze(t1213, 1)  # t1214: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1214 = prims.broadcast_in_dim(t1213, [1, 1, 768], [0, 2])  # t1214: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1213\n",
      "  t517 = Tensor.expand(t1214, (4, 64, 768))  # t517: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t517 = ltorch.expand(t1214, (4, 64, 768))  # t517: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t517 = prims.broadcast_in_dim(t1214, (4, 64, 768), (0, 1, 2))  # t517: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1214\n",
      "  t1216 = torch.unsqueeze(t_transformer_h_6_ln_2_bias, 0)  # t1216: \"cuda:0 f32[1, 768]\"\n",
      "    # t1216 = ltorch.unsqueeze(t_transformer_h_6_ln_2_bias, 0)  # t1216: \"cuda:0 f32[1, 768]\"\n",
      "      # t1216 = prims.broadcast_in_dim(t_transformer_h_6_ln_2_bias, [1, 768], [1])  # t1216: \"cuda:0 f32[1, 768]\"\n",
      "  t1217 = torch.unsqueeze(t1216, 1)  # t1217: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1217 = ltorch.unsqueeze(t1216, 1)  # t1217: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1217 = prims.broadcast_in_dim(t1216, [1, 1, 768], [0, 2])  # t1217: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1216\n",
      "  t519 = Tensor.expand(t1217, (4, 64, 768))  # t519: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t519 = ltorch.expand(t1217, (4, 64, 768))  # t519: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t519 = prims.broadcast_in_dim(t1217, (4, 64, 768), (0, 1, 2))  # t519: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1217\n",
      "  [t504, t508, t512, t514, t520] = nvFusion34(t456, t503, t517, t519)\n",
      "    # t504 = prims.add(t456, t503)  # t504: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # (t507, t508) = prims.var_mean(t504, (2,), correction=0)\n",
      "    # t509 = prims.broadcast_in_dim(t507, [4, 64, 1], [0, 1])  # t509: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t510 = prims.broadcast_in_dim(t508, [4, 64, 1], [0, 1])  # t510: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t511 = prims.add(t509, 1e-05)  # t511: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t512 = prims.rsqrt(t511)  # t512: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t513 = prims.broadcast_in_dim(t510, (4, 64, 768), (0, 1, 2))  # t513: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t514 = prims.sub(t504, t513)  # t514: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t515 = prims.broadcast_in_dim(t512, (4, 64, 768), (0, 1, 2))  # t515: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t516 = prims.mul(t514, t515)  # t516: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t518 = prims.mul(t516, t517)  # t518: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t520 = prims.add(t518, t519)  # t520: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t503, t519\n",
      "  t521 = torch.nn.functional.linear(t520, t_transformer_h_6_mlp_c_fc_weight, t_transformer_h_6_mlp_c_fc_bias)  # t521: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t521 = ltorch.linear(t520, t_transformer_h_6_mlp_c_fc_weight, t_transformer_h_6_mlp_c_fc_bias)  # t521: \"cuda:0 f32[4, 64, 3072]\"\n",
      "      # t521 = prims.linear(t520, t_transformer_h_6_mlp_c_fc_weight, t_transformer_h_6_mlp_c_fc_bias)  # t521: \"cuda:0 f32[4, 64, 3072]\"\n",
      "  [t529] = nvFusion35(t521)\n",
      "    # t522 = prims.mul(0.5, t521)  # t522: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t523 = prims.pow(t521, 3.0)  # t523: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t524 = prims.mul(0.044715, t523)  # t524: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t525 = prims.add(t521, t524)  # t525: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t526 = prims.mul(0.7978845608028654, t525)  # t526: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t527 = prims.tanh(t526)  # t527: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t528 = prims.add(1.0, t527)  # t528: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t529 = prims.mul(t522, t528)  # t529: \"cuda:0 f32[4, 64, 3072]\"\n",
      "  t530 = torch.nn.functional.linear(t529, t_transformer_h_6_mlp_c_proj_weight, t_transformer_h_6_mlp_c_proj_bias)  # t530: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t530 = ltorch.linear(t529, t_transformer_h_6_mlp_c_proj_weight, t_transformer_h_6_mlp_c_proj_bias)  # t530: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t530 = prims.linear(t529, t_transformer_h_6_mlp_c_proj_weight, t_transformer_h_6_mlp_c_proj_bias)  # t530: \"cuda:0 f32[4, 64, 768]\"\n",
      "  t1219 = torch.unsqueeze(t_transformer_h_7_ln_1_weight, 0)  # t1219: \"cuda:0 f32[1, 768]\"\n",
      "    # t1219 = ltorch.unsqueeze(t_transformer_h_7_ln_1_weight, 0)  # t1219: \"cuda:0 f32[1, 768]\"\n",
      "      # t1219 = prims.broadcast_in_dim(t_transformer_h_7_ln_1_weight, [1, 768], [1])  # t1219: \"cuda:0 f32[1, 768]\"\n",
      "  t1220 = torch.unsqueeze(t1219, 1)  # t1220: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1220 = ltorch.unsqueeze(t1219, 1)  # t1220: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1220 = prims.broadcast_in_dim(t1219, [1, 1, 768], [0, 2])  # t1220: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1219\n",
      "  t544 = Tensor.expand(t1220, (4, 64, 768))  # t544: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t544 = ltorch.expand(t1220, (4, 64, 768))  # t544: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t544 = prims.broadcast_in_dim(t1220, (4, 64, 768), (0, 1, 2))  # t544: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1220\n",
      "  t1222 = torch.unsqueeze(t_transformer_h_7_ln_1_bias, 0)  # t1222: \"cuda:0 f32[1, 768]\"\n",
      "    # t1222 = ltorch.unsqueeze(t_transformer_h_7_ln_1_bias, 0)  # t1222: \"cuda:0 f32[1, 768]\"\n",
      "      # t1222 = prims.broadcast_in_dim(t_transformer_h_7_ln_1_bias, [1, 768], [1])  # t1222: \"cuda:0 f32[1, 768]\"\n",
      "  t1223 = torch.unsqueeze(t1222, 1)  # t1223: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1223 = ltorch.unsqueeze(t1222, 1)  # t1223: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1223 = prims.broadcast_in_dim(t1222, [1, 1, 768], [0, 2])  # t1223: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1222\n",
      "  t546 = Tensor.expand(t1223, (4, 64, 768))  # t546: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t546 = ltorch.expand(t1223, (4, 64, 768))  # t546: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t546 = prims.broadcast_in_dim(t1223, (4, 64, 768), (0, 1, 2))  # t546: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1223\n",
      "  [t531, t535, t539, t541, t547] = nvFusion36(t504, t530, t544, t546)\n",
      "    # t531 = prims.add(t504, t530)  # t531: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # (t534, t535) = prims.var_mean(t531, (2,), correction=0)\n",
      "    # t536 = prims.broadcast_in_dim(t534, [4, 64, 1], [0, 1])  # t536: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t537 = prims.broadcast_in_dim(t535, [4, 64, 1], [0, 1])  # t537: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t538 = prims.add(t536, 1e-05)  # t538: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t539 = prims.rsqrt(t538)  # t539: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t540 = prims.broadcast_in_dim(t537, (4, 64, 768), (0, 1, 2))  # t540: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t541 = prims.sub(t531, t540)  # t541: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t542 = prims.broadcast_in_dim(t539, (4, 64, 768), (0, 1, 2))  # t542: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t543 = prims.mul(t541, t542)  # t543: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t545 = prims.mul(t543, t544)  # t545: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t547 = prims.add(t545, t546)  # t547: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t530, t546\n",
      "  t548 = torch.nn.functional.linear(t547, t_transformer_h_7_attn_c_attn_weight, t_transformer_h_7_attn_c_attn_bias)  # t548: \"cuda:0 f32[4, 64, 2304]\"\n",
      "    # t548 = ltorch.linear(t547, t_transformer_h_7_attn_c_attn_weight, t_transformer_h_7_attn_c_attn_bias)  # t548: \"cuda:0 f32[4, 64, 2304]\"\n",
      "      # t548 = prims.linear(t547, t_transformer_h_7_attn_c_attn_weight, t_transformer_h_7_attn_c_attn_bias)  # t548: \"cuda:0 f32[4, 64, 2304]\"\n",
      "  [t549, t550, t551] = torch.split(t548, 768, 2)\n",
      "    # [t549, t550, t551] = ltorch.split(t548, 768, 2)\n",
      "      # t549 = prims.slice_prim(t548, [0, 0, 0], [4, 64, 768], [1, 1, 1])  # t549: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t550 = prims.slice_prim(t548, [0, 0, 768], [4, 64, 1536], [1, 1, 1])  # t550: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t551 = prims.slice_prim(t548, [0, 0, 1536], [4, 64, 2304], [1, 1, 1])  # t551: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t548\n",
      "  t552 = torch.reshape(t549, (4, 64, 12, 64))  # t552: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t552 = ltorch.reshape(t549, (4, 64, 12, 64))  # t552: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t552 = prims.reshape(t549, (4, 64, 12, 64))  # t552: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t549\n",
      "  t553 = torch.permute(t552, (0, 2, 1, 3))  # t553: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t553 = ltorch.permute(t552, (0, 2, 1, 3))  # t553: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t553 = prims.transpose(t552, (0, 2, 1, 3))  # t553: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t552\n",
      "  t554 = torch.reshape(t550, (4, 64, 12, 64))  # t554: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t554 = ltorch.reshape(t550, (4, 64, 12, 64))  # t554: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t554 = prims.reshape(t550, (4, 64, 12, 64))  # t554: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t550\n",
      "  t555 = torch.permute(t554, (0, 2, 1, 3))  # t555: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t555 = ltorch.permute(t554, (0, 2, 1, 3))  # t555: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t555 = prims.transpose(t554, (0, 2, 1, 3))  # t555: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t554\n",
      "  t556 = torch.reshape(t551, (4, 64, 12, 64))  # t556: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t556 = ltorch.reshape(t551, (4, 64, 12, 64))  # t556: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t556 = prims.reshape(t551, (4, 64, 12, 64))  # t556: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t551\n",
      "  t557 = torch.permute(t556, (0, 2, 1, 3))  # t557: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t557 = ltorch.permute(t556, (0, 2, 1, 3))  # t557: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t557 = prims.transpose(t556, (0, 2, 1, 3))  # t557: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t556\n",
      "  t558 = torch.permute(t555, (0, 1, 3, 2))  # t558: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t558 = ltorch.permute(t555, (0, 1, 3, 2))  # t558: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t558 = prims.transpose(t555, (0, 1, 3, 2))  # t558: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t555\n",
      "  t559 = torch.matmul(t553, t558)  # t559: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t559 = ltorch.matmul(t553, t558)  # t559: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t559 = prims.matmul(t553, t558)  # t559: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  t561 = torch_slice_prim_impl(t_transformer_h_7_attn_bias, [0, 0, 0, 0], [1, 1, 64, 64], [1, 1, 1, 1])  # t561: \"cuda:0 f32[1, 1, 64, 64]\"\n",
      "  [t563, t573] = nvFusion37(t559, t561)\n",
      "    # t560 = prims.mul(t559, 0.125)  # t560: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t562 = prims.eq(t561, 0.0)  # t562: \"cuda:0 b8[1, 1, 64, 64]\"\n",
      "    # t563 = prims.broadcast_in_dim(t562, (4, 12, 64, 64), (0, 1, 2, 3))  # t563: \"cuda:0 b8[4, 12, 64, 64]\"\n",
      "    # t564 = prims.where(t563, -float('inf'), t560)  # t564: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t565 = prims.amax(t564, (3,))  # t565: \"cuda:0 f32[4, 12, 64]\"\n",
      "    # t566 = prims.broadcast_in_dim(t565, [4, 12, 64, 1], [0, 1, 2])  # t566: \"cuda:0 f32[4, 12, 64, 1]\"\n",
      "    # t567 = prims.broadcast_in_dim(t566, (4, 12, 64, 64), (0, 1, 2, 3))  # t567: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t568 = prims.sub(t564, t567)  # t568: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t569 = prims.exp(t568)  # t569: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t570 = prims.sum(t569, (3,))  # t570: \"cuda:0 f32[4, 12, 64]\"\n",
      "    # t571 = prims.broadcast_in_dim(t570, [4, 12, 64, 1], [0, 1, 2])  # t571: \"cuda:0 f32[4, 12, 64, 1]\"\n",
      "    # t572 = prims.broadcast_in_dim(t571, (4, 12, 64, 64), (0, 1, 2, 3))  # t572: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t573 = prims.div(t569, t572)  # t573: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t559, t561\n",
      "  t574 = torch.matmul(t573, t557)  # t574: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t574 = ltorch.matmul(t573, t557)  # t574: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t574 = prims.matmul(t573, t557)  # t574: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  t575 = torch.permute(t574, (0, 2, 1, 3))  # t575: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t575 = ltorch.permute(t574, (0, 2, 1, 3))  # t575: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t575 = prims.transpose(t574, (0, 2, 1, 3))  # t575: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t574\n",
      "  [t576] = nvFusion38(t575)\n",
      "    # t576 = prims.stride_order(t575, (3, 2, 1, 0))  # t576: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t575\n",
      "  t577 = torch.reshape(t576, (4, 64, 768))  # t577: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t577 = ltorch.reshape(t576, (4, 64, 768))  # t577: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t577 = prims.reshape(t576, (4, 64, 768))  # t577: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t576\n",
      "  t578 = torch.nn.functional.linear(t577, t_transformer_h_7_attn_c_proj_weight, t_transformer_h_7_attn_c_proj_bias)  # t578: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t578 = ltorch.linear(t577, t_transformer_h_7_attn_c_proj_weight, t_transformer_h_7_attn_c_proj_bias)  # t578: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t578 = prims.linear(t577, t_transformer_h_7_attn_c_proj_weight, t_transformer_h_7_attn_c_proj_bias)  # t578: \"cuda:0 f32[4, 64, 768]\"\n",
      "  t1238 = torch.unsqueeze(t_transformer_h_7_ln_2_weight, 0)  # t1238: \"cuda:0 f32[1, 768]\"\n",
      "    # t1238 = ltorch.unsqueeze(t_transformer_h_7_ln_2_weight, 0)  # t1238: \"cuda:0 f32[1, 768]\"\n",
      "      # t1238 = prims.broadcast_in_dim(t_transformer_h_7_ln_2_weight, [1, 768], [1])  # t1238: \"cuda:0 f32[1, 768]\"\n",
      "  t1239 = torch.unsqueeze(t1238, 1)  # t1239: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1239 = ltorch.unsqueeze(t1238, 1)  # t1239: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1239 = prims.broadcast_in_dim(t1238, [1, 1, 768], [0, 2])  # t1239: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1238\n",
      "  t592 = Tensor.expand(t1239, (4, 64, 768))  # t592: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t592 = ltorch.expand(t1239, (4, 64, 768))  # t592: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t592 = prims.broadcast_in_dim(t1239, (4, 64, 768), (0, 1, 2))  # t592: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1239\n",
      "  t1241 = torch.unsqueeze(t_transformer_h_7_ln_2_bias, 0)  # t1241: \"cuda:0 f32[1, 768]\"\n",
      "    # t1241 = ltorch.unsqueeze(t_transformer_h_7_ln_2_bias, 0)  # t1241: \"cuda:0 f32[1, 768]\"\n",
      "      # t1241 = prims.broadcast_in_dim(t_transformer_h_7_ln_2_bias, [1, 768], [1])  # t1241: \"cuda:0 f32[1, 768]\"\n",
      "  t1242 = torch.unsqueeze(t1241, 1)  # t1242: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1242 = ltorch.unsqueeze(t1241, 1)  # t1242: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1242 = prims.broadcast_in_dim(t1241, [1, 1, 768], [0, 2])  # t1242: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1241\n",
      "  t594 = Tensor.expand(t1242, (4, 64, 768))  # t594: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t594 = ltorch.expand(t1242, (4, 64, 768))  # t594: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t594 = prims.broadcast_in_dim(t1242, (4, 64, 768), (0, 1, 2))  # t594: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1242\n",
      "  [t579, t583, t587, t589, t595] = nvFusion39(t531, t578, t592, t594)\n",
      "    # t579 = prims.add(t531, t578)  # t579: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # (t582, t583) = prims.var_mean(t579, (2,), correction=0)\n",
      "    # t584 = prims.broadcast_in_dim(t582, [4, 64, 1], [0, 1])  # t584: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t585 = prims.broadcast_in_dim(t583, [4, 64, 1], [0, 1])  # t585: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t586 = prims.add(t584, 1e-05)  # t586: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t587 = prims.rsqrt(t586)  # t587: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t588 = prims.broadcast_in_dim(t585, (4, 64, 768), (0, 1, 2))  # t588: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t589 = prims.sub(t579, t588)  # t589: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t590 = prims.broadcast_in_dim(t587, (4, 64, 768), (0, 1, 2))  # t590: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t591 = prims.mul(t589, t590)  # t591: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t593 = prims.mul(t591, t592)  # t593: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t595 = prims.add(t593, t594)  # t595: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t578, t594\n",
      "  t596 = torch.nn.functional.linear(t595, t_transformer_h_7_mlp_c_fc_weight, t_transformer_h_7_mlp_c_fc_bias)  # t596: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t596 = ltorch.linear(t595, t_transformer_h_7_mlp_c_fc_weight, t_transformer_h_7_mlp_c_fc_bias)  # t596: \"cuda:0 f32[4, 64, 3072]\"\n",
      "      # t596 = prims.linear(t595, t_transformer_h_7_mlp_c_fc_weight, t_transformer_h_7_mlp_c_fc_bias)  # t596: \"cuda:0 f32[4, 64, 3072]\"\n",
      "  [t604] = nvFusion40(t596)\n",
      "    # t597 = prims.mul(0.5, t596)  # t597: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t598 = prims.pow(t596, 3.0)  # t598: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t599 = prims.mul(0.044715, t598)  # t599: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t600 = prims.add(t596, t599)  # t600: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t601 = prims.mul(0.7978845608028654, t600)  # t601: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t602 = prims.tanh(t601)  # t602: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t603 = prims.add(1.0, t602)  # t603: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t604 = prims.mul(t597, t603)  # t604: \"cuda:0 f32[4, 64, 3072]\"\n",
      "  t605 = torch.nn.functional.linear(t604, t_transformer_h_7_mlp_c_proj_weight, t_transformer_h_7_mlp_c_proj_bias)  # t605: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t605 = ltorch.linear(t604, t_transformer_h_7_mlp_c_proj_weight, t_transformer_h_7_mlp_c_proj_bias)  # t605: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t605 = prims.linear(t604, t_transformer_h_7_mlp_c_proj_weight, t_transformer_h_7_mlp_c_proj_bias)  # t605: \"cuda:0 f32[4, 64, 768]\"\n",
      "  t1244 = torch.unsqueeze(t_transformer_h_8_ln_1_weight, 0)  # t1244: \"cuda:0 f32[1, 768]\"\n",
      "    # t1244 = ltorch.unsqueeze(t_transformer_h_8_ln_1_weight, 0)  # t1244: \"cuda:0 f32[1, 768]\"\n",
      "      # t1244 = prims.broadcast_in_dim(t_transformer_h_8_ln_1_weight, [1, 768], [1])  # t1244: \"cuda:0 f32[1, 768]\"\n",
      "  t1245 = torch.unsqueeze(t1244, 1)  # t1245: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1245 = ltorch.unsqueeze(t1244, 1)  # t1245: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1245 = prims.broadcast_in_dim(t1244, [1, 1, 768], [0, 2])  # t1245: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1244\n",
      "  t619 = Tensor.expand(t1245, (4, 64, 768))  # t619: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t619 = ltorch.expand(t1245, (4, 64, 768))  # t619: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t619 = prims.broadcast_in_dim(t1245, (4, 64, 768), (0, 1, 2))  # t619: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1245\n",
      "  t1247 = torch.unsqueeze(t_transformer_h_8_ln_1_bias, 0)  # t1247: \"cuda:0 f32[1, 768]\"\n",
      "    # t1247 = ltorch.unsqueeze(t_transformer_h_8_ln_1_bias, 0)  # t1247: \"cuda:0 f32[1, 768]\"\n",
      "      # t1247 = prims.broadcast_in_dim(t_transformer_h_8_ln_1_bias, [1, 768], [1])  # t1247: \"cuda:0 f32[1, 768]\"\n",
      "  t1248 = torch.unsqueeze(t1247, 1)  # t1248: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1248 = ltorch.unsqueeze(t1247, 1)  # t1248: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1248 = prims.broadcast_in_dim(t1247, [1, 1, 768], [0, 2])  # t1248: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1247\n",
      "  t621 = Tensor.expand(t1248, (4, 64, 768))  # t621: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t621 = ltorch.expand(t1248, (4, 64, 768))  # t621: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t621 = prims.broadcast_in_dim(t1248, (4, 64, 768), (0, 1, 2))  # t621: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1248\n",
      "  [t606, t610, t614, t616, t622] = nvFusion41(t579, t605, t619, t621)\n",
      "    # t606 = prims.add(t579, t605)  # t606: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # (t609, t610) = prims.var_mean(t606, (2,), correction=0)\n",
      "    # t611 = prims.broadcast_in_dim(t609, [4, 64, 1], [0, 1])  # t611: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t612 = prims.broadcast_in_dim(t610, [4, 64, 1], [0, 1])  # t612: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t613 = prims.add(t611, 1e-05)  # t613: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t614 = prims.rsqrt(t613)  # t614: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t615 = prims.broadcast_in_dim(t612, (4, 64, 768), (0, 1, 2))  # t615: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t616 = prims.sub(t606, t615)  # t616: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t617 = prims.broadcast_in_dim(t614, (4, 64, 768), (0, 1, 2))  # t617: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t618 = prims.mul(t616, t617)  # t618: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t620 = prims.mul(t618, t619)  # t620: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t622 = prims.add(t620, t621)  # t622: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t605, t621\n",
      "  t623 = torch.nn.functional.linear(t622, t_transformer_h_8_attn_c_attn_weight, t_transformer_h_8_attn_c_attn_bias)  # t623: \"cuda:0 f32[4, 64, 2304]\"\n",
      "    # t623 = ltorch.linear(t622, t_transformer_h_8_attn_c_attn_weight, t_transformer_h_8_attn_c_attn_bias)  # t623: \"cuda:0 f32[4, 64, 2304]\"\n",
      "      # t623 = prims.linear(t622, t_transformer_h_8_attn_c_attn_weight, t_transformer_h_8_attn_c_attn_bias)  # t623: \"cuda:0 f32[4, 64, 2304]\"\n",
      "  [t624, t625, t626] = torch.split(t623, 768, 2)\n",
      "    # [t624, t625, t626] = ltorch.split(t623, 768, 2)\n",
      "      # t624 = prims.slice_prim(t623, [0, 0, 0], [4, 64, 768], [1, 1, 1])  # t624: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t625 = prims.slice_prim(t623, [0, 0, 768], [4, 64, 1536], [1, 1, 1])  # t625: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t626 = prims.slice_prim(t623, [0, 0, 1536], [4, 64, 2304], [1, 1, 1])  # t626: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t623\n",
      "  t627 = torch.reshape(t624, (4, 64, 12, 64))  # t627: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t627 = ltorch.reshape(t624, (4, 64, 12, 64))  # t627: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t627 = prims.reshape(t624, (4, 64, 12, 64))  # t627: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t624\n",
      "  t628 = torch.permute(t627, (0, 2, 1, 3))  # t628: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t628 = ltorch.permute(t627, (0, 2, 1, 3))  # t628: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t628 = prims.transpose(t627, (0, 2, 1, 3))  # t628: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t627\n",
      "  t629 = torch.reshape(t625, (4, 64, 12, 64))  # t629: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t629 = ltorch.reshape(t625, (4, 64, 12, 64))  # t629: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t629 = prims.reshape(t625, (4, 64, 12, 64))  # t629: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t625\n",
      "  t630 = torch.permute(t629, (0, 2, 1, 3))  # t630: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t630 = ltorch.permute(t629, (0, 2, 1, 3))  # t630: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t630 = prims.transpose(t629, (0, 2, 1, 3))  # t630: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t629\n",
      "  t631 = torch.reshape(t626, (4, 64, 12, 64))  # t631: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t631 = ltorch.reshape(t626, (4, 64, 12, 64))  # t631: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t631 = prims.reshape(t626, (4, 64, 12, 64))  # t631: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t626\n",
      "  t632 = torch.permute(t631, (0, 2, 1, 3))  # t632: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t632 = ltorch.permute(t631, (0, 2, 1, 3))  # t632: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t632 = prims.transpose(t631, (0, 2, 1, 3))  # t632: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t631\n",
      "  t633 = torch.permute(t630, (0, 1, 3, 2))  # t633: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t633 = ltorch.permute(t630, (0, 1, 3, 2))  # t633: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t633 = prims.transpose(t630, (0, 1, 3, 2))  # t633: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t630\n",
      "  t634 = torch.matmul(t628, t633)  # t634: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t634 = ltorch.matmul(t628, t633)  # t634: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t634 = prims.matmul(t628, t633)  # t634: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  t636 = torch_slice_prim_impl(t_transformer_h_8_attn_bias, [0, 0, 0, 0], [1, 1, 64, 64], [1, 1, 1, 1])  # t636: \"cuda:0 f32[1, 1, 64, 64]\"\n",
      "  [t638, t648] = nvFusion42(t634, t636)\n",
      "    # t635 = prims.mul(t634, 0.125)  # t635: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t637 = prims.eq(t636, 0.0)  # t637: \"cuda:0 b8[1, 1, 64, 64]\"\n",
      "    # t638 = prims.broadcast_in_dim(t637, (4, 12, 64, 64), (0, 1, 2, 3))  # t638: \"cuda:0 b8[4, 12, 64, 64]\"\n",
      "    # t639 = prims.where(t638, -float('inf'), t635)  # t639: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t640 = prims.amax(t639, (3,))  # t640: \"cuda:0 f32[4, 12, 64]\"\n",
      "    # t641 = prims.broadcast_in_dim(t640, [4, 12, 64, 1], [0, 1, 2])  # t641: \"cuda:0 f32[4, 12, 64, 1]\"\n",
      "    # t642 = prims.broadcast_in_dim(t641, (4, 12, 64, 64), (0, 1, 2, 3))  # t642: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t643 = prims.sub(t639, t642)  # t643: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t644 = prims.exp(t643)  # t644: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t645 = prims.sum(t644, (3,))  # t645: \"cuda:0 f32[4, 12, 64]\"\n",
      "    # t646 = prims.broadcast_in_dim(t645, [4, 12, 64, 1], [0, 1, 2])  # t646: \"cuda:0 f32[4, 12, 64, 1]\"\n",
      "    # t647 = prims.broadcast_in_dim(t646, (4, 12, 64, 64), (0, 1, 2, 3))  # t647: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t648 = prims.div(t644, t647)  # t648: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t634, t636\n",
      "  t649 = torch.matmul(t648, t632)  # t649: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t649 = ltorch.matmul(t648, t632)  # t649: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t649 = prims.matmul(t648, t632)  # t649: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  t650 = torch.permute(t649, (0, 2, 1, 3))  # t650: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t650 = ltorch.permute(t649, (0, 2, 1, 3))  # t650: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t650 = prims.transpose(t649, (0, 2, 1, 3))  # t650: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t649\n",
      "  [t651] = nvFusion43(t650)\n",
      "    # t651 = prims.stride_order(t650, (3, 2, 1, 0))  # t651: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t650\n",
      "  t652 = torch.reshape(t651, (4, 64, 768))  # t652: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t652 = ltorch.reshape(t651, (4, 64, 768))  # t652: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t652 = prims.reshape(t651, (4, 64, 768))  # t652: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t651\n",
      "  t653 = torch.nn.functional.linear(t652, t_transformer_h_8_attn_c_proj_weight, t_transformer_h_8_attn_c_proj_bias)  # t653: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t653 = ltorch.linear(t652, t_transformer_h_8_attn_c_proj_weight, t_transformer_h_8_attn_c_proj_bias)  # t653: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t653 = prims.linear(t652, t_transformer_h_8_attn_c_proj_weight, t_transformer_h_8_attn_c_proj_bias)  # t653: \"cuda:0 f32[4, 64, 768]\"\n",
      "  t1263 = torch.unsqueeze(t_transformer_h_8_ln_2_weight, 0)  # t1263: \"cuda:0 f32[1, 768]\"\n",
      "    # t1263 = ltorch.unsqueeze(t_transformer_h_8_ln_2_weight, 0)  # t1263: \"cuda:0 f32[1, 768]\"\n",
      "      # t1263 = prims.broadcast_in_dim(t_transformer_h_8_ln_2_weight, [1, 768], [1])  # t1263: \"cuda:0 f32[1, 768]\"\n",
      "  t1264 = torch.unsqueeze(t1263, 1)  # t1264: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1264 = ltorch.unsqueeze(t1263, 1)  # t1264: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1264 = prims.broadcast_in_dim(t1263, [1, 1, 768], [0, 2])  # t1264: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1263\n",
      "  t667 = Tensor.expand(t1264, (4, 64, 768))  # t667: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t667 = ltorch.expand(t1264, (4, 64, 768))  # t667: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t667 = prims.broadcast_in_dim(t1264, (4, 64, 768), (0, 1, 2))  # t667: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1264\n",
      "  t1266 = torch.unsqueeze(t_transformer_h_8_ln_2_bias, 0)  # t1266: \"cuda:0 f32[1, 768]\"\n",
      "    # t1266 = ltorch.unsqueeze(t_transformer_h_8_ln_2_bias, 0)  # t1266: \"cuda:0 f32[1, 768]\"\n",
      "      # t1266 = prims.broadcast_in_dim(t_transformer_h_8_ln_2_bias, [1, 768], [1])  # t1266: \"cuda:0 f32[1, 768]\"\n",
      "  t1267 = torch.unsqueeze(t1266, 1)  # t1267: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1267 = ltorch.unsqueeze(t1266, 1)  # t1267: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1267 = prims.broadcast_in_dim(t1266, [1, 1, 768], [0, 2])  # t1267: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1266\n",
      "  t669 = Tensor.expand(t1267, (4, 64, 768))  # t669: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t669 = ltorch.expand(t1267, (4, 64, 768))  # t669: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t669 = prims.broadcast_in_dim(t1267, (4, 64, 768), (0, 1, 2))  # t669: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1267\n",
      "  [t654, t658, t662, t664, t670] = nvFusion44(t606, t653, t667, t669)\n",
      "    # t654 = prims.add(t606, t653)  # t654: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # (t657, t658) = prims.var_mean(t654, (2,), correction=0)\n",
      "    # t659 = prims.broadcast_in_dim(t657, [4, 64, 1], [0, 1])  # t659: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t660 = prims.broadcast_in_dim(t658, [4, 64, 1], [0, 1])  # t660: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t661 = prims.add(t659, 1e-05)  # t661: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t662 = prims.rsqrt(t661)  # t662: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t663 = prims.broadcast_in_dim(t660, (4, 64, 768), (0, 1, 2))  # t663: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t664 = prims.sub(t654, t663)  # t664: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t665 = prims.broadcast_in_dim(t662, (4, 64, 768), (0, 1, 2))  # t665: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t666 = prims.mul(t664, t665)  # t666: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t668 = prims.mul(t666, t667)  # t668: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t670 = prims.add(t668, t669)  # t670: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t653, t669\n",
      "  t671 = torch.nn.functional.linear(t670, t_transformer_h_8_mlp_c_fc_weight, t_transformer_h_8_mlp_c_fc_bias)  # t671: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t671 = ltorch.linear(t670, t_transformer_h_8_mlp_c_fc_weight, t_transformer_h_8_mlp_c_fc_bias)  # t671: \"cuda:0 f32[4, 64, 3072]\"\n",
      "      # t671 = prims.linear(t670, t_transformer_h_8_mlp_c_fc_weight, t_transformer_h_8_mlp_c_fc_bias)  # t671: \"cuda:0 f32[4, 64, 3072]\"\n",
      "  [t679] = nvFusion45(t671)\n",
      "    # t672 = prims.mul(0.5, t671)  # t672: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t673 = prims.pow(t671, 3.0)  # t673: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t674 = prims.mul(0.044715, t673)  # t674: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t675 = prims.add(t671, t674)  # t675: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t676 = prims.mul(0.7978845608028654, t675)  # t676: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t677 = prims.tanh(t676)  # t677: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t678 = prims.add(1.0, t677)  # t678: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t679 = prims.mul(t672, t678)  # t679: \"cuda:0 f32[4, 64, 3072]\"\n",
      "  t680 = torch.nn.functional.linear(t679, t_transformer_h_8_mlp_c_proj_weight, t_transformer_h_8_mlp_c_proj_bias)  # t680: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t680 = ltorch.linear(t679, t_transformer_h_8_mlp_c_proj_weight, t_transformer_h_8_mlp_c_proj_bias)  # t680: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t680 = prims.linear(t679, t_transformer_h_8_mlp_c_proj_weight, t_transformer_h_8_mlp_c_proj_bias)  # t680: \"cuda:0 f32[4, 64, 768]\"\n",
      "  t1269 = torch.unsqueeze(t_transformer_h_9_ln_1_weight, 0)  # t1269: \"cuda:0 f32[1, 768]\"\n",
      "    # t1269 = ltorch.unsqueeze(t_transformer_h_9_ln_1_weight, 0)  # t1269: \"cuda:0 f32[1, 768]\"\n",
      "      # t1269 = prims.broadcast_in_dim(t_transformer_h_9_ln_1_weight, [1, 768], [1])  # t1269: \"cuda:0 f32[1, 768]\"\n",
      "  t1270 = torch.unsqueeze(t1269, 1)  # t1270: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1270 = ltorch.unsqueeze(t1269, 1)  # t1270: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1270 = prims.broadcast_in_dim(t1269, [1, 1, 768], [0, 2])  # t1270: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1269\n",
      "  t694 = Tensor.expand(t1270, (4, 64, 768))  # t694: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t694 = ltorch.expand(t1270, (4, 64, 768))  # t694: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t694 = prims.broadcast_in_dim(t1270, (4, 64, 768), (0, 1, 2))  # t694: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1270\n",
      "  t1272 = torch.unsqueeze(t_transformer_h_9_ln_1_bias, 0)  # t1272: \"cuda:0 f32[1, 768]\"\n",
      "    # t1272 = ltorch.unsqueeze(t_transformer_h_9_ln_1_bias, 0)  # t1272: \"cuda:0 f32[1, 768]\"\n",
      "      # t1272 = prims.broadcast_in_dim(t_transformer_h_9_ln_1_bias, [1, 768], [1])  # t1272: \"cuda:0 f32[1, 768]\"\n",
      "  t1273 = torch.unsqueeze(t1272, 1)  # t1273: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1273 = ltorch.unsqueeze(t1272, 1)  # t1273: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1273 = prims.broadcast_in_dim(t1272, [1, 1, 768], [0, 2])  # t1273: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1272\n",
      "  t696 = Tensor.expand(t1273, (4, 64, 768))  # t696: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t696 = ltorch.expand(t1273, (4, 64, 768))  # t696: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t696 = prims.broadcast_in_dim(t1273, (4, 64, 768), (0, 1, 2))  # t696: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1273\n",
      "  [t681, t685, t689, t691, t697] = nvFusion46(t654, t680, t694, t696)\n",
      "    # t681 = prims.add(t654, t680)  # t681: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # (t684, t685) = prims.var_mean(t681, (2,), correction=0)\n",
      "    # t686 = prims.broadcast_in_dim(t684, [4, 64, 1], [0, 1])  # t686: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t687 = prims.broadcast_in_dim(t685, [4, 64, 1], [0, 1])  # t687: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t688 = prims.add(t686, 1e-05)  # t688: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t689 = prims.rsqrt(t688)  # t689: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t690 = prims.broadcast_in_dim(t687, (4, 64, 768), (0, 1, 2))  # t690: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t691 = prims.sub(t681, t690)  # t691: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t692 = prims.broadcast_in_dim(t689, (4, 64, 768), (0, 1, 2))  # t692: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t693 = prims.mul(t691, t692)  # t693: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t695 = prims.mul(t693, t694)  # t695: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t697 = prims.add(t695, t696)  # t697: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t680, t696\n",
      "  t698 = torch.nn.functional.linear(t697, t_transformer_h_9_attn_c_attn_weight, t_transformer_h_9_attn_c_attn_bias)  # t698: \"cuda:0 f32[4, 64, 2304]\"\n",
      "    # t698 = ltorch.linear(t697, t_transformer_h_9_attn_c_attn_weight, t_transformer_h_9_attn_c_attn_bias)  # t698: \"cuda:0 f32[4, 64, 2304]\"\n",
      "      # t698 = prims.linear(t697, t_transformer_h_9_attn_c_attn_weight, t_transformer_h_9_attn_c_attn_bias)  # t698: \"cuda:0 f32[4, 64, 2304]\"\n",
      "  [t699, t700, t701] = torch.split(t698, 768, 2)\n",
      "    # [t699, t700, t701] = ltorch.split(t698, 768, 2)\n",
      "      # t699 = prims.slice_prim(t698, [0, 0, 0], [4, 64, 768], [1, 1, 1])  # t699: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t700 = prims.slice_prim(t698, [0, 0, 768], [4, 64, 1536], [1, 1, 1])  # t700: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t701 = prims.slice_prim(t698, [0, 0, 1536], [4, 64, 2304], [1, 1, 1])  # t701: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t698\n",
      "  t702 = torch.reshape(t699, (4, 64, 12, 64))  # t702: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t702 = ltorch.reshape(t699, (4, 64, 12, 64))  # t702: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t702 = prims.reshape(t699, (4, 64, 12, 64))  # t702: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t699\n",
      "  t703 = torch.permute(t702, (0, 2, 1, 3))  # t703: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t703 = ltorch.permute(t702, (0, 2, 1, 3))  # t703: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t703 = prims.transpose(t702, (0, 2, 1, 3))  # t703: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t702\n",
      "  t704 = torch.reshape(t700, (4, 64, 12, 64))  # t704: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t704 = ltorch.reshape(t700, (4, 64, 12, 64))  # t704: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t704 = prims.reshape(t700, (4, 64, 12, 64))  # t704: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t700\n",
      "  t705 = torch.permute(t704, (0, 2, 1, 3))  # t705: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t705 = ltorch.permute(t704, (0, 2, 1, 3))  # t705: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t705 = prims.transpose(t704, (0, 2, 1, 3))  # t705: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t704\n",
      "  t706 = torch.reshape(t701, (4, 64, 12, 64))  # t706: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t706 = ltorch.reshape(t701, (4, 64, 12, 64))  # t706: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t706 = prims.reshape(t701, (4, 64, 12, 64))  # t706: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t701\n",
      "  t707 = torch.permute(t706, (0, 2, 1, 3))  # t707: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t707 = ltorch.permute(t706, (0, 2, 1, 3))  # t707: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t707 = prims.transpose(t706, (0, 2, 1, 3))  # t707: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t706\n",
      "  t708 = torch.permute(t705, (0, 1, 3, 2))  # t708: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t708 = ltorch.permute(t705, (0, 1, 3, 2))  # t708: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t708 = prims.transpose(t705, (0, 1, 3, 2))  # t708: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t705\n",
      "  t709 = torch.matmul(t703, t708)  # t709: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t709 = ltorch.matmul(t703, t708)  # t709: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t709 = prims.matmul(t703, t708)  # t709: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  t711 = torch_slice_prim_impl(t_transformer_h_9_attn_bias, [0, 0, 0, 0], [1, 1, 64, 64], [1, 1, 1, 1])  # t711: \"cuda:0 f32[1, 1, 64, 64]\"\n",
      "  [t713, t723] = nvFusion47(t709, t711)\n",
      "    # t710 = prims.mul(t709, 0.125)  # t710: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t712 = prims.eq(t711, 0.0)  # t712: \"cuda:0 b8[1, 1, 64, 64]\"\n",
      "    # t713 = prims.broadcast_in_dim(t712, (4, 12, 64, 64), (0, 1, 2, 3))  # t713: \"cuda:0 b8[4, 12, 64, 64]\"\n",
      "    # t714 = prims.where(t713, -float('inf'), t710)  # t714: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t715 = prims.amax(t714, (3,))  # t715: \"cuda:0 f32[4, 12, 64]\"\n",
      "    # t716 = prims.broadcast_in_dim(t715, [4, 12, 64, 1], [0, 1, 2])  # t716: \"cuda:0 f32[4, 12, 64, 1]\"\n",
      "    # t717 = prims.broadcast_in_dim(t716, (4, 12, 64, 64), (0, 1, 2, 3))  # t717: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t718 = prims.sub(t714, t717)  # t718: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t719 = prims.exp(t718)  # t719: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t720 = prims.sum(t719, (3,))  # t720: \"cuda:0 f32[4, 12, 64]\"\n",
      "    # t721 = prims.broadcast_in_dim(t720, [4, 12, 64, 1], [0, 1, 2])  # t721: \"cuda:0 f32[4, 12, 64, 1]\"\n",
      "    # t722 = prims.broadcast_in_dim(t721, (4, 12, 64, 64), (0, 1, 2, 3))  # t722: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t723 = prims.div(t719, t722)  # t723: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t709, t711\n",
      "  t724 = torch.matmul(t723, t707)  # t724: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t724 = ltorch.matmul(t723, t707)  # t724: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t724 = prims.matmul(t723, t707)  # t724: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  t725 = torch.permute(t724, (0, 2, 1, 3))  # t725: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t725 = ltorch.permute(t724, (0, 2, 1, 3))  # t725: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t725 = prims.transpose(t724, (0, 2, 1, 3))  # t725: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t724\n",
      "  [t726] = nvFusion48(t725)\n",
      "    # t726 = prims.stride_order(t725, (3, 2, 1, 0))  # t726: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t725\n",
      "  t727 = torch.reshape(t726, (4, 64, 768))  # t727: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t727 = ltorch.reshape(t726, (4, 64, 768))  # t727: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t727 = prims.reshape(t726, (4, 64, 768))  # t727: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t726\n",
      "  t728 = torch.nn.functional.linear(t727, t_transformer_h_9_attn_c_proj_weight, t_transformer_h_9_attn_c_proj_bias)  # t728: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t728 = ltorch.linear(t727, t_transformer_h_9_attn_c_proj_weight, t_transformer_h_9_attn_c_proj_bias)  # t728: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t728 = prims.linear(t727, t_transformer_h_9_attn_c_proj_weight, t_transformer_h_9_attn_c_proj_bias)  # t728: \"cuda:0 f32[4, 64, 768]\"\n",
      "  t1288 = torch.unsqueeze(t_transformer_h_9_ln_2_weight, 0)  # t1288: \"cuda:0 f32[1, 768]\"\n",
      "    # t1288 = ltorch.unsqueeze(t_transformer_h_9_ln_2_weight, 0)  # t1288: \"cuda:0 f32[1, 768]\"\n",
      "      # t1288 = prims.broadcast_in_dim(t_transformer_h_9_ln_2_weight, [1, 768], [1])  # t1288: \"cuda:0 f32[1, 768]\"\n",
      "  t1289 = torch.unsqueeze(t1288, 1)  # t1289: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1289 = ltorch.unsqueeze(t1288, 1)  # t1289: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1289 = prims.broadcast_in_dim(t1288, [1, 1, 768], [0, 2])  # t1289: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1288\n",
      "  t742 = Tensor.expand(t1289, (4, 64, 768))  # t742: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t742 = ltorch.expand(t1289, (4, 64, 768))  # t742: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t742 = prims.broadcast_in_dim(t1289, (4, 64, 768), (0, 1, 2))  # t742: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1289\n",
      "  t1291 = torch.unsqueeze(t_transformer_h_9_ln_2_bias, 0)  # t1291: \"cuda:0 f32[1, 768]\"\n",
      "    # t1291 = ltorch.unsqueeze(t_transformer_h_9_ln_2_bias, 0)  # t1291: \"cuda:0 f32[1, 768]\"\n",
      "      # t1291 = prims.broadcast_in_dim(t_transformer_h_9_ln_2_bias, [1, 768], [1])  # t1291: \"cuda:0 f32[1, 768]\"\n",
      "  t1292 = torch.unsqueeze(t1291, 1)  # t1292: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1292 = ltorch.unsqueeze(t1291, 1)  # t1292: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1292 = prims.broadcast_in_dim(t1291, [1, 1, 768], [0, 2])  # t1292: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1291\n",
      "  t744 = Tensor.expand(t1292, (4, 64, 768))  # t744: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t744 = ltorch.expand(t1292, (4, 64, 768))  # t744: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t744 = prims.broadcast_in_dim(t1292, (4, 64, 768), (0, 1, 2))  # t744: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1292\n",
      "  [t729, t733, t737, t739, t745] = nvFusion49(t681, t728, t742, t744)\n",
      "    # t729 = prims.add(t681, t728)  # t729: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # (t732, t733) = prims.var_mean(t729, (2,), correction=0)\n",
      "    # t734 = prims.broadcast_in_dim(t732, [4, 64, 1], [0, 1])  # t734: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t735 = prims.broadcast_in_dim(t733, [4, 64, 1], [0, 1])  # t735: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t736 = prims.add(t734, 1e-05)  # t736: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t737 = prims.rsqrt(t736)  # t737: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t738 = prims.broadcast_in_dim(t735, (4, 64, 768), (0, 1, 2))  # t738: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t739 = prims.sub(t729, t738)  # t739: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t740 = prims.broadcast_in_dim(t737, (4, 64, 768), (0, 1, 2))  # t740: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t741 = prims.mul(t739, t740)  # t741: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t743 = prims.mul(t741, t742)  # t743: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t745 = prims.add(t743, t744)  # t745: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t728, t744\n",
      "  t746 = torch.nn.functional.linear(t745, t_transformer_h_9_mlp_c_fc_weight, t_transformer_h_9_mlp_c_fc_bias)  # t746: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t746 = ltorch.linear(t745, t_transformer_h_9_mlp_c_fc_weight, t_transformer_h_9_mlp_c_fc_bias)  # t746: \"cuda:0 f32[4, 64, 3072]\"\n",
      "      # t746 = prims.linear(t745, t_transformer_h_9_mlp_c_fc_weight, t_transformer_h_9_mlp_c_fc_bias)  # t746: \"cuda:0 f32[4, 64, 3072]\"\n",
      "  [t754] = nvFusion50(t746)\n",
      "    # t747 = prims.mul(0.5, t746)  # t747: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t748 = prims.pow(t746, 3.0)  # t748: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t749 = prims.mul(0.044715, t748)  # t749: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t750 = prims.add(t746, t749)  # t750: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t751 = prims.mul(0.7978845608028654, t750)  # t751: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t752 = prims.tanh(t751)  # t752: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t753 = prims.add(1.0, t752)  # t753: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t754 = prims.mul(t747, t753)  # t754: \"cuda:0 f32[4, 64, 3072]\"\n",
      "  t755 = torch.nn.functional.linear(t754, t_transformer_h_9_mlp_c_proj_weight, t_transformer_h_9_mlp_c_proj_bias)  # t755: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t755 = ltorch.linear(t754, t_transformer_h_9_mlp_c_proj_weight, t_transformer_h_9_mlp_c_proj_bias)  # t755: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t755 = prims.linear(t754, t_transformer_h_9_mlp_c_proj_weight, t_transformer_h_9_mlp_c_proj_bias)  # t755: \"cuda:0 f32[4, 64, 768]\"\n",
      "  t1294 = torch.unsqueeze(t_transformer_h_10_ln_1_weight, 0)  # t1294: \"cuda:0 f32[1, 768]\"\n",
      "    # t1294 = ltorch.unsqueeze(t_transformer_h_10_ln_1_weight, 0)  # t1294: \"cuda:0 f32[1, 768]\"\n",
      "      # t1294 = prims.broadcast_in_dim(t_transformer_h_10_ln_1_weight, [1, 768], [1])  # t1294: \"cuda:0 f32[1, 768]\"\n",
      "  t1295 = torch.unsqueeze(t1294, 1)  # t1295: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1295 = ltorch.unsqueeze(t1294, 1)  # t1295: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1295 = prims.broadcast_in_dim(t1294, [1, 1, 768], [0, 2])  # t1295: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1294\n",
      "  t769 = Tensor.expand(t1295, (4, 64, 768))  # t769: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t769 = ltorch.expand(t1295, (4, 64, 768))  # t769: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t769 = prims.broadcast_in_dim(t1295, (4, 64, 768), (0, 1, 2))  # t769: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1295\n",
      "  t1297 = torch.unsqueeze(t_transformer_h_10_ln_1_bias, 0)  # t1297: \"cuda:0 f32[1, 768]\"\n",
      "    # t1297 = ltorch.unsqueeze(t_transformer_h_10_ln_1_bias, 0)  # t1297: \"cuda:0 f32[1, 768]\"\n",
      "      # t1297 = prims.broadcast_in_dim(t_transformer_h_10_ln_1_bias, [1, 768], [1])  # t1297: \"cuda:0 f32[1, 768]\"\n",
      "  t1298 = torch.unsqueeze(t1297, 1)  # t1298: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1298 = ltorch.unsqueeze(t1297, 1)  # t1298: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1298 = prims.broadcast_in_dim(t1297, [1, 1, 768], [0, 2])  # t1298: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1297\n",
      "  t771 = Tensor.expand(t1298, (4, 64, 768))  # t771: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t771 = ltorch.expand(t1298, (4, 64, 768))  # t771: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t771 = prims.broadcast_in_dim(t1298, (4, 64, 768), (0, 1, 2))  # t771: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1298\n",
      "  [t756, t760, t764, t766, t772] = nvFusion51(t729, t755, t769, t771)\n",
      "    # t756 = prims.add(t729, t755)  # t756: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # (t759, t760) = prims.var_mean(t756, (2,), correction=0)\n",
      "    # t761 = prims.broadcast_in_dim(t759, [4, 64, 1], [0, 1])  # t761: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t762 = prims.broadcast_in_dim(t760, [4, 64, 1], [0, 1])  # t762: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t763 = prims.add(t761, 1e-05)  # t763: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t764 = prims.rsqrt(t763)  # t764: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t765 = prims.broadcast_in_dim(t762, (4, 64, 768), (0, 1, 2))  # t765: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t766 = prims.sub(t756, t765)  # t766: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t767 = prims.broadcast_in_dim(t764, (4, 64, 768), (0, 1, 2))  # t767: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t768 = prims.mul(t766, t767)  # t768: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t770 = prims.mul(t768, t769)  # t770: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t772 = prims.add(t770, t771)  # t772: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t755, t771\n",
      "  t773 = torch.nn.functional.linear(t772, t_transformer_h_10_attn_c_attn_weight, t_transformer_h_10_attn_c_attn_bias)  # t773: \"cuda:0 f32[4, 64, 2304]\"\n",
      "    # t773 = ltorch.linear(t772, t_transformer_h_10_attn_c_attn_weight, t_transformer_h_10_attn_c_attn_bias)  # t773: \"cuda:0 f32[4, 64, 2304]\"\n",
      "      # t773 = prims.linear(t772, t_transformer_h_10_attn_c_attn_weight, t_transformer_h_10_attn_c_attn_bias)  # t773: \"cuda:0 f32[4, 64, 2304]\"\n",
      "  [t774, t775, t776] = torch.split(t773, 768, 2)\n",
      "    # [t774, t775, t776] = ltorch.split(t773, 768, 2)\n",
      "      # t774 = prims.slice_prim(t773, [0, 0, 0], [4, 64, 768], [1, 1, 1])  # t774: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t775 = prims.slice_prim(t773, [0, 0, 768], [4, 64, 1536], [1, 1, 1])  # t775: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t776 = prims.slice_prim(t773, [0, 0, 1536], [4, 64, 2304], [1, 1, 1])  # t776: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t773\n",
      "  t777 = torch.reshape(t774, (4, 64, 12, 64))  # t777: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t777 = ltorch.reshape(t774, (4, 64, 12, 64))  # t777: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t777 = prims.reshape(t774, (4, 64, 12, 64))  # t777: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t774\n",
      "  t778 = torch.permute(t777, (0, 2, 1, 3))  # t778: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t778 = ltorch.permute(t777, (0, 2, 1, 3))  # t778: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t778 = prims.transpose(t777, (0, 2, 1, 3))  # t778: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t777\n",
      "  t779 = torch.reshape(t775, (4, 64, 12, 64))  # t779: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t779 = ltorch.reshape(t775, (4, 64, 12, 64))  # t779: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t779 = prims.reshape(t775, (4, 64, 12, 64))  # t779: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t775\n",
      "  t780 = torch.permute(t779, (0, 2, 1, 3))  # t780: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t780 = ltorch.permute(t779, (0, 2, 1, 3))  # t780: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t780 = prims.transpose(t779, (0, 2, 1, 3))  # t780: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t779\n",
      "  t781 = torch.reshape(t776, (4, 64, 12, 64))  # t781: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t781 = ltorch.reshape(t776, (4, 64, 12, 64))  # t781: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t781 = prims.reshape(t776, (4, 64, 12, 64))  # t781: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t776\n",
      "  t782 = torch.permute(t781, (0, 2, 1, 3))  # t782: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t782 = ltorch.permute(t781, (0, 2, 1, 3))  # t782: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t782 = prims.transpose(t781, (0, 2, 1, 3))  # t782: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t781\n",
      "  t783 = torch.permute(t780, (0, 1, 3, 2))  # t783: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t783 = ltorch.permute(t780, (0, 1, 3, 2))  # t783: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t783 = prims.transpose(t780, (0, 1, 3, 2))  # t783: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t780\n",
      "  t784 = torch.matmul(t778, t783)  # t784: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t784 = ltorch.matmul(t778, t783)  # t784: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t784 = prims.matmul(t778, t783)  # t784: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  t786 = torch_slice_prim_impl(t_transformer_h_10_attn_bias, [0, 0, 0, 0], [1, 1, 64, 64], [1, 1, 1, 1])  # t786: \"cuda:0 f32[1, 1, 64, 64]\"\n",
      "  [t788, t798] = nvFusion52(t784, t786)\n",
      "    # t785 = prims.mul(t784, 0.125)  # t785: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t787 = prims.eq(t786, 0.0)  # t787: \"cuda:0 b8[1, 1, 64, 64]\"\n",
      "    # t788 = prims.broadcast_in_dim(t787, (4, 12, 64, 64), (0, 1, 2, 3))  # t788: \"cuda:0 b8[4, 12, 64, 64]\"\n",
      "    # t789 = prims.where(t788, -float('inf'), t785)  # t789: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t790 = prims.amax(t789, (3,))  # t790: \"cuda:0 f32[4, 12, 64]\"\n",
      "    # t791 = prims.broadcast_in_dim(t790, [4, 12, 64, 1], [0, 1, 2])  # t791: \"cuda:0 f32[4, 12, 64, 1]\"\n",
      "    # t792 = prims.broadcast_in_dim(t791, (4, 12, 64, 64), (0, 1, 2, 3))  # t792: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t793 = prims.sub(t789, t792)  # t793: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t794 = prims.exp(t793)  # t794: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t795 = prims.sum(t794, (3,))  # t795: \"cuda:0 f32[4, 12, 64]\"\n",
      "    # t796 = prims.broadcast_in_dim(t795, [4, 12, 64, 1], [0, 1, 2])  # t796: \"cuda:0 f32[4, 12, 64, 1]\"\n",
      "    # t797 = prims.broadcast_in_dim(t796, (4, 12, 64, 64), (0, 1, 2, 3))  # t797: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t798 = prims.div(t794, t797)  # t798: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t784, t786\n",
      "  t799 = torch.matmul(t798, t782)  # t799: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t799 = ltorch.matmul(t798, t782)  # t799: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t799 = prims.matmul(t798, t782)  # t799: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  t800 = torch.permute(t799, (0, 2, 1, 3))  # t800: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t800 = ltorch.permute(t799, (0, 2, 1, 3))  # t800: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t800 = prims.transpose(t799, (0, 2, 1, 3))  # t800: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t799\n",
      "  [t801] = nvFusion53(t800)\n",
      "    # t801 = prims.stride_order(t800, (3, 2, 1, 0))  # t801: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t800\n",
      "  t802 = torch.reshape(t801, (4, 64, 768))  # t802: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t802 = ltorch.reshape(t801, (4, 64, 768))  # t802: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t802 = prims.reshape(t801, (4, 64, 768))  # t802: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t801\n",
      "  t803 = torch.nn.functional.linear(t802, t_transformer_h_10_attn_c_proj_weight, t_transformer_h_10_attn_c_proj_bias)  # t803: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t803 = ltorch.linear(t802, t_transformer_h_10_attn_c_proj_weight, t_transformer_h_10_attn_c_proj_bias)  # t803: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t803 = prims.linear(t802, t_transformer_h_10_attn_c_proj_weight, t_transformer_h_10_attn_c_proj_bias)  # t803: \"cuda:0 f32[4, 64, 768]\"\n",
      "  t1313 = torch.unsqueeze(t_transformer_h_10_ln_2_weight, 0)  # t1313: \"cuda:0 f32[1, 768]\"\n",
      "    # t1313 = ltorch.unsqueeze(t_transformer_h_10_ln_2_weight, 0)  # t1313: \"cuda:0 f32[1, 768]\"\n",
      "      # t1313 = prims.broadcast_in_dim(t_transformer_h_10_ln_2_weight, [1, 768], [1])  # t1313: \"cuda:0 f32[1, 768]\"\n",
      "  t1314 = torch.unsqueeze(t1313, 1)  # t1314: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1314 = ltorch.unsqueeze(t1313, 1)  # t1314: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1314 = prims.broadcast_in_dim(t1313, [1, 1, 768], [0, 2])  # t1314: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1313\n",
      "  t817 = Tensor.expand(t1314, (4, 64, 768))  # t817: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t817 = ltorch.expand(t1314, (4, 64, 768))  # t817: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t817 = prims.broadcast_in_dim(t1314, (4, 64, 768), (0, 1, 2))  # t817: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1314\n",
      "  t1316 = torch.unsqueeze(t_transformer_h_10_ln_2_bias, 0)  # t1316: \"cuda:0 f32[1, 768]\"\n",
      "    # t1316 = ltorch.unsqueeze(t_transformer_h_10_ln_2_bias, 0)  # t1316: \"cuda:0 f32[1, 768]\"\n",
      "      # t1316 = prims.broadcast_in_dim(t_transformer_h_10_ln_2_bias, [1, 768], [1])  # t1316: \"cuda:0 f32[1, 768]\"\n",
      "  t1317 = torch.unsqueeze(t1316, 1)  # t1317: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1317 = ltorch.unsqueeze(t1316, 1)  # t1317: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1317 = prims.broadcast_in_dim(t1316, [1, 1, 768], [0, 2])  # t1317: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1316\n",
      "  t819 = Tensor.expand(t1317, (4, 64, 768))  # t819: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t819 = ltorch.expand(t1317, (4, 64, 768))  # t819: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t819 = prims.broadcast_in_dim(t1317, (4, 64, 768), (0, 1, 2))  # t819: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1317\n",
      "  [t804, t808, t812, t814, t820] = nvFusion54(t756, t803, t817, t819)\n",
      "    # t804 = prims.add(t756, t803)  # t804: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # (t807, t808) = prims.var_mean(t804, (2,), correction=0)\n",
      "    # t809 = prims.broadcast_in_dim(t807, [4, 64, 1], [0, 1])  # t809: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t810 = prims.broadcast_in_dim(t808, [4, 64, 1], [0, 1])  # t810: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t811 = prims.add(t809, 1e-05)  # t811: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t812 = prims.rsqrt(t811)  # t812: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t813 = prims.broadcast_in_dim(t810, (4, 64, 768), (0, 1, 2))  # t813: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t814 = prims.sub(t804, t813)  # t814: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t815 = prims.broadcast_in_dim(t812, (4, 64, 768), (0, 1, 2))  # t815: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t816 = prims.mul(t814, t815)  # t816: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t818 = prims.mul(t816, t817)  # t818: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t820 = prims.add(t818, t819)  # t820: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t803, t819\n",
      "  t821 = torch.nn.functional.linear(t820, t_transformer_h_10_mlp_c_fc_weight, t_transformer_h_10_mlp_c_fc_bias)  # t821: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t821 = ltorch.linear(t820, t_transformer_h_10_mlp_c_fc_weight, t_transformer_h_10_mlp_c_fc_bias)  # t821: \"cuda:0 f32[4, 64, 3072]\"\n",
      "      # t821 = prims.linear(t820, t_transformer_h_10_mlp_c_fc_weight, t_transformer_h_10_mlp_c_fc_bias)  # t821: \"cuda:0 f32[4, 64, 3072]\"\n",
      "  [t829] = nvFusion55(t821)\n",
      "    # t822 = prims.mul(0.5, t821)  # t822: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t823 = prims.pow(t821, 3.0)  # t823: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t824 = prims.mul(0.044715, t823)  # t824: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t825 = prims.add(t821, t824)  # t825: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t826 = prims.mul(0.7978845608028654, t825)  # t826: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t827 = prims.tanh(t826)  # t827: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t828 = prims.add(1.0, t827)  # t828: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t829 = prims.mul(t822, t828)  # t829: \"cuda:0 f32[4, 64, 3072]\"\n",
      "  t830 = torch.nn.functional.linear(t829, t_transformer_h_10_mlp_c_proj_weight, t_transformer_h_10_mlp_c_proj_bias)  # t830: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t830 = ltorch.linear(t829, t_transformer_h_10_mlp_c_proj_weight, t_transformer_h_10_mlp_c_proj_bias)  # t830: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t830 = prims.linear(t829, t_transformer_h_10_mlp_c_proj_weight, t_transformer_h_10_mlp_c_proj_bias)  # t830: \"cuda:0 f32[4, 64, 768]\"\n",
      "  t1319 = torch.unsqueeze(t_transformer_h_11_ln_1_weight, 0)  # t1319: \"cuda:0 f32[1, 768]\"\n",
      "    # t1319 = ltorch.unsqueeze(t_transformer_h_11_ln_1_weight, 0)  # t1319: \"cuda:0 f32[1, 768]\"\n",
      "      # t1319 = prims.broadcast_in_dim(t_transformer_h_11_ln_1_weight, [1, 768], [1])  # t1319: \"cuda:0 f32[1, 768]\"\n",
      "  t1320 = torch.unsqueeze(t1319, 1)  # t1320: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1320 = ltorch.unsqueeze(t1319, 1)  # t1320: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1320 = prims.broadcast_in_dim(t1319, [1, 1, 768], [0, 2])  # t1320: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1319\n",
      "  t844 = Tensor.expand(t1320, (4, 64, 768))  # t844: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t844 = ltorch.expand(t1320, (4, 64, 768))  # t844: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t844 = prims.broadcast_in_dim(t1320, (4, 64, 768), (0, 1, 2))  # t844: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1320\n",
      "  t1322 = torch.unsqueeze(t_transformer_h_11_ln_1_bias, 0)  # t1322: \"cuda:0 f32[1, 768]\"\n",
      "    # t1322 = ltorch.unsqueeze(t_transformer_h_11_ln_1_bias, 0)  # t1322: \"cuda:0 f32[1, 768]\"\n",
      "      # t1322 = prims.broadcast_in_dim(t_transformer_h_11_ln_1_bias, [1, 768], [1])  # t1322: \"cuda:0 f32[1, 768]\"\n",
      "  t1323 = torch.unsqueeze(t1322, 1)  # t1323: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1323 = ltorch.unsqueeze(t1322, 1)  # t1323: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1323 = prims.broadcast_in_dim(t1322, [1, 1, 768], [0, 2])  # t1323: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1322\n",
      "  t846 = Tensor.expand(t1323, (4, 64, 768))  # t846: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t846 = ltorch.expand(t1323, (4, 64, 768))  # t846: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t846 = prims.broadcast_in_dim(t1323, (4, 64, 768), (0, 1, 2))  # t846: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1323\n",
      "  [t831, t835, t839, t841, t847] = nvFusion56(t804, t830, t844, t846)\n",
      "    # t831 = prims.add(t804, t830)  # t831: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # (t834, t835) = prims.var_mean(t831, (2,), correction=0)\n",
      "    # t836 = prims.broadcast_in_dim(t834, [4, 64, 1], [0, 1])  # t836: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t837 = prims.broadcast_in_dim(t835, [4, 64, 1], [0, 1])  # t837: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t838 = prims.add(t836, 1e-05)  # t838: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t839 = prims.rsqrt(t838)  # t839: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t840 = prims.broadcast_in_dim(t837, (4, 64, 768), (0, 1, 2))  # t840: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t841 = prims.sub(t831, t840)  # t841: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t842 = prims.broadcast_in_dim(t839, (4, 64, 768), (0, 1, 2))  # t842: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t843 = prims.mul(t841, t842)  # t843: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t845 = prims.mul(t843, t844)  # t845: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t847 = prims.add(t845, t846)  # t847: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t830, t846\n",
      "  t848 = torch.nn.functional.linear(t847, t_transformer_h_11_attn_c_attn_weight, t_transformer_h_11_attn_c_attn_bias)  # t848: \"cuda:0 f32[4, 64, 2304]\"\n",
      "    # t848 = ltorch.linear(t847, t_transformer_h_11_attn_c_attn_weight, t_transformer_h_11_attn_c_attn_bias)  # t848: \"cuda:0 f32[4, 64, 2304]\"\n",
      "      # t848 = prims.linear(t847, t_transformer_h_11_attn_c_attn_weight, t_transformer_h_11_attn_c_attn_bias)  # t848: \"cuda:0 f32[4, 64, 2304]\"\n",
      "  [t849, t850, t851] = torch.split(t848, 768, 2)\n",
      "    # [t849, t850, t851] = ltorch.split(t848, 768, 2)\n",
      "      # t849 = prims.slice_prim(t848, [0, 0, 0], [4, 64, 768], [1, 1, 1])  # t849: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t850 = prims.slice_prim(t848, [0, 0, 768], [4, 64, 1536], [1, 1, 1])  # t850: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t851 = prims.slice_prim(t848, [0, 0, 1536], [4, 64, 2304], [1, 1, 1])  # t851: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t848\n",
      "  t852 = torch.reshape(t849, (4, 64, 12, 64))  # t852: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t852 = ltorch.reshape(t849, (4, 64, 12, 64))  # t852: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t852 = prims.reshape(t849, (4, 64, 12, 64))  # t852: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t849\n",
      "  t853 = torch.permute(t852, (0, 2, 1, 3))  # t853: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t853 = ltorch.permute(t852, (0, 2, 1, 3))  # t853: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t853 = prims.transpose(t852, (0, 2, 1, 3))  # t853: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t852\n",
      "  t854 = torch.reshape(t850, (4, 64, 12, 64))  # t854: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t854 = ltorch.reshape(t850, (4, 64, 12, 64))  # t854: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t854 = prims.reshape(t850, (4, 64, 12, 64))  # t854: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t850\n",
      "  t855 = torch.permute(t854, (0, 2, 1, 3))  # t855: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t855 = ltorch.permute(t854, (0, 2, 1, 3))  # t855: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t855 = prims.transpose(t854, (0, 2, 1, 3))  # t855: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t854\n",
      "  t856 = torch.reshape(t851, (4, 64, 12, 64))  # t856: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t856 = ltorch.reshape(t851, (4, 64, 12, 64))  # t856: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t856 = prims.reshape(t851, (4, 64, 12, 64))  # t856: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t851\n",
      "  t857 = torch.permute(t856, (0, 2, 1, 3))  # t857: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t857 = ltorch.permute(t856, (0, 2, 1, 3))  # t857: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t857 = prims.transpose(t856, (0, 2, 1, 3))  # t857: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t856\n",
      "  t858 = torch.permute(t855, (0, 1, 3, 2))  # t858: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t858 = ltorch.permute(t855, (0, 1, 3, 2))  # t858: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t858 = prims.transpose(t855, (0, 1, 3, 2))  # t858: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t855\n",
      "  t859 = torch.matmul(t853, t858)  # t859: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t859 = ltorch.matmul(t853, t858)  # t859: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t859 = prims.matmul(t853, t858)  # t859: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  t861 = torch_slice_prim_impl(t_transformer_h_11_attn_bias, [0, 0, 0, 0], [1, 1, 64, 64], [1, 1, 1, 1])  # t861: \"cuda:0 f32[1, 1, 64, 64]\"\n",
      "  [t863, t873] = nvFusion57(t859, t861)\n",
      "    # t860 = prims.mul(t859, 0.125)  # t860: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t862 = prims.eq(t861, 0.0)  # t862: \"cuda:0 b8[1, 1, 64, 64]\"\n",
      "    # t863 = prims.broadcast_in_dim(t862, (4, 12, 64, 64), (0, 1, 2, 3))  # t863: \"cuda:0 b8[4, 12, 64, 64]\"\n",
      "    # t864 = prims.where(t863, -float('inf'), t860)  # t864: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t865 = prims.amax(t864, (3,))  # t865: \"cuda:0 f32[4, 12, 64]\"\n",
      "    # t866 = prims.broadcast_in_dim(t865, [4, 12, 64, 1], [0, 1, 2])  # t866: \"cuda:0 f32[4, 12, 64, 1]\"\n",
      "    # t867 = prims.broadcast_in_dim(t866, (4, 12, 64, 64), (0, 1, 2, 3))  # t867: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t868 = prims.sub(t864, t867)  # t868: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t869 = prims.exp(t868)  # t869: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t870 = prims.sum(t869, (3,))  # t870: \"cuda:0 f32[4, 12, 64]\"\n",
      "    # t871 = prims.broadcast_in_dim(t870, [4, 12, 64, 1], [0, 1, 2])  # t871: \"cuda:0 f32[4, 12, 64, 1]\"\n",
      "    # t872 = prims.broadcast_in_dim(t871, (4, 12, 64, 64), (0, 1, 2, 3))  # t872: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t873 = prims.div(t869, t872)  # t873: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t859, t861\n",
      "  t874 = torch.matmul(t873, t857)  # t874: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t874 = ltorch.matmul(t873, t857)  # t874: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t874 = prims.matmul(t873, t857)  # t874: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  t875 = torch.permute(t874, (0, 2, 1, 3))  # t875: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t875 = ltorch.permute(t874, (0, 2, 1, 3))  # t875: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t875 = prims.transpose(t874, (0, 2, 1, 3))  # t875: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t874\n",
      "  [t876] = nvFusion58(t875)\n",
      "    # t876 = prims.stride_order(t875, (3, 2, 1, 0))  # t876: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t875\n",
      "  t877 = torch.reshape(t876, (4, 64, 768))  # t877: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t877 = ltorch.reshape(t876, (4, 64, 768))  # t877: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t877 = prims.reshape(t876, (4, 64, 768))  # t877: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t876\n",
      "  t878 = torch.nn.functional.linear(t877, t_transformer_h_11_attn_c_proj_weight, t_transformer_h_11_attn_c_proj_bias)  # t878: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t878 = ltorch.linear(t877, t_transformer_h_11_attn_c_proj_weight, t_transformer_h_11_attn_c_proj_bias)  # t878: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t878 = prims.linear(t877, t_transformer_h_11_attn_c_proj_weight, t_transformer_h_11_attn_c_proj_bias)  # t878: \"cuda:0 f32[4, 64, 768]\"\n",
      "  t1338 = torch.unsqueeze(t_transformer_h_11_ln_2_weight, 0)  # t1338: \"cuda:0 f32[1, 768]\"\n",
      "    # t1338 = ltorch.unsqueeze(t_transformer_h_11_ln_2_weight, 0)  # t1338: \"cuda:0 f32[1, 768]\"\n",
      "      # t1338 = prims.broadcast_in_dim(t_transformer_h_11_ln_2_weight, [1, 768], [1])  # t1338: \"cuda:0 f32[1, 768]\"\n",
      "  t1339 = torch.unsqueeze(t1338, 1)  # t1339: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1339 = ltorch.unsqueeze(t1338, 1)  # t1339: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1339 = prims.broadcast_in_dim(t1338, [1, 1, 768], [0, 2])  # t1339: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1338\n",
      "  t892 = Tensor.expand(t1339, (4, 64, 768))  # t892: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t892 = ltorch.expand(t1339, (4, 64, 768))  # t892: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t892 = prims.broadcast_in_dim(t1339, (4, 64, 768), (0, 1, 2))  # t892: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1339\n",
      "  t1341 = torch.unsqueeze(t_transformer_h_11_ln_2_bias, 0)  # t1341: \"cuda:0 f32[1, 768]\"\n",
      "    # t1341 = ltorch.unsqueeze(t_transformer_h_11_ln_2_bias, 0)  # t1341: \"cuda:0 f32[1, 768]\"\n",
      "      # t1341 = prims.broadcast_in_dim(t_transformer_h_11_ln_2_bias, [1, 768], [1])  # t1341: \"cuda:0 f32[1, 768]\"\n",
      "  t1342 = torch.unsqueeze(t1341, 1)  # t1342: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1342 = ltorch.unsqueeze(t1341, 1)  # t1342: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1342 = prims.broadcast_in_dim(t1341, [1, 1, 768], [0, 2])  # t1342: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1341\n",
      "  t894 = Tensor.expand(t1342, (4, 64, 768))  # t894: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t894 = ltorch.expand(t1342, (4, 64, 768))  # t894: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t894 = prims.broadcast_in_dim(t1342, (4, 64, 768), (0, 1, 2))  # t894: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1342\n",
      "  [t879, t883, t887, t889, t895] = nvFusion59(t831, t878, t892, t894)\n",
      "    # t879 = prims.add(t831, t878)  # t879: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # (t882, t883) = prims.var_mean(t879, (2,), correction=0)\n",
      "    # t884 = prims.broadcast_in_dim(t882, [4, 64, 1], [0, 1])  # t884: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t885 = prims.broadcast_in_dim(t883, [4, 64, 1], [0, 1])  # t885: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t886 = prims.add(t884, 1e-05)  # t886: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t887 = prims.rsqrt(t886)  # t887: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t888 = prims.broadcast_in_dim(t885, (4, 64, 768), (0, 1, 2))  # t888: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t889 = prims.sub(t879, t888)  # t889: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t890 = prims.broadcast_in_dim(t887, (4, 64, 768), (0, 1, 2))  # t890: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t891 = prims.mul(t889, t890)  # t891: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t893 = prims.mul(t891, t892)  # t893: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t895 = prims.add(t893, t894)  # t895: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t878, t894\n",
      "  t896 = torch.nn.functional.linear(t895, t_transformer_h_11_mlp_c_fc_weight, t_transformer_h_11_mlp_c_fc_bias)  # t896: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t896 = ltorch.linear(t895, t_transformer_h_11_mlp_c_fc_weight, t_transformer_h_11_mlp_c_fc_bias)  # t896: \"cuda:0 f32[4, 64, 3072]\"\n",
      "      # t896 = prims.linear(t895, t_transformer_h_11_mlp_c_fc_weight, t_transformer_h_11_mlp_c_fc_bias)  # t896: \"cuda:0 f32[4, 64, 3072]\"\n",
      "  [t904] = nvFusion60(t896)\n",
      "    # t897 = prims.mul(0.5, t896)  # t897: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t898 = prims.pow(t896, 3.0)  # t898: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t899 = prims.mul(0.044715, t898)  # t899: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t900 = prims.add(t896, t899)  # t900: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t901 = prims.mul(0.7978845608028654, t900)  # t901: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t902 = prims.tanh(t901)  # t902: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t903 = prims.add(1.0, t902)  # t903: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t904 = prims.mul(t897, t903)  # t904: \"cuda:0 f32[4, 64, 3072]\"\n",
      "  t905 = torch.nn.functional.linear(t904, t_transformer_h_11_mlp_c_proj_weight, t_transformer_h_11_mlp_c_proj_bias)  # t905: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t905 = ltorch.linear(t904, t_transformer_h_11_mlp_c_proj_weight, t_transformer_h_11_mlp_c_proj_bias)  # t905: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t905 = prims.linear(t904, t_transformer_h_11_mlp_c_proj_weight, t_transformer_h_11_mlp_c_proj_bias)  # t905: \"cuda:0 f32[4, 64, 768]\"\n",
      "  t1344 = torch.unsqueeze(t_transformer_ln_f_weight, 0)  # t1344: \"cuda:0 f32[1, 768]\"\n",
      "    # t1344 = ltorch.unsqueeze(t_transformer_ln_f_weight, 0)  # t1344: \"cuda:0 f32[1, 768]\"\n",
      "      # t1344 = prims.broadcast_in_dim(t_transformer_ln_f_weight, [1, 768], [1])  # t1344: \"cuda:0 f32[1, 768]\"\n",
      "  t1345 = torch.unsqueeze(t1344, 1)  # t1345: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1345 = ltorch.unsqueeze(t1344, 1)  # t1345: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1345 = prims.broadcast_in_dim(t1344, [1, 1, 768], [0, 2])  # t1345: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1344\n",
      "  t919 = Tensor.expand(t1345, (4, 64, 768))  # t919: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t919 = ltorch.expand(t1345, (4, 64, 768))  # t919: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t919 = prims.broadcast_in_dim(t1345, (4, 64, 768), (0, 1, 2))  # t919: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1345\n",
      "  t1347 = torch.unsqueeze(t_transformer_ln_f_bias, 0)  # t1347: \"cuda:0 f32[1, 768]\"\n",
      "    # t1347 = ltorch.unsqueeze(t_transformer_ln_f_bias, 0)  # t1347: \"cuda:0 f32[1, 768]\"\n",
      "      # t1347 = prims.broadcast_in_dim(t_transformer_ln_f_bias, [1, 768], [1])  # t1347: \"cuda:0 f32[1, 768]\"\n",
      "  t1348 = torch.unsqueeze(t1347, 1)  # t1348: \"cuda:0 f32[1, 1, 768]\"\n",
      "    # t1348 = ltorch.unsqueeze(t1347, 1)  # t1348: \"cuda:0 f32[1, 1, 768]\"\n",
      "      # t1348 = prims.broadcast_in_dim(t1347, [1, 1, 768], [0, 2])  # t1348: \"cuda:0 f32[1, 1, 768]\"\n",
      "  del t1347\n",
      "  t921 = Tensor.expand(t1348, (4, 64, 768))  # t921: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t921 = ltorch.expand(t1348, (4, 64, 768))  # t921: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t921 = prims.broadcast_in_dim(t1348, (4, 64, 768), (0, 1, 2))  # t921: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t1348\n",
      "  [t906, t910, t914, t916, t922] = nvFusion61(t879, t905, t919, t921)\n",
      "    # t906 = prims.add(t879, t905)  # t906: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # (t909, t910) = prims.var_mean(t906, (2,), correction=0)\n",
      "    # t911 = prims.broadcast_in_dim(t909, [4, 64, 1], [0, 1])  # t911: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t912 = prims.broadcast_in_dim(t910, [4, 64, 1], [0, 1])  # t912: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t913 = prims.add(t911, 1e-05)  # t913: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t914 = prims.rsqrt(t913)  # t914: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t915 = prims.broadcast_in_dim(t912, (4, 64, 768), (0, 1, 2))  # t915: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t916 = prims.sub(t906, t915)  # t916: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t917 = prims.broadcast_in_dim(t914, (4, 64, 768), (0, 1, 2))  # t917: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t918 = prims.mul(t916, t917)  # t918: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t920 = prims.mul(t918, t919)  # t920: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t922 = prims.add(t920, t921)  # t922: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t905, t921\n",
      "  t923 = torch.nn.functional.linear(t922, t_lm_head_weight, None)  # t923: \"cuda:0 f32[4, 64, 50257]\"\n",
      "    # t923 = ltorch.linear(t922, t_lm_head_weight, None)  # t923: \"cuda:0 f32[4, 64, 50257]\"\n",
      "      # t923 = prims.linear(t922, t_lm_head_weight, None)  # t923: \"cuda:0 f32[4, 64, 50257]\"\n",
      "  t924 = torch.reshape(t923, (256, 50257))  # t924: \"cuda:0 f32[256, 50257]\"\n",
      "    # t924 = ltorch.reshape(t923, (256, 50257))  # t924: \"cuda:0 f32[256, 50257]\"\n",
      "      # t924 = prims.reshape(t923, (256, 50257))  # t924: \"cuda:0 f32[256, 50257]\"\n",
      "  t944 = torch.nn.functional.cross_entropy(t924, t925, None, None, -1, None, 'mean', 0.0)  # t944: \"cuda:0 f32[]\"\n",
      "    # t944 = ltorch.cross_entropy(t924, t925, None, None, -1, None, 'mean', 0.0)  # t944: \"cuda:0 f32[]\"\n",
      "      # t1035 = ltorch.log_softmax(t924, 1, dtype=None)  # t1035: \"cuda:0 f32[256, 50257]\"\n",
      "        # t1033 = ltorch.logsumexp(t924, 1, True)  # t1033: \"cuda:0 f32[256, 1]\"\n",
      "          # t1023 = ltorch.amax(t924, 1, True)  # t1023: \"cuda:0 f32[256, 1]\"\n",
      "            # t1022 = prims.amax(t924, (1,))  # t1022: \"cuda:0 f32[256]\"\n",
      "            # t1023 = prims.broadcast_in_dim(t1022, [256, 1], [0])  # t1023: \"cuda:0 f32[256, 1]\"\n",
      "          # t1024 = ltorch.abs(t1023)  # t1024: \"cuda:0 f32[256, 1]\"\n",
      "            # t1024 = prims.abs(t1023)  # t1024: \"cuda:0 f32[256, 1]\"\n",
      "          # t1025 = ltorch.eq(t1024, float('inf'))  # t1025: \"cuda:0 b8[256, 1]\"\n",
      "            # t1025 = prims.eq(t1024, float('inf'))  # t1025: \"cuda:0 b8[256, 1]\"\n",
      "          # t1026 = ltorch.where(t1025, 0, t1023)  # t1026: \"cuda:0 f32[256, 1]\"\n",
      "            # _ = prims.convert_element_type(0, float)\n",
      "            # t1026 = prims.where(t1025, 0.0, t1023)  # t1026: \"cuda:0 f32[256, 1]\"\n",
      "          # t1028 = ltorch.sub(t924, t1026, alpha=None)  # t1028: \"cuda:0 f32[256, 50257]\"\n",
      "            # t1027 = prims.broadcast_in_dim(t1026, (256, 50257), (0, 1))  # t1027: \"cuda:0 f32[256, 50257]\"\n",
      "            # t1028 = prims.sub(t924, t1027)  # t1028: \"cuda:0 f32[256, 50257]\"\n",
      "          # t1029 = ltorch.exp(t1028)  # t1029: \"cuda:0 f32[256, 50257]\"\n",
      "            # t1029 = prims.exp(t1028)  # t1029: \"cuda:0 f32[256, 50257]\"\n",
      "          # t1031 = ltorch.sum(t1029, 1, True, dtype=None)  # t1031: \"cuda:0 f32[256, 1]\"\n",
      "            # t1030 = prims.sum(t1029, (1,))  # t1030: \"cuda:0 f32[256]\"\n",
      "            # t1031 = prims.broadcast_in_dim(t1030, [256, 1], [0])  # t1031: \"cuda:0 f32[256, 1]\"\n",
      "          # t1032 = ltorch.log(t1031)  # t1032: \"cuda:0 f32[256, 1]\"\n",
      "            # t1032 = prims.log(t1031)  # t1032: \"cuda:0 f32[256, 1]\"\n",
      "          # t1033 = ltorch.add(t1032, t1026, alpha=None)  # t1033: \"cuda:0 f32[256, 1]\"\n",
      "            # t1033 = prims.add(t1032, t1026)  # t1033: \"cuda:0 f32[256, 1]\"\n",
      "        # t1035 = ltorch.sub(t924, t1033, alpha=None)  # t1035: \"cuda:0 f32[256, 50257]\"\n",
      "          # t1034 = prims.broadcast_in_dim(t1033, (256, 50257), (0, 1))  # t1034: \"cuda:0 f32[256, 50257]\"\n",
      "          # t1035 = prims.sub(t924, t1034)  # t1035: \"cuda:0 f32[256, 50257]\"\n",
      "      # t1036 = ltorch.neg(t1035)  # t1036: \"cuda:0 f32[256, 50257]\"\n",
      "        # t1036 = prims.neg(t1035)  # t1036: \"cuda:0 f32[256, 50257]\"\n",
      "      # t1037 = ltorch.reshape(t925, [256, 1])  # t1037: \"cuda:0 i64[256, 1]\"\n",
      "        # t1037 = prims.reshape(t925, (256, 1))  # t1037: \"cuda:0 i64[256, 1]\"\n",
      "      # t1038 = prims.take_along_axis(t1036, t1037, 1)  # t1038: \"cuda:0 f32[256, 1]\"\n",
      "      # t1039 = prims.sum(t1038, (0, 1))  # t1039: \"cuda:0 f32[]\"\n",
      "      # t944 = ltorch.true_divide(t1039, 256)  # t944: \"cuda:0 f32[]\"\n",
      "        # _ = prims.convert_element_type(256, float)\n",
      "        # t944 = prims.div(t1039, 256.0)  # t944: \"cuda:0 f32[]\"\n",
      "  return {'output': (t923, t944), 'flat_args': [idx, targets, t_transformer_h_0_ln_1_bias, t_transformer_h_0_attn_c_attn_bias, t_transformer_h_0_attn_c_proj_bias, t_transformer_h_0_ln_2_bias, t_transformer_h_0_mlp_c_fc_bias, t_transformer_h_0_mlp_c_proj_bias, t_transformer_h_1_ln_1_bias, t_transformer_h_1_attn_c_attn_bias, t_transformer_h_1_attn_c_proj_bias, t_transformer_h_1_ln_2_bias, t_transformer_h_1_mlp_c_fc_bias, t_transformer_h_1_mlp_c_proj_bias, t_transformer_h_2_ln_1_bias, t_transformer_h_2_attn_c_attn_bias, t_transformer_h_2_attn_c_proj_bias, t_transformer_h_2_ln_2_bias, t_transformer_h_2_mlp_c_fc_bias, t_transformer_h_2_mlp_c_proj_bias, t_transformer_h_3_ln_1_bias, t_transformer_h_3_attn_c_attn_bias, t_transformer_h_3_attn_c_proj_bias, t_transformer_h_3_ln_2_bias, t_transformer_h_3_mlp_c_fc_bias, t_transformer_h_3_mlp_c_proj_bias, t_transformer_h_4_ln_1_bias, t_transformer_h_4_attn_c_attn_bias, t_transformer_h_4_attn_c_proj_bias, t_transformer_h_4_ln_2_bias, t_transformer_h_4_mlp_c_fc_bias, t_transformer_h_4_mlp_c_proj_bias, t_transformer_h_5_ln_1_bias, t_transformer_h_5_attn_c_attn_bias, t_transformer_h_5_attn_c_proj_bias, t_transformer_h_5_ln_2_bias, t_transformer_h_5_mlp_c_fc_bias, t_transformer_h_5_mlp_c_proj_bias, t_transformer_h_6_ln_1_bias, t_transformer_h_6_attn_c_attn_bias, t_transformer_h_6_attn_c_proj_bias, t_transformer_h_6_ln_2_bias, t_transformer_h_6_mlp_c_fc_bias, t_transformer_h_6_mlp_c_proj_bias, t_transformer_h_7_ln_1_bias, t_transformer_h_7_attn_c_attn_bias, t_transformer_h_7_attn_c_proj_bias, t_transformer_h_7_ln_2_bias, t_transformer_h_7_mlp_c_fc_bias, t_transformer_h_7_mlp_c_proj_bias, t_transformer_h_8_ln_1_bias, t_transformer_h_8_attn_c_attn_bias, t_transformer_h_8_attn_c_proj_bias, t_transformer_h_8_ln_2_bias, t_transformer_h_8_mlp_c_fc_bias, t_transformer_h_8_mlp_c_proj_bias, t_transformer_h_9_ln_1_bias, t_transformer_h_9_attn_c_attn_bias, t_transformer_h_9_attn_c_proj_bias, t_transformer_h_9_ln_2_bias, t_transformer_h_9_mlp_c_fc_bias, t_transformer_h_9_mlp_c_proj_bias, t_transformer_h_10_ln_1_bias, t_transformer_h_10_attn_c_attn_bias, t_transformer_h_10_attn_c_proj_bias, t_transformer_h_10_ln_2_bias, t_transformer_h_10_mlp_c_fc_bias, t_transformer_h_10_mlp_c_proj_bias, t_transformer_h_11_ln_1_bias, t_transformer_h_11_attn_c_attn_bias, t_transformer_h_11_attn_c_proj_bias, t_transformer_h_11_ln_2_bias, t_transformer_h_11_mlp_c_fc_bias, t_transformer_h_11_mlp_c_proj_bias, t_transformer_ln_f_bias, bias, tos1, t_transformer_h_2_attn_bias, t_transformer_h_3_attn_bias, t_transformer_h_4_attn_bias, t_transformer_h_5_attn_bias, t_transformer_h_6_attn_bias, t_transformer_h_7_attn_bias, t_transformer_h_8_attn_bias, t_transformer_h_9_attn_bias, t_transformer_h_10_attn_bias, t_transformer_h_11_attn_bias, t_transformer_h_0_attn_c_attn_weight, t_transformer_h_1_attn_c_attn_weight, t_transformer_h_2_attn_c_attn_weight, t_transformer_h_3_attn_c_attn_weight, t_transformer_h_4_attn_c_attn_weight, t_transformer_h_5_attn_c_attn_weight, t_transformer_h_6_attn_c_attn_weight, t_transformer_h_7_attn_c_attn_weight, t_transformer_h_8_attn_c_attn_weight, t_transformer_h_9_attn_c_attn_weight, t_transformer_h_10_attn_c_attn_weight, t_transformer_h_11_attn_c_attn_weight, t_transformer_h_0_mlp_c_fc_weight, t_transformer_h_1_mlp_c_fc_weight, t_transformer_h_2_mlp_c_fc_weight, t_transformer_h_3_mlp_c_fc_weight, t_transformer_h_4_mlp_c_fc_weight, t_transformer_h_5_mlp_c_fc_weight, t_transformer_h_6_mlp_c_fc_weight, t_transformer_h_7_mlp_c_fc_weight, t_transformer_h_8_mlp_c_fc_weight, t_transformer_h_9_mlp_c_fc_weight, t_transformer_h_10_mlp_c_fc_weight, t_transformer_h_11_mlp_c_fc_weight, t_transformer_h_0_attn_c_proj_weight, t_transformer_h_0_mlp_c_proj_weight, t_transformer_h_1_attn_c_proj_weight, t_transformer_h_1_mlp_c_proj_weight, t_transformer_h_2_attn_c_proj_weight, t_transformer_h_2_mlp_c_proj_weight, t_transformer_h_3_attn_c_proj_weight, t_transformer_h_3_mlp_c_proj_weight, t_transformer_h_4_attn_c_proj_weight, t_transformer_h_4_mlp_c_proj_weight, t_transformer_h_5_attn_c_proj_weight, t_transformer_h_5_mlp_c_proj_weight, t_transformer_h_6_attn_c_proj_weight, t_transformer_h_6_mlp_c_proj_weight, t_transformer_h_7_attn_c_proj_weight, t_transformer_h_7_mlp_c_proj_weight, t_transformer_h_8_attn_c_proj_weight, t_transformer_h_8_mlp_c_proj_weight, t_transformer_h_9_attn_c_proj_weight, t_transformer_h_9_mlp_c_proj_weight, t_transformer_h_10_attn_c_proj_weight, t_transformer_h_10_mlp_c_proj_weight, t_transformer_h_11_attn_c_proj_weight, t_transformer_h_11_mlp_c_proj_weight, t_lm_head_weight, t_transformer_h_0_ln_1_weight, t_transformer_h_10_ln_1_weight, t_transformer_h_11_ln_1_weight, t_transformer_h_1_ln_1_weight, t_transformer_h_2_ln_1_weight, t_transformer_h_3_ln_1_weight, t_transformer_h_4_ln_1_weight, t_transformer_h_5_ln_1_weight, t_transformer_h_6_ln_1_weight, t_transformer_h_7_ln_1_weight, t_transformer_h_8_ln_1_weight, t_transformer_h_9_ln_1_weight, t_transformer_h_0_ln_2_weight, t_transformer_h_1_ln_2_weight, t_transformer_h_2_ln_2_weight, t_transformer_h_3_ln_2_weight, t_transformer_h_4_ln_2_weight, t_transformer_h_5_ln_2_weight, t_transformer_h_6_ln_2_weight, t_transformer_h_7_ln_2_weight, t_transformer_h_8_ln_2_weight, t_transformer_h_9_ln_2_weight, t_transformer_h_10_ln_2_weight, t_transformer_h_11_ln_2_weight, t_transformer_ln_f_weight, t_transformer_wpe_weight, t_transformer_wte_weight], 'flat_output': (t923, t944)}, ((idx, t0, t10, t103, t107, t108, t113, t123, t127, t129, t133, t137, t139, t14, t142, t145, t146, t154, t156, t16, t160, t164, t166, t169, t172, t178, t182, t183, t188, t19, t198, t202, t204, t208, t212, t214, t217, t22, t220, t221, t229, t231, t235, t239, t241, t244, t247, t253, t257, t258, t263, t273, t277, t279, t28, t283, t287, t289, t292, t295, t296, t304, t306, t310, t314, t316, t319, t32, t322, t328, t33, t332, t333, t338, t348, t352, t354, t358, t362, t364, t367, t370, t371, t379, t38, t381, t385, t389, t391, t394, t397, t403, t407, t408, t413, t423, t427, t429, t433, t437, t439, t442, t445, t446, t454, t456, t460, t464, t466, t469, t472, t478, t48, t482, t483, t488, t498, t502, t504, t508, t512, t514, t517, t52, t520, t521, t529, t531, t535, t539, t54, t541, t544, t547, t553, t557, t558, t563, t573, t577, t579, t58, t583, t587, t589, t592, t595, t596, t6, t604, t606, t610, t614, t616, t619, t62, t622, t628, t632, t633, t638, t64, t648, t652, t654, t658, t662, t664, t667, t67, t670, t671, t679, t681, t685, t689, t691, t694, t697, t70, t703, t707, t708, t71, t713, t723, t727, t729, t733, t737, t739, t742, t745, t746, t754, t756, t760, t764, t766, t769, t772, t778, t782, t783, t788, t79, t798, t802, t804, t808, t81, t812, t814, t817, t820, t821, t829, t831, t835, t839, t841, t844, t847, t85, t853, t857, t858, t863, t873, t877, t879, t883, t887, t889, t89, t892, t895, t896, t904, t906, t91, t910, t914, t916, t919, t922, t924, t925, t94, t97, t_lm_head_weight, t_transformer_h_0_attn_c_attn_weight, t_transformer_h_0_attn_c_proj_weight, t_transformer_h_0_mlp_c_fc_weight, t_transformer_h_0_mlp_c_proj_weight, t_transformer_h_10_attn_c_attn_weight, t_transformer_h_10_attn_c_proj_weight, t_transformer_h_10_mlp_c_fc_weight, t_transformer_h_10_mlp_c_proj_weight, t_transformer_h_11_attn_c_attn_weight, t_transformer_h_11_attn_c_proj_weight, t_transformer_h_11_mlp_c_fc_weight, t_transformer_h_11_mlp_c_proj_weight, t_transformer_h_1_attn_c_attn_weight, t_transformer_h_1_attn_c_proj_weight, t_transformer_h_1_mlp_c_fc_weight, t_transformer_h_1_mlp_c_proj_weight, t_transformer_h_2_attn_c_attn_weight, t_transformer_h_2_attn_c_proj_weight, t_transformer_h_2_mlp_c_fc_weight, t_transformer_h_2_mlp_c_proj_weight, t_transformer_h_3_attn_c_attn_weight, t_transformer_h_3_attn_c_proj_weight, t_transformer_h_3_mlp_c_fc_weight, t_transformer_h_3_mlp_c_proj_weight, t_transformer_h_4_attn_c_attn_weight, t_transformer_h_4_attn_c_proj_weight, t_transformer_h_4_mlp_c_fc_weight, t_transformer_h_4_mlp_c_proj_weight, t_transformer_h_5_attn_c_attn_weight, t_transformer_h_5_attn_c_proj_weight, t_transformer_h_5_mlp_c_fc_weight, t_transformer_h_5_mlp_c_proj_weight, t_transformer_h_6_attn_c_attn_weight, t_transformer_h_6_attn_c_proj_weight, t_transformer_h_6_mlp_c_fc_weight, t_transformer_h_6_mlp_c_proj_weight, t_transformer_h_7_attn_c_attn_weight, t_transformer_h_7_attn_c_proj_weight, t_transformer_h_7_mlp_c_fc_weight, t_transformer_h_7_mlp_c_proj_weight, t_transformer_h_8_attn_c_attn_weight, t_transformer_h_8_attn_c_proj_weight, t_transformer_h_8_mlp_c_fc_weight, t_transformer_h_8_mlp_c_proj_weight, t_transformer_h_9_attn_c_attn_weight, t_transformer_h_9_attn_c_proj_weight, t_transformer_h_9_mlp_c_fc_weight, t_transformer_h_9_mlp_c_proj_weight), (False, False, False, False, 0.125, 0.5, 3.0, 0.044715, 0.7978845608028654, 0.125, 0.5, 3.0, 0.044715, 0.7978845608028654, 0.125, 0.5, 3.0, 0.044715, 0.7978845608028654, 0.125, 0.5, 3.0, 0.044715, 0.7978845608028654, 0.125, 0.5, 3.0, 0.044715, 0.7978845608028654, 0.125, 0.125, 0.5, 3.0, 0.044715, 0.7978845608028654, 0.125, 0.5, 3.0, 0.044715, 0.7978845608028654, 0.125, 0.5, 3.0, 0.044715, 0.7978845608028654, 0.125, 0.5, 0.5, 3.0, 3.0, 0.044715, 0.7978845608028654, 0.044715, 0.7978845608028654, 0.125, 0.5, 3.0, 0.044715, 0.7978845608028654, 0.125, 0.5, 3.0, 0.044715, 0.7978845608028654, 0.0, 50257, -1, 2, 2, -1, 2, -1, 2, 1024, -1, 2, -1, 2, -1, 2, -1, -1, 2, -1, 2, -1, 2, -1, 2, -1, 2, -1))\n"
     ]
    }
   ],
   "source": [
    "forward_trace = thunder.last_traces(thunder_model)[-1]\n",
    "print(forward_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdfd443-538d-444b-a8fd-6382d793b131",
   "metadata": {},
   "source": [
    "And the backward trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ae660a7-dc71-4560-bc1d-142e3d62abef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Constructed by Delete Last Used (took 15 milliseconds)\n",
      "import torch\n",
      "from torch import Tensor\n",
      "from thunder.executors.torchex import no_autocast\n",
      "\n",
      "@torch.no_grad()\n",
      "@no_autocast()\n",
      "def backward_fn(saved_for_backward, cotangents):\n",
      "  # saved_for_backward: \"Collection\"\n",
      "  # cotangents: \"Collection\"\n",
      "  C0, C1, = saved_for_backward\n",
      "  clear_collection(saved_for_backward)\n",
      "  del saved_for_backward\n",
      "  t945, t946, = cotangents\n",
      "  clear_collection(cotangents)\n",
      "  del cotangents\n",
      "  idx, t0, t10, t103, t107, t108, t113, t123, t127, t129, t133, t137, t139, t14, \\\n",
      "  t142, t145, t146, t154, t156, t16, t160, t164, t166, t169, t172, t178, t182, \\\n",
      "  t183, t188, t19, t198, t202, t204, t208, t212, t214, t217, t22, t220, t221, \\\n",
      "  t229, t231, t235, t239, t241, t244, t247, t253, t257, t258, t263, t273, t277, \\\n",
      "  t279, t28, t283, t287, t289, t292, t295, t296, t304, t306, t310, t314, t316, \\\n",
      "  t319, t32, t322, t328, t33, t332, t333, t338, t348, t352, t354, t358, t362, \\\n",
      "  t364, t367, t370, t371, t379, t38, t381, t385, t389, t391, t394, t397, t403, \\\n",
      "  t407, t408, t413, t423, t427, t429, t433, t437, t439, t442, t445, t446, t454, \\\n",
      "  t456, t460, t464, t466, t469, t472, t478, t48, t482, t483, t488, t498, t502, \\\n",
      "  t504, t508, t512, t514, t517, t52, t520, t521, t529, t531, t535, t539, t54, \\\n",
      "  t541, t544, t547, t553, t557, t558, t563, t573, t577, t579, t58, t583, t587, \\\n",
      "  t589, t592, t595, t596, t6, t604, t606, t610, t614, t616, t619, t62, t622, \\\n",
      "  t628, t632, t633, t638, t64, t648, t652, t654, t658, t662, t664, t667, t67, \\\n",
      "  t670, t671, t679, t681, t685, t689, t691, t694, t697, t70, t703, t707, t708, \\\n",
      "  t71, t713, t723, t727, t729, t733, t737, t739, t742, t745, t746, t754, t756, \\\n",
      "  t760, t764, t766, t769, t772, t778, t782, t783, t788, t79, t798, t802, t804, \\\n",
      "  t808, t81, t812, t814, t817, t820, t821, t829, t831, t835, t839, t841, t844, \\\n",
      "  t847, t85, t853, t857, t858, t863, t873, t877, t879, t883, t887, t889, t89, \\\n",
      "  t892, t895, t896, t904, t906, t91, t910, t914, t916, t919, t922, t924, t925, \\\n",
      "  t94, t97, t_lm_head_weight, t_transformer_h_0_attn_c_attn_weight, \\\n",
      "  t_transformer_h_0_attn_c_proj_weight, t_transformer_h_0_mlp_c_fc_weight, \\\n",
      "  t_transformer_h_0_mlp_c_proj_weight, t_transformer_h_10_attn_c_attn_weight, \\\n",
      "  t_transformer_h_10_attn_c_proj_weight, t_transformer_h_10_mlp_c_fc_weight, \\\n",
      "  t_transformer_h_10_mlp_c_proj_weight, t_transformer_h_11_attn_c_attn_weight, \\\n",
      "  t_transformer_h_11_attn_c_proj_weight, t_transformer_h_11_mlp_c_fc_weight, \\\n",
      "  t_transformer_h_11_mlp_c_proj_weight, t_transformer_h_1_attn_c_attn_weight, \\\n",
      "  t_transformer_h_1_attn_c_proj_weight, t_transformer_h_1_mlp_c_fc_weight, \\\n",
      "  t_transformer_h_1_mlp_c_proj_weight, t_transformer_h_2_attn_c_attn_weight, \\\n",
      "  t_transformer_h_2_attn_c_proj_weight, t_transformer_h_2_mlp_c_fc_weight, \\\n",
      "  t_transformer_h_2_mlp_c_proj_weight, t_transformer_h_3_attn_c_attn_weight, \\\n",
      "  t_transformer_h_3_attn_c_proj_weight, t_transformer_h_3_mlp_c_fc_weight, \\\n",
      "  t_transformer_h_3_mlp_c_proj_weight, t_transformer_h_4_attn_c_attn_weight, \\\n",
      "  t_transformer_h_4_attn_c_proj_weight, t_transformer_h_4_mlp_c_fc_weight, \\\n",
      "  t_transformer_h_4_mlp_c_proj_weight, t_transformer_h_5_attn_c_attn_weight, \\\n",
      "  t_transformer_h_5_attn_c_proj_weight, t_transformer_h_5_mlp_c_fc_weight, \\\n",
      "  t_transformer_h_5_mlp_c_proj_weight, t_transformer_h_6_attn_c_attn_weight, \\\n",
      "  t_transformer_h_6_attn_c_proj_weight, t_transformer_h_6_mlp_c_fc_weight, \\\n",
      "  t_transformer_h_6_mlp_c_proj_weight, t_transformer_h_7_attn_c_attn_weight, \\\n",
      "  t_transformer_h_7_attn_c_proj_weight, t_transformer_h_7_mlp_c_fc_weight, \\\n",
      "  t_transformer_h_7_mlp_c_proj_weight, t_transformer_h_8_attn_c_attn_weight, \\\n",
      "  t_transformer_h_8_attn_c_proj_weight, t_transformer_h_8_mlp_c_fc_weight, \\\n",
      "  t_transformer_h_8_mlp_c_proj_weight, t_transformer_h_9_attn_c_attn_weight, \\\n",
      "  t_transformer_h_9_attn_c_proj_weight, t_transformer_h_9_mlp_c_fc_weight, \\\n",
      "  t_transformer_h_9_mlp_c_proj_weight, = C0\n",
      "  clear_collection(C0)\n",
      "  del C0\n",
      "  b1, b2, b4, b5, f114, f134, f136, f138, f140, f182, f202, f204, f206, f208, \\\n",
      "  f250, f270, f272, f274, f276, f318, f338, f340, f342, f344, f386, f406, f408, \\\n",
      "  f410, f412, f454, f46, f474, f476, f478, f480, f522, f542, f544, f546, f548, \\\n",
      "  f590, f610, f612, f614, f616, f658, f66, f678, f68, f680, f682, f684, f70, f72, \\\n",
      "  f726, f746, f748, f750, f752, f794, f814, f816, f818, f820, f835, i0, i118, \\\n",
      "  i12, i148, i186, i216, i254, i284, i3, i322, i352, i390, i420, i458, i488, i50, \\\n",
      "  i526, i556, i594, i624, i662, i692, i730, i760, i798, i80, i834, = C1\n",
      "  clear_collection(C1)\n",
      "  del C1\n",
      "  t4742 = torch.permute(t333, (0, 1, 3, 2))  # t4742: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4742 = ltorch.permute(t333, (0, 1, 3, 2))  # t4742: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4742 = prims.transpose(t333, (0, 1, 3, 2))  # t4742: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t333\n",
      "  t4744 = torch.permute(t328, (0, 1, 3, 2))  # t4744: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4744 = ltorch.permute(t328, (0, 1, 3, 2))  # t4744: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4744 = prims.transpose(t328, (0, 1, 3, 2))  # t4744: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t328\n",
      "  t3689 = torch.reshape(t922, (-1, 768))  # t3689: \"cuda:0 f32[256, 768]\"\n",
      "    # t3689 = ltorch.reshape(t922, (-1, 768))  # t3689: \"cuda:0 f32[256, 768]\"\n",
      "      # t3689 = prims.reshape(t922, (256, 768))  # t3689: \"cuda:0 f32[256, 768]\"\n",
      "  del t922\n",
      "  t4759 = torch.reshape(t322, (-1, 768))  # t4759: \"cuda:0 f32[256, 768]\"\n",
      "    # t4759 = ltorch.reshape(t322, (-1, 768))  # t4759: \"cuda:0 f32[256, 768]\"\n",
      "      # t4759 = prims.reshape(t322, (256, 768))  # t4759: \"cuda:0 f32[256, 768]\"\n",
      "  del t322\n",
      "  t3725 = torch.reshape(t904, (-1, 3072))  # t3725: \"cuda:0 f32[256, 3072]\"\n",
      "    # t3725 = ltorch.reshape(t904, (-1, 3072))  # t3725: \"cuda:0 f32[256, 3072]\"\n",
      "      # t3725 = prims.reshape(t904, (256, 3072))  # t3725: \"cuda:0 f32[256, 3072]\"\n",
      "  del t904\n",
      "  t4797 = torch.reshape(t304, (-1, 3072))  # t4797: \"cuda:0 f32[256, 3072]\"\n",
      "    # t4797 = ltorch.reshape(t304, (-1, 3072))  # t4797: \"cuda:0 f32[256, 3072]\"\n",
      "      # t4797 = prims.reshape(t304, (256, 3072))  # t4797: \"cuda:0 f32[256, 3072]\"\n",
      "  del t304\n",
      "  t3749 = torch.reshape(t895, (-1, 768))  # t3749: \"cuda:0 f32[256, 768]\"\n",
      "    # t3749 = ltorch.reshape(t895, (-1, 768))  # t3749: \"cuda:0 f32[256, 768]\"\n",
      "      # t3749 = prims.reshape(t895, (256, 768))  # t3749: \"cuda:0 f32[256, 768]\"\n",
      "  del t895\n",
      "  t4821 = torch.reshape(t295, (-1, 768))  # t4821: \"cuda:0 f32[256, 768]\"\n",
      "    # t4821 = ltorch.reshape(t295, (-1, 768))  # t4821: \"cuda:0 f32[256, 768]\"\n",
      "      # t4821 = prims.reshape(t295, (256, 768))  # t4821: \"cuda:0 f32[256, 768]\"\n",
      "  del t295\n",
      "  t3787 = torch.reshape(t877, (-1, 768))  # t3787: \"cuda:0 f32[256, 768]\"\n",
      "    # t3787 = ltorch.reshape(t877, (-1, 768))  # t3787: \"cuda:0 f32[256, 768]\"\n",
      "      # t3787 = prims.reshape(t877, (256, 768))  # t3787: \"cuda:0 f32[256, 768]\"\n",
      "  del t877\n",
      "  t3792 = torch.permute(t857, (0, 1, 3, 2))  # t3792: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t3792 = ltorch.permute(t857, (0, 1, 3, 2))  # t3792: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t3792 = prims.transpose(t857, (0, 1, 3, 2))  # t3792: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t857\n",
      "  t3794 = torch.permute(t873, (0, 1, 3, 2))  # t3794: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t3794 = ltorch.permute(t873, (0, 1, 3, 2))  # t3794: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t3794 = prims.transpose(t873, (0, 1, 3, 2))  # t3794: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  t4859 = torch.reshape(t277, (-1, 768))  # t4859: \"cuda:0 f32[256, 768]\"\n",
      "    # t4859 = ltorch.reshape(t277, (-1, 768))  # t4859: \"cuda:0 f32[256, 768]\"\n",
      "      # t4859 = prims.reshape(t277, (256, 768))  # t4859: \"cuda:0 f32[256, 768]\"\n",
      "  del t277\n",
      "  t4864 = torch.permute(t257, (0, 1, 3, 2))  # t4864: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4864 = ltorch.permute(t257, (0, 1, 3, 2))  # t4864: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4864 = prims.transpose(t257, (0, 1, 3, 2))  # t4864: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t257\n",
      "  t3804 = torch.permute(t858, (0, 1, 3, 2))  # t3804: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t3804 = ltorch.permute(t858, (0, 1, 3, 2))  # t3804: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t3804 = prims.transpose(t858, (0, 1, 3, 2))  # t3804: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t858\n",
      "  t4866 = torch.permute(t273, (0, 1, 3, 2))  # t4866: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4866 = ltorch.permute(t273, (0, 1, 3, 2))  # t4866: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4866 = prims.transpose(t273, (0, 1, 3, 2))  # t4866: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  t3806 = torch.permute(t853, (0, 1, 3, 2))  # t3806: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t3806 = ltorch.permute(t853, (0, 1, 3, 2))  # t3806: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t3806 = prims.transpose(t853, (0, 1, 3, 2))  # t3806: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t853\n",
      "  t4876 = torch.permute(t258, (0, 1, 3, 2))  # t4876: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4876 = ltorch.permute(t258, (0, 1, 3, 2))  # t4876: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4876 = prims.transpose(t258, (0, 1, 3, 2))  # t4876: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t258\n",
      "  t4878 = torch.permute(t253, (0, 1, 3, 2))  # t4878: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4878 = ltorch.permute(t253, (0, 1, 3, 2))  # t4878: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4878 = prims.transpose(t253, (0, 1, 3, 2))  # t4878: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t253\n",
      "  t3821 = torch.reshape(t847, (-1, 768))  # t3821: \"cuda:0 f32[256, 768]\"\n",
      "    # t3821 = ltorch.reshape(t847, (-1, 768))  # t3821: \"cuda:0 f32[256, 768]\"\n",
      "      # t3821 = prims.reshape(t847, (256, 768))  # t3821: \"cuda:0 f32[256, 768]\"\n",
      "  del t847\n",
      "  t4893 = torch.reshape(t247, (-1, 768))  # t4893: \"cuda:0 f32[256, 768]\"\n",
      "    # t4893 = ltorch.reshape(t247, (-1, 768))  # t4893: \"cuda:0 f32[256, 768]\"\n",
      "      # t4893 = prims.reshape(t247, (256, 768))  # t4893: \"cuda:0 f32[256, 768]\"\n",
      "  del t247\n",
      "  t3859 = torch.reshape(t829, (-1, 3072))  # t3859: \"cuda:0 f32[256, 3072]\"\n",
      "    # t3859 = ltorch.reshape(t829, (-1, 3072))  # t3859: \"cuda:0 f32[256, 3072]\"\n",
      "      # t3859 = prims.reshape(t829, (256, 3072))  # t3859: \"cuda:0 f32[256, 3072]\"\n",
      "  del t829\n",
      "  t4931 = torch.reshape(t229, (-1, 3072))  # t4931: \"cuda:0 f32[256, 3072]\"\n",
      "    # t4931 = ltorch.reshape(t229, (-1, 3072))  # t4931: \"cuda:0 f32[256, 3072]\"\n",
      "      # t4931 = prims.reshape(t229, (256, 3072))  # t4931: \"cuda:0 f32[256, 3072]\"\n",
      "  del t229\n",
      "  t3883 = torch.reshape(t820, (-1, 768))  # t3883: \"cuda:0 f32[256, 768]\"\n",
      "    # t3883 = ltorch.reshape(t820, (-1, 768))  # t3883: \"cuda:0 f32[256, 768]\"\n",
      "      # t3883 = prims.reshape(t820, (256, 768))  # t3883: \"cuda:0 f32[256, 768]\"\n",
      "  del t820\n",
      "  t4955 = torch.reshape(t220, (-1, 768))  # t4955: \"cuda:0 f32[256, 768]\"\n",
      "    # t4955 = ltorch.reshape(t220, (-1, 768))  # t4955: \"cuda:0 f32[256, 768]\"\n",
      "      # t4955 = prims.reshape(t220, (256, 768))  # t4955: \"cuda:0 f32[256, 768]\"\n",
      "  del t220\n",
      "  t3921 = torch.reshape(t802, (-1, 768))  # t3921: \"cuda:0 f32[256, 768]\"\n",
      "    # t3921 = ltorch.reshape(t802, (-1, 768))  # t3921: \"cuda:0 f32[256, 768]\"\n",
      "      # t3921 = prims.reshape(t802, (256, 768))  # t3921: \"cuda:0 f32[256, 768]\"\n",
      "  del t802\n",
      "  t3926 = torch.permute(t782, (0, 1, 3, 2))  # t3926: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t3926 = ltorch.permute(t782, (0, 1, 3, 2))  # t3926: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t3926 = prims.transpose(t782, (0, 1, 3, 2))  # t3926: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t782\n",
      "  t3928 = torch.permute(t798, (0, 1, 3, 2))  # t3928: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t3928 = ltorch.permute(t798, (0, 1, 3, 2))  # t3928: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t3928 = prims.transpose(t798, (0, 1, 3, 2))  # t3928: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  t4993 = torch.reshape(t202, (-1, 768))  # t4993: \"cuda:0 f32[256, 768]\"\n",
      "    # t4993 = ltorch.reshape(t202, (-1, 768))  # t4993: \"cuda:0 f32[256, 768]\"\n",
      "      # t4993 = prims.reshape(t202, (256, 768))  # t4993: \"cuda:0 f32[256, 768]\"\n",
      "  del t202\n",
      "  t4998 = torch.permute(t182, (0, 1, 3, 2))  # t4998: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4998 = ltorch.permute(t182, (0, 1, 3, 2))  # t4998: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4998 = prims.transpose(t182, (0, 1, 3, 2))  # t4998: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t182\n",
      "  t3938 = torch.permute(t783, (0, 1, 3, 2))  # t3938: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t3938 = ltorch.permute(t783, (0, 1, 3, 2))  # t3938: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t3938 = prims.transpose(t783, (0, 1, 3, 2))  # t3938: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t783\n",
      "  t5000 = torch.permute(t198, (0, 1, 3, 2))  # t5000: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t5000 = ltorch.permute(t198, (0, 1, 3, 2))  # t5000: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t5000 = prims.transpose(t198, (0, 1, 3, 2))  # t5000: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  t3940 = torch.permute(t778, (0, 1, 3, 2))  # t3940: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t3940 = ltorch.permute(t778, (0, 1, 3, 2))  # t3940: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t3940 = prims.transpose(t778, (0, 1, 3, 2))  # t3940: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t778\n",
      "  t5010 = torch.permute(t183, (0, 1, 3, 2))  # t5010: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t5010 = ltorch.permute(t183, (0, 1, 3, 2))  # t5010: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t5010 = prims.transpose(t183, (0, 1, 3, 2))  # t5010: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t183\n",
      "  t5012 = torch.permute(t178, (0, 1, 3, 2))  # t5012: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t5012 = ltorch.permute(t178, (0, 1, 3, 2))  # t5012: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t5012 = prims.transpose(t178, (0, 1, 3, 2))  # t5012: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t178\n",
      "  t3955 = torch.reshape(t772, (-1, 768))  # t3955: \"cuda:0 f32[256, 768]\"\n",
      "    # t3955 = ltorch.reshape(t772, (-1, 768))  # t3955: \"cuda:0 f32[256, 768]\"\n",
      "      # t3955 = prims.reshape(t772, (256, 768))  # t3955: \"cuda:0 f32[256, 768]\"\n",
      "  del t772\n",
      "  t5027 = torch.reshape(t172, (-1, 768))  # t5027: \"cuda:0 f32[256, 768]\"\n",
      "    # t5027 = ltorch.reshape(t172, (-1, 768))  # t5027: \"cuda:0 f32[256, 768]\"\n",
      "      # t5027 = prims.reshape(t172, (256, 768))  # t5027: \"cuda:0 f32[256, 768]\"\n",
      "  del t172\n",
      "  t3993 = torch.reshape(t754, (-1, 3072))  # t3993: \"cuda:0 f32[256, 3072]\"\n",
      "    # t3993 = ltorch.reshape(t754, (-1, 3072))  # t3993: \"cuda:0 f32[256, 3072]\"\n",
      "      # t3993 = prims.reshape(t754, (256, 3072))  # t3993: \"cuda:0 f32[256, 3072]\"\n",
      "  del t754\n",
      "  t5065 = torch.reshape(t154, (-1, 3072))  # t5065: \"cuda:0 f32[256, 3072]\"\n",
      "    # t5065 = ltorch.reshape(t154, (-1, 3072))  # t5065: \"cuda:0 f32[256, 3072]\"\n",
      "      # t5065 = prims.reshape(t154, (256, 3072))  # t5065: \"cuda:0 f32[256, 3072]\"\n",
      "  del t154\n",
      "  t4017 = torch.reshape(t745, (-1, 768))  # t4017: \"cuda:0 f32[256, 768]\"\n",
      "    # t4017 = ltorch.reshape(t745, (-1, 768))  # t4017: \"cuda:0 f32[256, 768]\"\n",
      "      # t4017 = prims.reshape(t745, (256, 768))  # t4017: \"cuda:0 f32[256, 768]\"\n",
      "  del t745\n",
      "  t5089 = torch.reshape(t145, (-1, 768))  # t5089: \"cuda:0 f32[256, 768]\"\n",
      "    # t5089 = ltorch.reshape(t145, (-1, 768))  # t5089: \"cuda:0 f32[256, 768]\"\n",
      "      # t5089 = prims.reshape(t145, (256, 768))  # t5089: \"cuda:0 f32[256, 768]\"\n",
      "  del t145\n",
      "  t4055 = torch.reshape(t727, (-1, 768))  # t4055: \"cuda:0 f32[256, 768]\"\n",
      "    # t4055 = ltorch.reshape(t727, (-1, 768))  # t4055: \"cuda:0 f32[256, 768]\"\n",
      "      # t4055 = prims.reshape(t727, (256, 768))  # t4055: \"cuda:0 f32[256, 768]\"\n",
      "  del t727\n",
      "  t4060 = torch.permute(t707, (0, 1, 3, 2))  # t4060: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4060 = ltorch.permute(t707, (0, 1, 3, 2))  # t4060: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4060 = prims.transpose(t707, (0, 1, 3, 2))  # t4060: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t707\n",
      "  t4062 = torch.permute(t723, (0, 1, 3, 2))  # t4062: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4062 = ltorch.permute(t723, (0, 1, 3, 2))  # t4062: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4062 = prims.transpose(t723, (0, 1, 3, 2))  # t4062: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  t5127 = torch.reshape(t127, (-1, 768))  # t5127: \"cuda:0 f32[256, 768]\"\n",
      "    # t5127 = ltorch.reshape(t127, (-1, 768))  # t5127: \"cuda:0 f32[256, 768]\"\n",
      "      # t5127 = prims.reshape(t127, (256, 768))  # t5127: \"cuda:0 f32[256, 768]\"\n",
      "  del t127\n",
      "  t5132 = torch.permute(t107, (0, 1, 3, 2))  # t5132: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t5132 = ltorch.permute(t107, (0, 1, 3, 2))  # t5132: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t5132 = prims.transpose(t107, (0, 1, 3, 2))  # t5132: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t107\n",
      "  t4072 = torch.permute(t708, (0, 1, 3, 2))  # t4072: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4072 = ltorch.permute(t708, (0, 1, 3, 2))  # t4072: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4072 = prims.transpose(t708, (0, 1, 3, 2))  # t4072: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t708\n",
      "  t5134 = torch.permute(t123, (0, 1, 3, 2))  # t5134: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t5134 = ltorch.permute(t123, (0, 1, 3, 2))  # t5134: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t5134 = prims.transpose(t123, (0, 1, 3, 2))  # t5134: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  t4074 = torch.permute(t703, (0, 1, 3, 2))  # t4074: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4074 = ltorch.permute(t703, (0, 1, 3, 2))  # t4074: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4074 = prims.transpose(t703, (0, 1, 3, 2))  # t4074: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t703\n",
      "  t5144 = torch.permute(t108, (0, 1, 3, 2))  # t5144: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t5144 = ltorch.permute(t108, (0, 1, 3, 2))  # t5144: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t5144 = prims.transpose(t108, (0, 1, 3, 2))  # t5144: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t108\n",
      "  t5146 = torch.permute(t103, (0, 1, 3, 2))  # t5146: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t5146 = ltorch.permute(t103, (0, 1, 3, 2))  # t5146: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t5146 = prims.transpose(t103, (0, 1, 3, 2))  # t5146: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t103\n",
      "  t4089 = torch.reshape(t697, (-1, 768))  # t4089: \"cuda:0 f32[256, 768]\"\n",
      "    # t4089 = ltorch.reshape(t697, (-1, 768))  # t4089: \"cuda:0 f32[256, 768]\"\n",
      "      # t4089 = prims.reshape(t697, (256, 768))  # t4089: \"cuda:0 f32[256, 768]\"\n",
      "  del t697\n",
      "  t5161 = torch.reshape(t97, (-1, 768))  # t5161: \"cuda:0 f32[256, 768]\"\n",
      "    # t5161 = ltorch.reshape(t97, (-1, 768))  # t5161: \"cuda:0 f32[256, 768]\"\n",
      "      # t5161 = prims.reshape(t97, (256, 768))  # t5161: \"cuda:0 f32[256, 768]\"\n",
      "  del t97\n",
      "  t4127 = torch.reshape(t679, (-1, 3072))  # t4127: \"cuda:0 f32[256, 3072]\"\n",
      "    # t4127 = ltorch.reshape(t679, (-1, 3072))  # t4127: \"cuda:0 f32[256, 3072]\"\n",
      "      # t4127 = prims.reshape(t679, (256, 3072))  # t4127: \"cuda:0 f32[256, 3072]\"\n",
      "  del t679\n",
      "  t5199 = torch.reshape(t79, (-1, 3072))  # t5199: \"cuda:0 f32[256, 3072]\"\n",
      "    # t5199 = ltorch.reshape(t79, (-1, 3072))  # t5199: \"cuda:0 f32[256, 3072]\"\n",
      "      # t5199 = prims.reshape(t79, (256, 3072))  # t5199: \"cuda:0 f32[256, 3072]\"\n",
      "  del t79\n",
      "  t4151 = torch.reshape(t670, (-1, 768))  # t4151: \"cuda:0 f32[256, 768]\"\n",
      "    # t4151 = ltorch.reshape(t670, (-1, 768))  # t4151: \"cuda:0 f32[256, 768]\"\n",
      "      # t4151 = prims.reshape(t670, (256, 768))  # t4151: \"cuda:0 f32[256, 768]\"\n",
      "  del t670\n",
      "  t5223 = torch.reshape(t70, (-1, 768))  # t5223: \"cuda:0 f32[256, 768]\"\n",
      "    # t5223 = ltorch.reshape(t70, (-1, 768))  # t5223: \"cuda:0 f32[256, 768]\"\n",
      "      # t5223 = prims.reshape(t70, (256, 768))  # t5223: \"cuda:0 f32[256, 768]\"\n",
      "  del t70\n",
      "  t4189 = torch.reshape(t652, (-1, 768))  # t4189: \"cuda:0 f32[256, 768]\"\n",
      "    # t4189 = ltorch.reshape(t652, (-1, 768))  # t4189: \"cuda:0 f32[256, 768]\"\n",
      "      # t4189 = prims.reshape(t652, (256, 768))  # t4189: \"cuda:0 f32[256, 768]\"\n",
      "  del t652\n",
      "  t4194 = torch.permute(t632, (0, 1, 3, 2))  # t4194: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4194 = ltorch.permute(t632, (0, 1, 3, 2))  # t4194: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4194 = prims.transpose(t632, (0, 1, 3, 2))  # t4194: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t632\n",
      "  t4196 = torch.permute(t648, (0, 1, 3, 2))  # t4196: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4196 = ltorch.permute(t648, (0, 1, 3, 2))  # t4196: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4196 = prims.transpose(t648, (0, 1, 3, 2))  # t4196: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  t5261 = torch.reshape(t52, (-1, 768))  # t5261: \"cuda:0 f32[256, 768]\"\n",
      "    # t5261 = ltorch.reshape(t52, (-1, 768))  # t5261: \"cuda:0 f32[256, 768]\"\n",
      "      # t5261 = prims.reshape(t52, (256, 768))  # t5261: \"cuda:0 f32[256, 768]\"\n",
      "  del t52\n",
      "  t5266 = torch.permute(t32, (0, 1, 3, 2))  # t5266: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t5266 = ltorch.permute(t32, (0, 1, 3, 2))  # t5266: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t5266 = prims.transpose(t32, (0, 1, 3, 2))  # t5266: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t32\n",
      "  t4206 = torch.permute(t633, (0, 1, 3, 2))  # t4206: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4206 = ltorch.permute(t633, (0, 1, 3, 2))  # t4206: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4206 = prims.transpose(t633, (0, 1, 3, 2))  # t4206: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t633\n",
      "  t5268 = torch.permute(t48, (0, 1, 3, 2))  # t5268: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t5268 = ltorch.permute(t48, (0, 1, 3, 2))  # t5268: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t5268 = prims.transpose(t48, (0, 1, 3, 2))  # t5268: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  t4208 = torch.permute(t628, (0, 1, 3, 2))  # t4208: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4208 = ltorch.permute(t628, (0, 1, 3, 2))  # t4208: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4208 = prims.transpose(t628, (0, 1, 3, 2))  # t4208: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t628\n",
      "  t5278 = torch.permute(t33, (0, 1, 3, 2))  # t5278: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t5278 = ltorch.permute(t33, (0, 1, 3, 2))  # t5278: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t5278 = prims.transpose(t33, (0, 1, 3, 2))  # t5278: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t33\n",
      "  t5280 = torch.permute(t28, (0, 1, 3, 2))  # t5280: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t5280 = ltorch.permute(t28, (0, 1, 3, 2))  # t5280: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t5280 = prims.transpose(t28, (0, 1, 3, 2))  # t5280: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t28\n",
      "  t4223 = torch.reshape(t622, (-1, 768))  # t4223: \"cuda:0 f32[256, 768]\"\n",
      "    # t4223 = ltorch.reshape(t622, (-1, 768))  # t4223: \"cuda:0 f32[256, 768]\"\n",
      "      # t4223 = prims.reshape(t622, (256, 768))  # t4223: \"cuda:0 f32[256, 768]\"\n",
      "  del t622\n",
      "  t5295 = torch.reshape(t22, (-1, 768))  # t5295: \"cuda:0 f32[256, 768]\"\n",
      "    # t5295 = ltorch.reshape(t22, (-1, 768))  # t5295: \"cuda:0 f32[256, 768]\"\n",
      "      # t5295 = prims.reshape(t22, (256, 768))  # t5295: \"cuda:0 f32[256, 768]\"\n",
      "  del t22\n",
      "  t4261 = torch.reshape(t604, (-1, 3072))  # t4261: \"cuda:0 f32[256, 3072]\"\n",
      "    # t4261 = ltorch.reshape(t604, (-1, 3072))  # t4261: \"cuda:0 f32[256, 3072]\"\n",
      "      # t4261 = prims.reshape(t604, (256, 3072))  # t4261: \"cuda:0 f32[256, 3072]\"\n",
      "  del t604\n",
      "  t4285 = torch.reshape(t595, (-1, 768))  # t4285: \"cuda:0 f32[256, 768]\"\n",
      "    # t4285 = ltorch.reshape(t595, (-1, 768))  # t4285: \"cuda:0 f32[256, 768]\"\n",
      "      # t4285 = prims.reshape(t595, (256, 768))  # t4285: \"cuda:0 f32[256, 768]\"\n",
      "  del t595\n",
      "  t4323 = torch.reshape(t577, (-1, 768))  # t4323: \"cuda:0 f32[256, 768]\"\n",
      "    # t4323 = ltorch.reshape(t577, (-1, 768))  # t4323: \"cuda:0 f32[256, 768]\"\n",
      "      # t4323 = prims.reshape(t577, (256, 768))  # t4323: \"cuda:0 f32[256, 768]\"\n",
      "  del t577\n",
      "  t4328 = torch.permute(t557, (0, 1, 3, 2))  # t4328: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4328 = ltorch.permute(t557, (0, 1, 3, 2))  # t4328: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4328 = prims.transpose(t557, (0, 1, 3, 2))  # t4328: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t557\n",
      "  t4330 = torch.permute(t573, (0, 1, 3, 2))  # t4330: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4330 = ltorch.permute(t573, (0, 1, 3, 2))  # t4330: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4330 = prims.transpose(t573, (0, 1, 3, 2))  # t4330: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  t4340 = torch.permute(t558, (0, 1, 3, 2))  # t4340: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4340 = ltorch.permute(t558, (0, 1, 3, 2))  # t4340: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4340 = prims.transpose(t558, (0, 1, 3, 2))  # t4340: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t558\n",
      "  t4342 = torch.permute(t553, (0, 1, 3, 2))  # t4342: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4342 = ltorch.permute(t553, (0, 1, 3, 2))  # t4342: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4342 = prims.transpose(t553, (0, 1, 3, 2))  # t4342: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t553\n",
      "  t4357 = torch.reshape(t547, (-1, 768))  # t4357: \"cuda:0 f32[256, 768]\"\n",
      "    # t4357 = ltorch.reshape(t547, (-1, 768))  # t4357: \"cuda:0 f32[256, 768]\"\n",
      "      # t4357 = prims.reshape(t547, (256, 768))  # t4357: \"cuda:0 f32[256, 768]\"\n",
      "  del t547\n",
      "  t4395 = torch.reshape(t529, (-1, 3072))  # t4395: \"cuda:0 f32[256, 3072]\"\n",
      "    # t4395 = ltorch.reshape(t529, (-1, 3072))  # t4395: \"cuda:0 f32[256, 3072]\"\n",
      "      # t4395 = prims.reshape(t529, (256, 3072))  # t4395: \"cuda:0 f32[256, 3072]\"\n",
      "  del t529\n",
      "  t4419 = torch.reshape(t520, (-1, 768))  # t4419: \"cuda:0 f32[256, 768]\"\n",
      "    # t4419 = ltorch.reshape(t520, (-1, 768))  # t4419: \"cuda:0 f32[256, 768]\"\n",
      "      # t4419 = prims.reshape(t520, (256, 768))  # t4419: \"cuda:0 f32[256, 768]\"\n",
      "  del t520\n",
      "  t4457 = torch.reshape(t502, (-1, 768))  # t4457: \"cuda:0 f32[256, 768]\"\n",
      "    # t4457 = ltorch.reshape(t502, (-1, 768))  # t4457: \"cuda:0 f32[256, 768]\"\n",
      "      # t4457 = prims.reshape(t502, (256, 768))  # t4457: \"cuda:0 f32[256, 768]\"\n",
      "  del t502\n",
      "  t4462 = torch.permute(t482, (0, 1, 3, 2))  # t4462: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4462 = ltorch.permute(t482, (0, 1, 3, 2))  # t4462: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4462 = prims.transpose(t482, (0, 1, 3, 2))  # t4462: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t482\n",
      "  t4464 = torch.permute(t498, (0, 1, 3, 2))  # t4464: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4464 = ltorch.permute(t498, (0, 1, 3, 2))  # t4464: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4464 = prims.transpose(t498, (0, 1, 3, 2))  # t4464: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  t4474 = torch.permute(t483, (0, 1, 3, 2))  # t4474: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4474 = ltorch.permute(t483, (0, 1, 3, 2))  # t4474: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4474 = prims.transpose(t483, (0, 1, 3, 2))  # t4474: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t483\n",
      "  t4476 = torch.permute(t478, (0, 1, 3, 2))  # t4476: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4476 = ltorch.permute(t478, (0, 1, 3, 2))  # t4476: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4476 = prims.transpose(t478, (0, 1, 3, 2))  # t4476: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t478\n",
      "  t4491 = torch.reshape(t472, (-1, 768))  # t4491: \"cuda:0 f32[256, 768]\"\n",
      "    # t4491 = ltorch.reshape(t472, (-1, 768))  # t4491: \"cuda:0 f32[256, 768]\"\n",
      "      # t4491 = prims.reshape(t472, (256, 768))  # t4491: \"cuda:0 f32[256, 768]\"\n",
      "  del t472\n",
      "  t4529 = torch.reshape(t454, (-1, 3072))  # t4529: \"cuda:0 f32[256, 3072]\"\n",
      "    # t4529 = ltorch.reshape(t454, (-1, 3072))  # t4529: \"cuda:0 f32[256, 3072]\"\n",
      "      # t4529 = prims.reshape(t454, (256, 3072))  # t4529: \"cuda:0 f32[256, 3072]\"\n",
      "  del t454\n",
      "  t4553 = torch.reshape(t445, (-1, 768))  # t4553: \"cuda:0 f32[256, 768]\"\n",
      "    # t4553 = ltorch.reshape(t445, (-1, 768))  # t4553: \"cuda:0 f32[256, 768]\"\n",
      "      # t4553 = prims.reshape(t445, (256, 768))  # t4553: \"cuda:0 f32[256, 768]\"\n",
      "  del t445\n",
      "  t4591 = torch.reshape(t427, (-1, 768))  # t4591: \"cuda:0 f32[256, 768]\"\n",
      "    # t4591 = ltorch.reshape(t427, (-1, 768))  # t4591: \"cuda:0 f32[256, 768]\"\n",
      "      # t4591 = prims.reshape(t427, (256, 768))  # t4591: \"cuda:0 f32[256, 768]\"\n",
      "  del t427\n",
      "  t4596 = torch.permute(t407, (0, 1, 3, 2))  # t4596: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4596 = ltorch.permute(t407, (0, 1, 3, 2))  # t4596: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4596 = prims.transpose(t407, (0, 1, 3, 2))  # t4596: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t407\n",
      "  t4598 = torch.permute(t423, (0, 1, 3, 2))  # t4598: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4598 = ltorch.permute(t423, (0, 1, 3, 2))  # t4598: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4598 = prims.transpose(t423, (0, 1, 3, 2))  # t4598: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  t4608 = torch.permute(t408, (0, 1, 3, 2))  # t4608: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4608 = ltorch.permute(t408, (0, 1, 3, 2))  # t4608: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4608 = prims.transpose(t408, (0, 1, 3, 2))  # t4608: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t408\n",
      "  t4610 = torch.permute(t403, (0, 1, 3, 2))  # t4610: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4610 = ltorch.permute(t403, (0, 1, 3, 2))  # t4610: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4610 = prims.transpose(t403, (0, 1, 3, 2))  # t4610: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t403\n",
      "  t4625 = torch.reshape(t397, (-1, 768))  # t4625: \"cuda:0 f32[256, 768]\"\n",
      "    # t4625 = ltorch.reshape(t397, (-1, 768))  # t4625: \"cuda:0 f32[256, 768]\"\n",
      "      # t4625 = prims.reshape(t397, (256, 768))  # t4625: \"cuda:0 f32[256, 768]\"\n",
      "  del t397\n",
      "  t4663 = torch.reshape(t379, (-1, 3072))  # t4663: \"cuda:0 f32[256, 3072]\"\n",
      "    # t4663 = ltorch.reshape(t379, (-1, 3072))  # t4663: \"cuda:0 f32[256, 3072]\"\n",
      "      # t4663 = prims.reshape(t379, (256, 3072))  # t4663: \"cuda:0 f32[256, 3072]\"\n",
      "  del t379\n",
      "  t4687 = torch.reshape(t370, (-1, 768))  # t4687: \"cuda:0 f32[256, 768]\"\n",
      "    # t4687 = ltorch.reshape(t370, (-1, 768))  # t4687: \"cuda:0 f32[256, 768]\"\n",
      "      # t4687 = prims.reshape(t370, (256, 768))  # t4687: \"cuda:0 f32[256, 768]\"\n",
      "  del t370\n",
      "  t4725 = torch.reshape(t352, (-1, 768))  # t4725: \"cuda:0 f32[256, 768]\"\n",
      "    # t4725 = ltorch.reshape(t352, (-1, 768))  # t4725: \"cuda:0 f32[256, 768]\"\n",
      "      # t4725 = prims.reshape(t352, (256, 768))  # t4725: \"cuda:0 f32[256, 768]\"\n",
      "  del t352\n",
      "  t4730 = torch.permute(t332, (0, 1, 3, 2))  # t4730: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4730 = ltorch.permute(t332, (0, 1, 3, 2))  # t4730: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4730 = prims.transpose(t332, (0, 1, 3, 2))  # t4730: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t332\n",
      "  t4732 = torch.permute(t348, (0, 1, 3, 2))  # t4732: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4732 = ltorch.permute(t348, (0, 1, 3, 2))  # t4732: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4732 = prims.transpose(t348, (0, 1, 3, 2))  # t4732: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  t3681 = torch_cross_entropy_backward_impl(t946, t924, t925, None, 'mean', i834, f835)  # t3681: \"cuda:0 f32[256, 50257]\"\n",
      "  del t946, t924, t925, i834, f835\n",
      "  t3682 = torch.reshape(t3681, (4, 64, 50257))  # t3682: \"cuda:0 f32[4, 64, 50257]\"\n",
      "    # t3682 = ltorch.reshape(t3681, (4, 64, 50257))  # t3682: \"cuda:0 f32[4, 64, 50257]\"\n",
      "      # t3682 = prims.reshape(t3681, (4, 64, 50257))  # t3682: \"cuda:0 f32[4, 64, 50257]\"\n",
      "  del t3681\n",
      "  [t3683] = nvFusion0(t3682, t945)\n",
      "    # t3683 = prims.add(t945, t3682)  # t3683: \"cuda:0 f32[4, 64, 50257]\"\n",
      "  del t3682, t945\n",
      "  t3684 = torch.reshape(t3683, (-1, 50257))  # t3684: \"cuda:0 f32[256, 50257]\"\n",
      "    # t3684 = ltorch.reshape(t3683, (-1, 50257))  # t3684: \"cuda:0 f32[256, 50257]\"\n",
      "      # t3684 = prims.reshape(t3683, (256, 50257))  # t3684: \"cuda:0 f32[256, 50257]\"\n",
      "  del t3683\n",
      "  t3688 = torch.permute(t3684, (1, 0))  # t3688: \"cuda:0 f32[50257, 256]\"\n",
      "    # t3688 = ltorch.permute(t3684, (1, 0))  # t3688: \"cuda:0 f32[50257, 256]\"\n",
      "      # t3688 = prims.transpose(t3684, (1, 0))  # t3688: \"cuda:0 f32[50257, 256]\"\n",
      "  t3685 = torch.matmul(t3684, t_lm_head_weight)  # t3685: \"cuda:0 f32[256, 768]\"\n",
      "    # t3685 = ltorch.matmul(t3684, t_lm_head_weight)  # t3685: \"cuda:0 f32[256, 768]\"\n",
      "      # t3685 = prims.matmul(t3684, t_lm_head_weight)  # t3685: \"cuda:0 f32[256, 768]\"\n",
      "  del t3684, t_lm_head_weight\n",
      "  t3690 = torch.matmul(t3688, t3689)  # t3690: \"cuda:0 f32[50257, 768]\"\n",
      "    # t3690 = ltorch.matmul(t3688, t3689)  # t3690: \"cuda:0 f32[50257, 768]\"\n",
      "      # t3690 = prims.matmul(t3688, t3689)  # t3690: \"cuda:0 f32[50257, 768]\"\n",
      "  del t3688, t3689\n",
      "  t3686 = torch.reshape(t3685, (4, 64, 768))  # t3686: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3686 = ltorch.reshape(t3685, (4, 64, 768))  # t3686: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t3686 = prims.reshape(t3685, (4, 64, 768))  # t3686: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t3685\n",
      "  t5581 = torch.unsqueeze(t910, 2)  # t5581: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5581 = ltorch.unsqueeze(t910, 2)  # t5581: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t5581 = prims.broadcast_in_dim(t910, [4, 64, 1], [0, 1])  # t5581: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t910\n",
      "  t3712 = Tensor.expand(t5581, [4, 64, 1])  # t3712: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t3712 = ltorch.expand(t5581, [4, 64, 1])  # t3712: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t3712 = prims.broadcast_in_dim(t5581, (4, 64, 1), (0, 1, 2))  # t3712: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t5581\n",
      "  t3713 = Tensor.expand(t3712, (4, 64, 768))  # t3713: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3713 = ltorch.expand(t3712, (4, 64, 768))  # t3713: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t3713 = prims.broadcast_in_dim(t3712, (4, 64, 768), (0, 1, 2))  # t3713: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t3712\n",
      "  [t3691, t3694, t3719, t3727] = nvFusion1(t3686, t3713, t906, t914, t916, t919)\n",
      "    # t917 = prims.broadcast_in_dim(t914, (4, 64, 768), (0, 1, 2))  # t917: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t918 = prims.mul(t916, t917)  # t918: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3691 = prims.sum(t3686, (0, 1))  # t3691: \"cuda:0 f32[768]\"\n",
      "    # t3692 = prims.mul(t919, t3686)  # t3692: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3693 = prims.mul(t918, t3686)  # t3693: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3694 = prims.sum(t3693, (0, 1))  # t3694: \"cuda:0 f32[768]\"\n",
      "    # t3695 = prims.mul(t917, t3692)  # t3695: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3696 = prims.mul(t916, t3692)  # t3696: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3697 = prims.sum(t3696, (2,))  # t3697: \"cuda:0 f32[4, 64]\"\n",
      "    # t3698 = prims.broadcast_in_dim(t3697, [4, 64, 1], [0, 1])  # t3698: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t3699 = prims.neg(t3695)  # t3699: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3700 = prims.sum(t3699, (2,))  # t3700: \"cuda:0 f32[4, 64]\"\n",
      "    # t3701 = prims.broadcast_in_dim(t3700, [4, 64, 1], [0, 1])  # t3701: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t3702 = prims.mul(-0.5, t3698)  # t3702: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t3703 = prims.pow(t914, 3.0)  # t3703: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t3704 = prims.mul(t3702, t3703)  # t3704: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t3705 = prims.sum(t3701, (2,))  # t3705: \"cuda:0 f32[4, 64]\"\n",
      "    # t3706 = prims.sum(t3704, (2,))  # t3706: \"cuda:0 f32[4, 64]\"\n",
      "    # t3707 = prims.broadcast_in_dim(t3705, [4, 64, 1], [0, 1])  # t3707: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t3708 = prims.broadcast_in_dim(t3707, (4, 64, 768), (0, 1, 2))  # t3708: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3709 = prims.mul(0.0013020833333333333, t3708)  # t3709: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3710 = prims.broadcast_in_dim(t3706, [4, 64, 1], [0, 1])  # t3710: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t3711 = prims.broadcast_in_dim(t3710, (4, 64, 768), (0, 1, 2))  # t3711: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3714 = prims.mul(2.0, t3711)  # t3714: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3715 = prims.sub(t906, t3713)  # t3715: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3716 = prims.mul(t3714, t3715)  # t3716: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3717 = prims.div(t3716, 768.0)  # t3717: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3718 = prims.add(t3709, t3717)  # t3718: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3719 = prims.add(t3695, t3718)  # t3719: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3727 = prims.sum(t3719, (0, 1))  # t3727: \"cuda:0 f32[768]\"\n",
      "  del t3686, t3713, t906, t914, t916, t919\n",
      "  t3720 = torch.reshape(t3719, (-1, 768))  # t3720: \"cuda:0 f32[256, 768]\"\n",
      "    # t3720 = ltorch.reshape(t3719, (-1, 768))  # t3720: \"cuda:0 f32[256, 768]\"\n",
      "      # t3720 = prims.reshape(t3719, (256, 768))  # t3720: \"cuda:0 f32[256, 768]\"\n",
      "  t3724 = torch.permute(t3720, (1, 0))  # t3724: \"cuda:0 f32[768, 256]\"\n",
      "    # t3724 = ltorch.permute(t3720, (1, 0))  # t3724: \"cuda:0 f32[768, 256]\"\n",
      "      # t3724 = prims.transpose(t3720, (1, 0))  # t3724: \"cuda:0 f32[768, 256]\"\n",
      "  t3721 = torch.matmul(t3720, t_transformer_h_11_mlp_c_proj_weight)  # t3721: \"cuda:0 f32[256, 3072]\"\n",
      "    # t3721 = ltorch.matmul(t3720, t_transformer_h_11_mlp_c_proj_weight)  # t3721: \"cuda:0 f32[256, 3072]\"\n",
      "      # t3721 = prims.matmul(t3720, t_transformer_h_11_mlp_c_proj_weight)  # t3721: \"cuda:0 f32[256, 3072]\"\n",
      "  del t3720, t_transformer_h_11_mlp_c_proj_weight\n",
      "  t3726 = torch.matmul(t3724, t3725)  # t3726: \"cuda:0 f32[768, 3072]\"\n",
      "    # t3726 = ltorch.matmul(t3724, t3725)  # t3726: \"cuda:0 f32[768, 3072]\"\n",
      "      # t3726 = prims.matmul(t3724, t3725)  # t3726: \"cuda:0 f32[768, 3072]\"\n",
      "  del t3724, t3725\n",
      "  t3722 = torch.reshape(t3721, (4, 64, 3072))  # t3722: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t3722 = ltorch.reshape(t3721, (4, 64, 3072))  # t3722: \"cuda:0 f32[4, 64, 3072]\"\n",
      "      # t3722 = prims.reshape(t3721, (4, 64, 3072))  # t3722: \"cuda:0 f32[4, 64, 3072]\"\n",
      "  del t3721\n",
      "  [t3743, t3751] = nvFusion2(f814, f816, f818, f820, t3722, t896)\n",
      "    # t897 = prims.mul(0.5, t896)  # t897: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t898 = prims.pow(t896, 3.0)  # t898: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t899 = prims.mul(0.044715, t898)  # t899: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t900 = prims.add(t896, t899)  # t900: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t901 = prims.mul(0.7978845608028654, t900)  # t901: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t902 = prims.tanh(t901)  # t902: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t903 = prims.add(1.0, t902)  # t903: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t3728 = prims.mul(t903, t3722)  # t3728: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t3729 = prims.mul(t897, t3722)  # t3729: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t3730 = prims.mul(t902, t902)  # t3730: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t3731 = prims.sub(1.0, t3730)  # t3731: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t3732 = prims.mul(t3729, t3731)  # t3732: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t3733 = prims.mul(f820, t3732)  # t3733: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t3734 = prims.mul(f818, t3733)  # t3734: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t3736 = prims.mul(t3734, f816)  # t3736: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t3737 = prims.pow(t896, 2.0)  # t3737: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t3738 = prims.mul(t3736, t3737)  # t3738: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t3741 = prims.add(t3733, t3738)  # t3741: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t3742 = prims.mul(f814, t3728)  # t3742: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t3743 = prims.add(t3741, t3742)  # t3743: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t3751 = prims.sum(t3743, (0, 1))  # t3751: \"cuda:0 f32[3072]\"\n",
      "  del f814, f816, f818, f820, t3722, t896\n",
      "  t3744 = torch.reshape(t3743, (-1, 3072))  # t3744: \"cuda:0 f32[256, 3072]\"\n",
      "    # t3744 = ltorch.reshape(t3743, (-1, 3072))  # t3744: \"cuda:0 f32[256, 3072]\"\n",
      "      # t3744 = prims.reshape(t3743, (256, 3072))  # t3744: \"cuda:0 f32[256, 3072]\"\n",
      "  del t3743\n",
      "  t3748 = torch.permute(t3744, (1, 0))  # t3748: \"cuda:0 f32[3072, 256]\"\n",
      "    # t3748 = ltorch.permute(t3744, (1, 0))  # t3748: \"cuda:0 f32[3072, 256]\"\n",
      "      # t3748 = prims.transpose(t3744, (1, 0))  # t3748: \"cuda:0 f32[3072, 256]\"\n",
      "  t3750 = torch.matmul(t3748, t3749)  # t3750: \"cuda:0 f32[3072, 768]\"\n",
      "    # t3750 = ltorch.matmul(t3748, t3749)  # t3750: \"cuda:0 f32[3072, 768]\"\n",
      "      # t3750 = prims.matmul(t3748, t3749)  # t3750: \"cuda:0 f32[3072, 768]\"\n",
      "  del t3748, t3749\n",
      "  t3745 = torch.matmul(t3744, t_transformer_h_11_mlp_c_fc_weight)  # t3745: \"cuda:0 f32[256, 768]\"\n",
      "    # t3745 = ltorch.matmul(t3744, t_transformer_h_11_mlp_c_fc_weight)  # t3745: \"cuda:0 f32[256, 768]\"\n",
      "      # t3745 = prims.matmul(t3744, t_transformer_h_11_mlp_c_fc_weight)  # t3745: \"cuda:0 f32[256, 768]\"\n",
      "  del t3744, t_transformer_h_11_mlp_c_fc_weight\n",
      "  t3746 = torch.reshape(t3745, (4, 64, 768))  # t3746: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3746 = ltorch.reshape(t3745, (4, 64, 768))  # t3746: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t3746 = prims.reshape(t3745, (4, 64, 768))  # t3746: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t3745\n",
      "  t5590 = torch.unsqueeze(t883, 2)  # t5590: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5590 = ltorch.unsqueeze(t883, 2)  # t5590: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t5590 = prims.broadcast_in_dim(t883, [4, 64, 1], [0, 1])  # t5590: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t883\n",
      "  t3773 = Tensor.expand(t5590, [4, 64, 1])  # t3773: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t3773 = ltorch.expand(t5590, [4, 64, 1])  # t3773: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t3773 = prims.broadcast_in_dim(t5590, (4, 64, 1), (0, 1, 2))  # t3773: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t5590\n",
      "  t3774 = Tensor.expand(t3773, (4, 64, 768))  # t3774: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3774 = ltorch.expand(t3773, (4, 64, 768))  # t3774: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t3774 = prims.broadcast_in_dim(t3773, (4, 64, 768), (0, 1, 2))  # t3774: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t3773\n",
      "  [t3752, t3755, t3781, t3789] = nvFusion3(t3719, t3746, t3774, t879, t887, t889, t892)\n",
      "    # t890 = prims.broadcast_in_dim(t887, (4, 64, 768), (0, 1, 2))  # t890: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t891 = prims.mul(t889, t890)  # t891: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3752 = prims.sum(t3746, (0, 1))  # t3752: \"cuda:0 f32[768]\"\n",
      "    # t3753 = prims.mul(t892, t3746)  # t3753: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3754 = prims.mul(t891, t3746)  # t3754: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3755 = prims.sum(t3754, (0, 1))  # t3755: \"cuda:0 f32[768]\"\n",
      "    # t3756 = prims.mul(t890, t3753)  # t3756: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3757 = prims.mul(t889, t3753)  # t3757: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3758 = prims.sum(t3757, (2,))  # t3758: \"cuda:0 f32[4, 64]\"\n",
      "    # t3759 = prims.broadcast_in_dim(t3758, [4, 64, 1], [0, 1])  # t3759: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t3760 = prims.neg(t3756)  # t3760: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3761 = prims.sum(t3760, (2,))  # t3761: \"cuda:0 f32[4, 64]\"\n",
      "    # t3762 = prims.broadcast_in_dim(t3761, [4, 64, 1], [0, 1])  # t3762: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t3763 = prims.mul(-0.5, t3759)  # t3763: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t3764 = prims.pow(t887, 3.0)  # t3764: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t3765 = prims.mul(t3763, t3764)  # t3765: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t3766 = prims.sum(t3762, (2,))  # t3766: \"cuda:0 f32[4, 64]\"\n",
      "    # t3767 = prims.sum(t3765, (2,))  # t3767: \"cuda:0 f32[4, 64]\"\n",
      "    # t3768 = prims.broadcast_in_dim(t3766, [4, 64, 1], [0, 1])  # t3768: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t3769 = prims.broadcast_in_dim(t3768, (4, 64, 768), (0, 1, 2))  # t3769: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3770 = prims.mul(0.0013020833333333333, t3769)  # t3770: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3771 = prims.broadcast_in_dim(t3767, [4, 64, 1], [0, 1])  # t3771: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t3772 = prims.broadcast_in_dim(t3771, (4, 64, 768), (0, 1, 2))  # t3772: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3775 = prims.mul(2.0, t3772)  # t3775: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3776 = prims.sub(t879, t3774)  # t3776: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3777 = prims.mul(t3775, t3776)  # t3777: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3778 = prims.div(t3777, 768.0)  # t3778: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3779 = prims.add(t3770, t3778)  # t3779: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3780 = prims.add(t3756, t3779)  # t3780: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3781 = prims.add(t3719, t3780)  # t3781: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3789 = prims.sum(t3781, (0, 1))  # t3789: \"cuda:0 f32[768]\"\n",
      "  del t3719, t3746, t3774, t879, t887, t889, t892\n",
      "  t3782 = torch.reshape(t3781, (-1, 768))  # t3782: \"cuda:0 f32[256, 768]\"\n",
      "    # t3782 = ltorch.reshape(t3781, (-1, 768))  # t3782: \"cuda:0 f32[256, 768]\"\n",
      "      # t3782 = prims.reshape(t3781, (256, 768))  # t3782: \"cuda:0 f32[256, 768]\"\n",
      "  t3786 = torch.permute(t3782, (1, 0))  # t3786: \"cuda:0 f32[768, 256]\"\n",
      "    # t3786 = ltorch.permute(t3782, (1, 0))  # t3786: \"cuda:0 f32[768, 256]\"\n",
      "      # t3786 = prims.transpose(t3782, (1, 0))  # t3786: \"cuda:0 f32[768, 256]\"\n",
      "  t3783 = torch.matmul(t3782, t_transformer_h_11_attn_c_proj_weight)  # t3783: \"cuda:0 f32[256, 768]\"\n",
      "    # t3783 = ltorch.matmul(t3782, t_transformer_h_11_attn_c_proj_weight)  # t3783: \"cuda:0 f32[256, 768]\"\n",
      "      # t3783 = prims.matmul(t3782, t_transformer_h_11_attn_c_proj_weight)  # t3783: \"cuda:0 f32[256, 768]\"\n",
      "  del t3782, t_transformer_h_11_attn_c_proj_weight\n",
      "  t3788 = torch.matmul(t3786, t3787)  # t3788: \"cuda:0 f32[768, 768]\"\n",
      "    # t3788 = ltorch.matmul(t3786, t3787)  # t3788: \"cuda:0 f32[768, 768]\"\n",
      "      # t3788 = prims.matmul(t3786, t3787)  # t3788: \"cuda:0 f32[768, 768]\"\n",
      "  del t3786, t3787\n",
      "  t3784 = torch.reshape(t3783, (4, 64, 768))  # t3784: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3784 = ltorch.reshape(t3783, (4, 64, 768))  # t3784: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t3784 = prims.reshape(t3783, (4, 64, 768))  # t3784: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t3783\n",
      "  t3790 = torch.reshape(t3784, (4, 64, 12, 64))  # t3790: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t3790 = ltorch.reshape(t3784, (4, 64, 12, 64))  # t3790: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t3790 = prims.reshape(t3784, (4, 64, 12, 64))  # t3790: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t3784\n",
      "  t3791 = torch.permute(t3790, (0, 2, 1, 3))  # t3791: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t3791 = ltorch.permute(t3790, (0, 2, 1, 3))  # t3791: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t3791 = prims.transpose(t3790, (0, 2, 1, 3))  # t3791: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t3790\n",
      "  t3793 = torch.matmul(t3791, t3792)  # t3793: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t3793 = ltorch.matmul(t3791, t3792)  # t3793: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t3793 = prims.matmul(t3791, t3792)  # t3793: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t3792\n",
      "  t3795 = torch.matmul(t3794, t3791)  # t3795: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t3795 = ltorch.matmul(t3794, t3791)  # t3795: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t3795 = prims.matmul(t3794, t3791)  # t3795: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t3794, t3791\n",
      "  [t3803] = nvFusion4(f794, i798, t3793, t863, t873)\n",
      "    # t3796 = prims.mul(t873, t3793)  # t3796: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t3797 = prims.sum(t3796, (3,))  # t3797: \"cuda:0 f32[4, 12, 64]\"\n",
      "    # t3798 = prims.broadcast_in_dim(t3797, [4, 12, 64, 1], [0, 1, 2])  # t3798: \"cuda:0 f32[4, 12, 64, 1]\"\n",
      "    # t3799 = prims.broadcast_in_dim(t3798, (4, 12, 64, 64), (0, 1, 2, 3))  # t3799: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t3800 = prims.sub(t3793, t3799)  # t3800: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t3801 = prims.mul(t873, t3800)  # t3801: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t3802 = prims.where(t863, 0.0, t3801)  # t3802: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t3803 = prims.mul(f794, t3802)  # t3803: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del f794, i798, t3793, t863, t873\n",
      "  t3805 = torch.matmul(t3803, t3804)  # t3805: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t3805 = ltorch.matmul(t3803, t3804)  # t3805: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t3805 = prims.matmul(t3803, t3804)  # t3805: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t3804\n",
      "  t3807 = torch.matmul(t3806, t3803)  # t3807: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t3807 = ltorch.matmul(t3806, t3803)  # t3807: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t3807 = prims.matmul(t3806, t3803)  # t3807: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t3806, t3803\n",
      "  t3808 = torch.permute(t3807, (0, 1, 3, 2))  # t3808: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t3808 = ltorch.permute(t3807, (0, 1, 3, 2))  # t3808: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t3808 = prims.transpose(t3807, (0, 1, 3, 2))  # t3808: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t3807\n",
      "  t3809 = torch.permute(t3795, (0, 2, 1, 3))  # t3809: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t3809 = ltorch.permute(t3795, (0, 2, 1, 3))  # t3809: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t3809 = prims.transpose(t3795, (0, 2, 1, 3))  # t3809: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t3795\n",
      "  t3810 = torch.reshape(t3809, (4, 64, 768))  # t3810: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3810 = ltorch.reshape(t3809, (4, 64, 768))  # t3810: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t3810 = prims.reshape(t3809, (4, 64, 768))  # t3810: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t3809\n",
      "  t3811 = torch.permute(t3808, (0, 2, 1, 3))  # t3811: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t3811 = ltorch.permute(t3808, (0, 2, 1, 3))  # t3811: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t3811 = prims.transpose(t3808, (0, 2, 1, 3))  # t3811: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t3808\n",
      "  t3812 = torch.reshape(t3811, (4, 64, 768))  # t3812: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3812 = ltorch.reshape(t3811, (4, 64, 768))  # t3812: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t3812 = prims.reshape(t3811, (4, 64, 768))  # t3812: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t3811\n",
      "  t3813 = torch.permute(t3805, (0, 2, 1, 3))  # t3813: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t3813 = ltorch.permute(t3805, (0, 2, 1, 3))  # t3813: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t3813 = prims.transpose(t3805, (0, 2, 1, 3))  # t3813: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t3805\n",
      "  t3814 = torch.reshape(t3813, (4, 64, 768))  # t3814: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3814 = ltorch.reshape(t3813, (4, 64, 768))  # t3814: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t3814 = prims.reshape(t3813, (4, 64, 768))  # t3814: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t3813\n",
      "  [t3815, t3823] = nvFusion5(i760, t3810, t3812, t3814)\n",
      "    # t3815 = prims.cat((t3814, t3812, t3810), i760)  # t3815: \"cuda:0 f32[4, 64, 2304]\"\n",
      "    # t3823 = prims.sum(t3815, (0, 1))  # t3823: \"cuda:0 f32[2304]\"\n",
      "  del i760, t3810, t3812, t3814\n",
      "  t3816 = torch.reshape(t3815, (-1, 2304))  # t3816: \"cuda:0 f32[256, 2304]\"\n",
      "    # t3816 = ltorch.reshape(t3815, (-1, 2304))  # t3816: \"cuda:0 f32[256, 2304]\"\n",
      "      # t3816 = prims.reshape(t3815, (256, 2304))  # t3816: \"cuda:0 f32[256, 2304]\"\n",
      "  del t3815\n",
      "  t3820 = torch.permute(t3816, (1, 0))  # t3820: \"cuda:0 f32[2304, 256]\"\n",
      "    # t3820 = ltorch.permute(t3816, (1, 0))  # t3820: \"cuda:0 f32[2304, 256]\"\n",
      "      # t3820 = prims.transpose(t3816, (1, 0))  # t3820: \"cuda:0 f32[2304, 256]\"\n",
      "  t3817 = torch.matmul(t3816, t_transformer_h_11_attn_c_attn_weight)  # t3817: \"cuda:0 f32[256, 768]\"\n",
      "    # t3817 = ltorch.matmul(t3816, t_transformer_h_11_attn_c_attn_weight)  # t3817: \"cuda:0 f32[256, 768]\"\n",
      "      # t3817 = prims.matmul(t3816, t_transformer_h_11_attn_c_attn_weight)  # t3817: \"cuda:0 f32[256, 768]\"\n",
      "  del t3816, t_transformer_h_11_attn_c_attn_weight\n",
      "  t3822 = torch.matmul(t3820, t3821)  # t3822: \"cuda:0 f32[2304, 768]\"\n",
      "    # t3822 = ltorch.matmul(t3820, t3821)  # t3822: \"cuda:0 f32[2304, 768]\"\n",
      "      # t3822 = prims.matmul(t3820, t3821)  # t3822: \"cuda:0 f32[2304, 768]\"\n",
      "  del t3820, t3821\n",
      "  t3818 = torch.reshape(t3817, (4, 64, 768))  # t3818: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3818 = ltorch.reshape(t3817, (4, 64, 768))  # t3818: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t3818 = prims.reshape(t3817, (4, 64, 768))  # t3818: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t3817\n",
      "  t5608 = torch.unsqueeze(t835, 2)  # t5608: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5608 = ltorch.unsqueeze(t835, 2)  # t5608: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t5608 = prims.broadcast_in_dim(t835, [4, 64, 1], [0, 1])  # t5608: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t835\n",
      "  t3845 = Tensor.expand(t5608, [4, 64, 1])  # t3845: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t3845 = ltorch.expand(t5608, [4, 64, 1])  # t3845: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t3845 = prims.broadcast_in_dim(t5608, (4, 64, 1), (0, 1, 2))  # t3845: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t5608\n",
      "  t3846 = Tensor.expand(t3845, (4, 64, 768))  # t3846: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3846 = ltorch.expand(t3845, (4, 64, 768))  # t3846: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t3846 = prims.broadcast_in_dim(t3845, (4, 64, 768), (0, 1, 2))  # t3846: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t3845\n",
      "  [t3824, t3827, t3853, t3861] = nvFusion6(t3781, t3818, t3846, t831, t839, t841, t844)\n",
      "    # t842 = prims.broadcast_in_dim(t839, (4, 64, 768), (0, 1, 2))  # t842: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t843 = prims.mul(t841, t842)  # t843: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3824 = prims.sum(t3818, (0, 1))  # t3824: \"cuda:0 f32[768]\"\n",
      "    # t3825 = prims.mul(t844, t3818)  # t3825: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3826 = prims.mul(t843, t3818)  # t3826: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3827 = prims.sum(t3826, (0, 1))  # t3827: \"cuda:0 f32[768]\"\n",
      "    # t3828 = prims.mul(t842, t3825)  # t3828: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3829 = prims.mul(t841, t3825)  # t3829: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3830 = prims.sum(t3829, (2,))  # t3830: \"cuda:0 f32[4, 64]\"\n",
      "    # t3831 = prims.broadcast_in_dim(t3830, [4, 64, 1], [0, 1])  # t3831: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t3832 = prims.neg(t3828)  # t3832: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3833 = prims.sum(t3832, (2,))  # t3833: \"cuda:0 f32[4, 64]\"\n",
      "    # t3834 = prims.broadcast_in_dim(t3833, [4, 64, 1], [0, 1])  # t3834: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t3835 = prims.mul(-0.5, t3831)  # t3835: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t3836 = prims.pow(t839, 3.0)  # t3836: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t3837 = prims.mul(t3835, t3836)  # t3837: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t3838 = prims.sum(t3834, (2,))  # t3838: \"cuda:0 f32[4, 64]\"\n",
      "    # t3839 = prims.sum(t3837, (2,))  # t3839: \"cuda:0 f32[4, 64]\"\n",
      "    # t3840 = prims.broadcast_in_dim(t3838, [4, 64, 1], [0, 1])  # t3840: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t3841 = prims.broadcast_in_dim(t3840, (4, 64, 768), (0, 1, 2))  # t3841: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3842 = prims.mul(0.0013020833333333333, t3841)  # t3842: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3843 = prims.broadcast_in_dim(t3839, [4, 64, 1], [0, 1])  # t3843: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t3844 = prims.broadcast_in_dim(t3843, (4, 64, 768), (0, 1, 2))  # t3844: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3847 = prims.mul(2.0, t3844)  # t3847: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3848 = prims.sub(t831, t3846)  # t3848: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3849 = prims.mul(t3847, t3848)  # t3849: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3850 = prims.div(t3849, 768.0)  # t3850: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3851 = prims.add(t3842, t3850)  # t3851: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3852 = prims.add(t3828, t3851)  # t3852: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3853 = prims.add(t3781, t3852)  # t3853: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3861 = prims.sum(t3853, (0, 1))  # t3861: \"cuda:0 f32[768]\"\n",
      "  del t3781, t3818, t3846, t831, t839, t841, t844\n",
      "  t3854 = torch.reshape(t3853, (-1, 768))  # t3854: \"cuda:0 f32[256, 768]\"\n",
      "    # t3854 = ltorch.reshape(t3853, (-1, 768))  # t3854: \"cuda:0 f32[256, 768]\"\n",
      "      # t3854 = prims.reshape(t3853, (256, 768))  # t3854: \"cuda:0 f32[256, 768]\"\n",
      "  t3858 = torch.permute(t3854, (1, 0))  # t3858: \"cuda:0 f32[768, 256]\"\n",
      "    # t3858 = ltorch.permute(t3854, (1, 0))  # t3858: \"cuda:0 f32[768, 256]\"\n",
      "      # t3858 = prims.transpose(t3854, (1, 0))  # t3858: \"cuda:0 f32[768, 256]\"\n",
      "  t3855 = torch.matmul(t3854, t_transformer_h_10_mlp_c_proj_weight)  # t3855: \"cuda:0 f32[256, 3072]\"\n",
      "    # t3855 = ltorch.matmul(t3854, t_transformer_h_10_mlp_c_proj_weight)  # t3855: \"cuda:0 f32[256, 3072]\"\n",
      "      # t3855 = prims.matmul(t3854, t_transformer_h_10_mlp_c_proj_weight)  # t3855: \"cuda:0 f32[256, 3072]\"\n",
      "  del t3854, t_transformer_h_10_mlp_c_proj_weight\n",
      "  t3860 = torch.matmul(t3858, t3859)  # t3860: \"cuda:0 f32[768, 3072]\"\n",
      "    # t3860 = ltorch.matmul(t3858, t3859)  # t3860: \"cuda:0 f32[768, 3072]\"\n",
      "      # t3860 = prims.matmul(t3858, t3859)  # t3860: \"cuda:0 f32[768, 3072]\"\n",
      "  del t3858, t3859\n",
      "  t3856 = torch.reshape(t3855, (4, 64, 3072))  # t3856: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t3856 = ltorch.reshape(t3855, (4, 64, 3072))  # t3856: \"cuda:0 f32[4, 64, 3072]\"\n",
      "      # t3856 = prims.reshape(t3855, (4, 64, 3072))  # t3856: \"cuda:0 f32[4, 64, 3072]\"\n",
      "  del t3855\n",
      "  [t3877, t3885] = nvFusion7(f746, f748, f750, f752, t3856, t821)\n",
      "    # t822 = prims.mul(0.5, t821)  # t822: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t823 = prims.pow(t821, 3.0)  # t823: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t824 = prims.mul(0.044715, t823)  # t824: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t825 = prims.add(t821, t824)  # t825: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t826 = prims.mul(0.7978845608028654, t825)  # t826: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t827 = prims.tanh(t826)  # t827: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t828 = prims.add(1.0, t827)  # t828: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t3862 = prims.mul(t828, t3856)  # t3862: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t3863 = prims.mul(t822, t3856)  # t3863: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t3864 = prims.mul(t827, t827)  # t3864: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t3865 = prims.sub(1.0, t3864)  # t3865: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t3866 = prims.mul(t3863, t3865)  # t3866: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t3867 = prims.mul(f752, t3866)  # t3867: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t3868 = prims.mul(f750, t3867)  # t3868: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t3870 = prims.mul(t3868, f748)  # t3870: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t3871 = prims.pow(t821, 2.0)  # t3871: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t3872 = prims.mul(t3870, t3871)  # t3872: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t3875 = prims.add(t3867, t3872)  # t3875: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t3876 = prims.mul(f746, t3862)  # t3876: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t3877 = prims.add(t3875, t3876)  # t3877: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t3885 = prims.sum(t3877, (0, 1))  # t3885: \"cuda:0 f32[3072]\"\n",
      "  del f746, f748, f750, f752, t3856, t821\n",
      "  t3878 = torch.reshape(t3877, (-1, 3072))  # t3878: \"cuda:0 f32[256, 3072]\"\n",
      "    # t3878 = ltorch.reshape(t3877, (-1, 3072))  # t3878: \"cuda:0 f32[256, 3072]\"\n",
      "      # t3878 = prims.reshape(t3877, (256, 3072))  # t3878: \"cuda:0 f32[256, 3072]\"\n",
      "  del t3877\n",
      "  t3882 = torch.permute(t3878, (1, 0))  # t3882: \"cuda:0 f32[3072, 256]\"\n",
      "    # t3882 = ltorch.permute(t3878, (1, 0))  # t3882: \"cuda:0 f32[3072, 256]\"\n",
      "      # t3882 = prims.transpose(t3878, (1, 0))  # t3882: \"cuda:0 f32[3072, 256]\"\n",
      "  t3884 = torch.matmul(t3882, t3883)  # t3884: \"cuda:0 f32[3072, 768]\"\n",
      "    # t3884 = ltorch.matmul(t3882, t3883)  # t3884: \"cuda:0 f32[3072, 768]\"\n",
      "      # t3884 = prims.matmul(t3882, t3883)  # t3884: \"cuda:0 f32[3072, 768]\"\n",
      "  del t3882, t3883\n",
      "  t3879 = torch.matmul(t3878, t_transformer_h_10_mlp_c_fc_weight)  # t3879: \"cuda:0 f32[256, 768]\"\n",
      "    # t3879 = ltorch.matmul(t3878, t_transformer_h_10_mlp_c_fc_weight)  # t3879: \"cuda:0 f32[256, 768]\"\n",
      "      # t3879 = prims.matmul(t3878, t_transformer_h_10_mlp_c_fc_weight)  # t3879: \"cuda:0 f32[256, 768]\"\n",
      "  del t3878, t_transformer_h_10_mlp_c_fc_weight\n",
      "  t3880 = torch.reshape(t3879, (4, 64, 768))  # t3880: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3880 = ltorch.reshape(t3879, (4, 64, 768))  # t3880: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t3880 = prims.reshape(t3879, (4, 64, 768))  # t3880: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t3879\n",
      "  t5617 = torch.unsqueeze(t808, 2)  # t5617: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5617 = ltorch.unsqueeze(t808, 2)  # t5617: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t5617 = prims.broadcast_in_dim(t808, [4, 64, 1], [0, 1])  # t5617: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t808\n",
      "  t3907 = Tensor.expand(t5617, [4, 64, 1])  # t3907: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t3907 = ltorch.expand(t5617, [4, 64, 1])  # t3907: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t3907 = prims.broadcast_in_dim(t5617, (4, 64, 1), (0, 1, 2))  # t3907: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t5617\n",
      "  t3908 = Tensor.expand(t3907, (4, 64, 768))  # t3908: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3908 = ltorch.expand(t3907, (4, 64, 768))  # t3908: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t3908 = prims.broadcast_in_dim(t3907, (4, 64, 768), (0, 1, 2))  # t3908: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t3907\n",
      "  [t3886, t3889, t3915, t3923] = nvFusion8(t3853, t3880, t3908, t804, t812, t814, t817)\n",
      "    # t815 = prims.broadcast_in_dim(t812, (4, 64, 768), (0, 1, 2))  # t815: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t816 = prims.mul(t814, t815)  # t816: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3886 = prims.sum(t3880, (0, 1))  # t3886: \"cuda:0 f32[768]\"\n",
      "    # t3887 = prims.mul(t817, t3880)  # t3887: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3888 = prims.mul(t816, t3880)  # t3888: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3889 = prims.sum(t3888, (0, 1))  # t3889: \"cuda:0 f32[768]\"\n",
      "    # t3890 = prims.mul(t815, t3887)  # t3890: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3891 = prims.mul(t814, t3887)  # t3891: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3892 = prims.sum(t3891, (2,))  # t3892: \"cuda:0 f32[4, 64]\"\n",
      "    # t3893 = prims.broadcast_in_dim(t3892, [4, 64, 1], [0, 1])  # t3893: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t3894 = prims.neg(t3890)  # t3894: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3895 = prims.sum(t3894, (2,))  # t3895: \"cuda:0 f32[4, 64]\"\n",
      "    # t3896 = prims.broadcast_in_dim(t3895, [4, 64, 1], [0, 1])  # t3896: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t3897 = prims.mul(-0.5, t3893)  # t3897: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t3898 = prims.pow(t812, 3.0)  # t3898: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t3899 = prims.mul(t3897, t3898)  # t3899: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t3900 = prims.sum(t3896, (2,))  # t3900: \"cuda:0 f32[4, 64]\"\n",
      "    # t3901 = prims.sum(t3899, (2,))  # t3901: \"cuda:0 f32[4, 64]\"\n",
      "    # t3902 = prims.broadcast_in_dim(t3900, [4, 64, 1], [0, 1])  # t3902: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t3903 = prims.broadcast_in_dim(t3902, (4, 64, 768), (0, 1, 2))  # t3903: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3904 = prims.mul(0.0013020833333333333, t3903)  # t3904: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3905 = prims.broadcast_in_dim(t3901, [4, 64, 1], [0, 1])  # t3905: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t3906 = prims.broadcast_in_dim(t3905, (4, 64, 768), (0, 1, 2))  # t3906: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3909 = prims.mul(2.0, t3906)  # t3909: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3910 = prims.sub(t804, t3908)  # t3910: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3911 = prims.mul(t3909, t3910)  # t3911: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3912 = prims.div(t3911, 768.0)  # t3912: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3913 = prims.add(t3904, t3912)  # t3913: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3914 = prims.add(t3890, t3913)  # t3914: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3915 = prims.add(t3853, t3914)  # t3915: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3923 = prims.sum(t3915, (0, 1))  # t3923: \"cuda:0 f32[768]\"\n",
      "  del t3853, t3880, t3908, t804, t812, t814, t817\n",
      "  t3916 = torch.reshape(t3915, (-1, 768))  # t3916: \"cuda:0 f32[256, 768]\"\n",
      "    # t3916 = ltorch.reshape(t3915, (-1, 768))  # t3916: \"cuda:0 f32[256, 768]\"\n",
      "      # t3916 = prims.reshape(t3915, (256, 768))  # t3916: \"cuda:0 f32[256, 768]\"\n",
      "  t3920 = torch.permute(t3916, (1, 0))  # t3920: \"cuda:0 f32[768, 256]\"\n",
      "    # t3920 = ltorch.permute(t3916, (1, 0))  # t3920: \"cuda:0 f32[768, 256]\"\n",
      "      # t3920 = prims.transpose(t3916, (1, 0))  # t3920: \"cuda:0 f32[768, 256]\"\n",
      "  t3917 = torch.matmul(t3916, t_transformer_h_10_attn_c_proj_weight)  # t3917: \"cuda:0 f32[256, 768]\"\n",
      "    # t3917 = ltorch.matmul(t3916, t_transformer_h_10_attn_c_proj_weight)  # t3917: \"cuda:0 f32[256, 768]\"\n",
      "      # t3917 = prims.matmul(t3916, t_transformer_h_10_attn_c_proj_weight)  # t3917: \"cuda:0 f32[256, 768]\"\n",
      "  del t3916, t_transformer_h_10_attn_c_proj_weight\n",
      "  t3922 = torch.matmul(t3920, t3921)  # t3922: \"cuda:0 f32[768, 768]\"\n",
      "    # t3922 = ltorch.matmul(t3920, t3921)  # t3922: \"cuda:0 f32[768, 768]\"\n",
      "      # t3922 = prims.matmul(t3920, t3921)  # t3922: \"cuda:0 f32[768, 768]\"\n",
      "  del t3920, t3921\n",
      "  t3918 = torch.reshape(t3917, (4, 64, 768))  # t3918: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3918 = ltorch.reshape(t3917, (4, 64, 768))  # t3918: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t3918 = prims.reshape(t3917, (4, 64, 768))  # t3918: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t3917\n",
      "  t3924 = torch.reshape(t3918, (4, 64, 12, 64))  # t3924: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t3924 = ltorch.reshape(t3918, (4, 64, 12, 64))  # t3924: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t3924 = prims.reshape(t3918, (4, 64, 12, 64))  # t3924: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t3918\n",
      "  t3925 = torch.permute(t3924, (0, 2, 1, 3))  # t3925: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t3925 = ltorch.permute(t3924, (0, 2, 1, 3))  # t3925: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t3925 = prims.transpose(t3924, (0, 2, 1, 3))  # t3925: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t3924\n",
      "  t3927 = torch.matmul(t3925, t3926)  # t3927: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t3927 = ltorch.matmul(t3925, t3926)  # t3927: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t3927 = prims.matmul(t3925, t3926)  # t3927: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t3926\n",
      "  t3929 = torch.matmul(t3928, t3925)  # t3929: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t3929 = ltorch.matmul(t3928, t3925)  # t3929: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t3929 = prims.matmul(t3928, t3925)  # t3929: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t3928, t3925\n",
      "  [t3937] = nvFusion9(f726, i730, t3927, t788, t798)\n",
      "    # t3930 = prims.mul(t798, t3927)  # t3930: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t3931 = prims.sum(t3930, (3,))  # t3931: \"cuda:0 f32[4, 12, 64]\"\n",
      "    # t3932 = prims.broadcast_in_dim(t3931, [4, 12, 64, 1], [0, 1, 2])  # t3932: \"cuda:0 f32[4, 12, 64, 1]\"\n",
      "    # t3933 = prims.broadcast_in_dim(t3932, (4, 12, 64, 64), (0, 1, 2, 3))  # t3933: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t3934 = prims.sub(t3927, t3933)  # t3934: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t3935 = prims.mul(t798, t3934)  # t3935: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t3936 = prims.where(t788, 0.0, t3935)  # t3936: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t3937 = prims.mul(f726, t3936)  # t3937: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del f726, i730, t3927, t788, t798\n",
      "  t3941 = torch.matmul(t3940, t3937)  # t3941: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t3941 = ltorch.matmul(t3940, t3937)  # t3941: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t3941 = prims.matmul(t3940, t3937)  # t3941: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t3940\n",
      "  t3939 = torch.matmul(t3937, t3938)  # t3939: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t3939 = ltorch.matmul(t3937, t3938)  # t3939: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t3939 = prims.matmul(t3937, t3938)  # t3939: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t3937, t3938\n",
      "  t3942 = torch.permute(t3941, (0, 1, 3, 2))  # t3942: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t3942 = ltorch.permute(t3941, (0, 1, 3, 2))  # t3942: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t3942 = prims.transpose(t3941, (0, 1, 3, 2))  # t3942: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t3941\n",
      "  t3943 = torch.permute(t3929, (0, 2, 1, 3))  # t3943: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t3943 = ltorch.permute(t3929, (0, 2, 1, 3))  # t3943: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t3943 = prims.transpose(t3929, (0, 2, 1, 3))  # t3943: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t3929\n",
      "  t3944 = torch.reshape(t3943, (4, 64, 768))  # t3944: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3944 = ltorch.reshape(t3943, (4, 64, 768))  # t3944: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t3944 = prims.reshape(t3943, (4, 64, 768))  # t3944: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t3943\n",
      "  t3945 = torch.permute(t3942, (0, 2, 1, 3))  # t3945: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t3945 = ltorch.permute(t3942, (0, 2, 1, 3))  # t3945: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t3945 = prims.transpose(t3942, (0, 2, 1, 3))  # t3945: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t3942\n",
      "  t3946 = torch.reshape(t3945, (4, 64, 768))  # t3946: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3946 = ltorch.reshape(t3945, (4, 64, 768))  # t3946: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t3946 = prims.reshape(t3945, (4, 64, 768))  # t3946: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t3945\n",
      "  t3947 = torch.permute(t3939, (0, 2, 1, 3))  # t3947: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t3947 = ltorch.permute(t3939, (0, 2, 1, 3))  # t3947: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t3947 = prims.transpose(t3939, (0, 2, 1, 3))  # t3947: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t3939\n",
      "  t3948 = torch.reshape(t3947, (4, 64, 768))  # t3948: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3948 = ltorch.reshape(t3947, (4, 64, 768))  # t3948: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t3948 = prims.reshape(t3947, (4, 64, 768))  # t3948: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t3947\n",
      "  [t3949, t3957] = nvFusion10(i692, t3944, t3946, t3948)\n",
      "    # t3949 = prims.cat((t3948, t3946, t3944), i692)  # t3949: \"cuda:0 f32[4, 64, 2304]\"\n",
      "    # t3957 = prims.sum(t3949, (0, 1))  # t3957: \"cuda:0 f32[2304]\"\n",
      "  del i692, t3944, t3946, t3948\n",
      "  t3950 = torch.reshape(t3949, (-1, 2304))  # t3950: \"cuda:0 f32[256, 2304]\"\n",
      "    # t3950 = ltorch.reshape(t3949, (-1, 2304))  # t3950: \"cuda:0 f32[256, 2304]\"\n",
      "      # t3950 = prims.reshape(t3949, (256, 2304))  # t3950: \"cuda:0 f32[256, 2304]\"\n",
      "  del t3949\n",
      "  t3954 = torch.permute(t3950, (1, 0))  # t3954: \"cuda:0 f32[2304, 256]\"\n",
      "    # t3954 = ltorch.permute(t3950, (1, 0))  # t3954: \"cuda:0 f32[2304, 256]\"\n",
      "      # t3954 = prims.transpose(t3950, (1, 0))  # t3954: \"cuda:0 f32[2304, 256]\"\n",
      "  t3951 = torch.matmul(t3950, t_transformer_h_10_attn_c_attn_weight)  # t3951: \"cuda:0 f32[256, 768]\"\n",
      "    # t3951 = ltorch.matmul(t3950, t_transformer_h_10_attn_c_attn_weight)  # t3951: \"cuda:0 f32[256, 768]\"\n",
      "      # t3951 = prims.matmul(t3950, t_transformer_h_10_attn_c_attn_weight)  # t3951: \"cuda:0 f32[256, 768]\"\n",
      "  del t3950, t_transformer_h_10_attn_c_attn_weight\n",
      "  t3956 = torch.matmul(t3954, t3955)  # t3956: \"cuda:0 f32[2304, 768]\"\n",
      "    # t3956 = ltorch.matmul(t3954, t3955)  # t3956: \"cuda:0 f32[2304, 768]\"\n",
      "      # t3956 = prims.matmul(t3954, t3955)  # t3956: \"cuda:0 f32[2304, 768]\"\n",
      "  del t3954, t3955\n",
      "  t3952 = torch.reshape(t3951, (4, 64, 768))  # t3952: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3952 = ltorch.reshape(t3951, (4, 64, 768))  # t3952: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t3952 = prims.reshape(t3951, (4, 64, 768))  # t3952: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t3951\n",
      "  t5635 = torch.unsqueeze(t760, 2)  # t5635: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5635 = ltorch.unsqueeze(t760, 2)  # t5635: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t5635 = prims.broadcast_in_dim(t760, [4, 64, 1], [0, 1])  # t5635: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t760\n",
      "  t3979 = Tensor.expand(t5635, [4, 64, 1])  # t3979: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t3979 = ltorch.expand(t5635, [4, 64, 1])  # t3979: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t3979 = prims.broadcast_in_dim(t5635, (4, 64, 1), (0, 1, 2))  # t3979: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t5635\n",
      "  t3980 = Tensor.expand(t3979, (4, 64, 768))  # t3980: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3980 = ltorch.expand(t3979, (4, 64, 768))  # t3980: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t3980 = prims.broadcast_in_dim(t3979, (4, 64, 768), (0, 1, 2))  # t3980: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t3979\n",
      "  [t3958, t3961, t3987, t3995] = nvFusion11(t3915, t3952, t3980, t756, t764, t766, t769)\n",
      "    # t767 = prims.broadcast_in_dim(t764, (4, 64, 768), (0, 1, 2))  # t767: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t768 = prims.mul(t766, t767)  # t768: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3958 = prims.sum(t3952, (0, 1))  # t3958: \"cuda:0 f32[768]\"\n",
      "    # t3959 = prims.mul(t769, t3952)  # t3959: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3960 = prims.mul(t768, t3952)  # t3960: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3961 = prims.sum(t3960, (0, 1))  # t3961: \"cuda:0 f32[768]\"\n",
      "    # t3962 = prims.mul(t767, t3959)  # t3962: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3963 = prims.mul(t766, t3959)  # t3963: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3964 = prims.sum(t3963, (2,))  # t3964: \"cuda:0 f32[4, 64]\"\n",
      "    # t3965 = prims.broadcast_in_dim(t3964, [4, 64, 1], [0, 1])  # t3965: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t3966 = prims.neg(t3962)  # t3966: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3967 = prims.sum(t3966, (2,))  # t3967: \"cuda:0 f32[4, 64]\"\n",
      "    # t3968 = prims.broadcast_in_dim(t3967, [4, 64, 1], [0, 1])  # t3968: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t3969 = prims.mul(-0.5, t3965)  # t3969: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t3970 = prims.pow(t764, 3.0)  # t3970: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t3971 = prims.mul(t3969, t3970)  # t3971: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t3972 = prims.sum(t3968, (2,))  # t3972: \"cuda:0 f32[4, 64]\"\n",
      "    # t3973 = prims.sum(t3971, (2,))  # t3973: \"cuda:0 f32[4, 64]\"\n",
      "    # t3974 = prims.broadcast_in_dim(t3972, [4, 64, 1], [0, 1])  # t3974: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t3975 = prims.broadcast_in_dim(t3974, (4, 64, 768), (0, 1, 2))  # t3975: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3976 = prims.mul(0.0013020833333333333, t3975)  # t3976: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3977 = prims.broadcast_in_dim(t3973, [4, 64, 1], [0, 1])  # t3977: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t3978 = prims.broadcast_in_dim(t3977, (4, 64, 768), (0, 1, 2))  # t3978: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3981 = prims.mul(2.0, t3978)  # t3981: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3982 = prims.sub(t756, t3980)  # t3982: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3983 = prims.mul(t3981, t3982)  # t3983: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3984 = prims.div(t3983, 768.0)  # t3984: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3985 = prims.add(t3976, t3984)  # t3985: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3986 = prims.add(t3962, t3985)  # t3986: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3987 = prims.add(t3915, t3986)  # t3987: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t3995 = prims.sum(t3987, (0, 1))  # t3995: \"cuda:0 f32[768]\"\n",
      "  del t3915, t3952, t3980, t756, t764, t766, t769\n",
      "  t3988 = torch.reshape(t3987, (-1, 768))  # t3988: \"cuda:0 f32[256, 768]\"\n",
      "    # t3988 = ltorch.reshape(t3987, (-1, 768))  # t3988: \"cuda:0 f32[256, 768]\"\n",
      "      # t3988 = prims.reshape(t3987, (256, 768))  # t3988: \"cuda:0 f32[256, 768]\"\n",
      "  t3992 = torch.permute(t3988, (1, 0))  # t3992: \"cuda:0 f32[768, 256]\"\n",
      "    # t3992 = ltorch.permute(t3988, (1, 0))  # t3992: \"cuda:0 f32[768, 256]\"\n",
      "      # t3992 = prims.transpose(t3988, (1, 0))  # t3992: \"cuda:0 f32[768, 256]\"\n",
      "  t3989 = torch.matmul(t3988, t_transformer_h_9_mlp_c_proj_weight)  # t3989: \"cuda:0 f32[256, 3072]\"\n",
      "    # t3989 = ltorch.matmul(t3988, t_transformer_h_9_mlp_c_proj_weight)  # t3989: \"cuda:0 f32[256, 3072]\"\n",
      "      # t3989 = prims.matmul(t3988, t_transformer_h_9_mlp_c_proj_weight)  # t3989: \"cuda:0 f32[256, 3072]\"\n",
      "  del t3988, t_transformer_h_9_mlp_c_proj_weight\n",
      "  t3994 = torch.matmul(t3992, t3993)  # t3994: \"cuda:0 f32[768, 3072]\"\n",
      "    # t3994 = ltorch.matmul(t3992, t3993)  # t3994: \"cuda:0 f32[768, 3072]\"\n",
      "      # t3994 = prims.matmul(t3992, t3993)  # t3994: \"cuda:0 f32[768, 3072]\"\n",
      "  del t3992, t3993\n",
      "  t3990 = torch.reshape(t3989, (4, 64, 3072))  # t3990: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t3990 = ltorch.reshape(t3989, (4, 64, 3072))  # t3990: \"cuda:0 f32[4, 64, 3072]\"\n",
      "      # t3990 = prims.reshape(t3989, (4, 64, 3072))  # t3990: \"cuda:0 f32[4, 64, 3072]\"\n",
      "  del t3989\n",
      "  [t4011, t4019] = nvFusion12(f678, f680, f682, f684, t3990, t746)\n",
      "    # t747 = prims.mul(0.5, t746)  # t747: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t748 = prims.pow(t746, 3.0)  # t748: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t749 = prims.mul(0.044715, t748)  # t749: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t750 = prims.add(t746, t749)  # t750: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t751 = prims.mul(0.7978845608028654, t750)  # t751: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t752 = prims.tanh(t751)  # t752: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t753 = prims.add(1.0, t752)  # t753: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t3996 = prims.mul(t753, t3990)  # t3996: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t3997 = prims.mul(t747, t3990)  # t3997: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t3998 = prims.mul(t752, t752)  # t3998: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t3999 = prims.sub(1.0, t3998)  # t3999: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4000 = prims.mul(t3997, t3999)  # t4000: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4001 = prims.mul(f684, t4000)  # t4001: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4002 = prims.mul(f682, t4001)  # t4002: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4004 = prims.mul(t4002, f680)  # t4004: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4005 = prims.pow(t746, 2.0)  # t4005: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4006 = prims.mul(t4004, t4005)  # t4006: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4009 = prims.add(t4001, t4006)  # t4009: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4010 = prims.mul(f678, t3996)  # t4010: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4011 = prims.add(t4009, t4010)  # t4011: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4019 = prims.sum(t4011, (0, 1))  # t4019: \"cuda:0 f32[3072]\"\n",
      "  del f678, f680, f682, f684, t3990, t746\n",
      "  t4012 = torch.reshape(t4011, (-1, 3072))  # t4012: \"cuda:0 f32[256, 3072]\"\n",
      "    # t4012 = ltorch.reshape(t4011, (-1, 3072))  # t4012: \"cuda:0 f32[256, 3072]\"\n",
      "      # t4012 = prims.reshape(t4011, (256, 3072))  # t4012: \"cuda:0 f32[256, 3072]\"\n",
      "  del t4011\n",
      "  t4016 = torch.permute(t4012, (1, 0))  # t4016: \"cuda:0 f32[3072, 256]\"\n",
      "    # t4016 = ltorch.permute(t4012, (1, 0))  # t4016: \"cuda:0 f32[3072, 256]\"\n",
      "      # t4016 = prims.transpose(t4012, (1, 0))  # t4016: \"cuda:0 f32[3072, 256]\"\n",
      "  t4018 = torch.matmul(t4016, t4017)  # t4018: \"cuda:0 f32[3072, 768]\"\n",
      "    # t4018 = ltorch.matmul(t4016, t4017)  # t4018: \"cuda:0 f32[3072, 768]\"\n",
      "      # t4018 = prims.matmul(t4016, t4017)  # t4018: \"cuda:0 f32[3072, 768]\"\n",
      "  del t4016, t4017\n",
      "  t4013 = torch.matmul(t4012, t_transformer_h_9_mlp_c_fc_weight)  # t4013: \"cuda:0 f32[256, 768]\"\n",
      "    # t4013 = ltorch.matmul(t4012, t_transformer_h_9_mlp_c_fc_weight)  # t4013: \"cuda:0 f32[256, 768]\"\n",
      "      # t4013 = prims.matmul(t4012, t_transformer_h_9_mlp_c_fc_weight)  # t4013: \"cuda:0 f32[256, 768]\"\n",
      "  del t4012, t_transformer_h_9_mlp_c_fc_weight\n",
      "  t4014 = torch.reshape(t4013, (4, 64, 768))  # t4014: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4014 = ltorch.reshape(t4013, (4, 64, 768))  # t4014: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4014 = prims.reshape(t4013, (4, 64, 768))  # t4014: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4013\n",
      "  t5644 = torch.unsqueeze(t733, 2)  # t5644: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5644 = ltorch.unsqueeze(t733, 2)  # t5644: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t5644 = prims.broadcast_in_dim(t733, [4, 64, 1], [0, 1])  # t5644: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t733\n",
      "  t4041 = Tensor.expand(t5644, [4, 64, 1])  # t4041: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4041 = ltorch.expand(t5644, [4, 64, 1])  # t4041: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t4041 = prims.broadcast_in_dim(t5644, (4, 64, 1), (0, 1, 2))  # t4041: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t5644\n",
      "  t4042 = Tensor.expand(t4041, (4, 64, 768))  # t4042: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4042 = ltorch.expand(t4041, (4, 64, 768))  # t4042: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4042 = prims.broadcast_in_dim(t4041, (4, 64, 768), (0, 1, 2))  # t4042: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4041\n",
      "  [t4020, t4023, t4049, t4057] = nvFusion13(t3987, t4014, t4042, t729, t737, t739, t742)\n",
      "    # t740 = prims.broadcast_in_dim(t737, (4, 64, 768), (0, 1, 2))  # t740: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t741 = prims.mul(t739, t740)  # t741: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4020 = prims.sum(t4014, (0, 1))  # t4020: \"cuda:0 f32[768]\"\n",
      "    # t4021 = prims.mul(t742, t4014)  # t4021: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4022 = prims.mul(t741, t4014)  # t4022: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4023 = prims.sum(t4022, (0, 1))  # t4023: \"cuda:0 f32[768]\"\n",
      "    # t4024 = prims.mul(t740, t4021)  # t4024: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4025 = prims.mul(t739, t4021)  # t4025: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4026 = prims.sum(t4025, (2,))  # t4026: \"cuda:0 f32[4, 64]\"\n",
      "    # t4027 = prims.broadcast_in_dim(t4026, [4, 64, 1], [0, 1])  # t4027: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4028 = prims.neg(t4024)  # t4028: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4029 = prims.sum(t4028, (2,))  # t4029: \"cuda:0 f32[4, 64]\"\n",
      "    # t4030 = prims.broadcast_in_dim(t4029, [4, 64, 1], [0, 1])  # t4030: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4031 = prims.mul(-0.5, t4027)  # t4031: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4032 = prims.pow(t737, 3.0)  # t4032: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4033 = prims.mul(t4031, t4032)  # t4033: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4034 = prims.sum(t4030, (2,))  # t4034: \"cuda:0 f32[4, 64]\"\n",
      "    # t4035 = prims.sum(t4033, (2,))  # t4035: \"cuda:0 f32[4, 64]\"\n",
      "    # t4036 = prims.broadcast_in_dim(t4034, [4, 64, 1], [0, 1])  # t4036: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4037 = prims.broadcast_in_dim(t4036, (4, 64, 768), (0, 1, 2))  # t4037: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4038 = prims.mul(0.0013020833333333333, t4037)  # t4038: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4039 = prims.broadcast_in_dim(t4035, [4, 64, 1], [0, 1])  # t4039: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4040 = prims.broadcast_in_dim(t4039, (4, 64, 768), (0, 1, 2))  # t4040: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4043 = prims.mul(2.0, t4040)  # t4043: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4044 = prims.sub(t729, t4042)  # t4044: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4045 = prims.mul(t4043, t4044)  # t4045: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4046 = prims.div(t4045, 768.0)  # t4046: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4047 = prims.add(t4038, t4046)  # t4047: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4048 = prims.add(t4024, t4047)  # t4048: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4049 = prims.add(t3987, t4048)  # t4049: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4057 = prims.sum(t4049, (0, 1))  # t4057: \"cuda:0 f32[768]\"\n",
      "  del t3987, t4014, t4042, t729, t737, t739, t742\n",
      "  t4050 = torch.reshape(t4049, (-1, 768))  # t4050: \"cuda:0 f32[256, 768]\"\n",
      "    # t4050 = ltorch.reshape(t4049, (-1, 768))  # t4050: \"cuda:0 f32[256, 768]\"\n",
      "      # t4050 = prims.reshape(t4049, (256, 768))  # t4050: \"cuda:0 f32[256, 768]\"\n",
      "  t4054 = torch.permute(t4050, (1, 0))  # t4054: \"cuda:0 f32[768, 256]\"\n",
      "    # t4054 = ltorch.permute(t4050, (1, 0))  # t4054: \"cuda:0 f32[768, 256]\"\n",
      "      # t4054 = prims.transpose(t4050, (1, 0))  # t4054: \"cuda:0 f32[768, 256]\"\n",
      "  t4051 = torch.matmul(t4050, t_transformer_h_9_attn_c_proj_weight)  # t4051: \"cuda:0 f32[256, 768]\"\n",
      "    # t4051 = ltorch.matmul(t4050, t_transformer_h_9_attn_c_proj_weight)  # t4051: \"cuda:0 f32[256, 768]\"\n",
      "      # t4051 = prims.matmul(t4050, t_transformer_h_9_attn_c_proj_weight)  # t4051: \"cuda:0 f32[256, 768]\"\n",
      "  del t4050, t_transformer_h_9_attn_c_proj_weight\n",
      "  t4056 = torch.matmul(t4054, t4055)  # t4056: \"cuda:0 f32[768, 768]\"\n",
      "    # t4056 = ltorch.matmul(t4054, t4055)  # t4056: \"cuda:0 f32[768, 768]\"\n",
      "      # t4056 = prims.matmul(t4054, t4055)  # t4056: \"cuda:0 f32[768, 768]\"\n",
      "  del t4054, t4055\n",
      "  t4052 = torch.reshape(t4051, (4, 64, 768))  # t4052: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4052 = ltorch.reshape(t4051, (4, 64, 768))  # t4052: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4052 = prims.reshape(t4051, (4, 64, 768))  # t4052: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4051\n",
      "  t4058 = torch.reshape(t4052, (4, 64, 12, 64))  # t4058: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t4058 = ltorch.reshape(t4052, (4, 64, 12, 64))  # t4058: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t4058 = prims.reshape(t4052, (4, 64, 12, 64))  # t4058: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t4052\n",
      "  t4059 = torch.permute(t4058, (0, 2, 1, 3))  # t4059: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4059 = ltorch.permute(t4058, (0, 2, 1, 3))  # t4059: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4059 = prims.transpose(t4058, (0, 2, 1, 3))  # t4059: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4058\n",
      "  t4061 = torch.matmul(t4059, t4060)  # t4061: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4061 = ltorch.matmul(t4059, t4060)  # t4061: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4061 = prims.matmul(t4059, t4060)  # t4061: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4060\n",
      "  t4063 = torch.matmul(t4062, t4059)  # t4063: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4063 = ltorch.matmul(t4062, t4059)  # t4063: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4063 = prims.matmul(t4062, t4059)  # t4063: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4062, t4059\n",
      "  [t4071] = nvFusion14(f658, i662, t4061, t713, t723)\n",
      "    # t4064 = prims.mul(t723, t4061)  # t4064: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4065 = prims.sum(t4064, (3,))  # t4065: \"cuda:0 f32[4, 12, 64]\"\n",
      "    # t4066 = prims.broadcast_in_dim(t4065, [4, 12, 64, 1], [0, 1, 2])  # t4066: \"cuda:0 f32[4, 12, 64, 1]\"\n",
      "    # t4067 = prims.broadcast_in_dim(t4066, (4, 12, 64, 64), (0, 1, 2, 3))  # t4067: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4068 = prims.sub(t4061, t4067)  # t4068: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4069 = prims.mul(t723, t4068)  # t4069: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4070 = prims.where(t713, 0.0, t4069)  # t4070: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4071 = prims.mul(f658, t4070)  # t4071: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del f658, i662, t4061, t713, t723\n",
      "  t4075 = torch.matmul(t4074, t4071)  # t4075: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4075 = ltorch.matmul(t4074, t4071)  # t4075: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4075 = prims.matmul(t4074, t4071)  # t4075: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4074\n",
      "  t4073 = torch.matmul(t4071, t4072)  # t4073: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4073 = ltorch.matmul(t4071, t4072)  # t4073: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4073 = prims.matmul(t4071, t4072)  # t4073: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4071, t4072\n",
      "  t4076 = torch.permute(t4075, (0, 1, 3, 2))  # t4076: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4076 = ltorch.permute(t4075, (0, 1, 3, 2))  # t4076: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4076 = prims.transpose(t4075, (0, 1, 3, 2))  # t4076: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4075\n",
      "  t4077 = torch.permute(t4063, (0, 2, 1, 3))  # t4077: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t4077 = ltorch.permute(t4063, (0, 2, 1, 3))  # t4077: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t4077 = prims.transpose(t4063, (0, 2, 1, 3))  # t4077: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t4063\n",
      "  t4078 = torch.reshape(t4077, (4, 64, 768))  # t4078: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4078 = ltorch.reshape(t4077, (4, 64, 768))  # t4078: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4078 = prims.reshape(t4077, (4, 64, 768))  # t4078: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4077\n",
      "  t4079 = torch.permute(t4076, (0, 2, 1, 3))  # t4079: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t4079 = ltorch.permute(t4076, (0, 2, 1, 3))  # t4079: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t4079 = prims.transpose(t4076, (0, 2, 1, 3))  # t4079: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t4076\n",
      "  t4080 = torch.reshape(t4079, (4, 64, 768))  # t4080: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4080 = ltorch.reshape(t4079, (4, 64, 768))  # t4080: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4080 = prims.reshape(t4079, (4, 64, 768))  # t4080: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4079\n",
      "  t4081 = torch.permute(t4073, (0, 2, 1, 3))  # t4081: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t4081 = ltorch.permute(t4073, (0, 2, 1, 3))  # t4081: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t4081 = prims.transpose(t4073, (0, 2, 1, 3))  # t4081: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t4073\n",
      "  t4082 = torch.reshape(t4081, (4, 64, 768))  # t4082: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4082 = ltorch.reshape(t4081, (4, 64, 768))  # t4082: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4082 = prims.reshape(t4081, (4, 64, 768))  # t4082: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4081\n",
      "  [t4083, t4091] = nvFusion15(i624, t4078, t4080, t4082)\n",
      "    # t4083 = prims.cat((t4082, t4080, t4078), i624)  # t4083: \"cuda:0 f32[4, 64, 2304]\"\n",
      "    # t4091 = prims.sum(t4083, (0, 1))  # t4091: \"cuda:0 f32[2304]\"\n",
      "  del i624, t4078, t4080, t4082\n",
      "  t4084 = torch.reshape(t4083, (-1, 2304))  # t4084: \"cuda:0 f32[256, 2304]\"\n",
      "    # t4084 = ltorch.reshape(t4083, (-1, 2304))  # t4084: \"cuda:0 f32[256, 2304]\"\n",
      "      # t4084 = prims.reshape(t4083, (256, 2304))  # t4084: \"cuda:0 f32[256, 2304]\"\n",
      "  del t4083\n",
      "  t4088 = torch.permute(t4084, (1, 0))  # t4088: \"cuda:0 f32[2304, 256]\"\n",
      "    # t4088 = ltorch.permute(t4084, (1, 0))  # t4088: \"cuda:0 f32[2304, 256]\"\n",
      "      # t4088 = prims.transpose(t4084, (1, 0))  # t4088: \"cuda:0 f32[2304, 256]\"\n",
      "  t4090 = torch.matmul(t4088, t4089)  # t4090: \"cuda:0 f32[2304, 768]\"\n",
      "    # t4090 = ltorch.matmul(t4088, t4089)  # t4090: \"cuda:0 f32[2304, 768]\"\n",
      "      # t4090 = prims.matmul(t4088, t4089)  # t4090: \"cuda:0 f32[2304, 768]\"\n",
      "  del t4088, t4089\n",
      "  t4085 = torch.matmul(t4084, t_transformer_h_9_attn_c_attn_weight)  # t4085: \"cuda:0 f32[256, 768]\"\n",
      "    # t4085 = ltorch.matmul(t4084, t_transformer_h_9_attn_c_attn_weight)  # t4085: \"cuda:0 f32[256, 768]\"\n",
      "      # t4085 = prims.matmul(t4084, t_transformer_h_9_attn_c_attn_weight)  # t4085: \"cuda:0 f32[256, 768]\"\n",
      "  del t4084, t_transformer_h_9_attn_c_attn_weight\n",
      "  t4086 = torch.reshape(t4085, (4, 64, 768))  # t4086: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4086 = ltorch.reshape(t4085, (4, 64, 768))  # t4086: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4086 = prims.reshape(t4085, (4, 64, 768))  # t4086: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4085\n",
      "  t5662 = torch.unsqueeze(t685, 2)  # t5662: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5662 = ltorch.unsqueeze(t685, 2)  # t5662: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t5662 = prims.broadcast_in_dim(t685, [4, 64, 1], [0, 1])  # t5662: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t685\n",
      "  t4113 = Tensor.expand(t5662, [4, 64, 1])  # t4113: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4113 = ltorch.expand(t5662, [4, 64, 1])  # t4113: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t4113 = prims.broadcast_in_dim(t5662, (4, 64, 1), (0, 1, 2))  # t4113: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t5662\n",
      "  t4114 = Tensor.expand(t4113, (4, 64, 768))  # t4114: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4114 = ltorch.expand(t4113, (4, 64, 768))  # t4114: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4114 = prims.broadcast_in_dim(t4113, (4, 64, 768), (0, 1, 2))  # t4114: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4113\n",
      "  [t4092, t4095, t4121, t4129] = nvFusion16(t4049, t4086, t4114, t681, t689, t691, t694)\n",
      "    # t692 = prims.broadcast_in_dim(t689, (4, 64, 768), (0, 1, 2))  # t692: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t693 = prims.mul(t691, t692)  # t693: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4092 = prims.sum(t4086, (0, 1))  # t4092: \"cuda:0 f32[768]\"\n",
      "    # t4093 = prims.mul(t694, t4086)  # t4093: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4094 = prims.mul(t693, t4086)  # t4094: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4095 = prims.sum(t4094, (0, 1))  # t4095: \"cuda:0 f32[768]\"\n",
      "    # t4096 = prims.mul(t692, t4093)  # t4096: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4097 = prims.mul(t691, t4093)  # t4097: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4098 = prims.sum(t4097, (2,))  # t4098: \"cuda:0 f32[4, 64]\"\n",
      "    # t4099 = prims.broadcast_in_dim(t4098, [4, 64, 1], [0, 1])  # t4099: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4100 = prims.neg(t4096)  # t4100: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4101 = prims.sum(t4100, (2,))  # t4101: \"cuda:0 f32[4, 64]\"\n",
      "    # t4102 = prims.broadcast_in_dim(t4101, [4, 64, 1], [0, 1])  # t4102: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4103 = prims.mul(-0.5, t4099)  # t4103: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4104 = prims.pow(t689, 3.0)  # t4104: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4105 = prims.mul(t4103, t4104)  # t4105: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4106 = prims.sum(t4102, (2,))  # t4106: \"cuda:0 f32[4, 64]\"\n",
      "    # t4107 = prims.sum(t4105, (2,))  # t4107: \"cuda:0 f32[4, 64]\"\n",
      "    # t4108 = prims.broadcast_in_dim(t4106, [4, 64, 1], [0, 1])  # t4108: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4109 = prims.broadcast_in_dim(t4108, (4, 64, 768), (0, 1, 2))  # t4109: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4110 = prims.mul(0.0013020833333333333, t4109)  # t4110: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4111 = prims.broadcast_in_dim(t4107, [4, 64, 1], [0, 1])  # t4111: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4112 = prims.broadcast_in_dim(t4111, (4, 64, 768), (0, 1, 2))  # t4112: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4115 = prims.mul(2.0, t4112)  # t4115: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4116 = prims.sub(t681, t4114)  # t4116: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4117 = prims.mul(t4115, t4116)  # t4117: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4118 = prims.div(t4117, 768.0)  # t4118: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4119 = prims.add(t4110, t4118)  # t4119: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4120 = prims.add(t4096, t4119)  # t4120: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4121 = prims.add(t4049, t4120)  # t4121: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4129 = prims.sum(t4121, (0, 1))  # t4129: \"cuda:0 f32[768]\"\n",
      "  del t4049, t4086, t4114, t681, t689, t691, t694\n",
      "  t4122 = torch.reshape(t4121, (-1, 768))  # t4122: \"cuda:0 f32[256, 768]\"\n",
      "    # t4122 = ltorch.reshape(t4121, (-1, 768))  # t4122: \"cuda:0 f32[256, 768]\"\n",
      "      # t4122 = prims.reshape(t4121, (256, 768))  # t4122: \"cuda:0 f32[256, 768]\"\n",
      "  t4126 = torch.permute(t4122, (1, 0))  # t4126: \"cuda:0 f32[768, 256]\"\n",
      "    # t4126 = ltorch.permute(t4122, (1, 0))  # t4126: \"cuda:0 f32[768, 256]\"\n",
      "      # t4126 = prims.transpose(t4122, (1, 0))  # t4126: \"cuda:0 f32[768, 256]\"\n",
      "  t4123 = torch.matmul(t4122, t_transformer_h_8_mlp_c_proj_weight)  # t4123: \"cuda:0 f32[256, 3072]\"\n",
      "    # t4123 = ltorch.matmul(t4122, t_transformer_h_8_mlp_c_proj_weight)  # t4123: \"cuda:0 f32[256, 3072]\"\n",
      "      # t4123 = prims.matmul(t4122, t_transformer_h_8_mlp_c_proj_weight)  # t4123: \"cuda:0 f32[256, 3072]\"\n",
      "  del t4122, t_transformer_h_8_mlp_c_proj_weight\n",
      "  t4128 = torch.matmul(t4126, t4127)  # t4128: \"cuda:0 f32[768, 3072]\"\n",
      "    # t4128 = ltorch.matmul(t4126, t4127)  # t4128: \"cuda:0 f32[768, 3072]\"\n",
      "      # t4128 = prims.matmul(t4126, t4127)  # t4128: \"cuda:0 f32[768, 3072]\"\n",
      "  del t4126, t4127\n",
      "  t4124 = torch.reshape(t4123, (4, 64, 3072))  # t4124: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4124 = ltorch.reshape(t4123, (4, 64, 3072))  # t4124: \"cuda:0 f32[4, 64, 3072]\"\n",
      "      # t4124 = prims.reshape(t4123, (4, 64, 3072))  # t4124: \"cuda:0 f32[4, 64, 3072]\"\n",
      "  del t4123\n",
      "  [t4145, t4153] = nvFusion17(f610, f612, f614, f616, t4124, t671)\n",
      "    # t672 = prims.mul(0.5, t671)  # t672: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t673 = prims.pow(t671, 3.0)  # t673: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t674 = prims.mul(0.044715, t673)  # t674: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t675 = prims.add(t671, t674)  # t675: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t676 = prims.mul(0.7978845608028654, t675)  # t676: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t677 = prims.tanh(t676)  # t677: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t678 = prims.add(1.0, t677)  # t678: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4130 = prims.mul(t678, t4124)  # t4130: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4131 = prims.mul(t672, t4124)  # t4131: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4132 = prims.mul(t677, t677)  # t4132: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4133 = prims.sub(1.0, t4132)  # t4133: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4134 = prims.mul(t4131, t4133)  # t4134: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4135 = prims.mul(f616, t4134)  # t4135: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4136 = prims.mul(f614, t4135)  # t4136: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4138 = prims.mul(t4136, f612)  # t4138: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4139 = prims.pow(t671, 2.0)  # t4139: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4140 = prims.mul(t4138, t4139)  # t4140: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4143 = prims.add(t4135, t4140)  # t4143: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4144 = prims.mul(f610, t4130)  # t4144: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4145 = prims.add(t4143, t4144)  # t4145: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4153 = prims.sum(t4145, (0, 1))  # t4153: \"cuda:0 f32[3072]\"\n",
      "  del f610, f612, f614, f616, t4124, t671\n",
      "  t4146 = torch.reshape(t4145, (-1, 3072))  # t4146: \"cuda:0 f32[256, 3072]\"\n",
      "    # t4146 = ltorch.reshape(t4145, (-1, 3072))  # t4146: \"cuda:0 f32[256, 3072]\"\n",
      "      # t4146 = prims.reshape(t4145, (256, 3072))  # t4146: \"cuda:0 f32[256, 3072]\"\n",
      "  del t4145\n",
      "  t4150 = torch.permute(t4146, (1, 0))  # t4150: \"cuda:0 f32[3072, 256]\"\n",
      "    # t4150 = ltorch.permute(t4146, (1, 0))  # t4150: \"cuda:0 f32[3072, 256]\"\n",
      "      # t4150 = prims.transpose(t4146, (1, 0))  # t4150: \"cuda:0 f32[3072, 256]\"\n",
      "  t4152 = torch.matmul(t4150, t4151)  # t4152: \"cuda:0 f32[3072, 768]\"\n",
      "    # t4152 = ltorch.matmul(t4150, t4151)  # t4152: \"cuda:0 f32[3072, 768]\"\n",
      "      # t4152 = prims.matmul(t4150, t4151)  # t4152: \"cuda:0 f32[3072, 768]\"\n",
      "  del t4150, t4151\n",
      "  t4147 = torch.matmul(t4146, t_transformer_h_8_mlp_c_fc_weight)  # t4147: \"cuda:0 f32[256, 768]\"\n",
      "    # t4147 = ltorch.matmul(t4146, t_transformer_h_8_mlp_c_fc_weight)  # t4147: \"cuda:0 f32[256, 768]\"\n",
      "      # t4147 = prims.matmul(t4146, t_transformer_h_8_mlp_c_fc_weight)  # t4147: \"cuda:0 f32[256, 768]\"\n",
      "  del t4146, t_transformer_h_8_mlp_c_fc_weight\n",
      "  t4148 = torch.reshape(t4147, (4, 64, 768))  # t4148: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4148 = ltorch.reshape(t4147, (4, 64, 768))  # t4148: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4148 = prims.reshape(t4147, (4, 64, 768))  # t4148: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4147\n",
      "  t5671 = torch.unsqueeze(t658, 2)  # t5671: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5671 = ltorch.unsqueeze(t658, 2)  # t5671: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t5671 = prims.broadcast_in_dim(t658, [4, 64, 1], [0, 1])  # t5671: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t658\n",
      "  t4175 = Tensor.expand(t5671, [4, 64, 1])  # t4175: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4175 = ltorch.expand(t5671, [4, 64, 1])  # t4175: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t4175 = prims.broadcast_in_dim(t5671, (4, 64, 1), (0, 1, 2))  # t4175: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t5671\n",
      "  t4176 = Tensor.expand(t4175, (4, 64, 768))  # t4176: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4176 = ltorch.expand(t4175, (4, 64, 768))  # t4176: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4176 = prims.broadcast_in_dim(t4175, (4, 64, 768), (0, 1, 2))  # t4176: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4175\n",
      "  [t4154, t4157, t4183, t4191] = nvFusion18(t4121, t4148, t4176, t654, t662, t664, t667)\n",
      "    # t665 = prims.broadcast_in_dim(t662, (4, 64, 768), (0, 1, 2))  # t665: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t666 = prims.mul(t664, t665)  # t666: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4154 = prims.sum(t4148, (0, 1))  # t4154: \"cuda:0 f32[768]\"\n",
      "    # t4155 = prims.mul(t667, t4148)  # t4155: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4156 = prims.mul(t666, t4148)  # t4156: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4157 = prims.sum(t4156, (0, 1))  # t4157: \"cuda:0 f32[768]\"\n",
      "    # t4158 = prims.mul(t665, t4155)  # t4158: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4159 = prims.mul(t664, t4155)  # t4159: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4160 = prims.sum(t4159, (2,))  # t4160: \"cuda:0 f32[4, 64]\"\n",
      "    # t4161 = prims.broadcast_in_dim(t4160, [4, 64, 1], [0, 1])  # t4161: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4162 = prims.neg(t4158)  # t4162: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4163 = prims.sum(t4162, (2,))  # t4163: \"cuda:0 f32[4, 64]\"\n",
      "    # t4164 = prims.broadcast_in_dim(t4163, [4, 64, 1], [0, 1])  # t4164: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4165 = prims.mul(-0.5, t4161)  # t4165: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4166 = prims.pow(t662, 3.0)  # t4166: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4167 = prims.mul(t4165, t4166)  # t4167: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4168 = prims.sum(t4164, (2,))  # t4168: \"cuda:0 f32[4, 64]\"\n",
      "    # t4169 = prims.sum(t4167, (2,))  # t4169: \"cuda:0 f32[4, 64]\"\n",
      "    # t4170 = prims.broadcast_in_dim(t4168, [4, 64, 1], [0, 1])  # t4170: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4171 = prims.broadcast_in_dim(t4170, (4, 64, 768), (0, 1, 2))  # t4171: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4172 = prims.mul(0.0013020833333333333, t4171)  # t4172: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4173 = prims.broadcast_in_dim(t4169, [4, 64, 1], [0, 1])  # t4173: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4174 = prims.broadcast_in_dim(t4173, (4, 64, 768), (0, 1, 2))  # t4174: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4177 = prims.mul(2.0, t4174)  # t4177: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4178 = prims.sub(t654, t4176)  # t4178: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4179 = prims.mul(t4177, t4178)  # t4179: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4180 = prims.div(t4179, 768.0)  # t4180: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4181 = prims.add(t4172, t4180)  # t4181: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4182 = prims.add(t4158, t4181)  # t4182: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4183 = prims.add(t4121, t4182)  # t4183: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4191 = prims.sum(t4183, (0, 1))  # t4191: \"cuda:0 f32[768]\"\n",
      "  del t4121, t4148, t4176, t654, t662, t664, t667\n",
      "  t4184 = torch.reshape(t4183, (-1, 768))  # t4184: \"cuda:0 f32[256, 768]\"\n",
      "    # t4184 = ltorch.reshape(t4183, (-1, 768))  # t4184: \"cuda:0 f32[256, 768]\"\n",
      "      # t4184 = prims.reshape(t4183, (256, 768))  # t4184: \"cuda:0 f32[256, 768]\"\n",
      "  t4188 = torch.permute(t4184, (1, 0))  # t4188: \"cuda:0 f32[768, 256]\"\n",
      "    # t4188 = ltorch.permute(t4184, (1, 0))  # t4188: \"cuda:0 f32[768, 256]\"\n",
      "      # t4188 = prims.transpose(t4184, (1, 0))  # t4188: \"cuda:0 f32[768, 256]\"\n",
      "  t4185 = torch.matmul(t4184, t_transformer_h_8_attn_c_proj_weight)  # t4185: \"cuda:0 f32[256, 768]\"\n",
      "    # t4185 = ltorch.matmul(t4184, t_transformer_h_8_attn_c_proj_weight)  # t4185: \"cuda:0 f32[256, 768]\"\n",
      "      # t4185 = prims.matmul(t4184, t_transformer_h_8_attn_c_proj_weight)  # t4185: \"cuda:0 f32[256, 768]\"\n",
      "  del t4184, t_transformer_h_8_attn_c_proj_weight\n",
      "  t4190 = torch.matmul(t4188, t4189)  # t4190: \"cuda:0 f32[768, 768]\"\n",
      "    # t4190 = ltorch.matmul(t4188, t4189)  # t4190: \"cuda:0 f32[768, 768]\"\n",
      "      # t4190 = prims.matmul(t4188, t4189)  # t4190: \"cuda:0 f32[768, 768]\"\n",
      "  del t4188, t4189\n",
      "  t4186 = torch.reshape(t4185, (4, 64, 768))  # t4186: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4186 = ltorch.reshape(t4185, (4, 64, 768))  # t4186: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4186 = prims.reshape(t4185, (4, 64, 768))  # t4186: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4185\n",
      "  t4192 = torch.reshape(t4186, (4, 64, 12, 64))  # t4192: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t4192 = ltorch.reshape(t4186, (4, 64, 12, 64))  # t4192: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t4192 = prims.reshape(t4186, (4, 64, 12, 64))  # t4192: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t4186\n",
      "  t4193 = torch.permute(t4192, (0, 2, 1, 3))  # t4193: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4193 = ltorch.permute(t4192, (0, 2, 1, 3))  # t4193: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4193 = prims.transpose(t4192, (0, 2, 1, 3))  # t4193: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4192\n",
      "  t4197 = torch.matmul(t4196, t4193)  # t4197: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4197 = ltorch.matmul(t4196, t4193)  # t4197: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4197 = prims.matmul(t4196, t4193)  # t4197: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4196\n",
      "  t4195 = torch.matmul(t4193, t4194)  # t4195: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4195 = ltorch.matmul(t4193, t4194)  # t4195: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4195 = prims.matmul(t4193, t4194)  # t4195: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4193, t4194\n",
      "  [t4205] = nvFusion19(f590, i594, t4195, t638, t648)\n",
      "    # t4198 = prims.mul(t648, t4195)  # t4198: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4199 = prims.sum(t4198, (3,))  # t4199: \"cuda:0 f32[4, 12, 64]\"\n",
      "    # t4200 = prims.broadcast_in_dim(t4199, [4, 12, 64, 1], [0, 1, 2])  # t4200: \"cuda:0 f32[4, 12, 64, 1]\"\n",
      "    # t4201 = prims.broadcast_in_dim(t4200, (4, 12, 64, 64), (0, 1, 2, 3))  # t4201: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4202 = prims.sub(t4195, t4201)  # t4202: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4203 = prims.mul(t648, t4202)  # t4203: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4204 = prims.where(t638, 0.0, t4203)  # t4204: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4205 = prims.mul(f590, t4204)  # t4205: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del f590, i594, t4195, t638, t648\n",
      "  t4207 = torch.matmul(t4205, t4206)  # t4207: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4207 = ltorch.matmul(t4205, t4206)  # t4207: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4207 = prims.matmul(t4205, t4206)  # t4207: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4206\n",
      "  t4209 = torch.matmul(t4208, t4205)  # t4209: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4209 = ltorch.matmul(t4208, t4205)  # t4209: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4209 = prims.matmul(t4208, t4205)  # t4209: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4208, t4205\n",
      "  t4210 = torch.permute(t4209, (0, 1, 3, 2))  # t4210: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4210 = ltorch.permute(t4209, (0, 1, 3, 2))  # t4210: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4210 = prims.transpose(t4209, (0, 1, 3, 2))  # t4210: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4209\n",
      "  t4211 = torch.permute(t4197, (0, 2, 1, 3))  # t4211: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t4211 = ltorch.permute(t4197, (0, 2, 1, 3))  # t4211: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t4211 = prims.transpose(t4197, (0, 2, 1, 3))  # t4211: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t4197\n",
      "  t4212 = torch.reshape(t4211, (4, 64, 768))  # t4212: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4212 = ltorch.reshape(t4211, (4, 64, 768))  # t4212: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4212 = prims.reshape(t4211, (4, 64, 768))  # t4212: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4211\n",
      "  t4213 = torch.permute(t4210, (0, 2, 1, 3))  # t4213: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t4213 = ltorch.permute(t4210, (0, 2, 1, 3))  # t4213: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t4213 = prims.transpose(t4210, (0, 2, 1, 3))  # t4213: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t4210\n",
      "  t4214 = torch.reshape(t4213, (4, 64, 768))  # t4214: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4214 = ltorch.reshape(t4213, (4, 64, 768))  # t4214: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4214 = prims.reshape(t4213, (4, 64, 768))  # t4214: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4213\n",
      "  t4215 = torch.permute(t4207, (0, 2, 1, 3))  # t4215: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t4215 = ltorch.permute(t4207, (0, 2, 1, 3))  # t4215: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t4215 = prims.transpose(t4207, (0, 2, 1, 3))  # t4215: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t4207\n",
      "  t4216 = torch.reshape(t4215, (4, 64, 768))  # t4216: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4216 = ltorch.reshape(t4215, (4, 64, 768))  # t4216: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4216 = prims.reshape(t4215, (4, 64, 768))  # t4216: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4215\n",
      "  [t4217, t4225] = nvFusion20(i556, t4212, t4214, t4216)\n",
      "    # t4217 = prims.cat((t4216, t4214, t4212), i556)  # t4217: \"cuda:0 f32[4, 64, 2304]\"\n",
      "    # t4225 = prims.sum(t4217, (0, 1))  # t4225: \"cuda:0 f32[2304]\"\n",
      "  del i556, t4212, t4214, t4216\n",
      "  t4218 = torch.reshape(t4217, (-1, 2304))  # t4218: \"cuda:0 f32[256, 2304]\"\n",
      "    # t4218 = ltorch.reshape(t4217, (-1, 2304))  # t4218: \"cuda:0 f32[256, 2304]\"\n",
      "      # t4218 = prims.reshape(t4217, (256, 2304))  # t4218: \"cuda:0 f32[256, 2304]\"\n",
      "  del t4217\n",
      "  t4222 = torch.permute(t4218, (1, 0))  # t4222: \"cuda:0 f32[2304, 256]\"\n",
      "    # t4222 = ltorch.permute(t4218, (1, 0))  # t4222: \"cuda:0 f32[2304, 256]\"\n",
      "      # t4222 = prims.transpose(t4218, (1, 0))  # t4222: \"cuda:0 f32[2304, 256]\"\n",
      "  t4224 = torch.matmul(t4222, t4223)  # t4224: \"cuda:0 f32[2304, 768]\"\n",
      "    # t4224 = ltorch.matmul(t4222, t4223)  # t4224: \"cuda:0 f32[2304, 768]\"\n",
      "      # t4224 = prims.matmul(t4222, t4223)  # t4224: \"cuda:0 f32[2304, 768]\"\n",
      "  del t4222, t4223\n",
      "  t4219 = torch.matmul(t4218, t_transformer_h_8_attn_c_attn_weight)  # t4219: \"cuda:0 f32[256, 768]\"\n",
      "    # t4219 = ltorch.matmul(t4218, t_transformer_h_8_attn_c_attn_weight)  # t4219: \"cuda:0 f32[256, 768]\"\n",
      "      # t4219 = prims.matmul(t4218, t_transformer_h_8_attn_c_attn_weight)  # t4219: \"cuda:0 f32[256, 768]\"\n",
      "  del t4218, t_transformer_h_8_attn_c_attn_weight\n",
      "  t4220 = torch.reshape(t4219, (4, 64, 768))  # t4220: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4220 = ltorch.reshape(t4219, (4, 64, 768))  # t4220: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4220 = prims.reshape(t4219, (4, 64, 768))  # t4220: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4219\n",
      "  t5689 = torch.unsqueeze(t610, 2)  # t5689: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5689 = ltorch.unsqueeze(t610, 2)  # t5689: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t5689 = prims.broadcast_in_dim(t610, [4, 64, 1], [0, 1])  # t5689: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t610\n",
      "  t4247 = Tensor.expand(t5689, [4, 64, 1])  # t4247: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4247 = ltorch.expand(t5689, [4, 64, 1])  # t4247: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t4247 = prims.broadcast_in_dim(t5689, (4, 64, 1), (0, 1, 2))  # t4247: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t5689\n",
      "  t4248 = Tensor.expand(t4247, (4, 64, 768))  # t4248: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4248 = ltorch.expand(t4247, (4, 64, 768))  # t4248: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4248 = prims.broadcast_in_dim(t4247, (4, 64, 768), (0, 1, 2))  # t4248: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4247\n",
      "  [t4226, t4229, t4255, t4263] = nvFusion21(t4183, t4220, t4248, t606, t614, t616, t619)\n",
      "    # t617 = prims.broadcast_in_dim(t614, (4, 64, 768), (0, 1, 2))  # t617: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t618 = prims.mul(t616, t617)  # t618: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4226 = prims.sum(t4220, (0, 1))  # t4226: \"cuda:0 f32[768]\"\n",
      "    # t4227 = prims.mul(t619, t4220)  # t4227: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4228 = prims.mul(t618, t4220)  # t4228: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4229 = prims.sum(t4228, (0, 1))  # t4229: \"cuda:0 f32[768]\"\n",
      "    # t4230 = prims.mul(t617, t4227)  # t4230: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4231 = prims.mul(t616, t4227)  # t4231: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4232 = prims.sum(t4231, (2,))  # t4232: \"cuda:0 f32[4, 64]\"\n",
      "    # t4233 = prims.broadcast_in_dim(t4232, [4, 64, 1], [0, 1])  # t4233: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4234 = prims.neg(t4230)  # t4234: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4235 = prims.sum(t4234, (2,))  # t4235: \"cuda:0 f32[4, 64]\"\n",
      "    # t4236 = prims.broadcast_in_dim(t4235, [4, 64, 1], [0, 1])  # t4236: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4237 = prims.mul(-0.5, t4233)  # t4237: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4238 = prims.pow(t614, 3.0)  # t4238: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4239 = prims.mul(t4237, t4238)  # t4239: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4240 = prims.sum(t4236, (2,))  # t4240: \"cuda:0 f32[4, 64]\"\n",
      "    # t4241 = prims.sum(t4239, (2,))  # t4241: \"cuda:0 f32[4, 64]\"\n",
      "    # t4242 = prims.broadcast_in_dim(t4240, [4, 64, 1], [0, 1])  # t4242: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4243 = prims.broadcast_in_dim(t4242, (4, 64, 768), (0, 1, 2))  # t4243: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4244 = prims.mul(0.0013020833333333333, t4243)  # t4244: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4245 = prims.broadcast_in_dim(t4241, [4, 64, 1], [0, 1])  # t4245: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4246 = prims.broadcast_in_dim(t4245, (4, 64, 768), (0, 1, 2))  # t4246: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4249 = prims.mul(2.0, t4246)  # t4249: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4250 = prims.sub(t606, t4248)  # t4250: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4251 = prims.mul(t4249, t4250)  # t4251: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4252 = prims.div(t4251, 768.0)  # t4252: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4253 = prims.add(t4244, t4252)  # t4253: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4254 = prims.add(t4230, t4253)  # t4254: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4255 = prims.add(t4183, t4254)  # t4255: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4263 = prims.sum(t4255, (0, 1))  # t4263: \"cuda:0 f32[768]\"\n",
      "  del t4183, t4220, t4248, t606, t614, t616, t619\n",
      "  t4256 = torch.reshape(t4255, (-1, 768))  # t4256: \"cuda:0 f32[256, 768]\"\n",
      "    # t4256 = ltorch.reshape(t4255, (-1, 768))  # t4256: \"cuda:0 f32[256, 768]\"\n",
      "      # t4256 = prims.reshape(t4255, (256, 768))  # t4256: \"cuda:0 f32[256, 768]\"\n",
      "  t4260 = torch.permute(t4256, (1, 0))  # t4260: \"cuda:0 f32[768, 256]\"\n",
      "    # t4260 = ltorch.permute(t4256, (1, 0))  # t4260: \"cuda:0 f32[768, 256]\"\n",
      "      # t4260 = prims.transpose(t4256, (1, 0))  # t4260: \"cuda:0 f32[768, 256]\"\n",
      "  t4257 = torch.matmul(t4256, t_transformer_h_7_mlp_c_proj_weight)  # t4257: \"cuda:0 f32[256, 3072]\"\n",
      "    # t4257 = ltorch.matmul(t4256, t_transformer_h_7_mlp_c_proj_weight)  # t4257: \"cuda:0 f32[256, 3072]\"\n",
      "      # t4257 = prims.matmul(t4256, t_transformer_h_7_mlp_c_proj_weight)  # t4257: \"cuda:0 f32[256, 3072]\"\n",
      "  del t4256, t_transformer_h_7_mlp_c_proj_weight\n",
      "  t4262 = torch.matmul(t4260, t4261)  # t4262: \"cuda:0 f32[768, 3072]\"\n",
      "    # t4262 = ltorch.matmul(t4260, t4261)  # t4262: \"cuda:0 f32[768, 3072]\"\n",
      "      # t4262 = prims.matmul(t4260, t4261)  # t4262: \"cuda:0 f32[768, 3072]\"\n",
      "  del t4260, t4261\n",
      "  t4258 = torch.reshape(t4257, (4, 64, 3072))  # t4258: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4258 = ltorch.reshape(t4257, (4, 64, 3072))  # t4258: \"cuda:0 f32[4, 64, 3072]\"\n",
      "      # t4258 = prims.reshape(t4257, (4, 64, 3072))  # t4258: \"cuda:0 f32[4, 64, 3072]\"\n",
      "  del t4257\n",
      "  [t4279, t4287] = nvFusion22(f542, f544, f546, f548, t4258, t596)\n",
      "    # t597 = prims.mul(0.5, t596)  # t597: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t598 = prims.pow(t596, 3.0)  # t598: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t599 = prims.mul(0.044715, t598)  # t599: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t600 = prims.add(t596, t599)  # t600: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t601 = prims.mul(0.7978845608028654, t600)  # t601: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t602 = prims.tanh(t601)  # t602: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t603 = prims.add(1.0, t602)  # t603: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4264 = prims.mul(t603, t4258)  # t4264: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4265 = prims.mul(t597, t4258)  # t4265: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4266 = prims.mul(t602, t602)  # t4266: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4267 = prims.sub(1.0, t4266)  # t4267: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4268 = prims.mul(t4265, t4267)  # t4268: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4269 = prims.mul(f548, t4268)  # t4269: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4270 = prims.mul(f546, t4269)  # t4270: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4272 = prims.mul(t4270, f544)  # t4272: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4273 = prims.pow(t596, 2.0)  # t4273: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4274 = prims.mul(t4272, t4273)  # t4274: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4277 = prims.add(t4269, t4274)  # t4277: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4278 = prims.mul(f542, t4264)  # t4278: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4279 = prims.add(t4277, t4278)  # t4279: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4287 = prims.sum(t4279, (0, 1))  # t4287: \"cuda:0 f32[3072]\"\n",
      "  del f542, f544, f546, f548, t4258, t596\n",
      "  t4280 = torch.reshape(t4279, (-1, 3072))  # t4280: \"cuda:0 f32[256, 3072]\"\n",
      "    # t4280 = ltorch.reshape(t4279, (-1, 3072))  # t4280: \"cuda:0 f32[256, 3072]\"\n",
      "      # t4280 = prims.reshape(t4279, (256, 3072))  # t4280: \"cuda:0 f32[256, 3072]\"\n",
      "  del t4279\n",
      "  t4284 = torch.permute(t4280, (1, 0))  # t4284: \"cuda:0 f32[3072, 256]\"\n",
      "    # t4284 = ltorch.permute(t4280, (1, 0))  # t4284: \"cuda:0 f32[3072, 256]\"\n",
      "      # t4284 = prims.transpose(t4280, (1, 0))  # t4284: \"cuda:0 f32[3072, 256]\"\n",
      "  t4281 = torch.matmul(t4280, t_transformer_h_7_mlp_c_fc_weight)  # t4281: \"cuda:0 f32[256, 768]\"\n",
      "    # t4281 = ltorch.matmul(t4280, t_transformer_h_7_mlp_c_fc_weight)  # t4281: \"cuda:0 f32[256, 768]\"\n",
      "      # t4281 = prims.matmul(t4280, t_transformer_h_7_mlp_c_fc_weight)  # t4281: \"cuda:0 f32[256, 768]\"\n",
      "  del t4280, t_transformer_h_7_mlp_c_fc_weight\n",
      "  t4286 = torch.matmul(t4284, t4285)  # t4286: \"cuda:0 f32[3072, 768]\"\n",
      "    # t4286 = ltorch.matmul(t4284, t4285)  # t4286: \"cuda:0 f32[3072, 768]\"\n",
      "      # t4286 = prims.matmul(t4284, t4285)  # t4286: \"cuda:0 f32[3072, 768]\"\n",
      "  del t4284, t4285\n",
      "  t4282 = torch.reshape(t4281, (4, 64, 768))  # t4282: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4282 = ltorch.reshape(t4281, (4, 64, 768))  # t4282: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4282 = prims.reshape(t4281, (4, 64, 768))  # t4282: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4281\n",
      "  t5698 = torch.unsqueeze(t583, 2)  # t5698: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5698 = ltorch.unsqueeze(t583, 2)  # t5698: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t5698 = prims.broadcast_in_dim(t583, [4, 64, 1], [0, 1])  # t5698: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t583\n",
      "  t4309 = Tensor.expand(t5698, [4, 64, 1])  # t4309: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4309 = ltorch.expand(t5698, [4, 64, 1])  # t4309: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t4309 = prims.broadcast_in_dim(t5698, (4, 64, 1), (0, 1, 2))  # t4309: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t5698\n",
      "  t4310 = Tensor.expand(t4309, (4, 64, 768))  # t4310: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4310 = ltorch.expand(t4309, (4, 64, 768))  # t4310: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4310 = prims.broadcast_in_dim(t4309, (4, 64, 768), (0, 1, 2))  # t4310: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4309\n",
      "  [t4288, t4291, t4317, t4325] = nvFusion23(t4255, t4282, t4310, t579, t587, t589, t592)\n",
      "    # t590 = prims.broadcast_in_dim(t587, (4, 64, 768), (0, 1, 2))  # t590: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t591 = prims.mul(t589, t590)  # t591: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4288 = prims.sum(t4282, (0, 1))  # t4288: \"cuda:0 f32[768]\"\n",
      "    # t4289 = prims.mul(t592, t4282)  # t4289: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4290 = prims.mul(t591, t4282)  # t4290: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4291 = prims.sum(t4290, (0, 1))  # t4291: \"cuda:0 f32[768]\"\n",
      "    # t4292 = prims.mul(t590, t4289)  # t4292: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4293 = prims.mul(t589, t4289)  # t4293: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4294 = prims.sum(t4293, (2,))  # t4294: \"cuda:0 f32[4, 64]\"\n",
      "    # t4295 = prims.broadcast_in_dim(t4294, [4, 64, 1], [0, 1])  # t4295: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4296 = prims.neg(t4292)  # t4296: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4297 = prims.sum(t4296, (2,))  # t4297: \"cuda:0 f32[4, 64]\"\n",
      "    # t4298 = prims.broadcast_in_dim(t4297, [4, 64, 1], [0, 1])  # t4298: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4299 = prims.mul(-0.5, t4295)  # t4299: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4300 = prims.pow(t587, 3.0)  # t4300: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4301 = prims.mul(t4299, t4300)  # t4301: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4302 = prims.sum(t4298, (2,))  # t4302: \"cuda:0 f32[4, 64]\"\n",
      "    # t4303 = prims.sum(t4301, (2,))  # t4303: \"cuda:0 f32[4, 64]\"\n",
      "    # t4304 = prims.broadcast_in_dim(t4302, [4, 64, 1], [0, 1])  # t4304: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4305 = prims.broadcast_in_dim(t4304, (4, 64, 768), (0, 1, 2))  # t4305: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4306 = prims.mul(0.0013020833333333333, t4305)  # t4306: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4307 = prims.broadcast_in_dim(t4303, [4, 64, 1], [0, 1])  # t4307: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4308 = prims.broadcast_in_dim(t4307, (4, 64, 768), (0, 1, 2))  # t4308: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4311 = prims.mul(2.0, t4308)  # t4311: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4312 = prims.sub(t579, t4310)  # t4312: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4313 = prims.mul(t4311, t4312)  # t4313: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4314 = prims.div(t4313, 768.0)  # t4314: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4315 = prims.add(t4306, t4314)  # t4315: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4316 = prims.add(t4292, t4315)  # t4316: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4317 = prims.add(t4255, t4316)  # t4317: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4325 = prims.sum(t4317, (0, 1))  # t4325: \"cuda:0 f32[768]\"\n",
      "  del t4255, t4282, t4310, t579, t587, t589, t592\n",
      "  t4318 = torch.reshape(t4317, (-1, 768))  # t4318: \"cuda:0 f32[256, 768]\"\n",
      "    # t4318 = ltorch.reshape(t4317, (-1, 768))  # t4318: \"cuda:0 f32[256, 768]\"\n",
      "      # t4318 = prims.reshape(t4317, (256, 768))  # t4318: \"cuda:0 f32[256, 768]\"\n",
      "  t4322 = torch.permute(t4318, (1, 0))  # t4322: \"cuda:0 f32[768, 256]\"\n",
      "    # t4322 = ltorch.permute(t4318, (1, 0))  # t4322: \"cuda:0 f32[768, 256]\"\n",
      "      # t4322 = prims.transpose(t4318, (1, 0))  # t4322: \"cuda:0 f32[768, 256]\"\n",
      "  t4319 = torch.matmul(t4318, t_transformer_h_7_attn_c_proj_weight)  # t4319: \"cuda:0 f32[256, 768]\"\n",
      "    # t4319 = ltorch.matmul(t4318, t_transformer_h_7_attn_c_proj_weight)  # t4319: \"cuda:0 f32[256, 768]\"\n",
      "      # t4319 = prims.matmul(t4318, t_transformer_h_7_attn_c_proj_weight)  # t4319: \"cuda:0 f32[256, 768]\"\n",
      "  del t4318, t_transformer_h_7_attn_c_proj_weight\n",
      "  t4324 = torch.matmul(t4322, t4323)  # t4324: \"cuda:0 f32[768, 768]\"\n",
      "    # t4324 = ltorch.matmul(t4322, t4323)  # t4324: \"cuda:0 f32[768, 768]\"\n",
      "      # t4324 = prims.matmul(t4322, t4323)  # t4324: \"cuda:0 f32[768, 768]\"\n",
      "  del t4322, t4323\n",
      "  t4320 = torch.reshape(t4319, (4, 64, 768))  # t4320: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4320 = ltorch.reshape(t4319, (4, 64, 768))  # t4320: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4320 = prims.reshape(t4319, (4, 64, 768))  # t4320: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4319\n",
      "  t4326 = torch.reshape(t4320, (4, 64, 12, 64))  # t4326: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t4326 = ltorch.reshape(t4320, (4, 64, 12, 64))  # t4326: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t4326 = prims.reshape(t4320, (4, 64, 12, 64))  # t4326: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t4320\n",
      "  t4327 = torch.permute(t4326, (0, 2, 1, 3))  # t4327: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4327 = ltorch.permute(t4326, (0, 2, 1, 3))  # t4327: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4327 = prims.transpose(t4326, (0, 2, 1, 3))  # t4327: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4326\n",
      "  t4331 = torch.matmul(t4330, t4327)  # t4331: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4331 = ltorch.matmul(t4330, t4327)  # t4331: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4331 = prims.matmul(t4330, t4327)  # t4331: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4330\n",
      "  t4329 = torch.matmul(t4327, t4328)  # t4329: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4329 = ltorch.matmul(t4327, t4328)  # t4329: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4329 = prims.matmul(t4327, t4328)  # t4329: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4327, t4328\n",
      "  [t4339] = nvFusion24(f522, i526, t4329, t563, t573)\n",
      "    # t4332 = prims.mul(t573, t4329)  # t4332: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4333 = prims.sum(t4332, (3,))  # t4333: \"cuda:0 f32[4, 12, 64]\"\n",
      "    # t4334 = prims.broadcast_in_dim(t4333, [4, 12, 64, 1], [0, 1, 2])  # t4334: \"cuda:0 f32[4, 12, 64, 1]\"\n",
      "    # t4335 = prims.broadcast_in_dim(t4334, (4, 12, 64, 64), (0, 1, 2, 3))  # t4335: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4336 = prims.sub(t4329, t4335)  # t4336: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4337 = prims.mul(t573, t4336)  # t4337: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4338 = prims.where(t563, 0.0, t4337)  # t4338: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4339 = prims.mul(f522, t4338)  # t4339: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del f522, i526, t4329, t563, t573\n",
      "  t4341 = torch.matmul(t4339, t4340)  # t4341: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4341 = ltorch.matmul(t4339, t4340)  # t4341: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4341 = prims.matmul(t4339, t4340)  # t4341: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4340\n",
      "  t4343 = torch.matmul(t4342, t4339)  # t4343: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4343 = ltorch.matmul(t4342, t4339)  # t4343: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4343 = prims.matmul(t4342, t4339)  # t4343: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4342, t4339\n",
      "  t4344 = torch.permute(t4343, (0, 1, 3, 2))  # t4344: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4344 = ltorch.permute(t4343, (0, 1, 3, 2))  # t4344: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4344 = prims.transpose(t4343, (0, 1, 3, 2))  # t4344: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4343\n",
      "  t4345 = torch.permute(t4331, (0, 2, 1, 3))  # t4345: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t4345 = ltorch.permute(t4331, (0, 2, 1, 3))  # t4345: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t4345 = prims.transpose(t4331, (0, 2, 1, 3))  # t4345: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t4331\n",
      "  t4346 = torch.reshape(t4345, (4, 64, 768))  # t4346: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4346 = ltorch.reshape(t4345, (4, 64, 768))  # t4346: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4346 = prims.reshape(t4345, (4, 64, 768))  # t4346: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4345\n",
      "  t4347 = torch.permute(t4344, (0, 2, 1, 3))  # t4347: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t4347 = ltorch.permute(t4344, (0, 2, 1, 3))  # t4347: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t4347 = prims.transpose(t4344, (0, 2, 1, 3))  # t4347: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t4344\n",
      "  t4348 = torch.reshape(t4347, (4, 64, 768))  # t4348: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4348 = ltorch.reshape(t4347, (4, 64, 768))  # t4348: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4348 = prims.reshape(t4347, (4, 64, 768))  # t4348: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4347\n",
      "  t4349 = torch.permute(t4341, (0, 2, 1, 3))  # t4349: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t4349 = ltorch.permute(t4341, (0, 2, 1, 3))  # t4349: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t4349 = prims.transpose(t4341, (0, 2, 1, 3))  # t4349: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t4341\n",
      "  t4350 = torch.reshape(t4349, (4, 64, 768))  # t4350: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4350 = ltorch.reshape(t4349, (4, 64, 768))  # t4350: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4350 = prims.reshape(t4349, (4, 64, 768))  # t4350: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4349\n",
      "  [t4351, t4359] = nvFusion25(i488, t4346, t4348, t4350)\n",
      "    # t4351 = prims.cat((t4350, t4348, t4346), i488)  # t4351: \"cuda:0 f32[4, 64, 2304]\"\n",
      "    # t4359 = prims.sum(t4351, (0, 1))  # t4359: \"cuda:0 f32[2304]\"\n",
      "  del i488, t4346, t4348, t4350\n",
      "  t4352 = torch.reshape(t4351, (-1, 2304))  # t4352: \"cuda:0 f32[256, 2304]\"\n",
      "    # t4352 = ltorch.reshape(t4351, (-1, 2304))  # t4352: \"cuda:0 f32[256, 2304]\"\n",
      "      # t4352 = prims.reshape(t4351, (256, 2304))  # t4352: \"cuda:0 f32[256, 2304]\"\n",
      "  del t4351\n",
      "  t4356 = torch.permute(t4352, (1, 0))  # t4356: \"cuda:0 f32[2304, 256]\"\n",
      "    # t4356 = ltorch.permute(t4352, (1, 0))  # t4356: \"cuda:0 f32[2304, 256]\"\n",
      "      # t4356 = prims.transpose(t4352, (1, 0))  # t4356: \"cuda:0 f32[2304, 256]\"\n",
      "  t4358 = torch.matmul(t4356, t4357)  # t4358: \"cuda:0 f32[2304, 768]\"\n",
      "    # t4358 = ltorch.matmul(t4356, t4357)  # t4358: \"cuda:0 f32[2304, 768]\"\n",
      "      # t4358 = prims.matmul(t4356, t4357)  # t4358: \"cuda:0 f32[2304, 768]\"\n",
      "  del t4356, t4357\n",
      "  t4353 = torch.matmul(t4352, t_transformer_h_7_attn_c_attn_weight)  # t4353: \"cuda:0 f32[256, 768]\"\n",
      "    # t4353 = ltorch.matmul(t4352, t_transformer_h_7_attn_c_attn_weight)  # t4353: \"cuda:0 f32[256, 768]\"\n",
      "      # t4353 = prims.matmul(t4352, t_transformer_h_7_attn_c_attn_weight)  # t4353: \"cuda:0 f32[256, 768]\"\n",
      "  del t4352, t_transformer_h_7_attn_c_attn_weight\n",
      "  t4354 = torch.reshape(t4353, (4, 64, 768))  # t4354: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4354 = ltorch.reshape(t4353, (4, 64, 768))  # t4354: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4354 = prims.reshape(t4353, (4, 64, 768))  # t4354: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4353\n",
      "  t5716 = torch.unsqueeze(t535, 2)  # t5716: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5716 = ltorch.unsqueeze(t535, 2)  # t5716: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t5716 = prims.broadcast_in_dim(t535, [4, 64, 1], [0, 1])  # t5716: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t535\n",
      "  t4381 = Tensor.expand(t5716, [4, 64, 1])  # t4381: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4381 = ltorch.expand(t5716, [4, 64, 1])  # t4381: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t4381 = prims.broadcast_in_dim(t5716, (4, 64, 1), (0, 1, 2))  # t4381: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t5716\n",
      "  t4382 = Tensor.expand(t4381, (4, 64, 768))  # t4382: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4382 = ltorch.expand(t4381, (4, 64, 768))  # t4382: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4382 = prims.broadcast_in_dim(t4381, (4, 64, 768), (0, 1, 2))  # t4382: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4381\n",
      "  [t4360, t4363, t4389, t4397] = nvFusion26(t4317, t4354, t4382, t531, t539, t541, t544)\n",
      "    # t542 = prims.broadcast_in_dim(t539, (4, 64, 768), (0, 1, 2))  # t542: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t543 = prims.mul(t541, t542)  # t543: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4360 = prims.sum(t4354, (0, 1))  # t4360: \"cuda:0 f32[768]\"\n",
      "    # t4361 = prims.mul(t544, t4354)  # t4361: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4362 = prims.mul(t543, t4354)  # t4362: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4363 = prims.sum(t4362, (0, 1))  # t4363: \"cuda:0 f32[768]\"\n",
      "    # t4364 = prims.mul(t542, t4361)  # t4364: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4365 = prims.mul(t541, t4361)  # t4365: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4366 = prims.sum(t4365, (2,))  # t4366: \"cuda:0 f32[4, 64]\"\n",
      "    # t4367 = prims.broadcast_in_dim(t4366, [4, 64, 1], [0, 1])  # t4367: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4368 = prims.neg(t4364)  # t4368: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4369 = prims.sum(t4368, (2,))  # t4369: \"cuda:0 f32[4, 64]\"\n",
      "    # t4370 = prims.broadcast_in_dim(t4369, [4, 64, 1], [0, 1])  # t4370: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4371 = prims.mul(-0.5, t4367)  # t4371: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4372 = prims.pow(t539, 3.0)  # t4372: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4373 = prims.mul(t4371, t4372)  # t4373: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4374 = prims.sum(t4370, (2,))  # t4374: \"cuda:0 f32[4, 64]\"\n",
      "    # t4375 = prims.sum(t4373, (2,))  # t4375: \"cuda:0 f32[4, 64]\"\n",
      "    # t4376 = prims.broadcast_in_dim(t4374, [4, 64, 1], [0, 1])  # t4376: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4377 = prims.broadcast_in_dim(t4376, (4, 64, 768), (0, 1, 2))  # t4377: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4378 = prims.mul(0.0013020833333333333, t4377)  # t4378: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4379 = prims.broadcast_in_dim(t4375, [4, 64, 1], [0, 1])  # t4379: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4380 = prims.broadcast_in_dim(t4379, (4, 64, 768), (0, 1, 2))  # t4380: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4383 = prims.mul(2.0, t4380)  # t4383: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4384 = prims.sub(t531, t4382)  # t4384: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4385 = prims.mul(t4383, t4384)  # t4385: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4386 = prims.div(t4385, 768.0)  # t4386: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4387 = prims.add(t4378, t4386)  # t4387: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4388 = prims.add(t4364, t4387)  # t4388: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4389 = prims.add(t4317, t4388)  # t4389: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4397 = prims.sum(t4389, (0, 1))  # t4397: \"cuda:0 f32[768]\"\n",
      "  del t4317, t4354, t4382, t531, t539, t541, t544\n",
      "  t4390 = torch.reshape(t4389, (-1, 768))  # t4390: \"cuda:0 f32[256, 768]\"\n",
      "    # t4390 = ltorch.reshape(t4389, (-1, 768))  # t4390: \"cuda:0 f32[256, 768]\"\n",
      "      # t4390 = prims.reshape(t4389, (256, 768))  # t4390: \"cuda:0 f32[256, 768]\"\n",
      "  t4394 = torch.permute(t4390, (1, 0))  # t4394: \"cuda:0 f32[768, 256]\"\n",
      "    # t4394 = ltorch.permute(t4390, (1, 0))  # t4394: \"cuda:0 f32[768, 256]\"\n",
      "      # t4394 = prims.transpose(t4390, (1, 0))  # t4394: \"cuda:0 f32[768, 256]\"\n",
      "  t4391 = torch.matmul(t4390, t_transformer_h_6_mlp_c_proj_weight)  # t4391: \"cuda:0 f32[256, 3072]\"\n",
      "    # t4391 = ltorch.matmul(t4390, t_transformer_h_6_mlp_c_proj_weight)  # t4391: \"cuda:0 f32[256, 3072]\"\n",
      "      # t4391 = prims.matmul(t4390, t_transformer_h_6_mlp_c_proj_weight)  # t4391: \"cuda:0 f32[256, 3072]\"\n",
      "  del t4390, t_transformer_h_6_mlp_c_proj_weight\n",
      "  t4396 = torch.matmul(t4394, t4395)  # t4396: \"cuda:0 f32[768, 3072]\"\n",
      "    # t4396 = ltorch.matmul(t4394, t4395)  # t4396: \"cuda:0 f32[768, 3072]\"\n",
      "      # t4396 = prims.matmul(t4394, t4395)  # t4396: \"cuda:0 f32[768, 3072]\"\n",
      "  del t4394, t4395\n",
      "  t4392 = torch.reshape(t4391, (4, 64, 3072))  # t4392: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4392 = ltorch.reshape(t4391, (4, 64, 3072))  # t4392: \"cuda:0 f32[4, 64, 3072]\"\n",
      "      # t4392 = prims.reshape(t4391, (4, 64, 3072))  # t4392: \"cuda:0 f32[4, 64, 3072]\"\n",
      "  del t4391\n",
      "  [t4413, t4421] = nvFusion27(f474, f476, f478, f480, t4392, t521)\n",
      "    # t522 = prims.mul(0.5, t521)  # t522: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t523 = prims.pow(t521, 3.0)  # t523: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t524 = prims.mul(0.044715, t523)  # t524: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t525 = prims.add(t521, t524)  # t525: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t526 = prims.mul(0.7978845608028654, t525)  # t526: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t527 = prims.tanh(t526)  # t527: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t528 = prims.add(1.0, t527)  # t528: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4398 = prims.mul(t528, t4392)  # t4398: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4399 = prims.mul(t522, t4392)  # t4399: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4400 = prims.mul(t527, t527)  # t4400: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4401 = prims.sub(1.0, t4400)  # t4401: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4402 = prims.mul(t4399, t4401)  # t4402: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4403 = prims.mul(f480, t4402)  # t4403: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4404 = prims.mul(f478, t4403)  # t4404: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4406 = prims.mul(t4404, f476)  # t4406: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4407 = prims.pow(t521, 2.0)  # t4407: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4408 = prims.mul(t4406, t4407)  # t4408: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4411 = prims.add(t4403, t4408)  # t4411: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4412 = prims.mul(f474, t4398)  # t4412: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4413 = prims.add(t4411, t4412)  # t4413: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4421 = prims.sum(t4413, (0, 1))  # t4421: \"cuda:0 f32[3072]\"\n",
      "  del f474, f476, f478, f480, t4392, t521\n",
      "  t4414 = torch.reshape(t4413, (-1, 3072))  # t4414: \"cuda:0 f32[256, 3072]\"\n",
      "    # t4414 = ltorch.reshape(t4413, (-1, 3072))  # t4414: \"cuda:0 f32[256, 3072]\"\n",
      "      # t4414 = prims.reshape(t4413, (256, 3072))  # t4414: \"cuda:0 f32[256, 3072]\"\n",
      "  del t4413\n",
      "  t4418 = torch.permute(t4414, (1, 0))  # t4418: \"cuda:0 f32[3072, 256]\"\n",
      "    # t4418 = ltorch.permute(t4414, (1, 0))  # t4418: \"cuda:0 f32[3072, 256]\"\n",
      "      # t4418 = prims.transpose(t4414, (1, 0))  # t4418: \"cuda:0 f32[3072, 256]\"\n",
      "  t4415 = torch.matmul(t4414, t_transformer_h_6_mlp_c_fc_weight)  # t4415: \"cuda:0 f32[256, 768]\"\n",
      "    # t4415 = ltorch.matmul(t4414, t_transformer_h_6_mlp_c_fc_weight)  # t4415: \"cuda:0 f32[256, 768]\"\n",
      "      # t4415 = prims.matmul(t4414, t_transformer_h_6_mlp_c_fc_weight)  # t4415: \"cuda:0 f32[256, 768]\"\n",
      "  del t4414, t_transformer_h_6_mlp_c_fc_weight\n",
      "  t4420 = torch.matmul(t4418, t4419)  # t4420: \"cuda:0 f32[3072, 768]\"\n",
      "    # t4420 = ltorch.matmul(t4418, t4419)  # t4420: \"cuda:0 f32[3072, 768]\"\n",
      "      # t4420 = prims.matmul(t4418, t4419)  # t4420: \"cuda:0 f32[3072, 768]\"\n",
      "  del t4418, t4419\n",
      "  t4416 = torch.reshape(t4415, (4, 64, 768))  # t4416: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4416 = ltorch.reshape(t4415, (4, 64, 768))  # t4416: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4416 = prims.reshape(t4415, (4, 64, 768))  # t4416: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4415\n",
      "  t5725 = torch.unsqueeze(t508, 2)  # t5725: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5725 = ltorch.unsqueeze(t508, 2)  # t5725: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t5725 = prims.broadcast_in_dim(t508, [4, 64, 1], [0, 1])  # t5725: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t508\n",
      "  t4443 = Tensor.expand(t5725, [4, 64, 1])  # t4443: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4443 = ltorch.expand(t5725, [4, 64, 1])  # t4443: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t4443 = prims.broadcast_in_dim(t5725, (4, 64, 1), (0, 1, 2))  # t4443: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t5725\n",
      "  t4444 = Tensor.expand(t4443, (4, 64, 768))  # t4444: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4444 = ltorch.expand(t4443, (4, 64, 768))  # t4444: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4444 = prims.broadcast_in_dim(t4443, (4, 64, 768), (0, 1, 2))  # t4444: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4443\n",
      "  [t4422, t4425, t4451, t4459] = nvFusion28(t4389, t4416, t4444, t504, t512, t514, t517)\n",
      "    # t515 = prims.broadcast_in_dim(t512, (4, 64, 768), (0, 1, 2))  # t515: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t516 = prims.mul(t514, t515)  # t516: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4422 = prims.sum(t4416, (0, 1))  # t4422: \"cuda:0 f32[768]\"\n",
      "    # t4423 = prims.mul(t517, t4416)  # t4423: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4424 = prims.mul(t516, t4416)  # t4424: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4425 = prims.sum(t4424, (0, 1))  # t4425: \"cuda:0 f32[768]\"\n",
      "    # t4426 = prims.mul(t515, t4423)  # t4426: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4427 = prims.mul(t514, t4423)  # t4427: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4428 = prims.sum(t4427, (2,))  # t4428: \"cuda:0 f32[4, 64]\"\n",
      "    # t4429 = prims.broadcast_in_dim(t4428, [4, 64, 1], [0, 1])  # t4429: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4430 = prims.neg(t4426)  # t4430: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4431 = prims.sum(t4430, (2,))  # t4431: \"cuda:0 f32[4, 64]\"\n",
      "    # t4432 = prims.broadcast_in_dim(t4431, [4, 64, 1], [0, 1])  # t4432: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4433 = prims.mul(-0.5, t4429)  # t4433: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4434 = prims.pow(t512, 3.0)  # t4434: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4435 = prims.mul(t4433, t4434)  # t4435: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4436 = prims.sum(t4432, (2,))  # t4436: \"cuda:0 f32[4, 64]\"\n",
      "    # t4437 = prims.sum(t4435, (2,))  # t4437: \"cuda:0 f32[4, 64]\"\n",
      "    # t4438 = prims.broadcast_in_dim(t4436, [4, 64, 1], [0, 1])  # t4438: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4439 = prims.broadcast_in_dim(t4438, (4, 64, 768), (0, 1, 2))  # t4439: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4440 = prims.mul(0.0013020833333333333, t4439)  # t4440: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4441 = prims.broadcast_in_dim(t4437, [4, 64, 1], [0, 1])  # t4441: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4442 = prims.broadcast_in_dim(t4441, (4, 64, 768), (0, 1, 2))  # t4442: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4445 = prims.mul(2.0, t4442)  # t4445: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4446 = prims.sub(t504, t4444)  # t4446: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4447 = prims.mul(t4445, t4446)  # t4447: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4448 = prims.div(t4447, 768.0)  # t4448: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4449 = prims.add(t4440, t4448)  # t4449: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4450 = prims.add(t4426, t4449)  # t4450: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4451 = prims.add(t4389, t4450)  # t4451: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4459 = prims.sum(t4451, (0, 1))  # t4459: \"cuda:0 f32[768]\"\n",
      "  del t4389, t4416, t4444, t504, t512, t514, t517\n",
      "  t4452 = torch.reshape(t4451, (-1, 768))  # t4452: \"cuda:0 f32[256, 768]\"\n",
      "    # t4452 = ltorch.reshape(t4451, (-1, 768))  # t4452: \"cuda:0 f32[256, 768]\"\n",
      "      # t4452 = prims.reshape(t4451, (256, 768))  # t4452: \"cuda:0 f32[256, 768]\"\n",
      "  t4456 = torch.permute(t4452, (1, 0))  # t4456: \"cuda:0 f32[768, 256]\"\n",
      "    # t4456 = ltorch.permute(t4452, (1, 0))  # t4456: \"cuda:0 f32[768, 256]\"\n",
      "      # t4456 = prims.transpose(t4452, (1, 0))  # t4456: \"cuda:0 f32[768, 256]\"\n",
      "  t4453 = torch.matmul(t4452, t_transformer_h_6_attn_c_proj_weight)  # t4453: \"cuda:0 f32[256, 768]\"\n",
      "    # t4453 = ltorch.matmul(t4452, t_transformer_h_6_attn_c_proj_weight)  # t4453: \"cuda:0 f32[256, 768]\"\n",
      "      # t4453 = prims.matmul(t4452, t_transformer_h_6_attn_c_proj_weight)  # t4453: \"cuda:0 f32[256, 768]\"\n",
      "  del t4452, t_transformer_h_6_attn_c_proj_weight\n",
      "  t4458 = torch.matmul(t4456, t4457)  # t4458: \"cuda:0 f32[768, 768]\"\n",
      "    # t4458 = ltorch.matmul(t4456, t4457)  # t4458: \"cuda:0 f32[768, 768]\"\n",
      "      # t4458 = prims.matmul(t4456, t4457)  # t4458: \"cuda:0 f32[768, 768]\"\n",
      "  del t4456, t4457\n",
      "  t4454 = torch.reshape(t4453, (4, 64, 768))  # t4454: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4454 = ltorch.reshape(t4453, (4, 64, 768))  # t4454: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4454 = prims.reshape(t4453, (4, 64, 768))  # t4454: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4453\n",
      "  t4460 = torch.reshape(t4454, (4, 64, 12, 64))  # t4460: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t4460 = ltorch.reshape(t4454, (4, 64, 12, 64))  # t4460: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t4460 = prims.reshape(t4454, (4, 64, 12, 64))  # t4460: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t4454\n",
      "  t4461 = torch.permute(t4460, (0, 2, 1, 3))  # t4461: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4461 = ltorch.permute(t4460, (0, 2, 1, 3))  # t4461: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4461 = prims.transpose(t4460, (0, 2, 1, 3))  # t4461: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4460\n",
      "  t4463 = torch.matmul(t4461, t4462)  # t4463: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4463 = ltorch.matmul(t4461, t4462)  # t4463: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4463 = prims.matmul(t4461, t4462)  # t4463: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4462\n",
      "  t4465 = torch.matmul(t4464, t4461)  # t4465: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4465 = ltorch.matmul(t4464, t4461)  # t4465: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4465 = prims.matmul(t4464, t4461)  # t4465: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4464, t4461\n",
      "  [t4473] = nvFusion29(f454, i458, t4463, t488, t498)\n",
      "    # t4466 = prims.mul(t498, t4463)  # t4466: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4467 = prims.sum(t4466, (3,))  # t4467: \"cuda:0 f32[4, 12, 64]\"\n",
      "    # t4468 = prims.broadcast_in_dim(t4467, [4, 12, 64, 1], [0, 1, 2])  # t4468: \"cuda:0 f32[4, 12, 64, 1]\"\n",
      "    # t4469 = prims.broadcast_in_dim(t4468, (4, 12, 64, 64), (0, 1, 2, 3))  # t4469: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4470 = prims.sub(t4463, t4469)  # t4470: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4471 = prims.mul(t498, t4470)  # t4471: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4472 = prims.where(t488, 0.0, t4471)  # t4472: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4473 = prims.mul(f454, t4472)  # t4473: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del f454, i458, t4463, t488, t498\n",
      "  t4475 = torch.matmul(t4473, t4474)  # t4475: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4475 = ltorch.matmul(t4473, t4474)  # t4475: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4475 = prims.matmul(t4473, t4474)  # t4475: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4474\n",
      "  t4477 = torch.matmul(t4476, t4473)  # t4477: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4477 = ltorch.matmul(t4476, t4473)  # t4477: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4477 = prims.matmul(t4476, t4473)  # t4477: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4476, t4473\n",
      "  t4478 = torch.permute(t4477, (0, 1, 3, 2))  # t4478: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4478 = ltorch.permute(t4477, (0, 1, 3, 2))  # t4478: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4478 = prims.transpose(t4477, (0, 1, 3, 2))  # t4478: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4477\n",
      "  t4479 = torch.permute(t4465, (0, 2, 1, 3))  # t4479: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t4479 = ltorch.permute(t4465, (0, 2, 1, 3))  # t4479: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t4479 = prims.transpose(t4465, (0, 2, 1, 3))  # t4479: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t4465\n",
      "  t4480 = torch.reshape(t4479, (4, 64, 768))  # t4480: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4480 = ltorch.reshape(t4479, (4, 64, 768))  # t4480: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4480 = prims.reshape(t4479, (4, 64, 768))  # t4480: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4479\n",
      "  t4481 = torch.permute(t4478, (0, 2, 1, 3))  # t4481: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t4481 = ltorch.permute(t4478, (0, 2, 1, 3))  # t4481: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t4481 = prims.transpose(t4478, (0, 2, 1, 3))  # t4481: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t4478\n",
      "  t4482 = torch.reshape(t4481, (4, 64, 768))  # t4482: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4482 = ltorch.reshape(t4481, (4, 64, 768))  # t4482: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4482 = prims.reshape(t4481, (4, 64, 768))  # t4482: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4481\n",
      "  t4483 = torch.permute(t4475, (0, 2, 1, 3))  # t4483: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t4483 = ltorch.permute(t4475, (0, 2, 1, 3))  # t4483: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t4483 = prims.transpose(t4475, (0, 2, 1, 3))  # t4483: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t4475\n",
      "  t4484 = torch.reshape(t4483, (4, 64, 768))  # t4484: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4484 = ltorch.reshape(t4483, (4, 64, 768))  # t4484: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4484 = prims.reshape(t4483, (4, 64, 768))  # t4484: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4483\n",
      "  [t4485, t4493] = nvFusion30(i420, t4480, t4482, t4484)\n",
      "    # t4485 = prims.cat((t4484, t4482, t4480), i420)  # t4485: \"cuda:0 f32[4, 64, 2304]\"\n",
      "    # t4493 = prims.sum(t4485, (0, 1))  # t4493: \"cuda:0 f32[2304]\"\n",
      "  del i420, t4480, t4482, t4484\n",
      "  t4486 = torch.reshape(t4485, (-1, 2304))  # t4486: \"cuda:0 f32[256, 2304]\"\n",
      "    # t4486 = ltorch.reshape(t4485, (-1, 2304))  # t4486: \"cuda:0 f32[256, 2304]\"\n",
      "      # t4486 = prims.reshape(t4485, (256, 2304))  # t4486: \"cuda:0 f32[256, 2304]\"\n",
      "  del t4485\n",
      "  t4490 = torch.permute(t4486, (1, 0))  # t4490: \"cuda:0 f32[2304, 256]\"\n",
      "    # t4490 = ltorch.permute(t4486, (1, 0))  # t4490: \"cuda:0 f32[2304, 256]\"\n",
      "      # t4490 = prims.transpose(t4486, (1, 0))  # t4490: \"cuda:0 f32[2304, 256]\"\n",
      "  t4492 = torch.matmul(t4490, t4491)  # t4492: \"cuda:0 f32[2304, 768]\"\n",
      "    # t4492 = ltorch.matmul(t4490, t4491)  # t4492: \"cuda:0 f32[2304, 768]\"\n",
      "      # t4492 = prims.matmul(t4490, t4491)  # t4492: \"cuda:0 f32[2304, 768]\"\n",
      "  del t4490, t4491\n",
      "  t4487 = torch.matmul(t4486, t_transformer_h_6_attn_c_attn_weight)  # t4487: \"cuda:0 f32[256, 768]\"\n",
      "    # t4487 = ltorch.matmul(t4486, t_transformer_h_6_attn_c_attn_weight)  # t4487: \"cuda:0 f32[256, 768]\"\n",
      "      # t4487 = prims.matmul(t4486, t_transformer_h_6_attn_c_attn_weight)  # t4487: \"cuda:0 f32[256, 768]\"\n",
      "  del t4486, t_transformer_h_6_attn_c_attn_weight\n",
      "  t4488 = torch.reshape(t4487, (4, 64, 768))  # t4488: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4488 = ltorch.reshape(t4487, (4, 64, 768))  # t4488: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4488 = prims.reshape(t4487, (4, 64, 768))  # t4488: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4487\n",
      "  t5743 = torch.unsqueeze(t460, 2)  # t5743: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5743 = ltorch.unsqueeze(t460, 2)  # t5743: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t5743 = prims.broadcast_in_dim(t460, [4, 64, 1], [0, 1])  # t5743: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t460\n",
      "  t4515 = Tensor.expand(t5743, [4, 64, 1])  # t4515: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4515 = ltorch.expand(t5743, [4, 64, 1])  # t4515: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t4515 = prims.broadcast_in_dim(t5743, (4, 64, 1), (0, 1, 2))  # t4515: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t5743\n",
      "  t4516 = Tensor.expand(t4515, (4, 64, 768))  # t4516: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4516 = ltorch.expand(t4515, (4, 64, 768))  # t4516: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4516 = prims.broadcast_in_dim(t4515, (4, 64, 768), (0, 1, 2))  # t4516: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4515\n",
      "  [t4494, t4497, t4523, t4531] = nvFusion31(t4451, t4488, t4516, t456, t464, t466, t469)\n",
      "    # t467 = prims.broadcast_in_dim(t464, (4, 64, 768), (0, 1, 2))  # t467: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t468 = prims.mul(t466, t467)  # t468: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4494 = prims.sum(t4488, (0, 1))  # t4494: \"cuda:0 f32[768]\"\n",
      "    # t4495 = prims.mul(t469, t4488)  # t4495: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4496 = prims.mul(t468, t4488)  # t4496: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4497 = prims.sum(t4496, (0, 1))  # t4497: \"cuda:0 f32[768]\"\n",
      "    # t4498 = prims.mul(t467, t4495)  # t4498: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4499 = prims.mul(t466, t4495)  # t4499: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4500 = prims.sum(t4499, (2,))  # t4500: \"cuda:0 f32[4, 64]\"\n",
      "    # t4501 = prims.broadcast_in_dim(t4500, [4, 64, 1], [0, 1])  # t4501: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4502 = prims.neg(t4498)  # t4502: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4503 = prims.sum(t4502, (2,))  # t4503: \"cuda:0 f32[4, 64]\"\n",
      "    # t4504 = prims.broadcast_in_dim(t4503, [4, 64, 1], [0, 1])  # t4504: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4505 = prims.mul(-0.5, t4501)  # t4505: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4506 = prims.pow(t464, 3.0)  # t4506: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4507 = prims.mul(t4505, t4506)  # t4507: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4508 = prims.sum(t4504, (2,))  # t4508: \"cuda:0 f32[4, 64]\"\n",
      "    # t4509 = prims.sum(t4507, (2,))  # t4509: \"cuda:0 f32[4, 64]\"\n",
      "    # t4510 = prims.broadcast_in_dim(t4508, [4, 64, 1], [0, 1])  # t4510: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4511 = prims.broadcast_in_dim(t4510, (4, 64, 768), (0, 1, 2))  # t4511: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4512 = prims.mul(0.0013020833333333333, t4511)  # t4512: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4513 = prims.broadcast_in_dim(t4509, [4, 64, 1], [0, 1])  # t4513: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4514 = prims.broadcast_in_dim(t4513, (4, 64, 768), (0, 1, 2))  # t4514: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4517 = prims.mul(2.0, t4514)  # t4517: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4518 = prims.sub(t456, t4516)  # t4518: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4519 = prims.mul(t4517, t4518)  # t4519: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4520 = prims.div(t4519, 768.0)  # t4520: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4521 = prims.add(t4512, t4520)  # t4521: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4522 = prims.add(t4498, t4521)  # t4522: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4523 = prims.add(t4451, t4522)  # t4523: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4531 = prims.sum(t4523, (0, 1))  # t4531: \"cuda:0 f32[768]\"\n",
      "  del t4451, t4488, t4516, t456, t464, t466, t469\n",
      "  t4524 = torch.reshape(t4523, (-1, 768))  # t4524: \"cuda:0 f32[256, 768]\"\n",
      "    # t4524 = ltorch.reshape(t4523, (-1, 768))  # t4524: \"cuda:0 f32[256, 768]\"\n",
      "      # t4524 = prims.reshape(t4523, (256, 768))  # t4524: \"cuda:0 f32[256, 768]\"\n",
      "  t4528 = torch.permute(t4524, (1, 0))  # t4528: \"cuda:0 f32[768, 256]\"\n",
      "    # t4528 = ltorch.permute(t4524, (1, 0))  # t4528: \"cuda:0 f32[768, 256]\"\n",
      "      # t4528 = prims.transpose(t4524, (1, 0))  # t4528: \"cuda:0 f32[768, 256]\"\n",
      "  t4530 = torch.matmul(t4528, t4529)  # t4530: \"cuda:0 f32[768, 3072]\"\n",
      "    # t4530 = ltorch.matmul(t4528, t4529)  # t4530: \"cuda:0 f32[768, 3072]\"\n",
      "      # t4530 = prims.matmul(t4528, t4529)  # t4530: \"cuda:0 f32[768, 3072]\"\n",
      "  del t4528, t4529\n",
      "  t4525 = torch.matmul(t4524, t_transformer_h_5_mlp_c_proj_weight)  # t4525: \"cuda:0 f32[256, 3072]\"\n",
      "    # t4525 = ltorch.matmul(t4524, t_transformer_h_5_mlp_c_proj_weight)  # t4525: \"cuda:0 f32[256, 3072]\"\n",
      "      # t4525 = prims.matmul(t4524, t_transformer_h_5_mlp_c_proj_weight)  # t4525: \"cuda:0 f32[256, 3072]\"\n",
      "  del t4524, t_transformer_h_5_mlp_c_proj_weight\n",
      "  t4526 = torch.reshape(t4525, (4, 64, 3072))  # t4526: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4526 = ltorch.reshape(t4525, (4, 64, 3072))  # t4526: \"cuda:0 f32[4, 64, 3072]\"\n",
      "      # t4526 = prims.reshape(t4525, (4, 64, 3072))  # t4526: \"cuda:0 f32[4, 64, 3072]\"\n",
      "  del t4525\n",
      "  [t4547, t4555] = nvFusion32(f406, f408, f410, f412, t446, t4526)\n",
      "    # t447 = prims.mul(0.5, t446)  # t447: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t448 = prims.pow(t446, 3.0)  # t448: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t449 = prims.mul(0.044715, t448)  # t449: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t450 = prims.add(t446, t449)  # t450: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t451 = prims.mul(0.7978845608028654, t450)  # t451: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t452 = prims.tanh(t451)  # t452: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t453 = prims.add(1.0, t452)  # t453: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4532 = prims.mul(t453, t4526)  # t4532: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4533 = prims.mul(t447, t4526)  # t4533: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4534 = prims.mul(t452, t452)  # t4534: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4535 = prims.sub(1.0, t4534)  # t4535: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4536 = prims.mul(t4533, t4535)  # t4536: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4537 = prims.mul(f412, t4536)  # t4537: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4538 = prims.mul(f410, t4537)  # t4538: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4540 = prims.mul(t4538, f408)  # t4540: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4541 = prims.pow(t446, 2.0)  # t4541: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4542 = prims.mul(t4540, t4541)  # t4542: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4545 = prims.add(t4537, t4542)  # t4545: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4546 = prims.mul(f406, t4532)  # t4546: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4547 = prims.add(t4545, t4546)  # t4547: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4555 = prims.sum(t4547, (0, 1))  # t4555: \"cuda:0 f32[3072]\"\n",
      "  del f406, f408, f410, f412, t446, t4526\n",
      "  t4548 = torch.reshape(t4547, (-1, 3072))  # t4548: \"cuda:0 f32[256, 3072]\"\n",
      "    # t4548 = ltorch.reshape(t4547, (-1, 3072))  # t4548: \"cuda:0 f32[256, 3072]\"\n",
      "      # t4548 = prims.reshape(t4547, (256, 3072))  # t4548: \"cuda:0 f32[256, 3072]\"\n",
      "  del t4547\n",
      "  t4552 = torch.permute(t4548, (1, 0))  # t4552: \"cuda:0 f32[3072, 256]\"\n",
      "    # t4552 = ltorch.permute(t4548, (1, 0))  # t4552: \"cuda:0 f32[3072, 256]\"\n",
      "      # t4552 = prims.transpose(t4548, (1, 0))  # t4552: \"cuda:0 f32[3072, 256]\"\n",
      "  t4549 = torch.matmul(t4548, t_transformer_h_5_mlp_c_fc_weight)  # t4549: \"cuda:0 f32[256, 768]\"\n",
      "    # t4549 = ltorch.matmul(t4548, t_transformer_h_5_mlp_c_fc_weight)  # t4549: \"cuda:0 f32[256, 768]\"\n",
      "      # t4549 = prims.matmul(t4548, t_transformer_h_5_mlp_c_fc_weight)  # t4549: \"cuda:0 f32[256, 768]\"\n",
      "  del t4548, t_transformer_h_5_mlp_c_fc_weight\n",
      "  t4554 = torch.matmul(t4552, t4553)  # t4554: \"cuda:0 f32[3072, 768]\"\n",
      "    # t4554 = ltorch.matmul(t4552, t4553)  # t4554: \"cuda:0 f32[3072, 768]\"\n",
      "      # t4554 = prims.matmul(t4552, t4553)  # t4554: \"cuda:0 f32[3072, 768]\"\n",
      "  del t4552, t4553\n",
      "  t4550 = torch.reshape(t4549, (4, 64, 768))  # t4550: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4550 = ltorch.reshape(t4549, (4, 64, 768))  # t4550: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4550 = prims.reshape(t4549, (4, 64, 768))  # t4550: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4549\n",
      "  t5752 = torch.unsqueeze(t433, 2)  # t5752: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5752 = ltorch.unsqueeze(t433, 2)  # t5752: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t5752 = prims.broadcast_in_dim(t433, [4, 64, 1], [0, 1])  # t5752: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t433\n",
      "  t4577 = Tensor.expand(t5752, [4, 64, 1])  # t4577: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4577 = ltorch.expand(t5752, [4, 64, 1])  # t4577: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t4577 = prims.broadcast_in_dim(t5752, (4, 64, 1), (0, 1, 2))  # t4577: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t5752\n",
      "  t4578 = Tensor.expand(t4577, (4, 64, 768))  # t4578: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4578 = ltorch.expand(t4577, (4, 64, 768))  # t4578: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4578 = prims.broadcast_in_dim(t4577, (4, 64, 768), (0, 1, 2))  # t4578: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4577\n",
      "  [t4556, t4559, t4585, t4593] = nvFusion33(t429, t437, t439, t442, t4523, t4550, t4578)\n",
      "    # t440 = prims.broadcast_in_dim(t437, (4, 64, 768), (0, 1, 2))  # t440: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t441 = prims.mul(t439, t440)  # t441: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4556 = prims.sum(t4550, (0, 1))  # t4556: \"cuda:0 f32[768]\"\n",
      "    # t4557 = prims.mul(t442, t4550)  # t4557: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4558 = prims.mul(t441, t4550)  # t4558: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4559 = prims.sum(t4558, (0, 1))  # t4559: \"cuda:0 f32[768]\"\n",
      "    # t4560 = prims.mul(t440, t4557)  # t4560: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4561 = prims.mul(t439, t4557)  # t4561: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4562 = prims.sum(t4561, (2,))  # t4562: \"cuda:0 f32[4, 64]\"\n",
      "    # t4563 = prims.broadcast_in_dim(t4562, [4, 64, 1], [0, 1])  # t4563: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4564 = prims.neg(t4560)  # t4564: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4565 = prims.sum(t4564, (2,))  # t4565: \"cuda:0 f32[4, 64]\"\n",
      "    # t4566 = prims.broadcast_in_dim(t4565, [4, 64, 1], [0, 1])  # t4566: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4567 = prims.mul(-0.5, t4563)  # t4567: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4568 = prims.pow(t437, 3.0)  # t4568: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4569 = prims.mul(t4567, t4568)  # t4569: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4570 = prims.sum(t4566, (2,))  # t4570: \"cuda:0 f32[4, 64]\"\n",
      "    # t4571 = prims.sum(t4569, (2,))  # t4571: \"cuda:0 f32[4, 64]\"\n",
      "    # t4572 = prims.broadcast_in_dim(t4570, [4, 64, 1], [0, 1])  # t4572: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4573 = prims.broadcast_in_dim(t4572, (4, 64, 768), (0, 1, 2))  # t4573: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4574 = prims.mul(0.0013020833333333333, t4573)  # t4574: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4575 = prims.broadcast_in_dim(t4571, [4, 64, 1], [0, 1])  # t4575: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4576 = prims.broadcast_in_dim(t4575, (4, 64, 768), (0, 1, 2))  # t4576: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4579 = prims.mul(2.0, t4576)  # t4579: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4580 = prims.sub(t429, t4578)  # t4580: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4581 = prims.mul(t4579, t4580)  # t4581: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4582 = prims.div(t4581, 768.0)  # t4582: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4583 = prims.add(t4574, t4582)  # t4583: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4584 = prims.add(t4560, t4583)  # t4584: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4585 = prims.add(t4523, t4584)  # t4585: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4593 = prims.sum(t4585, (0, 1))  # t4593: \"cuda:0 f32[768]\"\n",
      "  del t429, t437, t439, t442, t4523, t4550, t4578\n",
      "  t4586 = torch.reshape(t4585, (-1, 768))  # t4586: \"cuda:0 f32[256, 768]\"\n",
      "    # t4586 = ltorch.reshape(t4585, (-1, 768))  # t4586: \"cuda:0 f32[256, 768]\"\n",
      "      # t4586 = prims.reshape(t4585, (256, 768))  # t4586: \"cuda:0 f32[256, 768]\"\n",
      "  t4590 = torch.permute(t4586, (1, 0))  # t4590: \"cuda:0 f32[768, 256]\"\n",
      "    # t4590 = ltorch.permute(t4586, (1, 0))  # t4590: \"cuda:0 f32[768, 256]\"\n",
      "      # t4590 = prims.transpose(t4586, (1, 0))  # t4590: \"cuda:0 f32[768, 256]\"\n",
      "  t4587 = torch.matmul(t4586, t_transformer_h_5_attn_c_proj_weight)  # t4587: \"cuda:0 f32[256, 768]\"\n",
      "    # t4587 = ltorch.matmul(t4586, t_transformer_h_5_attn_c_proj_weight)  # t4587: \"cuda:0 f32[256, 768]\"\n",
      "      # t4587 = prims.matmul(t4586, t_transformer_h_5_attn_c_proj_weight)  # t4587: \"cuda:0 f32[256, 768]\"\n",
      "  del t4586, t_transformer_h_5_attn_c_proj_weight\n",
      "  t4592 = torch.matmul(t4590, t4591)  # t4592: \"cuda:0 f32[768, 768]\"\n",
      "    # t4592 = ltorch.matmul(t4590, t4591)  # t4592: \"cuda:0 f32[768, 768]\"\n",
      "      # t4592 = prims.matmul(t4590, t4591)  # t4592: \"cuda:0 f32[768, 768]\"\n",
      "  del t4590, t4591\n",
      "  t4588 = torch.reshape(t4587, (4, 64, 768))  # t4588: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4588 = ltorch.reshape(t4587, (4, 64, 768))  # t4588: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4588 = prims.reshape(t4587, (4, 64, 768))  # t4588: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4587\n",
      "  t4594 = torch.reshape(t4588, (4, 64, 12, 64))  # t4594: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t4594 = ltorch.reshape(t4588, (4, 64, 12, 64))  # t4594: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t4594 = prims.reshape(t4588, (4, 64, 12, 64))  # t4594: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t4588\n",
      "  t4595 = torch.permute(t4594, (0, 2, 1, 3))  # t4595: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4595 = ltorch.permute(t4594, (0, 2, 1, 3))  # t4595: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4595 = prims.transpose(t4594, (0, 2, 1, 3))  # t4595: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4594\n",
      "  t4597 = torch.matmul(t4595, t4596)  # t4597: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4597 = ltorch.matmul(t4595, t4596)  # t4597: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4597 = prims.matmul(t4595, t4596)  # t4597: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4596\n",
      "  t4599 = torch.matmul(t4598, t4595)  # t4599: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4599 = ltorch.matmul(t4598, t4595)  # t4599: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4599 = prims.matmul(t4598, t4595)  # t4599: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4598, t4595\n",
      "  [t4607] = nvFusion34(f386, i390, t413, t423, t4597)\n",
      "    # t4600 = prims.mul(t423, t4597)  # t4600: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4601 = prims.sum(t4600, (3,))  # t4601: \"cuda:0 f32[4, 12, 64]\"\n",
      "    # t4602 = prims.broadcast_in_dim(t4601, [4, 12, 64, 1], [0, 1, 2])  # t4602: \"cuda:0 f32[4, 12, 64, 1]\"\n",
      "    # t4603 = prims.broadcast_in_dim(t4602, (4, 12, 64, 64), (0, 1, 2, 3))  # t4603: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4604 = prims.sub(t4597, t4603)  # t4604: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4605 = prims.mul(t423, t4604)  # t4605: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4606 = prims.where(t413, 0.0, t4605)  # t4606: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4607 = prims.mul(f386, t4606)  # t4607: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del f386, i390, t413, t423, t4597\n",
      "  t4609 = torch.matmul(t4607, t4608)  # t4609: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4609 = ltorch.matmul(t4607, t4608)  # t4609: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4609 = prims.matmul(t4607, t4608)  # t4609: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4608\n",
      "  t4611 = torch.matmul(t4610, t4607)  # t4611: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4611 = ltorch.matmul(t4610, t4607)  # t4611: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4611 = prims.matmul(t4610, t4607)  # t4611: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4610, t4607\n",
      "  t4612 = torch.permute(t4611, (0, 1, 3, 2))  # t4612: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4612 = ltorch.permute(t4611, (0, 1, 3, 2))  # t4612: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4612 = prims.transpose(t4611, (0, 1, 3, 2))  # t4612: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4611\n",
      "  t4613 = torch.permute(t4599, (0, 2, 1, 3))  # t4613: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t4613 = ltorch.permute(t4599, (0, 2, 1, 3))  # t4613: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t4613 = prims.transpose(t4599, (0, 2, 1, 3))  # t4613: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t4599\n",
      "  t4614 = torch.reshape(t4613, (4, 64, 768))  # t4614: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4614 = ltorch.reshape(t4613, (4, 64, 768))  # t4614: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4614 = prims.reshape(t4613, (4, 64, 768))  # t4614: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4613\n",
      "  t4615 = torch.permute(t4612, (0, 2, 1, 3))  # t4615: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t4615 = ltorch.permute(t4612, (0, 2, 1, 3))  # t4615: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t4615 = prims.transpose(t4612, (0, 2, 1, 3))  # t4615: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t4612\n",
      "  t4616 = torch.reshape(t4615, (4, 64, 768))  # t4616: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4616 = ltorch.reshape(t4615, (4, 64, 768))  # t4616: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4616 = prims.reshape(t4615, (4, 64, 768))  # t4616: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4615\n",
      "  t4617 = torch.permute(t4609, (0, 2, 1, 3))  # t4617: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t4617 = ltorch.permute(t4609, (0, 2, 1, 3))  # t4617: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t4617 = prims.transpose(t4609, (0, 2, 1, 3))  # t4617: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t4609\n",
      "  t4618 = torch.reshape(t4617, (4, 64, 768))  # t4618: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4618 = ltorch.reshape(t4617, (4, 64, 768))  # t4618: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4618 = prims.reshape(t4617, (4, 64, 768))  # t4618: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4617\n",
      "  [t4619, t4627] = nvFusion35(i352, t4614, t4616, t4618)\n",
      "    # t4619 = prims.cat((t4618, t4616, t4614), i352)  # t4619: \"cuda:0 f32[4, 64, 2304]\"\n",
      "    # t4627 = prims.sum(t4619, (0, 1))  # t4627: \"cuda:0 f32[2304]\"\n",
      "  del i352, t4614, t4616, t4618\n",
      "  t4620 = torch.reshape(t4619, (-1, 2304))  # t4620: \"cuda:0 f32[256, 2304]\"\n",
      "    # t4620 = ltorch.reshape(t4619, (-1, 2304))  # t4620: \"cuda:0 f32[256, 2304]\"\n",
      "      # t4620 = prims.reshape(t4619, (256, 2304))  # t4620: \"cuda:0 f32[256, 2304]\"\n",
      "  del t4619\n",
      "  t4624 = torch.permute(t4620, (1, 0))  # t4624: \"cuda:0 f32[2304, 256]\"\n",
      "    # t4624 = ltorch.permute(t4620, (1, 0))  # t4624: \"cuda:0 f32[2304, 256]\"\n",
      "      # t4624 = prims.transpose(t4620, (1, 0))  # t4624: \"cuda:0 f32[2304, 256]\"\n",
      "  t4626 = torch.matmul(t4624, t4625)  # t4626: \"cuda:0 f32[2304, 768]\"\n",
      "    # t4626 = ltorch.matmul(t4624, t4625)  # t4626: \"cuda:0 f32[2304, 768]\"\n",
      "      # t4626 = prims.matmul(t4624, t4625)  # t4626: \"cuda:0 f32[2304, 768]\"\n",
      "  del t4624, t4625\n",
      "  t4621 = torch.matmul(t4620, t_transformer_h_5_attn_c_attn_weight)  # t4621: \"cuda:0 f32[256, 768]\"\n",
      "    # t4621 = ltorch.matmul(t4620, t_transformer_h_5_attn_c_attn_weight)  # t4621: \"cuda:0 f32[256, 768]\"\n",
      "      # t4621 = prims.matmul(t4620, t_transformer_h_5_attn_c_attn_weight)  # t4621: \"cuda:0 f32[256, 768]\"\n",
      "  del t4620, t_transformer_h_5_attn_c_attn_weight\n",
      "  t4622 = torch.reshape(t4621, (4, 64, 768))  # t4622: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4622 = ltorch.reshape(t4621, (4, 64, 768))  # t4622: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4622 = prims.reshape(t4621, (4, 64, 768))  # t4622: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4621\n",
      "  t5770 = torch.unsqueeze(t385, 2)  # t5770: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5770 = ltorch.unsqueeze(t385, 2)  # t5770: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t5770 = prims.broadcast_in_dim(t385, [4, 64, 1], [0, 1])  # t5770: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t385\n",
      "  t4649 = Tensor.expand(t5770, [4, 64, 1])  # t4649: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4649 = ltorch.expand(t5770, [4, 64, 1])  # t4649: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t4649 = prims.broadcast_in_dim(t5770, (4, 64, 1), (0, 1, 2))  # t4649: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t5770\n",
      "  t4650 = Tensor.expand(t4649, (4, 64, 768))  # t4650: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4650 = ltorch.expand(t4649, (4, 64, 768))  # t4650: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4650 = prims.broadcast_in_dim(t4649, (4, 64, 768), (0, 1, 2))  # t4650: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4649\n",
      "  [t4628, t4631, t4657, t4665] = nvFusion36(t381, t389, t391, t394, t4585, t4622, t4650)\n",
      "    # t392 = prims.broadcast_in_dim(t389, (4, 64, 768), (0, 1, 2))  # t392: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t393 = prims.mul(t391, t392)  # t393: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4628 = prims.sum(t4622, (0, 1))  # t4628: \"cuda:0 f32[768]\"\n",
      "    # t4629 = prims.mul(t394, t4622)  # t4629: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4630 = prims.mul(t393, t4622)  # t4630: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4631 = prims.sum(t4630, (0, 1))  # t4631: \"cuda:0 f32[768]\"\n",
      "    # t4632 = prims.mul(t392, t4629)  # t4632: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4633 = prims.mul(t391, t4629)  # t4633: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4634 = prims.sum(t4633, (2,))  # t4634: \"cuda:0 f32[4, 64]\"\n",
      "    # t4635 = prims.broadcast_in_dim(t4634, [4, 64, 1], [0, 1])  # t4635: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4636 = prims.neg(t4632)  # t4636: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4637 = prims.sum(t4636, (2,))  # t4637: \"cuda:0 f32[4, 64]\"\n",
      "    # t4638 = prims.broadcast_in_dim(t4637, [4, 64, 1], [0, 1])  # t4638: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4639 = prims.mul(-0.5, t4635)  # t4639: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4640 = prims.pow(t389, 3.0)  # t4640: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4641 = prims.mul(t4639, t4640)  # t4641: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4642 = prims.sum(t4638, (2,))  # t4642: \"cuda:0 f32[4, 64]\"\n",
      "    # t4643 = prims.sum(t4641, (2,))  # t4643: \"cuda:0 f32[4, 64]\"\n",
      "    # t4644 = prims.broadcast_in_dim(t4642, [4, 64, 1], [0, 1])  # t4644: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4645 = prims.broadcast_in_dim(t4644, (4, 64, 768), (0, 1, 2))  # t4645: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4646 = prims.mul(0.0013020833333333333, t4645)  # t4646: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4647 = prims.broadcast_in_dim(t4643, [4, 64, 1], [0, 1])  # t4647: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4648 = prims.broadcast_in_dim(t4647, (4, 64, 768), (0, 1, 2))  # t4648: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4651 = prims.mul(2.0, t4648)  # t4651: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4652 = prims.sub(t381, t4650)  # t4652: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4653 = prims.mul(t4651, t4652)  # t4653: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4654 = prims.div(t4653, 768.0)  # t4654: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4655 = prims.add(t4646, t4654)  # t4655: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4656 = prims.add(t4632, t4655)  # t4656: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4657 = prims.add(t4585, t4656)  # t4657: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4665 = prims.sum(t4657, (0, 1))  # t4665: \"cuda:0 f32[768]\"\n",
      "  del t381, t389, t391, t394, t4585, t4622, t4650\n",
      "  t4658 = torch.reshape(t4657, (-1, 768))  # t4658: \"cuda:0 f32[256, 768]\"\n",
      "    # t4658 = ltorch.reshape(t4657, (-1, 768))  # t4658: \"cuda:0 f32[256, 768]\"\n",
      "      # t4658 = prims.reshape(t4657, (256, 768))  # t4658: \"cuda:0 f32[256, 768]\"\n",
      "  t4662 = torch.permute(t4658, (1, 0))  # t4662: \"cuda:0 f32[768, 256]\"\n",
      "    # t4662 = ltorch.permute(t4658, (1, 0))  # t4662: \"cuda:0 f32[768, 256]\"\n",
      "      # t4662 = prims.transpose(t4658, (1, 0))  # t4662: \"cuda:0 f32[768, 256]\"\n",
      "  t4659 = torch.matmul(t4658, t_transformer_h_4_mlp_c_proj_weight)  # t4659: \"cuda:0 f32[256, 3072]\"\n",
      "    # t4659 = ltorch.matmul(t4658, t_transformer_h_4_mlp_c_proj_weight)  # t4659: \"cuda:0 f32[256, 3072]\"\n",
      "      # t4659 = prims.matmul(t4658, t_transformer_h_4_mlp_c_proj_weight)  # t4659: \"cuda:0 f32[256, 3072]\"\n",
      "  del t4658, t_transformer_h_4_mlp_c_proj_weight\n",
      "  t4664 = torch.matmul(t4662, t4663)  # t4664: \"cuda:0 f32[768, 3072]\"\n",
      "    # t4664 = ltorch.matmul(t4662, t4663)  # t4664: \"cuda:0 f32[768, 3072]\"\n",
      "      # t4664 = prims.matmul(t4662, t4663)  # t4664: \"cuda:0 f32[768, 3072]\"\n",
      "  del t4662, t4663\n",
      "  t4660 = torch.reshape(t4659, (4, 64, 3072))  # t4660: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4660 = ltorch.reshape(t4659, (4, 64, 3072))  # t4660: \"cuda:0 f32[4, 64, 3072]\"\n",
      "      # t4660 = prims.reshape(t4659, (4, 64, 3072))  # t4660: \"cuda:0 f32[4, 64, 3072]\"\n",
      "  del t4659\n",
      "  [t4681, t4689] = nvFusion37(f338, f340, f342, f344, t371, t4660)\n",
      "    # t372 = prims.mul(0.5, t371)  # t372: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t373 = prims.pow(t371, 3.0)  # t373: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t374 = prims.mul(0.044715, t373)  # t374: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t375 = prims.add(t371, t374)  # t375: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t376 = prims.mul(0.7978845608028654, t375)  # t376: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t377 = prims.tanh(t376)  # t377: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t378 = prims.add(1.0, t377)  # t378: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4666 = prims.mul(t378, t4660)  # t4666: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4667 = prims.mul(t372, t4660)  # t4667: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4668 = prims.mul(t377, t377)  # t4668: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4669 = prims.sub(1.0, t4668)  # t4669: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4670 = prims.mul(t4667, t4669)  # t4670: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4671 = prims.mul(f344, t4670)  # t4671: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4672 = prims.mul(f342, t4671)  # t4672: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4674 = prims.mul(t4672, f340)  # t4674: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4675 = prims.pow(t371, 2.0)  # t4675: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4676 = prims.mul(t4674, t4675)  # t4676: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4679 = prims.add(t4671, t4676)  # t4679: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4680 = prims.mul(f338, t4666)  # t4680: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4681 = prims.add(t4679, t4680)  # t4681: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4689 = prims.sum(t4681, (0, 1))  # t4689: \"cuda:0 f32[3072]\"\n",
      "  del f338, f340, f342, f344, t371, t4660\n",
      "  t4682 = torch.reshape(t4681, (-1, 3072))  # t4682: \"cuda:0 f32[256, 3072]\"\n",
      "    # t4682 = ltorch.reshape(t4681, (-1, 3072))  # t4682: \"cuda:0 f32[256, 3072]\"\n",
      "      # t4682 = prims.reshape(t4681, (256, 3072))  # t4682: \"cuda:0 f32[256, 3072]\"\n",
      "  del t4681\n",
      "  t4686 = torch.permute(t4682, (1, 0))  # t4686: \"cuda:0 f32[3072, 256]\"\n",
      "    # t4686 = ltorch.permute(t4682, (1, 0))  # t4686: \"cuda:0 f32[3072, 256]\"\n",
      "      # t4686 = prims.transpose(t4682, (1, 0))  # t4686: \"cuda:0 f32[3072, 256]\"\n",
      "  t4683 = torch.matmul(t4682, t_transformer_h_4_mlp_c_fc_weight)  # t4683: \"cuda:0 f32[256, 768]\"\n",
      "    # t4683 = ltorch.matmul(t4682, t_transformer_h_4_mlp_c_fc_weight)  # t4683: \"cuda:0 f32[256, 768]\"\n",
      "      # t4683 = prims.matmul(t4682, t_transformer_h_4_mlp_c_fc_weight)  # t4683: \"cuda:0 f32[256, 768]\"\n",
      "  del t4682, t_transformer_h_4_mlp_c_fc_weight\n",
      "  t4688 = torch.matmul(t4686, t4687)  # t4688: \"cuda:0 f32[3072, 768]\"\n",
      "    # t4688 = ltorch.matmul(t4686, t4687)  # t4688: \"cuda:0 f32[3072, 768]\"\n",
      "      # t4688 = prims.matmul(t4686, t4687)  # t4688: \"cuda:0 f32[3072, 768]\"\n",
      "  del t4686, t4687\n",
      "  t4684 = torch.reshape(t4683, (4, 64, 768))  # t4684: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4684 = ltorch.reshape(t4683, (4, 64, 768))  # t4684: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4684 = prims.reshape(t4683, (4, 64, 768))  # t4684: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4683\n",
      "  t5779 = torch.unsqueeze(t358, 2)  # t5779: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5779 = ltorch.unsqueeze(t358, 2)  # t5779: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t5779 = prims.broadcast_in_dim(t358, [4, 64, 1], [0, 1])  # t5779: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t358\n",
      "  t4711 = Tensor.expand(t5779, [4, 64, 1])  # t4711: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4711 = ltorch.expand(t5779, [4, 64, 1])  # t4711: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t4711 = prims.broadcast_in_dim(t5779, (4, 64, 1), (0, 1, 2))  # t4711: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t5779\n",
      "  t4712 = Tensor.expand(t4711, (4, 64, 768))  # t4712: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4712 = ltorch.expand(t4711, (4, 64, 768))  # t4712: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4712 = prims.broadcast_in_dim(t4711, (4, 64, 768), (0, 1, 2))  # t4712: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4711\n",
      "  [t4690, t4693, t4719, t4727] = nvFusion38(t354, t362, t364, t367, t4657, t4684, t4712)\n",
      "    # t365 = prims.broadcast_in_dim(t362, (4, 64, 768), (0, 1, 2))  # t365: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t366 = prims.mul(t364, t365)  # t366: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4690 = prims.sum(t4684, (0, 1))  # t4690: \"cuda:0 f32[768]\"\n",
      "    # t4691 = prims.mul(t367, t4684)  # t4691: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4692 = prims.mul(t366, t4684)  # t4692: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4693 = prims.sum(t4692, (0, 1))  # t4693: \"cuda:0 f32[768]\"\n",
      "    # t4694 = prims.mul(t365, t4691)  # t4694: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4695 = prims.mul(t364, t4691)  # t4695: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4696 = prims.sum(t4695, (2,))  # t4696: \"cuda:0 f32[4, 64]\"\n",
      "    # t4697 = prims.broadcast_in_dim(t4696, [4, 64, 1], [0, 1])  # t4697: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4698 = prims.neg(t4694)  # t4698: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4699 = prims.sum(t4698, (2,))  # t4699: \"cuda:0 f32[4, 64]\"\n",
      "    # t4700 = prims.broadcast_in_dim(t4699, [4, 64, 1], [0, 1])  # t4700: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4701 = prims.mul(-0.5, t4697)  # t4701: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4702 = prims.pow(t362, 3.0)  # t4702: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4703 = prims.mul(t4701, t4702)  # t4703: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4704 = prims.sum(t4700, (2,))  # t4704: \"cuda:0 f32[4, 64]\"\n",
      "    # t4705 = prims.sum(t4703, (2,))  # t4705: \"cuda:0 f32[4, 64]\"\n",
      "    # t4706 = prims.broadcast_in_dim(t4704, [4, 64, 1], [0, 1])  # t4706: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4707 = prims.broadcast_in_dim(t4706, (4, 64, 768), (0, 1, 2))  # t4707: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4708 = prims.mul(0.0013020833333333333, t4707)  # t4708: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4709 = prims.broadcast_in_dim(t4705, [4, 64, 1], [0, 1])  # t4709: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4710 = prims.broadcast_in_dim(t4709, (4, 64, 768), (0, 1, 2))  # t4710: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4713 = prims.mul(2.0, t4710)  # t4713: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4714 = prims.sub(t354, t4712)  # t4714: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4715 = prims.mul(t4713, t4714)  # t4715: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4716 = prims.div(t4715, 768.0)  # t4716: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4717 = prims.add(t4708, t4716)  # t4717: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4718 = prims.add(t4694, t4717)  # t4718: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4719 = prims.add(t4657, t4718)  # t4719: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4727 = prims.sum(t4719, (0, 1))  # t4727: \"cuda:0 f32[768]\"\n",
      "  del t354, t362, t364, t367, t4657, t4684, t4712\n",
      "  t4720 = torch.reshape(t4719, (-1, 768))  # t4720: \"cuda:0 f32[256, 768]\"\n",
      "    # t4720 = ltorch.reshape(t4719, (-1, 768))  # t4720: \"cuda:0 f32[256, 768]\"\n",
      "      # t4720 = prims.reshape(t4719, (256, 768))  # t4720: \"cuda:0 f32[256, 768]\"\n",
      "  t4724 = torch.permute(t4720, (1, 0))  # t4724: \"cuda:0 f32[768, 256]\"\n",
      "    # t4724 = ltorch.permute(t4720, (1, 0))  # t4724: \"cuda:0 f32[768, 256]\"\n",
      "      # t4724 = prims.transpose(t4720, (1, 0))  # t4724: \"cuda:0 f32[768, 256]\"\n",
      "  t4721 = torch.matmul(t4720, t_transformer_h_4_attn_c_proj_weight)  # t4721: \"cuda:0 f32[256, 768]\"\n",
      "    # t4721 = ltorch.matmul(t4720, t_transformer_h_4_attn_c_proj_weight)  # t4721: \"cuda:0 f32[256, 768]\"\n",
      "      # t4721 = prims.matmul(t4720, t_transformer_h_4_attn_c_proj_weight)  # t4721: \"cuda:0 f32[256, 768]\"\n",
      "  del t4720, t_transformer_h_4_attn_c_proj_weight\n",
      "  t4726 = torch.matmul(t4724, t4725)  # t4726: \"cuda:0 f32[768, 768]\"\n",
      "    # t4726 = ltorch.matmul(t4724, t4725)  # t4726: \"cuda:0 f32[768, 768]\"\n",
      "      # t4726 = prims.matmul(t4724, t4725)  # t4726: \"cuda:0 f32[768, 768]\"\n",
      "  del t4724, t4725\n",
      "  t4722 = torch.reshape(t4721, (4, 64, 768))  # t4722: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4722 = ltorch.reshape(t4721, (4, 64, 768))  # t4722: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4722 = prims.reshape(t4721, (4, 64, 768))  # t4722: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4721\n",
      "  t4728 = torch.reshape(t4722, (4, 64, 12, 64))  # t4728: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t4728 = ltorch.reshape(t4722, (4, 64, 12, 64))  # t4728: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t4728 = prims.reshape(t4722, (4, 64, 12, 64))  # t4728: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t4722\n",
      "  t4729 = torch.permute(t4728, (0, 2, 1, 3))  # t4729: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4729 = ltorch.permute(t4728, (0, 2, 1, 3))  # t4729: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4729 = prims.transpose(t4728, (0, 2, 1, 3))  # t4729: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4728\n",
      "  t4731 = torch.matmul(t4729, t4730)  # t4731: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4731 = ltorch.matmul(t4729, t4730)  # t4731: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4731 = prims.matmul(t4729, t4730)  # t4731: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4730\n",
      "  t4733 = torch.matmul(t4732, t4729)  # t4733: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4733 = ltorch.matmul(t4732, t4729)  # t4733: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4733 = prims.matmul(t4732, t4729)  # t4733: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4732, t4729\n",
      "  [t4741] = nvFusion39(f318, i322, t338, t348, t4731)\n",
      "    # t4734 = prims.mul(t348, t4731)  # t4734: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4735 = prims.sum(t4734, (3,))  # t4735: \"cuda:0 f32[4, 12, 64]\"\n",
      "    # t4736 = prims.broadcast_in_dim(t4735, [4, 12, 64, 1], [0, 1, 2])  # t4736: \"cuda:0 f32[4, 12, 64, 1]\"\n",
      "    # t4737 = prims.broadcast_in_dim(t4736, (4, 12, 64, 64), (0, 1, 2, 3))  # t4737: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4738 = prims.sub(t4731, t4737)  # t4738: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4739 = prims.mul(t348, t4738)  # t4739: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4740 = prims.where(t338, 0.0, t4739)  # t4740: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4741 = prims.mul(f318, t4740)  # t4741: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del f318, i322, t338, t348, t4731\n",
      "  t4743 = torch.matmul(t4741, t4742)  # t4743: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4743 = ltorch.matmul(t4741, t4742)  # t4743: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4743 = prims.matmul(t4741, t4742)  # t4743: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4742\n",
      "  t4745 = torch.matmul(t4744, t4741)  # t4745: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4745 = ltorch.matmul(t4744, t4741)  # t4745: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4745 = prims.matmul(t4744, t4741)  # t4745: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4744, t4741\n",
      "  t4746 = torch.permute(t4745, (0, 1, 3, 2))  # t4746: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4746 = ltorch.permute(t4745, (0, 1, 3, 2))  # t4746: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4746 = prims.transpose(t4745, (0, 1, 3, 2))  # t4746: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4745\n",
      "  t4747 = torch.permute(t4733, (0, 2, 1, 3))  # t4747: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t4747 = ltorch.permute(t4733, (0, 2, 1, 3))  # t4747: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t4747 = prims.transpose(t4733, (0, 2, 1, 3))  # t4747: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t4733\n",
      "  t4748 = torch.reshape(t4747, (4, 64, 768))  # t4748: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4748 = ltorch.reshape(t4747, (4, 64, 768))  # t4748: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4748 = prims.reshape(t4747, (4, 64, 768))  # t4748: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4747\n",
      "  t4749 = torch.permute(t4746, (0, 2, 1, 3))  # t4749: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t4749 = ltorch.permute(t4746, (0, 2, 1, 3))  # t4749: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t4749 = prims.transpose(t4746, (0, 2, 1, 3))  # t4749: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t4746\n",
      "  t4750 = torch.reshape(t4749, (4, 64, 768))  # t4750: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4750 = ltorch.reshape(t4749, (4, 64, 768))  # t4750: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4750 = prims.reshape(t4749, (4, 64, 768))  # t4750: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4749\n",
      "  t4751 = torch.permute(t4743, (0, 2, 1, 3))  # t4751: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t4751 = ltorch.permute(t4743, (0, 2, 1, 3))  # t4751: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t4751 = prims.transpose(t4743, (0, 2, 1, 3))  # t4751: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t4743\n",
      "  t4752 = torch.reshape(t4751, (4, 64, 768))  # t4752: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4752 = ltorch.reshape(t4751, (4, 64, 768))  # t4752: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4752 = prims.reshape(t4751, (4, 64, 768))  # t4752: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4751\n",
      "  [t4753, t4761] = nvFusion40(i284, t4748, t4750, t4752)\n",
      "    # t4753 = prims.cat((t4752, t4750, t4748), i284)  # t4753: \"cuda:0 f32[4, 64, 2304]\"\n",
      "    # t4761 = prims.sum(t4753, (0, 1))  # t4761: \"cuda:0 f32[2304]\"\n",
      "  del i284, t4748, t4750, t4752\n",
      "  t4754 = torch.reshape(t4753, (-1, 2304))  # t4754: \"cuda:0 f32[256, 2304]\"\n",
      "    # t4754 = ltorch.reshape(t4753, (-1, 2304))  # t4754: \"cuda:0 f32[256, 2304]\"\n",
      "      # t4754 = prims.reshape(t4753, (256, 2304))  # t4754: \"cuda:0 f32[256, 2304]\"\n",
      "  del t4753\n",
      "  t4758 = torch.permute(t4754, (1, 0))  # t4758: \"cuda:0 f32[2304, 256]\"\n",
      "    # t4758 = ltorch.permute(t4754, (1, 0))  # t4758: \"cuda:0 f32[2304, 256]\"\n",
      "      # t4758 = prims.transpose(t4754, (1, 0))  # t4758: \"cuda:0 f32[2304, 256]\"\n",
      "  t4755 = torch.matmul(t4754, t_transformer_h_4_attn_c_attn_weight)  # t4755: \"cuda:0 f32[256, 768]\"\n",
      "    # t4755 = ltorch.matmul(t4754, t_transformer_h_4_attn_c_attn_weight)  # t4755: \"cuda:0 f32[256, 768]\"\n",
      "      # t4755 = prims.matmul(t4754, t_transformer_h_4_attn_c_attn_weight)  # t4755: \"cuda:0 f32[256, 768]\"\n",
      "  del t4754, t_transformer_h_4_attn_c_attn_weight\n",
      "  t4760 = torch.matmul(t4758, t4759)  # t4760: \"cuda:0 f32[2304, 768]\"\n",
      "    # t4760 = ltorch.matmul(t4758, t4759)  # t4760: \"cuda:0 f32[2304, 768]\"\n",
      "      # t4760 = prims.matmul(t4758, t4759)  # t4760: \"cuda:0 f32[2304, 768]\"\n",
      "  del t4758, t4759\n",
      "  t4756 = torch.reshape(t4755, (4, 64, 768))  # t4756: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4756 = ltorch.reshape(t4755, (4, 64, 768))  # t4756: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4756 = prims.reshape(t4755, (4, 64, 768))  # t4756: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4755\n",
      "  t5797 = torch.unsqueeze(t310, 2)  # t5797: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5797 = ltorch.unsqueeze(t310, 2)  # t5797: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t5797 = prims.broadcast_in_dim(t310, [4, 64, 1], [0, 1])  # t5797: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t310\n",
      "  t4783 = Tensor.expand(t5797, [4, 64, 1])  # t4783: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4783 = ltorch.expand(t5797, [4, 64, 1])  # t4783: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t4783 = prims.broadcast_in_dim(t5797, (4, 64, 1), (0, 1, 2))  # t4783: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t5797\n",
      "  t4784 = Tensor.expand(t4783, (4, 64, 768))  # t4784: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4784 = ltorch.expand(t4783, (4, 64, 768))  # t4784: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4784 = prims.broadcast_in_dim(t4783, (4, 64, 768), (0, 1, 2))  # t4784: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4783\n",
      "  [t4762, t4765, t4791, t4799] = nvFusion41(t306, t314, t316, t319, t4719, t4756, t4784)\n",
      "    # t317 = prims.broadcast_in_dim(t314, (4, 64, 768), (0, 1, 2))  # t317: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t318 = prims.mul(t316, t317)  # t318: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4762 = prims.sum(t4756, (0, 1))  # t4762: \"cuda:0 f32[768]\"\n",
      "    # t4763 = prims.mul(t319, t4756)  # t4763: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4764 = prims.mul(t318, t4756)  # t4764: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4765 = prims.sum(t4764, (0, 1))  # t4765: \"cuda:0 f32[768]\"\n",
      "    # t4766 = prims.mul(t317, t4763)  # t4766: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4767 = prims.mul(t316, t4763)  # t4767: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4768 = prims.sum(t4767, (2,))  # t4768: \"cuda:0 f32[4, 64]\"\n",
      "    # t4769 = prims.broadcast_in_dim(t4768, [4, 64, 1], [0, 1])  # t4769: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4770 = prims.neg(t4766)  # t4770: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4771 = prims.sum(t4770, (2,))  # t4771: \"cuda:0 f32[4, 64]\"\n",
      "    # t4772 = prims.broadcast_in_dim(t4771, [4, 64, 1], [0, 1])  # t4772: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4773 = prims.mul(-0.5, t4769)  # t4773: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4774 = prims.pow(t314, 3.0)  # t4774: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4775 = prims.mul(t4773, t4774)  # t4775: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4776 = prims.sum(t4772, (2,))  # t4776: \"cuda:0 f32[4, 64]\"\n",
      "    # t4777 = prims.sum(t4775, (2,))  # t4777: \"cuda:0 f32[4, 64]\"\n",
      "    # t4778 = prims.broadcast_in_dim(t4776, [4, 64, 1], [0, 1])  # t4778: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4779 = prims.broadcast_in_dim(t4778, (4, 64, 768), (0, 1, 2))  # t4779: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4780 = prims.mul(0.0013020833333333333, t4779)  # t4780: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4781 = prims.broadcast_in_dim(t4777, [4, 64, 1], [0, 1])  # t4781: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4782 = prims.broadcast_in_dim(t4781, (4, 64, 768), (0, 1, 2))  # t4782: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4785 = prims.mul(2.0, t4782)  # t4785: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4786 = prims.sub(t306, t4784)  # t4786: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4787 = prims.mul(t4785, t4786)  # t4787: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4788 = prims.div(t4787, 768.0)  # t4788: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4789 = prims.add(t4780, t4788)  # t4789: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4790 = prims.add(t4766, t4789)  # t4790: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4791 = prims.add(t4719, t4790)  # t4791: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4799 = prims.sum(t4791, (0, 1))  # t4799: \"cuda:0 f32[768]\"\n",
      "  del t306, t314, t316, t319, t4719, t4756, t4784\n",
      "  t4792 = torch.reshape(t4791, (-1, 768))  # t4792: \"cuda:0 f32[256, 768]\"\n",
      "    # t4792 = ltorch.reshape(t4791, (-1, 768))  # t4792: \"cuda:0 f32[256, 768]\"\n",
      "      # t4792 = prims.reshape(t4791, (256, 768))  # t4792: \"cuda:0 f32[256, 768]\"\n",
      "  t4796 = torch.permute(t4792, (1, 0))  # t4796: \"cuda:0 f32[768, 256]\"\n",
      "    # t4796 = ltorch.permute(t4792, (1, 0))  # t4796: \"cuda:0 f32[768, 256]\"\n",
      "      # t4796 = prims.transpose(t4792, (1, 0))  # t4796: \"cuda:0 f32[768, 256]\"\n",
      "  t4798 = torch.matmul(t4796, t4797)  # t4798: \"cuda:0 f32[768, 3072]\"\n",
      "    # t4798 = ltorch.matmul(t4796, t4797)  # t4798: \"cuda:0 f32[768, 3072]\"\n",
      "      # t4798 = prims.matmul(t4796, t4797)  # t4798: \"cuda:0 f32[768, 3072]\"\n",
      "  del t4796, t4797\n",
      "  t4793 = torch.matmul(t4792, t_transformer_h_3_mlp_c_proj_weight)  # t4793: \"cuda:0 f32[256, 3072]\"\n",
      "    # t4793 = ltorch.matmul(t4792, t_transformer_h_3_mlp_c_proj_weight)  # t4793: \"cuda:0 f32[256, 3072]\"\n",
      "      # t4793 = prims.matmul(t4792, t_transformer_h_3_mlp_c_proj_weight)  # t4793: \"cuda:0 f32[256, 3072]\"\n",
      "  del t4792, t_transformer_h_3_mlp_c_proj_weight\n",
      "  t4794 = torch.reshape(t4793, (4, 64, 3072))  # t4794: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4794 = ltorch.reshape(t4793, (4, 64, 3072))  # t4794: \"cuda:0 f32[4, 64, 3072]\"\n",
      "      # t4794 = prims.reshape(t4793, (4, 64, 3072))  # t4794: \"cuda:0 f32[4, 64, 3072]\"\n",
      "  del t4793\n",
      "  [t4815, t4823] = nvFusion42(f270, f272, f274, f276, t296, t4794)\n",
      "    # t297 = prims.mul(0.5, t296)  # t297: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t298 = prims.pow(t296, 3.0)  # t298: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t299 = prims.mul(0.044715, t298)  # t299: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t300 = prims.add(t296, t299)  # t300: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t301 = prims.mul(0.7978845608028654, t300)  # t301: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t302 = prims.tanh(t301)  # t302: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t303 = prims.add(1.0, t302)  # t303: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4800 = prims.mul(t303, t4794)  # t4800: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4801 = prims.mul(t297, t4794)  # t4801: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4802 = prims.mul(t302, t302)  # t4802: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4803 = prims.sub(1.0, t4802)  # t4803: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4804 = prims.mul(t4801, t4803)  # t4804: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4805 = prims.mul(f276, t4804)  # t4805: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4806 = prims.mul(f274, t4805)  # t4806: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4808 = prims.mul(t4806, f272)  # t4808: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4809 = prims.pow(t296, 2.0)  # t4809: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4810 = prims.mul(t4808, t4809)  # t4810: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4813 = prims.add(t4805, t4810)  # t4813: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4814 = prims.mul(f270, t4800)  # t4814: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4815 = prims.add(t4813, t4814)  # t4815: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4823 = prims.sum(t4815, (0, 1))  # t4823: \"cuda:0 f32[3072]\"\n",
      "  del f270, f272, f274, f276, t296, t4794\n",
      "  t4816 = torch.reshape(t4815, (-1, 3072))  # t4816: \"cuda:0 f32[256, 3072]\"\n",
      "    # t4816 = ltorch.reshape(t4815, (-1, 3072))  # t4816: \"cuda:0 f32[256, 3072]\"\n",
      "      # t4816 = prims.reshape(t4815, (256, 3072))  # t4816: \"cuda:0 f32[256, 3072]\"\n",
      "  del t4815\n",
      "  t4820 = torch.permute(t4816, (1, 0))  # t4820: \"cuda:0 f32[3072, 256]\"\n",
      "    # t4820 = ltorch.permute(t4816, (1, 0))  # t4820: \"cuda:0 f32[3072, 256]\"\n",
      "      # t4820 = prims.transpose(t4816, (1, 0))  # t4820: \"cuda:0 f32[3072, 256]\"\n",
      "  t4822 = torch.matmul(t4820, t4821)  # t4822: \"cuda:0 f32[3072, 768]\"\n",
      "    # t4822 = ltorch.matmul(t4820, t4821)  # t4822: \"cuda:0 f32[3072, 768]\"\n",
      "      # t4822 = prims.matmul(t4820, t4821)  # t4822: \"cuda:0 f32[3072, 768]\"\n",
      "  del t4820, t4821\n",
      "  t4817 = torch.matmul(t4816, t_transformer_h_3_mlp_c_fc_weight)  # t4817: \"cuda:0 f32[256, 768]\"\n",
      "    # t4817 = ltorch.matmul(t4816, t_transformer_h_3_mlp_c_fc_weight)  # t4817: \"cuda:0 f32[256, 768]\"\n",
      "      # t4817 = prims.matmul(t4816, t_transformer_h_3_mlp_c_fc_weight)  # t4817: \"cuda:0 f32[256, 768]\"\n",
      "  del t4816, t_transformer_h_3_mlp_c_fc_weight\n",
      "  t4818 = torch.reshape(t4817, (4, 64, 768))  # t4818: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4818 = ltorch.reshape(t4817, (4, 64, 768))  # t4818: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4818 = prims.reshape(t4817, (4, 64, 768))  # t4818: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4817\n",
      "  t5806 = torch.unsqueeze(t283, 2)  # t5806: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5806 = ltorch.unsqueeze(t283, 2)  # t5806: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t5806 = prims.broadcast_in_dim(t283, [4, 64, 1], [0, 1])  # t5806: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t283\n",
      "  t4845 = Tensor.expand(t5806, [4, 64, 1])  # t4845: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4845 = ltorch.expand(t5806, [4, 64, 1])  # t4845: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t4845 = prims.broadcast_in_dim(t5806, (4, 64, 1), (0, 1, 2))  # t4845: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t5806\n",
      "  t4846 = Tensor.expand(t4845, (4, 64, 768))  # t4846: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4846 = ltorch.expand(t4845, (4, 64, 768))  # t4846: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4846 = prims.broadcast_in_dim(t4845, (4, 64, 768), (0, 1, 2))  # t4846: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4845\n",
      "  [t4824, t4827, t4853, t4861] = nvFusion43(t279, t287, t289, t292, t4791, t4818, t4846)\n",
      "    # t290 = prims.broadcast_in_dim(t287, (4, 64, 768), (0, 1, 2))  # t290: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t291 = prims.mul(t289, t290)  # t291: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4824 = prims.sum(t4818, (0, 1))  # t4824: \"cuda:0 f32[768]\"\n",
      "    # t4825 = prims.mul(t292, t4818)  # t4825: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4826 = prims.mul(t291, t4818)  # t4826: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4827 = prims.sum(t4826, (0, 1))  # t4827: \"cuda:0 f32[768]\"\n",
      "    # t4828 = prims.mul(t290, t4825)  # t4828: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4829 = prims.mul(t289, t4825)  # t4829: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4830 = prims.sum(t4829, (2,))  # t4830: \"cuda:0 f32[4, 64]\"\n",
      "    # t4831 = prims.broadcast_in_dim(t4830, [4, 64, 1], [0, 1])  # t4831: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4832 = prims.neg(t4828)  # t4832: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4833 = prims.sum(t4832, (2,))  # t4833: \"cuda:0 f32[4, 64]\"\n",
      "    # t4834 = prims.broadcast_in_dim(t4833, [4, 64, 1], [0, 1])  # t4834: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4835 = prims.mul(-0.5, t4831)  # t4835: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4836 = prims.pow(t287, 3.0)  # t4836: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4837 = prims.mul(t4835, t4836)  # t4837: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4838 = prims.sum(t4834, (2,))  # t4838: \"cuda:0 f32[4, 64]\"\n",
      "    # t4839 = prims.sum(t4837, (2,))  # t4839: \"cuda:0 f32[4, 64]\"\n",
      "    # t4840 = prims.broadcast_in_dim(t4838, [4, 64, 1], [0, 1])  # t4840: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4841 = prims.broadcast_in_dim(t4840, (4, 64, 768), (0, 1, 2))  # t4841: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4842 = prims.mul(0.0013020833333333333, t4841)  # t4842: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4843 = prims.broadcast_in_dim(t4839, [4, 64, 1], [0, 1])  # t4843: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4844 = prims.broadcast_in_dim(t4843, (4, 64, 768), (0, 1, 2))  # t4844: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4847 = prims.mul(2.0, t4844)  # t4847: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4848 = prims.sub(t279, t4846)  # t4848: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4849 = prims.mul(t4847, t4848)  # t4849: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4850 = prims.div(t4849, 768.0)  # t4850: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4851 = prims.add(t4842, t4850)  # t4851: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4852 = prims.add(t4828, t4851)  # t4852: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4853 = prims.add(t4791, t4852)  # t4853: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4861 = prims.sum(t4853, (0, 1))  # t4861: \"cuda:0 f32[768]\"\n",
      "  del t279, t287, t289, t292, t4791, t4818, t4846\n",
      "  t4854 = torch.reshape(t4853, (-1, 768))  # t4854: \"cuda:0 f32[256, 768]\"\n",
      "    # t4854 = ltorch.reshape(t4853, (-1, 768))  # t4854: \"cuda:0 f32[256, 768]\"\n",
      "      # t4854 = prims.reshape(t4853, (256, 768))  # t4854: \"cuda:0 f32[256, 768]\"\n",
      "  t4858 = torch.permute(t4854, (1, 0))  # t4858: \"cuda:0 f32[768, 256]\"\n",
      "    # t4858 = ltorch.permute(t4854, (1, 0))  # t4858: \"cuda:0 f32[768, 256]\"\n",
      "      # t4858 = prims.transpose(t4854, (1, 0))  # t4858: \"cuda:0 f32[768, 256]\"\n",
      "  t4855 = torch.matmul(t4854, t_transformer_h_3_attn_c_proj_weight)  # t4855: \"cuda:0 f32[256, 768]\"\n",
      "    # t4855 = ltorch.matmul(t4854, t_transformer_h_3_attn_c_proj_weight)  # t4855: \"cuda:0 f32[256, 768]\"\n",
      "      # t4855 = prims.matmul(t4854, t_transformer_h_3_attn_c_proj_weight)  # t4855: \"cuda:0 f32[256, 768]\"\n",
      "  del t4854, t_transformer_h_3_attn_c_proj_weight\n",
      "  t4860 = torch.matmul(t4858, t4859)  # t4860: \"cuda:0 f32[768, 768]\"\n",
      "    # t4860 = ltorch.matmul(t4858, t4859)  # t4860: \"cuda:0 f32[768, 768]\"\n",
      "      # t4860 = prims.matmul(t4858, t4859)  # t4860: \"cuda:0 f32[768, 768]\"\n",
      "  del t4858, t4859\n",
      "  t4856 = torch.reshape(t4855, (4, 64, 768))  # t4856: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4856 = ltorch.reshape(t4855, (4, 64, 768))  # t4856: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4856 = prims.reshape(t4855, (4, 64, 768))  # t4856: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4855\n",
      "  t4862 = torch.reshape(t4856, (4, 64, 12, 64))  # t4862: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t4862 = ltorch.reshape(t4856, (4, 64, 12, 64))  # t4862: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t4862 = prims.reshape(t4856, (4, 64, 12, 64))  # t4862: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t4856\n",
      "  t4863 = torch.permute(t4862, (0, 2, 1, 3))  # t4863: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4863 = ltorch.permute(t4862, (0, 2, 1, 3))  # t4863: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4863 = prims.transpose(t4862, (0, 2, 1, 3))  # t4863: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4862\n",
      "  t4865 = torch.matmul(t4863, t4864)  # t4865: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4865 = ltorch.matmul(t4863, t4864)  # t4865: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4865 = prims.matmul(t4863, t4864)  # t4865: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4864\n",
      "  t4867 = torch.matmul(t4866, t4863)  # t4867: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4867 = ltorch.matmul(t4866, t4863)  # t4867: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4867 = prims.matmul(t4866, t4863)  # t4867: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4866, t4863\n",
      "  [t4875] = nvFusion44(f250, i254, t263, t273, t4865)\n",
      "    # t4868 = prims.mul(t273, t4865)  # t4868: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4869 = prims.sum(t4868, (3,))  # t4869: \"cuda:0 f32[4, 12, 64]\"\n",
      "    # t4870 = prims.broadcast_in_dim(t4869, [4, 12, 64, 1], [0, 1, 2])  # t4870: \"cuda:0 f32[4, 12, 64, 1]\"\n",
      "    # t4871 = prims.broadcast_in_dim(t4870, (4, 12, 64, 64), (0, 1, 2, 3))  # t4871: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4872 = prims.sub(t4865, t4871)  # t4872: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4873 = prims.mul(t273, t4872)  # t4873: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4874 = prims.where(t263, 0.0, t4873)  # t4874: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4875 = prims.mul(f250, t4874)  # t4875: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del f250, i254, t263, t273, t4865\n",
      "  t4877 = torch.matmul(t4875, t4876)  # t4877: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4877 = ltorch.matmul(t4875, t4876)  # t4877: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4877 = prims.matmul(t4875, t4876)  # t4877: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4876\n",
      "  t4879 = torch.matmul(t4878, t4875)  # t4879: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4879 = ltorch.matmul(t4878, t4875)  # t4879: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4879 = prims.matmul(t4878, t4875)  # t4879: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4878, t4875\n",
      "  t4880 = torch.permute(t4879, (0, 1, 3, 2))  # t4880: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4880 = ltorch.permute(t4879, (0, 1, 3, 2))  # t4880: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4880 = prims.transpose(t4879, (0, 1, 3, 2))  # t4880: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4879\n",
      "  t4881 = torch.permute(t4867, (0, 2, 1, 3))  # t4881: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t4881 = ltorch.permute(t4867, (0, 2, 1, 3))  # t4881: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t4881 = prims.transpose(t4867, (0, 2, 1, 3))  # t4881: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t4867\n",
      "  t4882 = torch.reshape(t4881, (4, 64, 768))  # t4882: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4882 = ltorch.reshape(t4881, (4, 64, 768))  # t4882: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4882 = prims.reshape(t4881, (4, 64, 768))  # t4882: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4881\n",
      "  t4883 = torch.permute(t4880, (0, 2, 1, 3))  # t4883: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t4883 = ltorch.permute(t4880, (0, 2, 1, 3))  # t4883: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t4883 = prims.transpose(t4880, (0, 2, 1, 3))  # t4883: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t4880\n",
      "  t4884 = torch.reshape(t4883, (4, 64, 768))  # t4884: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4884 = ltorch.reshape(t4883, (4, 64, 768))  # t4884: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4884 = prims.reshape(t4883, (4, 64, 768))  # t4884: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4883\n",
      "  t4885 = torch.permute(t4877, (0, 2, 1, 3))  # t4885: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t4885 = ltorch.permute(t4877, (0, 2, 1, 3))  # t4885: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t4885 = prims.transpose(t4877, (0, 2, 1, 3))  # t4885: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t4877\n",
      "  t4886 = torch.reshape(t4885, (4, 64, 768))  # t4886: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4886 = ltorch.reshape(t4885, (4, 64, 768))  # t4886: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4886 = prims.reshape(t4885, (4, 64, 768))  # t4886: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4885\n",
      "  [t4887, t4895] = nvFusion45(i216, t4882, t4884, t4886)\n",
      "    # t4887 = prims.cat((t4886, t4884, t4882), i216)  # t4887: \"cuda:0 f32[4, 64, 2304]\"\n",
      "    # t4895 = prims.sum(t4887, (0, 1))  # t4895: \"cuda:0 f32[2304]\"\n",
      "  del i216, t4882, t4884, t4886\n",
      "  t4888 = torch.reshape(t4887, (-1, 2304))  # t4888: \"cuda:0 f32[256, 2304]\"\n",
      "    # t4888 = ltorch.reshape(t4887, (-1, 2304))  # t4888: \"cuda:0 f32[256, 2304]\"\n",
      "      # t4888 = prims.reshape(t4887, (256, 2304))  # t4888: \"cuda:0 f32[256, 2304]\"\n",
      "  del t4887\n",
      "  t4892 = torch.permute(t4888, (1, 0))  # t4892: \"cuda:0 f32[2304, 256]\"\n",
      "    # t4892 = ltorch.permute(t4888, (1, 0))  # t4892: \"cuda:0 f32[2304, 256]\"\n",
      "      # t4892 = prims.transpose(t4888, (1, 0))  # t4892: \"cuda:0 f32[2304, 256]\"\n",
      "  t4889 = torch.matmul(t4888, t_transformer_h_3_attn_c_attn_weight)  # t4889: \"cuda:0 f32[256, 768]\"\n",
      "    # t4889 = ltorch.matmul(t4888, t_transformer_h_3_attn_c_attn_weight)  # t4889: \"cuda:0 f32[256, 768]\"\n",
      "      # t4889 = prims.matmul(t4888, t_transformer_h_3_attn_c_attn_weight)  # t4889: \"cuda:0 f32[256, 768]\"\n",
      "  del t4888, t_transformer_h_3_attn_c_attn_weight\n",
      "  t4894 = torch.matmul(t4892, t4893)  # t4894: \"cuda:0 f32[2304, 768]\"\n",
      "    # t4894 = ltorch.matmul(t4892, t4893)  # t4894: \"cuda:0 f32[2304, 768]\"\n",
      "      # t4894 = prims.matmul(t4892, t4893)  # t4894: \"cuda:0 f32[2304, 768]\"\n",
      "  del t4892, t4893\n",
      "  t4890 = torch.reshape(t4889, (4, 64, 768))  # t4890: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4890 = ltorch.reshape(t4889, (4, 64, 768))  # t4890: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4890 = prims.reshape(t4889, (4, 64, 768))  # t4890: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4889\n",
      "  t5824 = torch.unsqueeze(t235, 2)  # t5824: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5824 = ltorch.unsqueeze(t235, 2)  # t5824: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t5824 = prims.broadcast_in_dim(t235, [4, 64, 1], [0, 1])  # t5824: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t235\n",
      "  t4917 = Tensor.expand(t5824, [4, 64, 1])  # t4917: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4917 = ltorch.expand(t5824, [4, 64, 1])  # t4917: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t4917 = prims.broadcast_in_dim(t5824, (4, 64, 1), (0, 1, 2))  # t4917: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t5824\n",
      "  t4918 = Tensor.expand(t4917, (4, 64, 768))  # t4918: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4918 = ltorch.expand(t4917, (4, 64, 768))  # t4918: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4918 = prims.broadcast_in_dim(t4917, (4, 64, 768), (0, 1, 2))  # t4918: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4917\n",
      "  [t4896, t4899, t4925, t4933] = nvFusion46(t231, t239, t241, t244, t4853, t4890, t4918)\n",
      "    # t242 = prims.broadcast_in_dim(t239, (4, 64, 768), (0, 1, 2))  # t242: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t243 = prims.mul(t241, t242)  # t243: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4896 = prims.sum(t4890, (0, 1))  # t4896: \"cuda:0 f32[768]\"\n",
      "    # t4897 = prims.mul(t244, t4890)  # t4897: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4898 = prims.mul(t243, t4890)  # t4898: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4899 = prims.sum(t4898, (0, 1))  # t4899: \"cuda:0 f32[768]\"\n",
      "    # t4900 = prims.mul(t242, t4897)  # t4900: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4901 = prims.mul(t241, t4897)  # t4901: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4902 = prims.sum(t4901, (2,))  # t4902: \"cuda:0 f32[4, 64]\"\n",
      "    # t4903 = prims.broadcast_in_dim(t4902, [4, 64, 1], [0, 1])  # t4903: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4904 = prims.neg(t4900)  # t4904: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4905 = prims.sum(t4904, (2,))  # t4905: \"cuda:0 f32[4, 64]\"\n",
      "    # t4906 = prims.broadcast_in_dim(t4905, [4, 64, 1], [0, 1])  # t4906: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4907 = prims.mul(-0.5, t4903)  # t4907: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4908 = prims.pow(t239, 3.0)  # t4908: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4909 = prims.mul(t4907, t4908)  # t4909: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4910 = prims.sum(t4906, (2,))  # t4910: \"cuda:0 f32[4, 64]\"\n",
      "    # t4911 = prims.sum(t4909, (2,))  # t4911: \"cuda:0 f32[4, 64]\"\n",
      "    # t4912 = prims.broadcast_in_dim(t4910, [4, 64, 1], [0, 1])  # t4912: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4913 = prims.broadcast_in_dim(t4912, (4, 64, 768), (0, 1, 2))  # t4913: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4914 = prims.mul(0.0013020833333333333, t4913)  # t4914: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4915 = prims.broadcast_in_dim(t4911, [4, 64, 1], [0, 1])  # t4915: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4916 = prims.broadcast_in_dim(t4915, (4, 64, 768), (0, 1, 2))  # t4916: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4919 = prims.mul(2.0, t4916)  # t4919: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4920 = prims.sub(t231, t4918)  # t4920: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4921 = prims.mul(t4919, t4920)  # t4921: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4922 = prims.div(t4921, 768.0)  # t4922: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4923 = prims.add(t4914, t4922)  # t4923: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4924 = prims.add(t4900, t4923)  # t4924: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4925 = prims.add(t4853, t4924)  # t4925: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4933 = prims.sum(t4925, (0, 1))  # t4933: \"cuda:0 f32[768]\"\n",
      "  del t231, t239, t241, t244, t4853, t4890, t4918\n",
      "  t4926 = torch.reshape(t4925, (-1, 768))  # t4926: \"cuda:0 f32[256, 768]\"\n",
      "    # t4926 = ltorch.reshape(t4925, (-1, 768))  # t4926: \"cuda:0 f32[256, 768]\"\n",
      "      # t4926 = prims.reshape(t4925, (256, 768))  # t4926: \"cuda:0 f32[256, 768]\"\n",
      "  t4930 = torch.permute(t4926, (1, 0))  # t4930: \"cuda:0 f32[768, 256]\"\n",
      "    # t4930 = ltorch.permute(t4926, (1, 0))  # t4930: \"cuda:0 f32[768, 256]\"\n",
      "      # t4930 = prims.transpose(t4926, (1, 0))  # t4930: \"cuda:0 f32[768, 256]\"\n",
      "  t4927 = torch.matmul(t4926, t_transformer_h_2_mlp_c_proj_weight)  # t4927: \"cuda:0 f32[256, 3072]\"\n",
      "    # t4927 = ltorch.matmul(t4926, t_transformer_h_2_mlp_c_proj_weight)  # t4927: \"cuda:0 f32[256, 3072]\"\n",
      "      # t4927 = prims.matmul(t4926, t_transformer_h_2_mlp_c_proj_weight)  # t4927: \"cuda:0 f32[256, 3072]\"\n",
      "  del t4926, t_transformer_h_2_mlp_c_proj_weight\n",
      "  t4932 = torch.matmul(t4930, t4931)  # t4932: \"cuda:0 f32[768, 3072]\"\n",
      "    # t4932 = ltorch.matmul(t4930, t4931)  # t4932: \"cuda:0 f32[768, 3072]\"\n",
      "      # t4932 = prims.matmul(t4930, t4931)  # t4932: \"cuda:0 f32[768, 3072]\"\n",
      "  del t4930, t4931\n",
      "  t4928 = torch.reshape(t4927, (4, 64, 3072))  # t4928: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4928 = ltorch.reshape(t4927, (4, 64, 3072))  # t4928: \"cuda:0 f32[4, 64, 3072]\"\n",
      "      # t4928 = prims.reshape(t4927, (4, 64, 3072))  # t4928: \"cuda:0 f32[4, 64, 3072]\"\n",
      "  del t4927\n",
      "  [t4949, t4957] = nvFusion47(f202, f204, f206, f208, t221, t4928)\n",
      "    # t222 = prims.mul(0.5, t221)  # t222: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t223 = prims.pow(t221, 3.0)  # t223: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t224 = prims.mul(0.044715, t223)  # t224: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t225 = prims.add(t221, t224)  # t225: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t226 = prims.mul(0.7978845608028654, t225)  # t226: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t227 = prims.tanh(t226)  # t227: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t228 = prims.add(1.0, t227)  # t228: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4934 = prims.mul(t228, t4928)  # t4934: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4935 = prims.mul(t222, t4928)  # t4935: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4936 = prims.mul(t227, t227)  # t4936: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4937 = prims.sub(1.0, t4936)  # t4937: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4938 = prims.mul(t4935, t4937)  # t4938: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4939 = prims.mul(f208, t4938)  # t4939: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4940 = prims.mul(f206, t4939)  # t4940: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4942 = prims.mul(t4940, f204)  # t4942: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4943 = prims.pow(t221, 2.0)  # t4943: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4944 = prims.mul(t4942, t4943)  # t4944: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4947 = prims.add(t4939, t4944)  # t4947: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4948 = prims.mul(f202, t4934)  # t4948: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4949 = prims.add(t4947, t4948)  # t4949: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t4957 = prims.sum(t4949, (0, 1))  # t4957: \"cuda:0 f32[3072]\"\n",
      "  del f202, f204, f206, f208, t221, t4928\n",
      "  t4950 = torch.reshape(t4949, (-1, 3072))  # t4950: \"cuda:0 f32[256, 3072]\"\n",
      "    # t4950 = ltorch.reshape(t4949, (-1, 3072))  # t4950: \"cuda:0 f32[256, 3072]\"\n",
      "      # t4950 = prims.reshape(t4949, (256, 3072))  # t4950: \"cuda:0 f32[256, 3072]\"\n",
      "  del t4949\n",
      "  t4954 = torch.permute(t4950, (1, 0))  # t4954: \"cuda:0 f32[3072, 256]\"\n",
      "    # t4954 = ltorch.permute(t4950, (1, 0))  # t4954: \"cuda:0 f32[3072, 256]\"\n",
      "      # t4954 = prims.transpose(t4950, (1, 0))  # t4954: \"cuda:0 f32[3072, 256]\"\n",
      "  t4956 = torch.matmul(t4954, t4955)  # t4956: \"cuda:0 f32[3072, 768]\"\n",
      "    # t4956 = ltorch.matmul(t4954, t4955)  # t4956: \"cuda:0 f32[3072, 768]\"\n",
      "      # t4956 = prims.matmul(t4954, t4955)  # t4956: \"cuda:0 f32[3072, 768]\"\n",
      "  del t4954, t4955\n",
      "  t4951 = torch.matmul(t4950, t_transformer_h_2_mlp_c_fc_weight)  # t4951: \"cuda:0 f32[256, 768]\"\n",
      "    # t4951 = ltorch.matmul(t4950, t_transformer_h_2_mlp_c_fc_weight)  # t4951: \"cuda:0 f32[256, 768]\"\n",
      "      # t4951 = prims.matmul(t4950, t_transformer_h_2_mlp_c_fc_weight)  # t4951: \"cuda:0 f32[256, 768]\"\n",
      "  del t4950, t_transformer_h_2_mlp_c_fc_weight\n",
      "  t4952 = torch.reshape(t4951, (4, 64, 768))  # t4952: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4952 = ltorch.reshape(t4951, (4, 64, 768))  # t4952: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4952 = prims.reshape(t4951, (4, 64, 768))  # t4952: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4951\n",
      "  t5833 = torch.unsqueeze(t208, 2)  # t5833: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5833 = ltorch.unsqueeze(t208, 2)  # t5833: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t5833 = prims.broadcast_in_dim(t208, [4, 64, 1], [0, 1])  # t5833: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t208\n",
      "  t4979 = Tensor.expand(t5833, [4, 64, 1])  # t4979: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4979 = ltorch.expand(t5833, [4, 64, 1])  # t4979: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t4979 = prims.broadcast_in_dim(t5833, (4, 64, 1), (0, 1, 2))  # t4979: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t5833\n",
      "  t4980 = Tensor.expand(t4979, (4, 64, 768))  # t4980: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4980 = ltorch.expand(t4979, (4, 64, 768))  # t4980: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4980 = prims.broadcast_in_dim(t4979, (4, 64, 768), (0, 1, 2))  # t4980: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4979\n",
      "  [t4958, t4961, t4987, t4995] = nvFusion48(t204, t212, t214, t217, t4925, t4952, t4980)\n",
      "    # t215 = prims.broadcast_in_dim(t212, (4, 64, 768), (0, 1, 2))  # t215: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t216 = prims.mul(t214, t215)  # t216: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4958 = prims.sum(t4952, (0, 1))  # t4958: \"cuda:0 f32[768]\"\n",
      "    # t4959 = prims.mul(t217, t4952)  # t4959: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4960 = prims.mul(t216, t4952)  # t4960: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4961 = prims.sum(t4960, (0, 1))  # t4961: \"cuda:0 f32[768]\"\n",
      "    # t4962 = prims.mul(t215, t4959)  # t4962: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4963 = prims.mul(t214, t4959)  # t4963: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4964 = prims.sum(t4963, (2,))  # t4964: \"cuda:0 f32[4, 64]\"\n",
      "    # t4965 = prims.broadcast_in_dim(t4964, [4, 64, 1], [0, 1])  # t4965: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4966 = prims.neg(t4962)  # t4966: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4967 = prims.sum(t4966, (2,))  # t4967: \"cuda:0 f32[4, 64]\"\n",
      "    # t4968 = prims.broadcast_in_dim(t4967, [4, 64, 1], [0, 1])  # t4968: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4969 = prims.mul(-0.5, t4965)  # t4969: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4970 = prims.pow(t212, 3.0)  # t4970: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4971 = prims.mul(t4969, t4970)  # t4971: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4972 = prims.sum(t4968, (2,))  # t4972: \"cuda:0 f32[4, 64]\"\n",
      "    # t4973 = prims.sum(t4971, (2,))  # t4973: \"cuda:0 f32[4, 64]\"\n",
      "    # t4974 = prims.broadcast_in_dim(t4972, [4, 64, 1], [0, 1])  # t4974: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4975 = prims.broadcast_in_dim(t4974, (4, 64, 768), (0, 1, 2))  # t4975: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4976 = prims.mul(0.0013020833333333333, t4975)  # t4976: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4977 = prims.broadcast_in_dim(t4973, [4, 64, 1], [0, 1])  # t4977: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t4978 = prims.broadcast_in_dim(t4977, (4, 64, 768), (0, 1, 2))  # t4978: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4981 = prims.mul(2.0, t4978)  # t4981: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4982 = prims.sub(t204, t4980)  # t4982: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4983 = prims.mul(t4981, t4982)  # t4983: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4984 = prims.div(t4983, 768.0)  # t4984: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4985 = prims.add(t4976, t4984)  # t4985: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4986 = prims.add(t4962, t4985)  # t4986: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4987 = prims.add(t4925, t4986)  # t4987: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4995 = prims.sum(t4987, (0, 1))  # t4995: \"cuda:0 f32[768]\"\n",
      "  del t204, t212, t214, t217, t4925, t4952, t4980\n",
      "  t4988 = torch.reshape(t4987, (-1, 768))  # t4988: \"cuda:0 f32[256, 768]\"\n",
      "    # t4988 = ltorch.reshape(t4987, (-1, 768))  # t4988: \"cuda:0 f32[256, 768]\"\n",
      "      # t4988 = prims.reshape(t4987, (256, 768))  # t4988: \"cuda:0 f32[256, 768]\"\n",
      "  t4992 = torch.permute(t4988, (1, 0))  # t4992: \"cuda:0 f32[768, 256]\"\n",
      "    # t4992 = ltorch.permute(t4988, (1, 0))  # t4992: \"cuda:0 f32[768, 256]\"\n",
      "      # t4992 = prims.transpose(t4988, (1, 0))  # t4992: \"cuda:0 f32[768, 256]\"\n",
      "  t4989 = torch.matmul(t4988, t_transformer_h_2_attn_c_proj_weight)  # t4989: \"cuda:0 f32[256, 768]\"\n",
      "    # t4989 = ltorch.matmul(t4988, t_transformer_h_2_attn_c_proj_weight)  # t4989: \"cuda:0 f32[256, 768]\"\n",
      "      # t4989 = prims.matmul(t4988, t_transformer_h_2_attn_c_proj_weight)  # t4989: \"cuda:0 f32[256, 768]\"\n",
      "  del t4988, t_transformer_h_2_attn_c_proj_weight\n",
      "  t4994 = torch.matmul(t4992, t4993)  # t4994: \"cuda:0 f32[768, 768]\"\n",
      "    # t4994 = ltorch.matmul(t4992, t4993)  # t4994: \"cuda:0 f32[768, 768]\"\n",
      "      # t4994 = prims.matmul(t4992, t4993)  # t4994: \"cuda:0 f32[768, 768]\"\n",
      "  del t4992, t4993\n",
      "  t4990 = torch.reshape(t4989, (4, 64, 768))  # t4990: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t4990 = ltorch.reshape(t4989, (4, 64, 768))  # t4990: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t4990 = prims.reshape(t4989, (4, 64, 768))  # t4990: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t4989\n",
      "  t4996 = torch.reshape(t4990, (4, 64, 12, 64))  # t4996: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t4996 = ltorch.reshape(t4990, (4, 64, 12, 64))  # t4996: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t4996 = prims.reshape(t4990, (4, 64, 12, 64))  # t4996: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t4990\n",
      "  t4997 = torch.permute(t4996, (0, 2, 1, 3))  # t4997: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4997 = ltorch.permute(t4996, (0, 2, 1, 3))  # t4997: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4997 = prims.transpose(t4996, (0, 2, 1, 3))  # t4997: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4996\n",
      "  t4999 = torch.matmul(t4997, t4998)  # t4999: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t4999 = ltorch.matmul(t4997, t4998)  # t4999: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t4999 = prims.matmul(t4997, t4998)  # t4999: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t4998\n",
      "  t5001 = torch.matmul(t5000, t4997)  # t5001: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t5001 = ltorch.matmul(t5000, t4997)  # t5001: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t5001 = prims.matmul(t5000, t4997)  # t5001: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t5000, t4997\n",
      "  [t5009] = nvFusion49(f182, i186, t188, t198, t4999)\n",
      "    # t5002 = prims.mul(t198, t4999)  # t5002: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t5003 = prims.sum(t5002, (3,))  # t5003: \"cuda:0 f32[4, 12, 64]\"\n",
      "    # t5004 = prims.broadcast_in_dim(t5003, [4, 12, 64, 1], [0, 1, 2])  # t5004: \"cuda:0 f32[4, 12, 64, 1]\"\n",
      "    # t5005 = prims.broadcast_in_dim(t5004, (4, 12, 64, 64), (0, 1, 2, 3))  # t5005: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t5006 = prims.sub(t4999, t5005)  # t5006: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t5007 = prims.mul(t198, t5006)  # t5007: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t5008 = prims.where(t188, 0.0, t5007)  # t5008: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t5009 = prims.mul(f182, t5008)  # t5009: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del f182, i186, t188, t198, t4999\n",
      "  t5013 = torch.matmul(t5012, t5009)  # t5013: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t5013 = ltorch.matmul(t5012, t5009)  # t5013: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t5013 = prims.matmul(t5012, t5009)  # t5013: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t5012\n",
      "  t5011 = torch.matmul(t5009, t5010)  # t5011: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t5011 = ltorch.matmul(t5009, t5010)  # t5011: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t5011 = prims.matmul(t5009, t5010)  # t5011: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t5009, t5010\n",
      "  t5014 = torch.permute(t5013, (0, 1, 3, 2))  # t5014: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t5014 = ltorch.permute(t5013, (0, 1, 3, 2))  # t5014: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t5014 = prims.transpose(t5013, (0, 1, 3, 2))  # t5014: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t5013\n",
      "  t5015 = torch.permute(t5001, (0, 2, 1, 3))  # t5015: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t5015 = ltorch.permute(t5001, (0, 2, 1, 3))  # t5015: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t5015 = prims.transpose(t5001, (0, 2, 1, 3))  # t5015: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t5001\n",
      "  t5016 = torch.reshape(t5015, (4, 64, 768))  # t5016: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5016 = ltorch.reshape(t5015, (4, 64, 768))  # t5016: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t5016 = prims.reshape(t5015, (4, 64, 768))  # t5016: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t5015\n",
      "  t5017 = torch.permute(t5014, (0, 2, 1, 3))  # t5017: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t5017 = ltorch.permute(t5014, (0, 2, 1, 3))  # t5017: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t5017 = prims.transpose(t5014, (0, 2, 1, 3))  # t5017: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t5014\n",
      "  t5018 = torch.reshape(t5017, (4, 64, 768))  # t5018: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5018 = ltorch.reshape(t5017, (4, 64, 768))  # t5018: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t5018 = prims.reshape(t5017, (4, 64, 768))  # t5018: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t5017\n",
      "  t5019 = torch.permute(t5011, (0, 2, 1, 3))  # t5019: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t5019 = ltorch.permute(t5011, (0, 2, 1, 3))  # t5019: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t5019 = prims.transpose(t5011, (0, 2, 1, 3))  # t5019: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t5011\n",
      "  t5020 = torch.reshape(t5019, (4, 64, 768))  # t5020: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5020 = ltorch.reshape(t5019, (4, 64, 768))  # t5020: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t5020 = prims.reshape(t5019, (4, 64, 768))  # t5020: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t5019\n",
      "  [t5021, t5029] = nvFusion50(i148, t5016, t5018, t5020)\n",
      "    # t5021 = prims.cat((t5020, t5018, t5016), i148)  # t5021: \"cuda:0 f32[4, 64, 2304]\"\n",
      "    # t5029 = prims.sum(t5021, (0, 1))  # t5029: \"cuda:0 f32[2304]\"\n",
      "  del i148, t5016, t5018, t5020\n",
      "  t5022 = torch.reshape(t5021, (-1, 2304))  # t5022: \"cuda:0 f32[256, 2304]\"\n",
      "    # t5022 = ltorch.reshape(t5021, (-1, 2304))  # t5022: \"cuda:0 f32[256, 2304]\"\n",
      "      # t5022 = prims.reshape(t5021, (256, 2304))  # t5022: \"cuda:0 f32[256, 2304]\"\n",
      "  del t5021\n",
      "  t5026 = torch.permute(t5022, (1, 0))  # t5026: \"cuda:0 f32[2304, 256]\"\n",
      "    # t5026 = ltorch.permute(t5022, (1, 0))  # t5026: \"cuda:0 f32[2304, 256]\"\n",
      "      # t5026 = prims.transpose(t5022, (1, 0))  # t5026: \"cuda:0 f32[2304, 256]\"\n",
      "  t5023 = torch.matmul(t5022, t_transformer_h_2_attn_c_attn_weight)  # t5023: \"cuda:0 f32[256, 768]\"\n",
      "    # t5023 = ltorch.matmul(t5022, t_transformer_h_2_attn_c_attn_weight)  # t5023: \"cuda:0 f32[256, 768]\"\n",
      "      # t5023 = prims.matmul(t5022, t_transformer_h_2_attn_c_attn_weight)  # t5023: \"cuda:0 f32[256, 768]\"\n",
      "  del t5022, t_transformer_h_2_attn_c_attn_weight\n",
      "  t5028 = torch.matmul(t5026, t5027)  # t5028: \"cuda:0 f32[2304, 768]\"\n",
      "    # t5028 = ltorch.matmul(t5026, t5027)  # t5028: \"cuda:0 f32[2304, 768]\"\n",
      "      # t5028 = prims.matmul(t5026, t5027)  # t5028: \"cuda:0 f32[2304, 768]\"\n",
      "  del t5026, t5027\n",
      "  t5024 = torch.reshape(t5023, (4, 64, 768))  # t5024: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5024 = ltorch.reshape(t5023, (4, 64, 768))  # t5024: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t5024 = prims.reshape(t5023, (4, 64, 768))  # t5024: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t5023\n",
      "  t5851 = torch.unsqueeze(t160, 2)  # t5851: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5851 = ltorch.unsqueeze(t160, 2)  # t5851: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t5851 = prims.broadcast_in_dim(t160, [4, 64, 1], [0, 1])  # t5851: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t160\n",
      "  t5051 = Tensor.expand(t5851, [4, 64, 1])  # t5051: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5051 = ltorch.expand(t5851, [4, 64, 1])  # t5051: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t5051 = prims.broadcast_in_dim(t5851, (4, 64, 1), (0, 1, 2))  # t5051: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t5851\n",
      "  t5052 = Tensor.expand(t5051, (4, 64, 768))  # t5052: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5052 = ltorch.expand(t5051, (4, 64, 768))  # t5052: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t5052 = prims.broadcast_in_dim(t5051, (4, 64, 768), (0, 1, 2))  # t5052: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t5051\n",
      "  [t5030, t5033, t5059, t5067] = nvFusion51(t156, t164, t166, t169, t4987, t5024, t5052)\n",
      "    # t167 = prims.broadcast_in_dim(t164, (4, 64, 768), (0, 1, 2))  # t167: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t168 = prims.mul(t166, t167)  # t168: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5030 = prims.sum(t5024, (0, 1))  # t5030: \"cuda:0 f32[768]\"\n",
      "    # t5031 = prims.mul(t169, t5024)  # t5031: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5032 = prims.mul(t168, t5024)  # t5032: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5033 = prims.sum(t5032, (0, 1))  # t5033: \"cuda:0 f32[768]\"\n",
      "    # t5034 = prims.mul(t167, t5031)  # t5034: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5035 = prims.mul(t166, t5031)  # t5035: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5036 = prims.sum(t5035, (2,))  # t5036: \"cuda:0 f32[4, 64]\"\n",
      "    # t5037 = prims.broadcast_in_dim(t5036, [4, 64, 1], [0, 1])  # t5037: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5038 = prims.neg(t5034)  # t5038: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5039 = prims.sum(t5038, (2,))  # t5039: \"cuda:0 f32[4, 64]\"\n",
      "    # t5040 = prims.broadcast_in_dim(t5039, [4, 64, 1], [0, 1])  # t5040: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5041 = prims.mul(-0.5, t5037)  # t5041: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5042 = prims.pow(t164, 3.0)  # t5042: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5043 = prims.mul(t5041, t5042)  # t5043: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5044 = prims.sum(t5040, (2,))  # t5044: \"cuda:0 f32[4, 64]\"\n",
      "    # t5045 = prims.sum(t5043, (2,))  # t5045: \"cuda:0 f32[4, 64]\"\n",
      "    # t5046 = prims.broadcast_in_dim(t5044, [4, 64, 1], [0, 1])  # t5046: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5047 = prims.broadcast_in_dim(t5046, (4, 64, 768), (0, 1, 2))  # t5047: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5048 = prims.mul(0.0013020833333333333, t5047)  # t5048: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5049 = prims.broadcast_in_dim(t5045, [4, 64, 1], [0, 1])  # t5049: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5050 = prims.broadcast_in_dim(t5049, (4, 64, 768), (0, 1, 2))  # t5050: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5053 = prims.mul(2.0, t5050)  # t5053: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5054 = prims.sub(t156, t5052)  # t5054: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5055 = prims.mul(t5053, t5054)  # t5055: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5056 = prims.div(t5055, 768.0)  # t5056: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5057 = prims.add(t5048, t5056)  # t5057: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5058 = prims.add(t5034, t5057)  # t5058: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5059 = prims.add(t4987, t5058)  # t5059: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5067 = prims.sum(t5059, (0, 1))  # t5067: \"cuda:0 f32[768]\"\n",
      "  del t156, t164, t166, t169, t4987, t5024, t5052\n",
      "  t5060 = torch.reshape(t5059, (-1, 768))  # t5060: \"cuda:0 f32[256, 768]\"\n",
      "    # t5060 = ltorch.reshape(t5059, (-1, 768))  # t5060: \"cuda:0 f32[256, 768]\"\n",
      "      # t5060 = prims.reshape(t5059, (256, 768))  # t5060: \"cuda:0 f32[256, 768]\"\n",
      "  t5064 = torch.permute(t5060, (1, 0))  # t5064: \"cuda:0 f32[768, 256]\"\n",
      "    # t5064 = ltorch.permute(t5060, (1, 0))  # t5064: \"cuda:0 f32[768, 256]\"\n",
      "      # t5064 = prims.transpose(t5060, (1, 0))  # t5064: \"cuda:0 f32[768, 256]\"\n",
      "  t5061 = torch.matmul(t5060, t_transformer_h_1_mlp_c_proj_weight)  # t5061: \"cuda:0 f32[256, 3072]\"\n",
      "    # t5061 = ltorch.matmul(t5060, t_transformer_h_1_mlp_c_proj_weight)  # t5061: \"cuda:0 f32[256, 3072]\"\n",
      "      # t5061 = prims.matmul(t5060, t_transformer_h_1_mlp_c_proj_weight)  # t5061: \"cuda:0 f32[256, 3072]\"\n",
      "  del t5060, t_transformer_h_1_mlp_c_proj_weight\n",
      "  t5066 = torch.matmul(t5064, t5065)  # t5066: \"cuda:0 f32[768, 3072]\"\n",
      "    # t5066 = ltorch.matmul(t5064, t5065)  # t5066: \"cuda:0 f32[768, 3072]\"\n",
      "      # t5066 = prims.matmul(t5064, t5065)  # t5066: \"cuda:0 f32[768, 3072]\"\n",
      "  del t5064, t5065\n",
      "  t5062 = torch.reshape(t5061, (4, 64, 3072))  # t5062: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t5062 = ltorch.reshape(t5061, (4, 64, 3072))  # t5062: \"cuda:0 f32[4, 64, 3072]\"\n",
      "      # t5062 = prims.reshape(t5061, (4, 64, 3072))  # t5062: \"cuda:0 f32[4, 64, 3072]\"\n",
      "  del t5061\n",
      "  [t5083, t5091] = nvFusion52(f134, f136, f138, f140, t146, t5062)\n",
      "    # t147 = prims.mul(0.5, t146)  # t147: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t148 = prims.pow(t146, 3.0)  # t148: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t149 = prims.mul(0.044715, t148)  # t149: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t150 = prims.add(t146, t149)  # t150: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t151 = prims.mul(0.7978845608028654, t150)  # t151: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t152 = prims.tanh(t151)  # t152: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t153 = prims.add(1.0, t152)  # t153: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t5068 = prims.mul(t153, t5062)  # t5068: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t5069 = prims.mul(t147, t5062)  # t5069: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t5070 = prims.mul(t152, t152)  # t5070: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t5071 = prims.sub(1.0, t5070)  # t5071: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t5072 = prims.mul(t5069, t5071)  # t5072: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t5073 = prims.mul(f140, t5072)  # t5073: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t5074 = prims.mul(f138, t5073)  # t5074: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t5076 = prims.mul(t5074, f136)  # t5076: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t5077 = prims.pow(t146, 2.0)  # t5077: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t5078 = prims.mul(t5076, t5077)  # t5078: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t5081 = prims.add(t5073, t5078)  # t5081: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t5082 = prims.mul(f134, t5068)  # t5082: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t5083 = prims.add(t5081, t5082)  # t5083: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t5091 = prims.sum(t5083, (0, 1))  # t5091: \"cuda:0 f32[3072]\"\n",
      "  del f134, f136, f138, f140, t146, t5062\n",
      "  t5084 = torch.reshape(t5083, (-1, 3072))  # t5084: \"cuda:0 f32[256, 3072]\"\n",
      "    # t5084 = ltorch.reshape(t5083, (-1, 3072))  # t5084: \"cuda:0 f32[256, 3072]\"\n",
      "      # t5084 = prims.reshape(t5083, (256, 3072))  # t5084: \"cuda:0 f32[256, 3072]\"\n",
      "  del t5083\n",
      "  t5088 = torch.permute(t5084, (1, 0))  # t5088: \"cuda:0 f32[3072, 256]\"\n",
      "    # t5088 = ltorch.permute(t5084, (1, 0))  # t5088: \"cuda:0 f32[3072, 256]\"\n",
      "      # t5088 = prims.transpose(t5084, (1, 0))  # t5088: \"cuda:0 f32[3072, 256]\"\n",
      "  t5090 = torch.matmul(t5088, t5089)  # t5090: \"cuda:0 f32[3072, 768]\"\n",
      "    # t5090 = ltorch.matmul(t5088, t5089)  # t5090: \"cuda:0 f32[3072, 768]\"\n",
      "      # t5090 = prims.matmul(t5088, t5089)  # t5090: \"cuda:0 f32[3072, 768]\"\n",
      "  del t5088, t5089\n",
      "  t5085 = torch.matmul(t5084, t_transformer_h_1_mlp_c_fc_weight)  # t5085: \"cuda:0 f32[256, 768]\"\n",
      "    # t5085 = ltorch.matmul(t5084, t_transformer_h_1_mlp_c_fc_weight)  # t5085: \"cuda:0 f32[256, 768]\"\n",
      "      # t5085 = prims.matmul(t5084, t_transformer_h_1_mlp_c_fc_weight)  # t5085: \"cuda:0 f32[256, 768]\"\n",
      "  del t5084, t_transformer_h_1_mlp_c_fc_weight\n",
      "  t5086 = torch.reshape(t5085, (4, 64, 768))  # t5086: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5086 = ltorch.reshape(t5085, (4, 64, 768))  # t5086: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t5086 = prims.reshape(t5085, (4, 64, 768))  # t5086: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t5085\n",
      "  t5860 = torch.unsqueeze(t133, 2)  # t5860: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5860 = ltorch.unsqueeze(t133, 2)  # t5860: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t5860 = prims.broadcast_in_dim(t133, [4, 64, 1], [0, 1])  # t5860: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t133\n",
      "  t5113 = Tensor.expand(t5860, [4, 64, 1])  # t5113: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5113 = ltorch.expand(t5860, [4, 64, 1])  # t5113: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t5113 = prims.broadcast_in_dim(t5860, (4, 64, 1), (0, 1, 2))  # t5113: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t5860\n",
      "  t5114 = Tensor.expand(t5113, (4, 64, 768))  # t5114: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5114 = ltorch.expand(t5113, (4, 64, 768))  # t5114: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t5114 = prims.broadcast_in_dim(t5113, (4, 64, 768), (0, 1, 2))  # t5114: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t5113\n",
      "  [t5092, t5095, t5121, t5129] = nvFusion53(t129, t137, t139, t142, t5059, t5086, t5114)\n",
      "    # t140 = prims.broadcast_in_dim(t137, (4, 64, 768), (0, 1, 2))  # t140: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t141 = prims.mul(t139, t140)  # t141: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5092 = prims.sum(t5086, (0, 1))  # t5092: \"cuda:0 f32[768]\"\n",
      "    # t5093 = prims.mul(t142, t5086)  # t5093: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5094 = prims.mul(t141, t5086)  # t5094: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5095 = prims.sum(t5094, (0, 1))  # t5095: \"cuda:0 f32[768]\"\n",
      "    # t5096 = prims.mul(t140, t5093)  # t5096: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5097 = prims.mul(t139, t5093)  # t5097: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5098 = prims.sum(t5097, (2,))  # t5098: \"cuda:0 f32[4, 64]\"\n",
      "    # t5099 = prims.broadcast_in_dim(t5098, [4, 64, 1], [0, 1])  # t5099: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5100 = prims.neg(t5096)  # t5100: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5101 = prims.sum(t5100, (2,))  # t5101: \"cuda:0 f32[4, 64]\"\n",
      "    # t5102 = prims.broadcast_in_dim(t5101, [4, 64, 1], [0, 1])  # t5102: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5103 = prims.mul(-0.5, t5099)  # t5103: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5104 = prims.pow(t137, 3.0)  # t5104: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5105 = prims.mul(t5103, t5104)  # t5105: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5106 = prims.sum(t5102, (2,))  # t5106: \"cuda:0 f32[4, 64]\"\n",
      "    # t5107 = prims.sum(t5105, (2,))  # t5107: \"cuda:0 f32[4, 64]\"\n",
      "    # t5108 = prims.broadcast_in_dim(t5106, [4, 64, 1], [0, 1])  # t5108: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5109 = prims.broadcast_in_dim(t5108, (4, 64, 768), (0, 1, 2))  # t5109: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5110 = prims.mul(0.0013020833333333333, t5109)  # t5110: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5111 = prims.broadcast_in_dim(t5107, [4, 64, 1], [0, 1])  # t5111: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5112 = prims.broadcast_in_dim(t5111, (4, 64, 768), (0, 1, 2))  # t5112: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5115 = prims.mul(2.0, t5112)  # t5115: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5116 = prims.sub(t129, t5114)  # t5116: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5117 = prims.mul(t5115, t5116)  # t5117: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5118 = prims.div(t5117, 768.0)  # t5118: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5119 = prims.add(t5110, t5118)  # t5119: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5120 = prims.add(t5096, t5119)  # t5120: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5121 = prims.add(t5059, t5120)  # t5121: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5129 = prims.sum(t5121, (0, 1))  # t5129: \"cuda:0 f32[768]\"\n",
      "  del t129, t137, t139, t142, t5059, t5086, t5114\n",
      "  t5122 = torch.reshape(t5121, (-1, 768))  # t5122: \"cuda:0 f32[256, 768]\"\n",
      "    # t5122 = ltorch.reshape(t5121, (-1, 768))  # t5122: \"cuda:0 f32[256, 768]\"\n",
      "      # t5122 = prims.reshape(t5121, (256, 768))  # t5122: \"cuda:0 f32[256, 768]\"\n",
      "  t5126 = torch.permute(t5122, (1, 0))  # t5126: \"cuda:0 f32[768, 256]\"\n",
      "    # t5126 = ltorch.permute(t5122, (1, 0))  # t5126: \"cuda:0 f32[768, 256]\"\n",
      "      # t5126 = prims.transpose(t5122, (1, 0))  # t5126: \"cuda:0 f32[768, 256]\"\n",
      "  t5123 = torch.matmul(t5122, t_transformer_h_1_attn_c_proj_weight)  # t5123: \"cuda:0 f32[256, 768]\"\n",
      "    # t5123 = ltorch.matmul(t5122, t_transformer_h_1_attn_c_proj_weight)  # t5123: \"cuda:0 f32[256, 768]\"\n",
      "      # t5123 = prims.matmul(t5122, t_transformer_h_1_attn_c_proj_weight)  # t5123: \"cuda:0 f32[256, 768]\"\n",
      "  del t5122, t_transformer_h_1_attn_c_proj_weight\n",
      "  t5128 = torch.matmul(t5126, t5127)  # t5128: \"cuda:0 f32[768, 768]\"\n",
      "    # t5128 = ltorch.matmul(t5126, t5127)  # t5128: \"cuda:0 f32[768, 768]\"\n",
      "      # t5128 = prims.matmul(t5126, t5127)  # t5128: \"cuda:0 f32[768, 768]\"\n",
      "  del t5126, t5127\n",
      "  t5124 = torch.reshape(t5123, (4, 64, 768))  # t5124: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5124 = ltorch.reshape(t5123, (4, 64, 768))  # t5124: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t5124 = prims.reshape(t5123, (4, 64, 768))  # t5124: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t5123\n",
      "  t5130 = torch.reshape(t5124, (4, 64, 12, 64))  # t5130: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t5130 = ltorch.reshape(t5124, (4, 64, 12, 64))  # t5130: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t5130 = prims.reshape(t5124, (4, 64, 12, 64))  # t5130: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t5124\n",
      "  t5131 = torch.permute(t5130, (0, 2, 1, 3))  # t5131: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t5131 = ltorch.permute(t5130, (0, 2, 1, 3))  # t5131: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t5131 = prims.transpose(t5130, (0, 2, 1, 3))  # t5131: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t5130\n",
      "  t5133 = torch.matmul(t5131, t5132)  # t5133: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t5133 = ltorch.matmul(t5131, t5132)  # t5133: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t5133 = prims.matmul(t5131, t5132)  # t5133: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t5132\n",
      "  t5135 = torch.matmul(t5134, t5131)  # t5135: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t5135 = ltorch.matmul(t5134, t5131)  # t5135: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t5135 = prims.matmul(t5134, t5131)  # t5135: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t5134, t5131\n",
      "  [t5143] = nvFusion54(f114, i118, t113, t123, t5133)\n",
      "    # t5136 = prims.mul(t123, t5133)  # t5136: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t5137 = prims.sum(t5136, (3,))  # t5137: \"cuda:0 f32[4, 12, 64]\"\n",
      "    # t5138 = prims.broadcast_in_dim(t5137, [4, 12, 64, 1], [0, 1, 2])  # t5138: \"cuda:0 f32[4, 12, 64, 1]\"\n",
      "    # t5139 = prims.broadcast_in_dim(t5138, (4, 12, 64, 64), (0, 1, 2, 3))  # t5139: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t5140 = prims.sub(t5133, t5139)  # t5140: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t5141 = prims.mul(t123, t5140)  # t5141: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t5142 = prims.where(t113, 0.0, t5141)  # t5142: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t5143 = prims.mul(f114, t5142)  # t5143: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del f114, i118, t113, t123, t5133\n",
      "  t5147 = torch.matmul(t5146, t5143)  # t5147: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t5147 = ltorch.matmul(t5146, t5143)  # t5147: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t5147 = prims.matmul(t5146, t5143)  # t5147: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t5146\n",
      "  t5145 = torch.matmul(t5143, t5144)  # t5145: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t5145 = ltorch.matmul(t5143, t5144)  # t5145: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t5145 = prims.matmul(t5143, t5144)  # t5145: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t5143, t5144\n",
      "  t5148 = torch.permute(t5147, (0, 1, 3, 2))  # t5148: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t5148 = ltorch.permute(t5147, (0, 1, 3, 2))  # t5148: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t5148 = prims.transpose(t5147, (0, 1, 3, 2))  # t5148: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t5147\n",
      "  t5149 = torch.permute(t5135, (0, 2, 1, 3))  # t5149: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t5149 = ltorch.permute(t5135, (0, 2, 1, 3))  # t5149: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t5149 = prims.transpose(t5135, (0, 2, 1, 3))  # t5149: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t5135\n",
      "  t5150 = torch.reshape(t5149, (4, 64, 768))  # t5150: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5150 = ltorch.reshape(t5149, (4, 64, 768))  # t5150: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t5150 = prims.reshape(t5149, (4, 64, 768))  # t5150: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t5149\n",
      "  t5151 = torch.permute(t5148, (0, 2, 1, 3))  # t5151: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t5151 = ltorch.permute(t5148, (0, 2, 1, 3))  # t5151: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t5151 = prims.transpose(t5148, (0, 2, 1, 3))  # t5151: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t5148\n",
      "  t5152 = torch.reshape(t5151, (4, 64, 768))  # t5152: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5152 = ltorch.reshape(t5151, (4, 64, 768))  # t5152: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t5152 = prims.reshape(t5151, (4, 64, 768))  # t5152: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t5151\n",
      "  t5153 = torch.permute(t5145, (0, 2, 1, 3))  # t5153: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t5153 = ltorch.permute(t5145, (0, 2, 1, 3))  # t5153: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t5153 = prims.transpose(t5145, (0, 2, 1, 3))  # t5153: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t5145\n",
      "  t5154 = torch.reshape(t5153, (4, 64, 768))  # t5154: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5154 = ltorch.reshape(t5153, (4, 64, 768))  # t5154: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t5154 = prims.reshape(t5153, (4, 64, 768))  # t5154: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t5153\n",
      "  [t5155, t5163] = nvFusion55(i80, t5150, t5152, t5154)\n",
      "    # t5155 = prims.cat((t5154, t5152, t5150), i80)  # t5155: \"cuda:0 f32[4, 64, 2304]\"\n",
      "    # t5163 = prims.sum(t5155, (0, 1))  # t5163: \"cuda:0 f32[2304]\"\n",
      "  del i80, t5150, t5152, t5154\n",
      "  t5156 = torch.reshape(t5155, (-1, 2304))  # t5156: \"cuda:0 f32[256, 2304]\"\n",
      "    # t5156 = ltorch.reshape(t5155, (-1, 2304))  # t5156: \"cuda:0 f32[256, 2304]\"\n",
      "      # t5156 = prims.reshape(t5155, (256, 2304))  # t5156: \"cuda:0 f32[256, 2304]\"\n",
      "  del t5155\n",
      "  t5160 = torch.permute(t5156, (1, 0))  # t5160: \"cuda:0 f32[2304, 256]\"\n",
      "    # t5160 = ltorch.permute(t5156, (1, 0))  # t5160: \"cuda:0 f32[2304, 256]\"\n",
      "      # t5160 = prims.transpose(t5156, (1, 0))  # t5160: \"cuda:0 f32[2304, 256]\"\n",
      "  t5162 = torch.matmul(t5160, t5161)  # t5162: \"cuda:0 f32[2304, 768]\"\n",
      "    # t5162 = ltorch.matmul(t5160, t5161)  # t5162: \"cuda:0 f32[2304, 768]\"\n",
      "      # t5162 = prims.matmul(t5160, t5161)  # t5162: \"cuda:0 f32[2304, 768]\"\n",
      "  del t5160, t5161\n",
      "  t5157 = torch.matmul(t5156, t_transformer_h_1_attn_c_attn_weight)  # t5157: \"cuda:0 f32[256, 768]\"\n",
      "    # t5157 = ltorch.matmul(t5156, t_transformer_h_1_attn_c_attn_weight)  # t5157: \"cuda:0 f32[256, 768]\"\n",
      "      # t5157 = prims.matmul(t5156, t_transformer_h_1_attn_c_attn_weight)  # t5157: \"cuda:0 f32[256, 768]\"\n",
      "  del t5156, t_transformer_h_1_attn_c_attn_weight\n",
      "  t5158 = torch.reshape(t5157, (4, 64, 768))  # t5158: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5158 = ltorch.reshape(t5157, (4, 64, 768))  # t5158: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t5158 = prims.reshape(t5157, (4, 64, 768))  # t5158: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t5157\n",
      "  t5878 = torch.unsqueeze(t85, 2)  # t5878: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5878 = ltorch.unsqueeze(t85, 2)  # t5878: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t5878 = prims.broadcast_in_dim(t85, [4, 64, 1], [0, 1])  # t5878: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t85\n",
      "  t5185 = Tensor.expand(t5878, [4, 64, 1])  # t5185: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5185 = ltorch.expand(t5878, [4, 64, 1])  # t5185: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t5185 = prims.broadcast_in_dim(t5878, (4, 64, 1), (0, 1, 2))  # t5185: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t5878\n",
      "  t5186 = Tensor.expand(t5185, (4, 64, 768))  # t5186: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5186 = ltorch.expand(t5185, (4, 64, 768))  # t5186: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t5186 = prims.broadcast_in_dim(t5185, (4, 64, 768), (0, 1, 2))  # t5186: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t5185\n",
      "  [t5164, t5167, t5193, t5201] = nvFusion56(t5121, t5158, t5186, t81, t89, t91, t94)\n",
      "    # t92 = prims.broadcast_in_dim(t89, (4, 64, 768), (0, 1, 2))  # t92: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t93 = prims.mul(t91, t92)  # t93: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5164 = prims.sum(t5158, (0, 1))  # t5164: \"cuda:0 f32[768]\"\n",
      "    # t5165 = prims.mul(t94, t5158)  # t5165: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5166 = prims.mul(t93, t5158)  # t5166: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5167 = prims.sum(t5166, (0, 1))  # t5167: \"cuda:0 f32[768]\"\n",
      "    # t5168 = prims.mul(t92, t5165)  # t5168: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5169 = prims.mul(t91, t5165)  # t5169: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5170 = prims.sum(t5169, (2,))  # t5170: \"cuda:0 f32[4, 64]\"\n",
      "    # t5171 = prims.broadcast_in_dim(t5170, [4, 64, 1], [0, 1])  # t5171: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5172 = prims.neg(t5168)  # t5172: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5173 = prims.sum(t5172, (2,))  # t5173: \"cuda:0 f32[4, 64]\"\n",
      "    # t5174 = prims.broadcast_in_dim(t5173, [4, 64, 1], [0, 1])  # t5174: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5175 = prims.mul(-0.5, t5171)  # t5175: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5176 = prims.pow(t89, 3.0)  # t5176: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5177 = prims.mul(t5175, t5176)  # t5177: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5178 = prims.sum(t5174, (2,))  # t5178: \"cuda:0 f32[4, 64]\"\n",
      "    # t5179 = prims.sum(t5177, (2,))  # t5179: \"cuda:0 f32[4, 64]\"\n",
      "    # t5180 = prims.broadcast_in_dim(t5178, [4, 64, 1], [0, 1])  # t5180: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5181 = prims.broadcast_in_dim(t5180, (4, 64, 768), (0, 1, 2))  # t5181: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5182 = prims.mul(0.0013020833333333333, t5181)  # t5182: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5183 = prims.broadcast_in_dim(t5179, [4, 64, 1], [0, 1])  # t5183: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5184 = prims.broadcast_in_dim(t5183, (4, 64, 768), (0, 1, 2))  # t5184: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5187 = prims.mul(2.0, t5184)  # t5187: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5188 = prims.sub(t81, t5186)  # t5188: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5189 = prims.mul(t5187, t5188)  # t5189: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5190 = prims.div(t5189, 768.0)  # t5190: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5191 = prims.add(t5182, t5190)  # t5191: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5192 = prims.add(t5168, t5191)  # t5192: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5193 = prims.add(t5121, t5192)  # t5193: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5201 = prims.sum(t5193, (0, 1))  # t5201: \"cuda:0 f32[768]\"\n",
      "  del t5121, t5158, t5186, t81, t89, t91, t94\n",
      "  t5194 = torch.reshape(t5193, (-1, 768))  # t5194: \"cuda:0 f32[256, 768]\"\n",
      "    # t5194 = ltorch.reshape(t5193, (-1, 768))  # t5194: \"cuda:0 f32[256, 768]\"\n",
      "      # t5194 = prims.reshape(t5193, (256, 768))  # t5194: \"cuda:0 f32[256, 768]\"\n",
      "  t5198 = torch.permute(t5194, (1, 0))  # t5198: \"cuda:0 f32[768, 256]\"\n",
      "    # t5198 = ltorch.permute(t5194, (1, 0))  # t5198: \"cuda:0 f32[768, 256]\"\n",
      "      # t5198 = prims.transpose(t5194, (1, 0))  # t5198: \"cuda:0 f32[768, 256]\"\n",
      "  t5195 = torch.matmul(t5194, t_transformer_h_0_mlp_c_proj_weight)  # t5195: \"cuda:0 f32[256, 3072]\"\n",
      "    # t5195 = ltorch.matmul(t5194, t_transformer_h_0_mlp_c_proj_weight)  # t5195: \"cuda:0 f32[256, 3072]\"\n",
      "      # t5195 = prims.matmul(t5194, t_transformer_h_0_mlp_c_proj_weight)  # t5195: \"cuda:0 f32[256, 3072]\"\n",
      "  del t5194, t_transformer_h_0_mlp_c_proj_weight\n",
      "  t5200 = torch.matmul(t5198, t5199)  # t5200: \"cuda:0 f32[768, 3072]\"\n",
      "    # t5200 = ltorch.matmul(t5198, t5199)  # t5200: \"cuda:0 f32[768, 3072]\"\n",
      "      # t5200 = prims.matmul(t5198, t5199)  # t5200: \"cuda:0 f32[768, 3072]\"\n",
      "  del t5198, t5199\n",
      "  t5196 = torch.reshape(t5195, (4, 64, 3072))  # t5196: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t5196 = ltorch.reshape(t5195, (4, 64, 3072))  # t5196: \"cuda:0 f32[4, 64, 3072]\"\n",
      "      # t5196 = prims.reshape(t5195, (4, 64, 3072))  # t5196: \"cuda:0 f32[4, 64, 3072]\"\n",
      "  del t5195\n",
      "  [t5217, t5225] = nvFusion57(f66, f68, f70, f72, t5196, t71)\n",
      "    # t72 = prims.mul(0.5, t71)  # t72: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t73 = prims.pow(t71, 3.0)  # t73: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t74 = prims.mul(0.044715, t73)  # t74: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t75 = prims.add(t71, t74)  # t75: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t76 = prims.mul(0.7978845608028654, t75)  # t76: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t77 = prims.tanh(t76)  # t77: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t78 = prims.add(1.0, t77)  # t78: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t5202 = prims.mul(t78, t5196)  # t5202: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t5203 = prims.mul(t72, t5196)  # t5203: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t5204 = prims.mul(t77, t77)  # t5204: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t5205 = prims.sub(1.0, t5204)  # t5205: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t5206 = prims.mul(t5203, t5205)  # t5206: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t5207 = prims.mul(f72, t5206)  # t5207: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t5208 = prims.mul(f70, t5207)  # t5208: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t5210 = prims.mul(t5208, f68)  # t5210: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t5211 = prims.pow(t71, 2.0)  # t5211: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t5212 = prims.mul(t5210, t5211)  # t5212: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t5215 = prims.add(t5207, t5212)  # t5215: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t5216 = prims.mul(f66, t5202)  # t5216: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t5217 = prims.add(t5215, t5216)  # t5217: \"cuda:0 f32[4, 64, 3072]\"\n",
      "    # t5225 = prims.sum(t5217, (0, 1))  # t5225: \"cuda:0 f32[3072]\"\n",
      "  del f66, f68, f70, f72, t5196, t71\n",
      "  t5218 = torch.reshape(t5217, (-1, 3072))  # t5218: \"cuda:0 f32[256, 3072]\"\n",
      "    # t5218 = ltorch.reshape(t5217, (-1, 3072))  # t5218: \"cuda:0 f32[256, 3072]\"\n",
      "      # t5218 = prims.reshape(t5217, (256, 3072))  # t5218: \"cuda:0 f32[256, 3072]\"\n",
      "  del t5217\n",
      "  t5222 = torch.permute(t5218, (1, 0))  # t5222: \"cuda:0 f32[3072, 256]\"\n",
      "    # t5222 = ltorch.permute(t5218, (1, 0))  # t5222: \"cuda:0 f32[3072, 256]\"\n",
      "      # t5222 = prims.transpose(t5218, (1, 0))  # t5222: \"cuda:0 f32[3072, 256]\"\n",
      "  t5224 = torch.matmul(t5222, t5223)  # t5224: \"cuda:0 f32[3072, 768]\"\n",
      "    # t5224 = ltorch.matmul(t5222, t5223)  # t5224: \"cuda:0 f32[3072, 768]\"\n",
      "      # t5224 = prims.matmul(t5222, t5223)  # t5224: \"cuda:0 f32[3072, 768]\"\n",
      "  del t5222, t5223\n",
      "  t5219 = torch.matmul(t5218, t_transformer_h_0_mlp_c_fc_weight)  # t5219: \"cuda:0 f32[256, 768]\"\n",
      "    # t5219 = ltorch.matmul(t5218, t_transformer_h_0_mlp_c_fc_weight)  # t5219: \"cuda:0 f32[256, 768]\"\n",
      "      # t5219 = prims.matmul(t5218, t_transformer_h_0_mlp_c_fc_weight)  # t5219: \"cuda:0 f32[256, 768]\"\n",
      "  del t5218, t_transformer_h_0_mlp_c_fc_weight\n",
      "  t5220 = torch.reshape(t5219, (4, 64, 768))  # t5220: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5220 = ltorch.reshape(t5219, (4, 64, 768))  # t5220: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t5220 = prims.reshape(t5219, (4, 64, 768))  # t5220: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t5219\n",
      "  t5887 = torch.unsqueeze(t58, 2)  # t5887: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5887 = ltorch.unsqueeze(t58, 2)  # t5887: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t5887 = prims.broadcast_in_dim(t58, [4, 64, 1], [0, 1])  # t5887: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t58\n",
      "  t5247 = Tensor.expand(t5887, [4, 64, 1])  # t5247: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5247 = ltorch.expand(t5887, [4, 64, 1])  # t5247: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t5247 = prims.broadcast_in_dim(t5887, (4, 64, 1), (0, 1, 2))  # t5247: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t5887\n",
      "  t5248 = Tensor.expand(t5247, (4, 64, 768))  # t5248: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5248 = ltorch.expand(t5247, (4, 64, 768))  # t5248: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t5248 = prims.broadcast_in_dim(t5247, (4, 64, 768), (0, 1, 2))  # t5248: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t5247\n",
      "  [t5226, t5229, t5255, t5263] = nvFusion58(t5193, t5220, t5248, t54, t62, t64, t67)\n",
      "    # t65 = prims.broadcast_in_dim(t62, (4, 64, 768), (0, 1, 2))  # t65: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t66 = prims.mul(t64, t65)  # t66: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5226 = prims.sum(t5220, (0, 1))  # t5226: \"cuda:0 f32[768]\"\n",
      "    # t5227 = prims.mul(t67, t5220)  # t5227: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5228 = prims.mul(t66, t5220)  # t5228: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5229 = prims.sum(t5228, (0, 1))  # t5229: \"cuda:0 f32[768]\"\n",
      "    # t5230 = prims.mul(t65, t5227)  # t5230: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5231 = prims.mul(t64, t5227)  # t5231: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5232 = prims.sum(t5231, (2,))  # t5232: \"cuda:0 f32[4, 64]\"\n",
      "    # t5233 = prims.broadcast_in_dim(t5232, [4, 64, 1], [0, 1])  # t5233: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5234 = prims.neg(t5230)  # t5234: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5235 = prims.sum(t5234, (2,))  # t5235: \"cuda:0 f32[4, 64]\"\n",
      "    # t5236 = prims.broadcast_in_dim(t5235, [4, 64, 1], [0, 1])  # t5236: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5237 = prims.mul(-0.5, t5233)  # t5237: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5238 = prims.pow(t62, 3.0)  # t5238: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5239 = prims.mul(t5237, t5238)  # t5239: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5240 = prims.sum(t5236, (2,))  # t5240: \"cuda:0 f32[4, 64]\"\n",
      "    # t5241 = prims.sum(t5239, (2,))  # t5241: \"cuda:0 f32[4, 64]\"\n",
      "    # t5242 = prims.broadcast_in_dim(t5240, [4, 64, 1], [0, 1])  # t5242: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5243 = prims.broadcast_in_dim(t5242, (4, 64, 768), (0, 1, 2))  # t5243: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5244 = prims.mul(0.0013020833333333333, t5243)  # t5244: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5245 = prims.broadcast_in_dim(t5241, [4, 64, 1], [0, 1])  # t5245: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5246 = prims.broadcast_in_dim(t5245, (4, 64, 768), (0, 1, 2))  # t5246: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5249 = prims.mul(2.0, t5246)  # t5249: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5250 = prims.sub(t54, t5248)  # t5250: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5251 = prims.mul(t5249, t5250)  # t5251: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5252 = prims.div(t5251, 768.0)  # t5252: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5253 = prims.add(t5244, t5252)  # t5253: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5254 = prims.add(t5230, t5253)  # t5254: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5255 = prims.add(t5193, t5254)  # t5255: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5263 = prims.sum(t5255, (0, 1))  # t5263: \"cuda:0 f32[768]\"\n",
      "  del t5193, t5220, t5248, t54, t62, t64, t67\n",
      "  t5256 = torch.reshape(t5255, (-1, 768))  # t5256: \"cuda:0 f32[256, 768]\"\n",
      "    # t5256 = ltorch.reshape(t5255, (-1, 768))  # t5256: \"cuda:0 f32[256, 768]\"\n",
      "      # t5256 = prims.reshape(t5255, (256, 768))  # t5256: \"cuda:0 f32[256, 768]\"\n",
      "  t5260 = torch.permute(t5256, (1, 0))  # t5260: \"cuda:0 f32[768, 256]\"\n",
      "    # t5260 = ltorch.permute(t5256, (1, 0))  # t5260: \"cuda:0 f32[768, 256]\"\n",
      "      # t5260 = prims.transpose(t5256, (1, 0))  # t5260: \"cuda:0 f32[768, 256]\"\n",
      "  t5257 = torch.matmul(t5256, t_transformer_h_0_attn_c_proj_weight)  # t5257: \"cuda:0 f32[256, 768]\"\n",
      "    # t5257 = ltorch.matmul(t5256, t_transformer_h_0_attn_c_proj_weight)  # t5257: \"cuda:0 f32[256, 768]\"\n",
      "      # t5257 = prims.matmul(t5256, t_transformer_h_0_attn_c_proj_weight)  # t5257: \"cuda:0 f32[256, 768]\"\n",
      "  del t5256, t_transformer_h_0_attn_c_proj_weight\n",
      "  t5262 = torch.matmul(t5260, t5261)  # t5262: \"cuda:0 f32[768, 768]\"\n",
      "    # t5262 = ltorch.matmul(t5260, t5261)  # t5262: \"cuda:0 f32[768, 768]\"\n",
      "      # t5262 = prims.matmul(t5260, t5261)  # t5262: \"cuda:0 f32[768, 768]\"\n",
      "  del t5260, t5261\n",
      "  t5258 = torch.reshape(t5257, (4, 64, 768))  # t5258: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5258 = ltorch.reshape(t5257, (4, 64, 768))  # t5258: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t5258 = prims.reshape(t5257, (4, 64, 768))  # t5258: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t5257\n",
      "  t5264 = torch.reshape(t5258, (4, 64, 12, 64))  # t5264: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t5264 = ltorch.reshape(t5258, (4, 64, 12, 64))  # t5264: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t5264 = prims.reshape(t5258, (4, 64, 12, 64))  # t5264: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t5258\n",
      "  t5265 = torch.permute(t5264, (0, 2, 1, 3))  # t5265: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t5265 = ltorch.permute(t5264, (0, 2, 1, 3))  # t5265: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t5265 = prims.transpose(t5264, (0, 2, 1, 3))  # t5265: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t5264\n",
      "  t5269 = torch.matmul(t5268, t5265)  # t5269: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t5269 = ltorch.matmul(t5268, t5265)  # t5269: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t5269 = prims.matmul(t5268, t5265)  # t5269: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t5268\n",
      "  t5267 = torch.matmul(t5265, t5266)  # t5267: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t5267 = ltorch.matmul(t5265, t5266)  # t5267: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t5267 = prims.matmul(t5265, t5266)  # t5267: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t5265, t5266\n",
      "  [t5277] = nvFusion59(f46, i50, t38, t48, t5267)\n",
      "    # t5270 = prims.mul(t48, t5267)  # t5270: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t5271 = prims.sum(t5270, (3,))  # t5271: \"cuda:0 f32[4, 12, 64]\"\n",
      "    # t5272 = prims.broadcast_in_dim(t5271, [4, 12, 64, 1], [0, 1, 2])  # t5272: \"cuda:0 f32[4, 12, 64, 1]\"\n",
      "    # t5273 = prims.broadcast_in_dim(t5272, (4, 12, 64, 64), (0, 1, 2, 3))  # t5273: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t5274 = prims.sub(t5267, t5273)  # t5274: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t5275 = prims.mul(t48, t5274)  # t5275: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t5276 = prims.where(t38, 0.0, t5275)  # t5276: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t5277 = prims.mul(f46, t5276)  # t5277: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del f46, i50, t38, t48, t5267\n",
      "  t5279 = torch.matmul(t5277, t5278)  # t5279: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t5279 = ltorch.matmul(t5277, t5278)  # t5279: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t5279 = prims.matmul(t5277, t5278)  # t5279: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t5278\n",
      "  t5281 = torch.matmul(t5280, t5277)  # t5281: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t5281 = ltorch.matmul(t5280, t5277)  # t5281: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t5281 = prims.matmul(t5280, t5277)  # t5281: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t5280, t5277\n",
      "  t5282 = torch.permute(t5281, (0, 1, 3, 2))  # t5282: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "    # t5282 = ltorch.permute(t5281, (0, 1, 3, 2))  # t5282: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "      # t5282 = prims.transpose(t5281, (0, 1, 3, 2))  # t5282: \"cuda:0 f32[4, 12, 64, 64]\"\n",
      "  del t5281\n",
      "  t5283 = torch.permute(t5269, (0, 2, 1, 3))  # t5283: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t5283 = ltorch.permute(t5269, (0, 2, 1, 3))  # t5283: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t5283 = prims.transpose(t5269, (0, 2, 1, 3))  # t5283: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t5269\n",
      "  t5284 = torch.reshape(t5283, (4, 64, 768))  # t5284: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5284 = ltorch.reshape(t5283, (4, 64, 768))  # t5284: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t5284 = prims.reshape(t5283, (4, 64, 768))  # t5284: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t5283\n",
      "  t5285 = torch.permute(t5282, (0, 2, 1, 3))  # t5285: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t5285 = ltorch.permute(t5282, (0, 2, 1, 3))  # t5285: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t5285 = prims.transpose(t5282, (0, 2, 1, 3))  # t5285: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t5282\n",
      "  t5286 = torch.reshape(t5285, (4, 64, 768))  # t5286: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5286 = ltorch.reshape(t5285, (4, 64, 768))  # t5286: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t5286 = prims.reshape(t5285, (4, 64, 768))  # t5286: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t5285\n",
      "  t5287 = torch.permute(t5279, (0, 2, 1, 3))  # t5287: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "    # t5287 = ltorch.permute(t5279, (0, 2, 1, 3))  # t5287: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "      # t5287 = prims.transpose(t5279, (0, 2, 1, 3))  # t5287: \"cuda:0 f32[4, 64, 12, 64]\"\n",
      "  del t5279\n",
      "  t5288 = torch.reshape(t5287, (4, 64, 768))  # t5288: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5288 = ltorch.reshape(t5287, (4, 64, 768))  # t5288: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t5288 = prims.reshape(t5287, (4, 64, 768))  # t5288: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t5287\n",
      "  [t5289, t5297] = nvFusion60(i12, t5284, t5286, t5288)\n",
      "    # t5289 = prims.cat((t5288, t5286, t5284), i12)  # t5289: \"cuda:0 f32[4, 64, 2304]\"\n",
      "    # t5297 = prims.sum(t5289, (0, 1))  # t5297: \"cuda:0 f32[2304]\"\n",
      "  del i12, t5284, t5286, t5288\n",
      "  t5290 = torch.reshape(t5289, (-1, 2304))  # t5290: \"cuda:0 f32[256, 2304]\"\n",
      "    # t5290 = ltorch.reshape(t5289, (-1, 2304))  # t5290: \"cuda:0 f32[256, 2304]\"\n",
      "      # t5290 = prims.reshape(t5289, (256, 2304))  # t5290: \"cuda:0 f32[256, 2304]\"\n",
      "  del t5289\n",
      "  t5294 = torch.permute(t5290, (1, 0))  # t5294: \"cuda:0 f32[2304, 256]\"\n",
      "    # t5294 = ltorch.permute(t5290, (1, 0))  # t5294: \"cuda:0 f32[2304, 256]\"\n",
      "      # t5294 = prims.transpose(t5290, (1, 0))  # t5294: \"cuda:0 f32[2304, 256]\"\n",
      "  t5296 = torch.matmul(t5294, t5295)  # t5296: \"cuda:0 f32[2304, 768]\"\n",
      "    # t5296 = ltorch.matmul(t5294, t5295)  # t5296: \"cuda:0 f32[2304, 768]\"\n",
      "      # t5296 = prims.matmul(t5294, t5295)  # t5296: \"cuda:0 f32[2304, 768]\"\n",
      "  del t5294, t5295\n",
      "  t5291 = torch.matmul(t5290, t_transformer_h_0_attn_c_attn_weight)  # t5291: \"cuda:0 f32[256, 768]\"\n",
      "    # t5291 = ltorch.matmul(t5290, t_transformer_h_0_attn_c_attn_weight)  # t5291: \"cuda:0 f32[256, 768]\"\n",
      "      # t5291 = prims.matmul(t5290, t_transformer_h_0_attn_c_attn_weight)  # t5291: \"cuda:0 f32[256, 768]\"\n",
      "  del t5290, t_transformer_h_0_attn_c_attn_weight\n",
      "  t5292 = torch.reshape(t5291, (4, 64, 768))  # t5292: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5292 = ltorch.reshape(t5291, (4, 64, 768))  # t5292: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t5292 = prims.reshape(t5291, (4, 64, 768))  # t5292: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t5291\n",
      "  t5905 = torch.unsqueeze(t10, 2)  # t5905: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5905 = ltorch.unsqueeze(t10, 2)  # t5905: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t5905 = prims.broadcast_in_dim(t10, [4, 64, 1], [0, 1])  # t5905: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t10\n",
      "  t5319 = Tensor.expand(t5905, [4, 64, 1])  # t5319: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5319 = ltorch.expand(t5905, [4, 64, 1])  # t5319: \"cuda:0 f32[4, 64, 1]\"\n",
      "      # t5319 = prims.broadcast_in_dim(t5905, (4, 64, 1), (0, 1, 2))  # t5319: \"cuda:0 f32[4, 64, 1]\"\n",
      "  del t5905\n",
      "  t5320 = Tensor.expand(t5319, (4, 64, 768))  # t5320: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5320 = ltorch.expand(t5319, (4, 64, 768))  # t5320: \"cuda:0 f32[4, 64, 768]\"\n",
      "      # t5320 = prims.broadcast_in_dim(t5319, (4, 64, 768), (0, 1, 2))  # t5320: \"cuda:0 f32[4, 64, 768]\"\n",
      "  del t5319\n",
      "  [t5298, t5301, t5327, t5328] = nvFusion61(t14, t16, t19, t5255, t5292, t5320, t6)\n",
      "    # t17 = prims.broadcast_in_dim(t14, (4, 64, 768), (0, 1, 2))  # t17: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t18 = prims.mul(t16, t17)  # t18: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5298 = prims.sum(t5292, (0, 1))  # t5298: \"cuda:0 f32[768]\"\n",
      "    # t5299 = prims.mul(t19, t5292)  # t5299: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5300 = prims.mul(t18, t5292)  # t5300: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5301 = prims.sum(t5300, (0, 1))  # t5301: \"cuda:0 f32[768]\"\n",
      "    # t5302 = prims.mul(t17, t5299)  # t5302: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5303 = prims.mul(t16, t5299)  # t5303: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5304 = prims.sum(t5303, (2,))  # t5304: \"cuda:0 f32[4, 64]\"\n",
      "    # t5305 = prims.broadcast_in_dim(t5304, [4, 64, 1], [0, 1])  # t5305: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5306 = prims.neg(t5302)  # t5306: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5307 = prims.sum(t5306, (2,))  # t5307: \"cuda:0 f32[4, 64]\"\n",
      "    # t5308 = prims.broadcast_in_dim(t5307, [4, 64, 1], [0, 1])  # t5308: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5309 = prims.mul(-0.5, t5305)  # t5309: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5310 = prims.pow(t14, 3.0)  # t5310: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5311 = prims.mul(t5309, t5310)  # t5311: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5312 = prims.sum(t5308, (2,))  # t5312: \"cuda:0 f32[4, 64]\"\n",
      "    # t5313 = prims.sum(t5311, (2,))  # t5313: \"cuda:0 f32[4, 64]\"\n",
      "    # t5314 = prims.broadcast_in_dim(t5312, [4, 64, 1], [0, 1])  # t5314: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5315 = prims.broadcast_in_dim(t5314, (4, 64, 768), (0, 1, 2))  # t5315: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5316 = prims.mul(0.0013020833333333333, t5315)  # t5316: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5317 = prims.broadcast_in_dim(t5313, [4, 64, 1], [0, 1])  # t5317: \"cuda:0 f32[4, 64, 1]\"\n",
      "    # t5318 = prims.broadcast_in_dim(t5317, (4, 64, 768), (0, 1, 2))  # t5318: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5321 = prims.mul(2.0, t5318)  # t5321: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5322 = prims.sub(t6, t5320)  # t5322: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5323 = prims.mul(t5321, t5322)  # t5323: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5324 = prims.div(t5323, 768.0)  # t5324: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5325 = prims.add(t5316, t5324)  # t5325: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5326 = prims.add(t5302, t5325)  # t5326: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5327 = prims.add(t5255, t5326)  # t5327: \"cuda:0 f32[4, 64, 768]\"\n",
      "    # t5328 = prims.sum(t5327, (0,))  # t5328: \"cuda:0 f32[64, 768]\"\n",
      "  del t14, t16, t19, t5255, t5292, t5320, t6\n",
      "  t5329 = torch.torch.ops.aten.embedding_backward(t5328, t0, i3, -1, b4, b5)  # t5329: \"cuda:0 f32[1024, 768]\"\n",
      "    # t5329 = ltorch.embedding_backward(t5328, t0, i3, -1, b4, b5)  # t5329: \"cuda:0 f32[1024, 768]\"\n",
      "      # t5329 = prims.embedding_backward(t5328, t0, i3, -1, b4, b5)  # t5329: \"cuda:0 f32[1024, 768]\"\n",
      "  del t5328, t0, i3, b4, b5\n",
      "  t5330 = torch.torch.ops.aten.embedding_backward(t5327, idx, i0, -1, b1, b2)  # t5330: \"cuda:0 f32[50257, 768]\"\n",
      "    # t5330 = ltorch.embedding_backward(t5327, idx, i0, -1, b1, b2)  # t5330: \"cuda:0 f32[50257, 768]\"\n",
      "      # t5330 = prims.embedding_backward(t5327, idx, i0, -1, b1, b2)  # t5330: \"cuda:0 f32[50257, 768]\"\n",
      "  del t5327, idx, i0, b1, b2\n",
      "  return (None, None, t5298, t5297, t5263, t5226, t5225, t5201, t5164, t5163, t5129, t5092, t5091, t5067, t5030, t5029, t4995, t4958, t4957, t4933, t4896, t4895, t4861, t4824, t4823, t4799, t4762, t4761, t4727, t4690, t4689, t4665, t4628, t4627, t4593, t4556, t4555, t4531, t4494, t4493, t4459, t4422, t4421, t4397, t4360, t4359, t4325, t4288, t4287, t4263, t4226, t4225, t4191, t4154, t4153, t4129, t4092, t4091, t4057, t4020, t4019, t3995, t3958, t3957, t3923, t3886, t3885, t3861, t3824, t3823, t3789, t3752, t3751, t3727, t3691, None, None, None, None, None, None, None, None, None, None, None, None, t5296, t5162, t5028, t4894, t4760, t4626, t4492, t4358, t4224, t4090, t3956, t3822, t5224, t5090, t4956, t4822, t4688, t4554, t4420, t4286, t4152, t4018, t3884, t3750, t5262, t5200, t5128, t5066, t4994, t4932, t4860, t4798, t4726, t4664, t4592, t4530, t4458, t4396, t4324, t4262, t4190, t4128, t4056, t3994, t3922, t3860, t3788, t3726, t3690, t5301, t3961, t3827, t5167, t5033, t4899, t4765, t4631, t4497, t4363, t4229, t4095, t5229, t5095, t4961, t4827, t4693, t4559, t4425, t4291, t4157, t4023, t3889, t3755, t3694, t5329, t5330)\n"
     ]
    }
   ],
   "source": [
    "backward_trace = thunder.last_backward_traces(thunder_model)[-1]\n",
    "print(backward_trace)\n",
    "del thunder_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20b7ac9-67cd-438f-abf6-879ce866fd60",
   "metadata": {},
   "source": [
    "## Compiling CUDA kernels with [CUDA-Python](https://github.com/NVIDIA/cuda-python)\n",
    "We are using [CUDA-Python](https://github.com/NVIDIA/cuda-python) to compile CUDA kernels and provide bindings for Python.\n",
    "We already have an excellent resource, [Extend Thunder with CUDA-Python](./extend_thunder_with_cuda_python.ipynb), that explains the topic in greater detail. For now, we will reuse some of its helper functions that allow us to compile and run CUDA kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ce3daaf-ebd0-48e2-8c09-a28d2eaace49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuda import cuda, nvrtc\n",
    "\n",
    "\n",
    "def check_error(results):\n",
    "    err, *results = results\n",
    "    if isinstance(err, cuda.CUresult):\n",
    "        if err != cuda.CUresult.CUDA_SUCCESS:\n",
    "            raise RuntimeError(f\"CUDA error: {cuda.cuGetErrorString(err)}\")\n",
    "    elif isinstance(err, nvrtc.nvrtcResult):\n",
    "        if err != nvrtc.nvrtcResult.NVRTC_SUCCESS:\n",
    "            raise RuntimeError(f\"NVRTC error: {nvrtc.nvrtcGetErrorString(err)}\")\n",
    "    else:\n",
    "        raise TypeError(\"Unknown error type: {err}\")\n",
    "    if len(results) == 0:\n",
    "        return\n",
    "    if len(results) == 1:\n",
    "        return results[0]\n",
    "    return results\n",
    "\n",
    "\n",
    "def compile_program_and_get_module(cuda_src, program_name):\n",
    "    \"\"\"\n",
    "    Compiles a kernel from the CUDA source code provided in the string `cuda_src` with the name `program_name`\n",
    "    and returns a PTX represented as a module data.\n",
    "\n",
    "    The module is then could be used to retrieve CUDA kernels which in turn could be run with `launch_kernel`\n",
    "    \"\"\"\n",
    "\n",
    "    torch.cuda.current_stream()  # this initializes the device context for us. we don't need the stream specifically.\n",
    "    \n",
    "    # Create program\n",
    "    prog = check_error(nvrtc.nvrtcCreateProgram(str.encode(cuda_src), (program_name + '.cu').encode(), 0, [], []))    \n",
    "    \n",
    "    # Compile program\n",
    "    min, maj = torch.cuda.get_device_capability()\n",
    "    opts = [\n",
    "        f\"--gpu-architecture=compute_{min}{maj}\".encode(),\n",
    "        b\"--include-path=/usr/local/cuda/include/\",\n",
    "        b\"--include-path=/usr/include/\",\n",
    "        b\"--use_fast_math\",\n",
    "        b\"--dopt=on\",\n",
    "    ] #, b\"--expt-relaxed-constexpr\"]\n",
    "    check_error(nvrtc.nvrtcCompileProgram(prog, len(opts), opts))\n",
    "    \n",
    "    ## Get PTX from compilation\n",
    "    ptxSize = check_error(nvrtc.nvrtcGetPTXSize(prog))\n",
    "    ptx = b\" \" * ptxSize\n",
    "    check_error(nvrtc.nvrtcGetPTX(prog, ptx))\n",
    "    \n",
    "    logSize = check_error(nvrtc.nvrtcGetProgramLogSize(prog))\n",
    "    log = b\" \" * logSize\n",
    "    check_error(nvrtc.nvrtcGetProgramLog(prog, log))\n",
    "    print(log.decode())\n",
    "    \n",
    "    \n",
    "    # Load PTX as module data and retrieve function\n",
    "    module = check_error(cuda.cuModuleLoadData(ptx))\n",
    "    return module\n",
    "\n",
    "\n",
    "def launch_kernel(kernel, grid, block, /, *args, shmem=0):\n",
    "    \"\"\"utility function to launch kernels.\n",
    "    Args can be tensors (corresponding to float* etc kernel params or numpy scalars (which have precision info))\n",
    "    \"\"\"\n",
    "\n",
    "    # collect values (data_ptr as uint64 array for tensors, the values as an array for values)\n",
    "    addresses = []\n",
    "    wrapped_args = []\n",
    "    for a in args:\n",
    "        if isinstance(a, torch.Tensor):\n",
    "            # for tensor pass in data_ptr\n",
    "            wrapped_args.append(numpy.array(a.data_ptr(), dtype=numpy.uint64))\n",
    "        elif isinstance(a, numpy.number):\n",
    "            wrapped_args.append(numpy.array([a]))\n",
    "        else:\n",
    "            raise TypeError(\"please only pass tensors and numpy numbers to launch_kernel\")\n",
    "\n",
    "    # assemble an array of pointers to the args\n",
    "    args = numpy.array([a.ctypes.data for a in wrapped_args], dtype=numpy.uint64)\n",
    "\n",
    "    # set up grid / block layout to be 3d\n",
    "    grid = tuple(grid)\n",
    "    block = tuple(block)\n",
    "    assert 1 <= len(block) <= 3 and 1 <= len(grid) <= 3\n",
    "    grid = grid + (3 - len(grid)) * (1,)\n",
    "    block = block + (3 - len(block)) * (1,)\n",
    "\n",
    "    # Launch!\n",
    "    err, = cuda.cuLaunchKernel(\n",
    "       kernel,\n",
    "       *grid, *block, # xyz each\n",
    "       shmem,  # dynamic shared memory\n",
    "       torch.cuda.current_stream().stream_id,  # stream\n",
    "       args.ctypes.data,  # kernel arguments\n",
    "       0,  # extra (ignore)\n",
    "    )\n",
    "    if err != cuda.CUresult.CUDA_SUCCESS:\n",
    "        raise RuntimeError(f\"CUDA error: {err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3e3da7-b933-4081-941a-c1cfcd2dbdb2",
   "metadata": {},
   "source": [
    "## [llm.c](https://github.com/karpathy/llm.c): CUDA kernels\n",
    "Below we list all the CUDA kernels from [llm.c/train_gpt2.py](./llm.c/train_gpt2.py). Note that some of these kernels are defined with `extern \"C\"`. This is needed to avoid name mangling when accessing the kernels with [CUDA-Python](https://github.com/NVIDIA/cuda-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d53514d8-5ae1-4edc-ab15-2091cf492eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_kernels = r\"\"\"\n",
    "#include <cooperative_groups.h>\n",
    "#include <cooperative_groups/reduce.h>\n",
    "\n",
    "#define NEG_INFINITY __int_as_float(0xff800000)\n",
    "#define FLT_MAX 3.402823466e+38F\n",
    "#define M_PI 3.14159265358979323846\n",
    "\n",
    "\n",
    "// convenience macro for calculating grid/block dimensions for kernels\n",
    "#define CEIL_DIV(M, N) (((M) + (N)-1) / (N))\n",
    "\n",
    "// ----------------------------------------------------------------------------\n",
    "// all the kernels\n",
    "\n",
    "// warp-level reduction for finding the maximum value\n",
    "__device__ float warpReduceMax(float val) {\n",
    "    for (int offset = 16; offset > 0; offset /= 2) {\n",
    "        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n",
    "    }\n",
    "    return val;\n",
    "}\n",
    "\n",
    "// warp-level reduction for summing values\n",
    "__device__ float warpReduceSum(float val) {\n",
    "    for (int offset = 16; offset > 0; offset /= 2) {\n",
    "        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n",
    "    }\n",
    "    return val;\n",
    "}\n",
    "\n",
    "__global__ void encoder_forward_kernel2(float* out,\n",
    "                               int* inp, float* wte, float* wpe,\n",
    "                               int B, int T, int C) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int N = B * T * C;\n",
    "\n",
    "    if (idx < N) {\n",
    "        int bt = idx / C;\n",
    "        int b = bt / T;\n",
    "        int t = bt % T;\n",
    "        int c = idx % C;\n",
    "\n",
    "        int ix = inp[b * T + t];\n",
    "\n",
    "        float* out_btc = out + b * T * C + t * C + c;\n",
    "        float* wte_ix = wte + ix * C + c;\n",
    "        float* wpe_tc = wpe + t * C + c;\n",
    "        *out_btc = *wte_ix + *wpe_tc;\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "extern \"C\"\n",
    "__global__ void layernorm_forward_kernel3(float* __restrict__ out, float* __restrict__ mean, float* __restrict__ rstd,\n",
    "                                    const float*  __restrict__ inp, const float*  __restrict__ weight,\n",
    "                                    const float* __restrict__ bias, int N, int C) {\n",
    "    namespace cg = cooperative_groups;\n",
    "    cg::thread_block block = cg::this_thread_block();\n",
    "    cg::thread_block_tile<32> warp = cg::tiled_partition<32>(block);\n",
    "    int idx = blockIdx.x * warp.meta_group_size() + warp.meta_group_rank();\n",
    "    if(idx >= N) {\n",
    "        return;\n",
    "    }\n",
    "\n",
    "    // the row of input that this group of threads is responsible for\n",
    "    const float* x = inp + idx * C;\n",
    "\n",
    "    // mean\n",
    "    float sum = 0.0f;\n",
    "    for (int i = warp.thread_rank(); i < C; i += warp.size()) {\n",
    "        sum += x[i];\n",
    "    }\n",
    "    sum = cg::reduce(warp, sum, cg::plus<float>{});\n",
    "    float m = sum / C;\n",
    "    if(warp.thread_rank() == 0 && mean != nullptr) {\n",
    "        __stcs(mean + idx, m);\n",
    "    }\n",
    "\n",
    "    // rstd\n",
    "    sum = 0.0f;\n",
    "    for (int i = warp.thread_rank(); i < C; i += warp.size()) {\n",
    "        float diff = x[i] - m;\n",
    "        sum += diff * diff;\n",
    "    }\n",
    "    sum = cg::reduce(warp, sum, cg::plus<float>{});\n",
    "    float s = rsqrtf(sum / C + 1e-5f);\n",
    "    if(warp.thread_rank() == 0 && rstd != nullptr) {\n",
    "        __stcs(rstd + idx, s);\n",
    "    }\n",
    "\n",
    "    // final normalization and scaling by weight/bias\n",
    "    float* o = out + idx * C;\n",
    "    for (int c = warp.thread_rank(); c < C; c += warp.size()) {\n",
    "        // load and store using the .cs \"streaming\" hint to the compiler,\n",
    "        // indicating that this data will not be reused soon, and can be streamed through the caches\n",
    "        // this allows the threads to get more cache-hits for the (shared) weight and bias parameters\n",
    "        float n = s * (__ldcs(x+c) - m);\n",
    "        __stcs(o+c, n * weight[c] + bias[c]);\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void add_bias(float* out, float* bias, int B, int T, int OC) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    for (int i = idx; i < B*T*OC; i += stride) {\n",
    "        int col = i % OC;\n",
    "        out[i] += bias[col];\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "extern \"C\"\n",
    "__global__ void permute_kernel(float* q, float* k, float* v,\n",
    "                               const float* inp,\n",
    "                               int B, int N, int NH, int d) {\n",
    "    // okay so now, this kernel wants Q,K,V to all be of shape (B, NH, N, d)\n",
    "    // but instead, we have a single tensor QKV (inp) of shape (B, N, 3, NH, d)\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    // Q[b][nh_][n][d_] = inp[b][n][0][nh_][d_]\n",
    "\n",
    "    if (idx < B * NH * N * d) {\n",
    "        int b = idx / (NH * N * d);\n",
    "        int rest = idx % (NH * N * d);\n",
    "        int nh_ = rest / (N * d);\n",
    "        rest = rest % (N * d);\n",
    "        int n = rest / d;\n",
    "        int d_ = rest % d;\n",
    "\n",
    "        int inp_idx = \\\n",
    "            (b * N * 3 * NH * d)\n",
    "            +   (n * 3 * NH * d)\n",
    "            +       (0 * NH * d)\n",
    "            +          (nh_ * d)\n",
    "            +                d_;\n",
    "\n",
    "        q[idx] = __ldcs(&inp[inp_idx]);\n",
    "        k[idx] = __ldcs(&inp[inp_idx + NH * d]);\n",
    "        v[idx] = __ldcs(&inp[inp_idx + 2 * (NH * d)]);\n",
    "    }\n",
    "}\n",
    "\n",
    "extern \"C\"\n",
    "__global__ void permute_kernel_backward(float* dinp,\n",
    "                                        const float* dq, const float* dk, const float* dv,\n",
    "                                        int B, int N, int NH, int d) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < B * NH * N * d) {\n",
    "        int b = idx / (NH * N * d);\n",
    "        int rest = idx % (NH * N * d);\n",
    "        int nh_ = rest / (N * d);\n",
    "        rest = rest % (N * d);\n",
    "        int n = rest / d;\n",
    "        int d_ = rest % d;\n",
    "\n",
    "        int inp_idx = (b * N * 3 * NH * d) + (n * 3 * NH * d) + (0 * NH * d) + (nh_ * d) + d_;\n",
    "        dinp[inp_idx] += dq[idx];\n",
    "        dinp[inp_idx + NH * d] += dk[idx];\n",
    "        dinp[inp_idx + 2 * (NH * d)] += dv[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "extern \"C\"\n",
    "__global__ void unpermute_kernel(float* inp, float *out, int B, int N, int NH, int d) {\n",
    "   // out has shape (B, nh, N, d) but we need to unpermute it to (B, N, nh, d)\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    // out[b][n][nh_][d_] <- inp[b][nh_][n][d_]\n",
    "    if (idx < B * NH * N * d) {\n",
    "        int b = idx / (NH * N * d);\n",
    "        int rest = idx % (NH * N * d);\n",
    "        int nh_ = rest / (N * d);\n",
    "        rest = rest % (N * d);\n",
    "        int n = rest / d;\n",
    "        int d_ = rest % d;\n",
    "\n",
    "        int other_idx = (b * NH * N * d) + (n * NH * d) + (nh_ * d) + d_;\n",
    "        out[other_idx] = __ldcs(&inp[idx]);\n",
    "    }\n",
    "}\n",
    "\n",
    "extern \"C\"\n",
    "__global__ void unpermute_kernel_backward(float* dinp, const float *dout, int B, int N, int NH, int d) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < B * NH * N * d) {\n",
    "        int b = idx / (NH * N * d);\n",
    "        int rest = idx % (NH * N * d);\n",
    "        int nh_ = rest / (N * d);\n",
    "        rest = rest % (N * d);\n",
    "        int n = rest / d;\n",
    "        int d_ = rest % d;\n",
    "\n",
    "        int other_idx = (b * NH * N * d) + (n * NH * d) + (nh_ * d) + d_;\n",
    "        dinp[idx] += dout[other_idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "__device__ float& vec_at(float4& vec, int index) {\n",
    "    return reinterpret_cast<float*>(&vec)[index];\n",
    "}\n",
    "\n",
    "__device__ float vec_at(const float4& vec, int index) {\n",
    "    return reinterpret_cast<const float*>(&vec)[index];\n",
    "}\n",
    "\n",
    "__global__ void softmax_forward_kernel5(float* out, float inv_temperature, const float* inp, int N, int T) {\n",
    "    // inp, out shape: (N, T, T), where N = B * NH\n",
    "    // fuses the multiplication by scale inside attention\n",
    "    // directly autoregressive, so we only compute the lower triangular part\n",
    "    // uses the online softmax algorithm\n",
    "    assert(T % 4  == 0);\n",
    "    namespace cg = cooperative_groups;\n",
    "    cg::thread_block block = cg::this_thread_block();\n",
    "    cg::thread_block_tile<32> warp = cg::tiled_partition<32>(block);\n",
    "    int idx = blockIdx.x * warp.meta_group_size() + warp.meta_group_rank();\n",
    "    if(idx >= N * T) {\n",
    "        return;\n",
    "    }\n",
    "    int own_pos = idx % T;\n",
    "    int pos_by_4 = own_pos / 4;\n",
    "\n",
    "    // one row of inp, i.e. inp[idx, :] of shape (T,)\n",
    "    const float* x = inp + idx * T;\n",
    "\n",
    "    // not INF, so we don't get NaNs accidentally when subtracting two values.\n",
    "    float maxval = -FLT_MAX;\n",
    "    float sumval = 0.0f;\n",
    "\n",
    "    const float4* x_vec = reinterpret_cast<const float4*>(x);\n",
    "    for (int i = warp.thread_rank(); i < pos_by_4; i += warp.size()) {\n",
    "        float4 v = x_vec[i];\n",
    "        float old_maxval = maxval;\n",
    "        for(int k = 0; k < 4; ++k) {\n",
    "            maxval = fmaxf(maxval, vec_at(v, k));\n",
    "        }\n",
    "        sumval *= expf(inv_temperature * (old_maxval - maxval));\n",
    "        for(int k = 0; k < 4; ++k) {\n",
    "            sumval += expf(inv_temperature * (vec_at(v, k) - maxval));\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if(4*pos_by_4 + warp.thread_rank() <= own_pos) {\n",
    "        float old_maxval = maxval;\n",
    "        maxval = fmaxf(maxval, x[4*pos_by_4 + warp.thread_rank()]);\n",
    "        sumval *= expf(inv_temperature * (old_maxval - maxval));\n",
    "        sumval += expf(inv_temperature * (x[4*pos_by_4 + warp.thread_rank()] - maxval));\n",
    "    }\n",
    "\n",
    "    float global_maxval = cg::reduce(warp, maxval, cg::greater<float>{});\n",
    "    sumval *= expf(inv_temperature * (maxval - global_maxval));\n",
    "\n",
    "    float sum = cg::reduce(warp, sumval, cg::plus<float>{});\n",
    "    float norm = 1.f / sum;\n",
    "\n",
    "    // divide the whole row by the sum\n",
    "    for (int i = warp.thread_rank(); i <= own_pos; i += warp.size()) {\n",
    "        // recalculation is faster than doing the round-trip through memory.\n",
    "        float ev = expf(inv_temperature * (__ldcs(x + i) - global_maxval));\n",
    "        __stcs(out + idx * T + i, ev * norm);\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void residual_forward_kernel(float* out, float* inp1, float* inp2, int N) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < N) {\n",
    "        out[idx] = __ldcs(&inp1[idx]) + __ldcs(&inp2[idx]);\n",
    "    }\n",
    "}\n",
    "\n",
    "#define GELU_SCALING_FACTOR sqrtf(2.0f / M_PI)\n",
    "\n",
    "extern \"C\"\n",
    "__global__ void gelu_forward_kernel(float* out, const float* inp, int N) {\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (i < N) {\n",
    "        float xi = inp[i];\n",
    "        float cube = 0.044715f * xi * xi * xi;\n",
    "        out[i] = 0.5f * xi * (1.0f + tanhf(GELU_SCALING_FACTOR * (xi + cube)));\n",
    "    }\n",
    "}\n",
    "\n",
    "extern \"C\"\n",
    "__global__ void gelu_backward_kernel(float* dinp, const float* inp, const float* dout, const int N) {\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (i < N) {\n",
    "        float x = inp[i];\n",
    "        float cube = 0.044715f * x * x * x;\n",
    "        float tanh_arg = GELU_SCALING_FACTOR * (x + cube);\n",
    "        float tanh_out = tanhf(tanh_arg);\n",
    "        float coshf_out = coshf(tanh_arg);\n",
    "        float sech_out = 1.0f / (coshf_out * coshf_out);\n",
    "        float local_grad = 0.5f * (1.0f + tanh_out) + x * 0.5f * sech_out * GELU_SCALING_FACTOR * (1.0f + 3.0f * 0.044715f * x * x);\n",
    "        dinp[i] += local_grad * dout[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "extern \"C\"\n",
    "__global__ void crossentropy_forward_kernel1(float* losses,\n",
    "                            float* probs, int* targets,\n",
    "                            int B, int T, int V) {\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (i < B * T) {\n",
    "        int b = i / T;\n",
    "        int t = i % T;\n",
    "        float* probs_bt = probs + b * T * V + t * V;\n",
    "        int ix = targets[b * T + t];\n",
    "        losses[b * T + t] = -logf(probs_bt[ix]);\n",
    "    }\n",
    "}\n",
    "\n",
    "extern \"C\"\n",
    "__global__ void softmax_forward_kernel7(float* out, const float* inp, int N, int C) {\n",
    "    // out is (N, C) just like inp. Each row of inp will get softmaxed.\n",
    "    // same as kernel4, but optimised for very large Cs with advanced unrolling\n",
    "\n",
    "    // The trick is to read into a register array (all indices known at compile time)\n",
    "    // and always read UNROLL_FACTOR values to maximise memory level parallelism\n",
    "    // even if we would be out of bounds, we set the index to min(C-1, idx)\n",
    "    // so we just do some unnecessary reads (obviously bad for small C)\n",
    "    // the writes are in a separate loop with a conditional check for out of bounds\n",
    "    // making it separate is necessary to convince the compiler to do the right thing\n",
    "    const int UNROLL_FACTOR = 8;\n",
    "    const int warpsPerBlock = blockDim.x / 32;\n",
    "\n",
    "    extern __shared__ float shared[];\n",
    "    int idx = blockIdx.x;\n",
    "    int tid = threadIdx.x;\n",
    "    int warpId = threadIdx.x / 32; // warp index within a block\n",
    "    int laneId = threadIdx.x % 32; // thread index within a warp\n",
    "\n",
    "    // shared[] must be allocated to have 2 * warpsPerBlock elements\n",
    "    // first half for max values, the second half for sum values\n",
    "    float* maxvals = shared;\n",
    "    float* sumvals = &shared[warpsPerBlock];\n",
    "\n",
    "    if (tid >= C) {\n",
    "        maxvals[warpId] = NEG_INFINITY;\n",
    "        sumvals[warpId] = 0.0f;\n",
    "        return;\n",
    "    }\n",
    "\n",
    "    const float* x = inp + idx * C; // input\n",
    "    float* y = out + idx * C; // output\n",
    "\n",
    "    // first, thread coarsening by directly accessing global memory in series\n",
    "    float maxval = NEG_INFINITY;\n",
    "    for (int i = tid; i < C; i += blockDim.x * UNROLL_FACTOR) {\n",
    "        #pragma unroll\n",
    "        for (int u = 0; u < UNROLL_FACTOR; u++) {\n",
    "            maxval = fmaxf(maxval, x[min(C - 1, i + u*blockDim.x)]);\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // now within-warp reductions for maxval\n",
    "    maxval = warpReduceMax(maxval);\n",
    "    // the 0th thread of each warp writes the maxval of that warp to shared memory\n",
    "    if (laneId == 0) maxvals[warpId] = maxval;\n",
    "    __syncthreads();\n",
    "    // now the 0th thread reduces the maxvals in shared memory, i.e. across warps\n",
    "    if (tid == 0) {\n",
    "        float val = maxvals[tid];\n",
    "        #pragma unroll\n",
    "        for (int i = 1; i < warpsPerBlock; i++) {\n",
    "            val = fmaxf(val, maxvals[i]);\n",
    "        }\n",
    "        // store the final max in the first position\n",
    "        maxvals[0] = val;\n",
    "    }\n",
    "    __syncthreads();\n",
    "    // broadcast the max to all threads\n",
    "    float offset = maxvals[0];\n",
    "\n",
    "    // compute expf and write the result to global memory\n",
    "    // + thread coarsening for sum\n",
    "    float sumval = 0.0f;\n",
    "    for (int i = tid; i < C; i += blockDim.x * UNROLL_FACTOR) {\n",
    "        float reg_array[UNROLL_FACTOR];\n",
    "        #pragma unroll\n",
    "        for (int u = 0; u < UNROLL_FACTOR; u++) {\n",
    "            reg_array[u] = __ldcs(&x[min(C - 1, i + u*blockDim.x)]);\n",
    "        }\n",
    "        #pragma unroll\n",
    "        for (int u = 0; u < UNROLL_FACTOR; u++) {\n",
    "            if (i + u*blockDim.x < C) {\n",
    "                float output = expf(reg_array[u] - offset);\n",
    "                y[min(C - 1, i + u*blockDim.x)] = output; // compiler likes redundant min()?!\n",
    "                sumval += output; // combined into the same loop unlike kernel3\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // okay now we calculated exp(x - max(x))\n",
    "    // step 2: sum all the values and divide by the sum\n",
    "\n",
    "    // within-warp reduction for sumval\n",
    "    sumval = warpReduceSum(sumval);\n",
    "    // write sumval to shared memory\n",
    "    if (laneId == 0) sumvals[warpId] = sumval;\n",
    "    __syncthreads();\n",
    "    // inter-thread reduction of sum\n",
    "    if (tid == 0) {\n",
    "        float val = sumvals[tid];\n",
    "        #pragma unroll\n",
    "        for (int i = 1; i < warpsPerBlock; ++i) {\n",
    "            val += sumvals[i];\n",
    "        }\n",
    "        sumvals[0] = val;\n",
    "    }\n",
    "    __syncthreads();\n",
    "    // broadcast the sum to all threads\n",
    "    float sum = sumvals[0];\n",
    "\n",
    "    // divide the whole row by the sum\n",
    "    for (int i = tid; i < C; i += blockDim.x * UNROLL_FACTOR) {\n",
    "        float reg_array[UNROLL_FACTOR];\n",
    "        #pragma unroll\n",
    "        for (int u = 0; u < UNROLL_FACTOR; u++) {\n",
    "            reg_array[u] = y[min(C - 1, i + u*blockDim.x)];\n",
    "        }\n",
    "        #pragma unroll\n",
    "        for (int u = 0; u < UNROLL_FACTOR; u++) {\n",
    "            if (i + u*blockDim.x < C) {\n",
    "                y[i + u*blockDim.x] = reg_array[u] / sum;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "extern \"C\"\n",
    "__global__ void crossentropy_softmax_backward_kernel1(float* dlogits,\n",
    "                           const float* dlosses, const float* probs, const int* targets,\n",
    "                           int B, int T, int V) {\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (i < B * T * V) {\n",
    "        int b = i / (T * V);\n",
    "        int t = (i / V) % T;\n",
    "        int v = i % V;\n",
    "        float* dlogits_bt = dlogits + b * T * V + t * V;\n",
    "        const float* probs_bt = probs + b * T * V + t * V;\n",
    "        float dloss = dlosses[b * T + t];\n",
    "        int ix = targets[b * T + t];\n",
    "        float p = probs_bt[v];\n",
    "        float indicator = v == ix ? 1.0f : 0.0f;\n",
    "        dlogits_bt[v] += (p - indicator) * dloss;\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void matmul_backward_bias_kernel_faster(float* dbias, const float* dout, int B, int T, int OC) {\n",
    "    extern __shared__ float shared[];\n",
    "    int o = blockIdx.x; // range [0, OC)\n",
    "    int tid = threadIdx.x; // range [0, block_size)\n",
    "    int block_size = blockDim.x;\n",
    "    const float* x = dout + o;\n",
    "    // thread coarsening\n",
    "    double sum = 0.0f;\n",
    "    for (int i = tid; i < B * T; i += block_size) {\n",
    "        sum += x[i * OC];\n",
    "    }\n",
    "    shared[tid] = (float) sum;\n",
    "    __syncthreads();\n",
    "    // reductions\n",
    "    for (int stride = block_size / 2; stride >= 1; stride /= 2) {\n",
    "        __syncthreads();\n",
    "        if (tid < stride) {\n",
    "            shared[tid] += shared[tid + stride];\n",
    "        }\n",
    "    }\n",
    "    // write the final result (at thread 0) to global memory\n",
    "    if (tid == 0) {\n",
    "        dbias[o] = shared[0];\n",
    "    }\n",
    "}\n",
    "\n",
    "// super naive layernorm backward kernel that just parallelizes over B,T and loops over C\n",
    "extern \"C\"\n",
    "__global__ void layernorm_backward_kernel1(float* dinp, float* dweight, float* dbias,\n",
    "                        float* dout, float* inp, float* weight, float* mean, float* rstd,\n",
    "                        int B, int T, int C) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx >= B*T) return;\n",
    "    int b = idx / T;\n",
    "    int t = idx % T;\n",
    "\n",
    "    float* dout_bt = dout + b * T * C + t * C;\n",
    "    float* inp_bt = inp + b * T * C + t * C;\n",
    "    float* dinp_bt = dinp + b * T * C + t * C;\n",
    "    float mean_bt = mean[b * T + t];\n",
    "    float rstd_bt = rstd[b * T + t];\n",
    "\n",
    "    // first: two reduce operations\n",
    "    float dnorm_mean = 0.0f;\n",
    "    float dnorm_norm_mean = 0.0f;\n",
    "    for (int i = 0; i < C; i++) {\n",
    "        float norm_bti = (inp_bt[i] - mean_bt) * rstd_bt;\n",
    "        float dnorm_i = weight[i] * dout_bt[i];\n",
    "        dnorm_mean += dnorm_i;\n",
    "        dnorm_norm_mean += dnorm_i * norm_bti;\n",
    "    }\n",
    "    dnorm_mean = dnorm_mean / C;\n",
    "    dnorm_norm_mean = dnorm_norm_mean / C;\n",
    "\n",
    "    // now iterate again and accumulate all the gradients\n",
    "    for (int i = 0; i < C; i++) {\n",
    "        float norm_bti = (inp_bt[i] - mean_bt) * rstd_bt;\n",
    "        float dnorm_i = weight[i] * dout_bt[i];\n",
    "        // gradient contribution to bias\n",
    "        atomicAdd(&dbias[i], dout_bt[i]);\n",
    "        // gradient contribution to weight\n",
    "        atomicAdd(&dweight[i], norm_bti * dout_bt[i]);\n",
    "        // gradient contribution to input\n",
    "        float dval = 0.0f;\n",
    "        dval += dnorm_i; // term 1\n",
    "        dval -= dnorm_mean; // term 2\n",
    "        dval -= norm_bti * dnorm_norm_mean; // term 3\n",
    "        dval *= rstd_bt; // final scale\n",
    "        dinp_bt[i] += dval;\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void setConstant(float* vec, float constant, int N) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < N) {\n",
    "        vec[idx] = constant;\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4359ce9-5a1d-45ca-b862-38e5b0c55de0",
   "metadata": {},
   "source": [
    "Let us try and compile the source code of `all_kernels` using the helper functions defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfdfa9cd-f1fe-4284-a684-c6c5051665da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llmc_cuda_module.cu(450): warning #1556-D: name linkage conflicts with previous declaration of variable \"shared\" (declared at line 326)\n",
      "      extern __shared__ float shared[];\n",
      "                              ^\n",
      "\n",
      "Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n",
      "\u0000\n"
     ]
    }
   ],
   "source": [
    "llmc_cuda_module = compile_program_and_get_module(all_kernels, \"llmc_cuda_module\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e276be1d-6ce8-408b-a2ce-0387b1bb0620",
   "metadata": {},
   "source": [
    "Having compiled the code, we need to be able to access specific kernels by their names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec99f030-3adc-4f79-8b90-529b6d5b2e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "@functools.cache\n",
    "def extract_cuda_kernel(function_name):\n",
    "    return check_error(cuda.cuModuleGetFunction(llmc_cuda_module, function_name.encode()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb7c8d0-b368-415a-910d-b2e48fde54e1",
   "metadata": {},
   "source": [
    "Note that we decorated `extract_cuda_kernel` with `functools.cache` for potentially faster kernel look-ups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492ced04-8d47-40dd-a9ff-6d2947910845",
   "metadata": {},
   "source": [
    "## High-level Python wrapper for launching CUDA kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eec40c-17cd-4224-9504-5c5136c7c679",
   "metadata": {},
   "source": [
    "Here we present a high-level Python wrappers that accept PyTorch tensors and execute the compiled CUDA kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ee56ff5-383a-49bc-a3d2-31e83a4ef1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "\n",
    "def map_arg_tensors(map_fn):\n",
    "    def arg_mapper(f):\n",
    "        def wrapper(*args):\n",
    "            def map_tensor_fn(inp):\n",
    "                if isinstance(inp, torch.Tensor):\n",
    "                    return map_fn(inp)\n",
    "                else:\n",
    "                    return inp\n",
    "\n",
    "            new_args = map(map_tensor_fn, args)\n",
    "            return f(*new_args)\n",
    "        return wrapper\n",
    "    return arg_mapper\n",
    "\n",
    "\n",
    "def force_contiguous_inputs(f):\n",
    "    return map_arg_tensors(lambda t: t.contiguous())(f)\n",
    "\n",
    "\n",
    "def index_tensors_to_int32(f):\n",
    "    def map_fn(t):\n",
    "        if t.dtype.is_floating_point:\n",
    "            return t\n",
    "        else:\n",
    "            return t.to(torch.int32)\n",
    "\n",
    "    return map_arg_tensors(lambda t: map_fn(t))(f)\n",
    "\n",
    "\n",
    "@force_contiguous_inputs\n",
    "def permute_split(input):\n",
    "    assert input.ndim == 5\n",
    "    assert input.shape[2] == 3\n",
    "\n",
    "    B, N, _, NH, d = input.shape\n",
    "\n",
    "    permute_kernel = extract_cuda_kernel(\"permute_kernel\")\n",
    "\n",
    "    q, k, v = torch.empty(3, B, NH, N, d, device=input.device, dtype=input.dtype).unbind(dim=0)\n",
    "\n",
    "    block_size = 256\n",
    "    grid_size = (input.numel() // 3 + block_size - 1) // block_size\n",
    "    launch_kernel(\n",
    "        permute_kernel,\n",
    "        (grid_size,),\n",
    "        (block_size,),\n",
    "        q, k, v,\n",
    "        input,\n",
    "        numpy.int32(B),\n",
    "        numpy.int32(N),\n",
    "        numpy.int32(NH),\n",
    "        numpy.int32(d),\n",
    "    )\n",
    "\n",
    "    return q, k, v\n",
    "\n",
    "\n",
    "@force_contiguous_inputs\n",
    "def permute_split_backward(q_grad, k_grad, v_grad):\n",
    "    assert q_grad.shape == k_grad.shape == v_grad.shape\n",
    "\n",
    "    permute_kernel_backward = extract_cuda_kernel(\"permute_kernel_backward\")\n",
    "\n",
    "    B, NH, N, d = q_grad.shape\n",
    "\n",
    "    grad = torch.zeros(B, N, 3, NH, d, dtype=q_grad.dtype, device=q_grad.device)\n",
    "\n",
    "    block_size = 256\n",
    "    grid_size = (q_grad.numel() + block_size - 1) // block_size\n",
    "    launch_kernel(\n",
    "        permute_kernel_backward,\n",
    "        (grid_size,),\n",
    "        (block_size,),\n",
    "        grad,\n",
    "        q_grad, k_grad, v_grad,\n",
    "        numpy.int32(B),\n",
    "        numpy.int32(N),\n",
    "        numpy.int32(NH),\n",
    "        numpy.int32(d),\n",
    "    )\n",
    "\n",
    "    return grad\n",
    "\n",
    "\n",
    "@force_contiguous_inputs\n",
    "def unpermute_forward(input):\n",
    "    assert input.ndim == 4\n",
    "\n",
    "    unpermute_kernel = extract_cuda_kernel(\"unpermute_kernel\")\n",
    "\n",
    "    B, NH, N, d = input.shape\n",
    "\n",
    "    output = torch.empty(B, N, NH, d, dtype=input.dtype, device=input.device)\n",
    "\n",
    "    block_size = 256\n",
    "    grid_size = (input.numel() + block_size - 1) // block_size\n",
    "    launch_kernel(\n",
    "        unpermute_kernel,\n",
    "        (grid_size,),\n",
    "        (block_size,),\n",
    "        input,\n",
    "        output,\n",
    "        numpy.int32(B),\n",
    "        numpy.int32(N),\n",
    "        numpy.int32(NH),\n",
    "        numpy.int32(d),\n",
    "    )\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "@force_contiguous_inputs\n",
    "def unpermute_backward(grad):\n",
    "    assert grad.ndim == 4\n",
    "\n",
    "    unpermute_backward_kernel = extract_cuda_kernel(\"unpermute_kernel_backward\")\n",
    "\n",
    "    B, N, NH, d = grad.shape\n",
    "\n",
    "    res = torch.zeros(B, NH, N, d, dtype=grad.dtype, device=grad.device)\n",
    "\n",
    "    block_size = 256\n",
    "    grid_size = (grad.numel() + block_size - 1) // block_size\n",
    "    launch_kernel(\n",
    "        unpermute_backward_kernel,\n",
    "        (grid_size,),\n",
    "        (block_size,),\n",
    "        res,\n",
    "        grad,\n",
    "        numpy.int32(B),\n",
    "        numpy.int32(N),\n",
    "        numpy.int32(NH),\n",
    "        numpy.int32(d),\n",
    "    )\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "@force_contiguous_inputs\n",
    "def softmax_forward(input):\n",
    "    softmax_kernel = extract_cuda_kernel(\"softmax_forward_kernel7\")\n",
    "\n",
    "    output = input.new_empty(*input.shape)\n",
    "\n",
    "    if input.ndim == 0:\n",
    "        C = 1\n",
    "    else:\n",
    "        C = input.shape[-1]\n",
    "    N = input.numel() // C\n",
    "\n",
    "    block_size = 512\n",
    "    grid_size = N\n",
    "    shared_mem_size = 2 * block_size / 32 * input.dtype.itemsize\n",
    "    launch_kernel(\n",
    "        softmax_kernel,\n",
    "        (grid_size,),\n",
    "        (block_size,),\n",
    "        output,\n",
    "        input,\n",
    "        numpy.int32(N),\n",
    "        numpy.int32(C),\n",
    "        shmem=shared_mem_size,\n",
    "    )\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "@force_contiguous_inputs\n",
    "@index_tensors_to_int32\n",
    "def crossentropy_forward(probs, targets):\n",
    "    assert probs.ndim - targets.ndim == 1\n",
    "\n",
    "    if probs.ndim <= 1:\n",
    "        probs = probs.unsqueeze(0)\n",
    "        targets = targets.unsqueeze(0)\n",
    "\n",
    "    *BT, V = probs.shape\n",
    "    assert tuple(targets.shape) == tuple(BT)\n",
    "\n",
    "    if len(BT) == 2:\n",
    "        B, T = BT\n",
    "    else:\n",
    "        B, T = 1, BT[0]\n",
    "\n",
    "    crossentropy_kernel = extract_cuda_kernel(\"crossentropy_forward_kernel1\")\n",
    "\n",
    "    losses = probs.new_empty(*targets.shape)\n",
    "\n",
    "    block_size = 128\n",
    "    grid_size = (probs.numel() // V + block_size - 1) // block_size\n",
    "\n",
    "    launch_kernel(\n",
    "        crossentropy_kernel,\n",
    "        (grid_size,),\n",
    "        (block_size,),\n",
    "        losses,\n",
    "        probs,\n",
    "        targets,\n",
    "        numpy.int32(B),\n",
    "        numpy.int32(T),\n",
    "        numpy.int32(V),\n",
    "    )\n",
    "\n",
    "    return losses\n",
    "\n",
    "\n",
    "@force_contiguous_inputs\n",
    "def crossentropy_softmax_forward(scores, targets):\n",
    "    probs = softmax_forward(scores)\n",
    "    return crossentropy_forward(probs, targets)\n",
    "\n",
    "\n",
    "@force_contiguous_inputs\n",
    "@index_tensors_to_int32\n",
    "def crossentropy_softmax_backward(glogits, glosses, probs, targets):\n",
    "    crossentropy_softmax_backward_kernel = extract_cuda_kernel(\"crossentropy_softmax_backward_kernel1\")\n",
    "\n",
    "    *BT, V = probs.shape\n",
    "    if len(BT) == 2:\n",
    "        B, T = BT\n",
    "    else:\n",
    "        B, T = 1, BT[0]\n",
    "\n",
    "    block_size = 256\n",
    "    grid_size = (probs.numel() + block_size - 1) // block_size\n",
    "\n",
    "    launch_kernel(\n",
    "        crossentropy_softmax_backward_kernel,\n",
    "        (grid_size,),\n",
    "        (block_size,),\n",
    "        glogits,\n",
    "        glosses,\n",
    "        probs,\n",
    "        targets,\n",
    "        numpy.int32(B),\n",
    "        numpy.int32(T),\n",
    "        numpy.int32(V),\n",
    "    )\n",
    "\n",
    "    return glogits\n",
    "\n",
    "\n",
    "@force_contiguous_inputs\n",
    "def gelu_forward(input):\n",
    "    gelu_kernel = extract_cuda_kernel(\"gelu_forward_kernel\")\n",
    "\n",
    "    output = input.new_empty(*input.shape)\n",
    "\n",
    "    block_size = 128\n",
    "    grid_size = (input.numel() + block_size - 1) // block_size\n",
    "    launch_kernel(gelu_kernel, (grid_size,), (block_size,), output, input, numpy.int32(input.numel()))\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "@force_contiguous_inputs\n",
    "def gelu_backward(input, grad):\n",
    "    gelu_backward_kernel = extract_cuda_kernel(\"gelu_backward_kernel\")\n",
    "\n",
    "    input_grad = torch.zeros(*input.shape, dtype=input.dtype, device=input.device)\n",
    "\n",
    "    block_size = 128\n",
    "    grid_size = (input.numel() + block_size - 1) // block_size\n",
    "    launch_kernel(gelu_backward_kernel, (grid_size,), (block_size,), input_grad, input, grad, numpy.int32(input.numel()))\n",
    "\n",
    "    return input_grad\n",
    "\n",
    "\n",
    "@force_contiguous_inputs\n",
    "def layernorm_forward(input, weight, bias):\n",
    "    layernorm_forward_kernel = extract_cuda_kernel(\"layernorm_forward_kernel3\")\n",
    "\n",
    "    B, T, C = input.shape\n",
    "    N = B * T\n",
    "\n",
    "    out = input.new_empty(*input.shape)\n",
    "    mean = input.new_empty(B, T)\n",
    "    rstd = input.new_empty(B, T)\n",
    "\n",
    "    block_size = 512\n",
    "    grid_size = (N * 32 + block_size - 1) // block_size\n",
    "    launch_kernel(\n",
    "        layernorm_forward_kernel,\n",
    "        (grid_size,),\n",
    "        (block_size,),\n",
    "        out, mean, rstd,\n",
    "        input, weight, bias,\n",
    "        numpy.int32(N), numpy.int32(C),\n",
    "    )\n",
    "\n",
    "    return out, mean, rstd\n",
    "\n",
    "\n",
    "@force_contiguous_inputs\n",
    "def layernorm_backward(grad, input, weight, bias, mean, rstd):\n",
    "    layernorm_backward_kernel = extract_cuda_kernel(\"layernorm_backward_kernel1\")\n",
    "\n",
    "    B, T, C = input.shape\n",
    "    N = B * T\n",
    "\n",
    "    input_grad = torch.zeros(*input.shape, dtype=input.dtype, device=input.device)\n",
    "    weight_grad = torch.zeros(*weight.shape, dtype=weight.dtype, device=weight.device)\n",
    "    bias_grad = torch.zeros(*bias.shape, dtype=bias.dtype, device=bias.device)\n",
    "\n",
    "    block_size = 64\n",
    "    grid_size = (N + block_size - 1) // block_size\n",
    "    launch_kernel(\n",
    "        layernorm_backward_kernel,\n",
    "        (grid_size,),\n",
    "        (block_size,),\n",
    "        input_grad, weight_grad, bias_grad,\n",
    "        grad, input, weight, mean, rstd,\n",
    "        numpy.int32(B), numpy.int32(T), numpy.int32(C),\n",
    "    )\n",
    "\n",
    "    return input_grad, weight_grad, bias_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cebe263-2c9c-40ec-a542-3cd6b749de55",
   "metadata": {},
   "source": [
    "A couple of notes. All the kernels expect contiguous inputs, hence all the functions are decorated with `force_contiguous_inputs`. Some of them accept index tensors of type `int32`, but PyTorch uses `int64` by default, so we additionally pre-process index inputs with `index_tensors_to_int32`. We do not convert floating point inputs to `float` as we use this `dtype` by default. Aside from these nuances, the implementation is straightforward - acquire the kernel, prepare inputs and launch configurations, run the kernel with `launch_kernel`.\n",
    "\n",
    "Let's test one of the custom CUDA kernels! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bca7bf8a-91b3-429f-b714-72c93cba58f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1921e-07, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(256, 256, device='cuda')\n",
    "torch_gelu = F.gelu(x, approximate='tanh')\n",
    "custom_gelu = gelu_forward(x)\n",
    "print((torch_gelu - custom_gelu).abs().max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb9828a-244c-44c0-98d6-ab296dae95d2",
   "metadata": {},
   "source": [
    "## Swap PyTorch implementations with custom ones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bea6d4-9abd-415c-a352-f6240cef24b2",
   "metadata": {},
   "source": [
    "In this section we will finally achieve our goal - we will replace PyTorch operations with our own defined above that utilize native CUDA kernels from [llm.c](https://github.com/karpathy/llm.c).\n",
    "\n",
    "Before reading on, however, we highly recommend checking the following resources first:\n",
    "* [Zero to Thunder](./zero_to_thunder.ipynb) for a very short introduction to Thunder and its capabilities.\n",
    "* [Defining new Thunder operators](./adding_custom_operator.ipynb) for understanding how to introduce new operators and executors to Thunder.\n",
    "* [Defining custom forward and backward for existing operators](./adding_custom_operator_backward.ipynb) to learn how to make these new operators also differentiable using custom backward implementations.\n",
    "\n",
    "In a nutshell, what we are doing below is as follows:\n",
    "* A new executor is created.\n",
    "* A new forward/backward symbol is registered for this executor with the corresponding implementation defined in the previous section.\n",
    "* A new symbol is registered with the executor which in `execution_transform` calls the \"forward\" symbol, and in `grad_transforms` calls both the \"forward\" and the \"backward\" symbols, all from the previous step. This new symbol is what specific PyTorch implementations are going to be mapped to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5a77925-0af0-475b-944a-ab3079bf0720",
   "metadata": {},
   "outputs": [],
   "source": [
    "from thunder.core.transforms import get_grad, put_grad, put_grads\n",
    "from thunder.core.langctxs import langctx, Languages\n",
    "from thunder import TensorProxy\n",
    "\n",
    "\n",
    "# Register a new executor\n",
    "llmc = thunder.extend.OperatorExecutor(\"llm.c\")\n",
    "thunder.extend.register_executor(llmc)\n",
    "\n",
    "# Permute/unpermute + backward {\n",
    "def permute_meta(input: TensorProxy):\n",
    "    B, N, _, NH, d = input.shape\n",
    "    shape = (B, NH, N, d)\n",
    "    return (\n",
    "        TensorProxy(like=input, shape=shape),\n",
    "        TensorProxy(like=input, shape=shape),\n",
    "        TensorProxy(like=input, shape=shape),\n",
    "    )\n",
    "\n",
    "\n",
    "# permute forward symbol\n",
    "llmc_permute = llmc.register_operator(\n",
    "    \"llmc_permute\",\n",
    "    like=permute_meta,\n",
    "    fn=permute_split,\n",
    ")\n",
    "\n",
    "\n",
    "def unpermute_meta(input: TensorProxy):\n",
    "    B, NH, N, d = input.shape\n",
    "    return TensorProxy(like=input, shape=(B, N, NH, d))\n",
    "\n",
    "\n",
    "# unpermute forward symbol\n",
    "llmc_unpermute = llmc.register_operator(\n",
    "    \"llmc_unpermute\",\n",
    "    like=unpermute_meta,\n",
    "    fn=unpermute_forward,\n",
    ")\n",
    "\n",
    "\n",
    "def permute_backward_meta(q_grad: TensorProxy, k_grad: TensorProxy, v_grad: TensorProxy):\n",
    "    B, NH, N, d = q_grad.shape\n",
    "    shape = (B, N, 3, NH, d)\n",
    "    return TensorProxy(like=q_grad, shape=shape)\n",
    "\n",
    "\n",
    "# permute backward symbol\n",
    "llmc_permute_backward = llmc.register_operator(\n",
    "    \"llmc_permute_backward\",\n",
    "    like=permute_backward_meta,\n",
    "    fn=permute_split_backward,\n",
    ")\n",
    "\n",
    "\n",
    "def unpermute_backward_meta(grad: TensorProxy):\n",
    "    B, N, NH, d = grad.shape\n",
    "    return TensorProxy(like=grad, shape=(B, NH, N, d))\n",
    "\n",
    "\n",
    "# unpermute backward symbol\n",
    "llmc_unpermute_backward = llmc.register_operator(\n",
    "    \"llmc_unpermute_backward\",\n",
    "    like=unpermute_backward_meta,\n",
    "    fn=unpermute_backward,\n",
    ")\n",
    "\n",
    "def llmc_permute_util_meta(qkv, n_embd, n_head, B, T, C):\n",
    "    shape = (B, n_head, T, C // n_head)\n",
    "    return (\n",
    "        TensorProxy(like=qkv, shape=shape),\n",
    "        TensorProxy(like=qkv, shape=shape),\n",
    "        TensorProxy(like=qkv, shape=shape),\n",
    "    )\n",
    "\n",
    "\n",
    "# composite permute symbol\n",
    "llmc_permute_util = llmc.register_operator(\n",
    "    \"llmc_permute_util\",\n",
    "    like=llmc_permute_util_meta,\n",
    "    fn=_permute,\n",
    "    replaces=_permute,\n",
    ")\n",
    "\n",
    "\n",
    "def llmc_permute_util_execution_transform(qkv, n_embd, n_head, B, T, C):\n",
    "    qkv = thunder.torch.reshape(qkv, (B, T, 3, n_head, C // n_head))\n",
    "    return llmc_permute(qkv)\n",
    "\n",
    "\n",
    "@langctx(Languages.TORCH)\n",
    "def llmc_permute_util_grad_transform(qkv, n_embd, n_head, B, T, C):\n",
    "    qkv_reshaped = thunder.torch.reshape(qkv, (B, T, 3, n_head, C // n_head))\n",
    "\n",
    "    q, k, v = llmc_permute(qkv_reshaped)\n",
    "\n",
    "    q_grad = get_grad(q)\n",
    "    k_grad = get_grad(k)\n",
    "    v_grad = get_grad(v)\n",
    "\n",
    "    grad = llmc_permute_backward(q_grad, k_grad, v_grad)\n",
    "    grad = thunder.torch.reshape(grad, qkv.shape)\n",
    "\n",
    "    put_grad(qkv, grad)\n",
    "\n",
    "    return q, k, v\n",
    "\n",
    "\n",
    "llmc.register_implementation(\n",
    "    llmc_permute_util,\n",
    "    checker=lambda *args, **kwargs: True,\n",
    "    execution_transform=llmc_permute_util_execution_transform,\n",
    "    grad_transform=llmc_permute_util_grad_transform,\n",
    ")\n",
    "\n",
    "\n",
    "def llmc_unpermute_util_meta(input: TensorProxy, B: int, T: int, C: int):\n",
    "    return TensorProxy(like=input, shape=(B, T, C))\n",
    "\n",
    "\n",
    "# composite unpermute symbol\n",
    "llmc_unpermute_util = llmc.register_operator(\n",
    "    \"llmc_unpermute_util\",\n",
    "    like=llmc_unpermute_util_meta,\n",
    "    fn=_unpermute,\n",
    "    replaces=_unpermute,\n",
    ")\n",
    "\n",
    "\n",
    "def llmc_unpermute_util_execution_transform(input: TensorProxy, B: int, T: int, C: int):\n",
    "    res = llmc_unpermute(input)\n",
    "    return thunder.torch.view(res, B, T, C)\n",
    "\n",
    "\n",
    "def llmc_unpermute_util_grad_transform(input: TensorProxy, B: int, T: int, C: int):\n",
    "    x = llmc_unpermute(input)\n",
    "    fwd = thunder.torch.view(x, B, T, C)\n",
    "\n",
    "    # NOTE: get_grad(x) breaks things. Why?\n",
    "    fwd_grad = get_grad(fwd)\n",
    "    fwd_grad = thunder.torch.view(fwd_grad, *x.shape)\n",
    "\n",
    "    input_grad = llmc_unpermute_backward(fwd_grad)\n",
    "    put_grad(input, input_grad)\n",
    "\n",
    "    return fwd\n",
    "\n",
    "\n",
    "llmc.register_implementation(\n",
    "    llmc_unpermute_util,\n",
    "    checker=lambda *args, **kwargs: True,\n",
    "    execution_transform=llmc_unpermute_util_execution_transform,\n",
    "    grad_transform=llmc_unpermute_util_grad_transform,\n",
    ")\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bbe897e-6f46-4dca-bd2b-ee974cb02ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax {\n",
    "llmc_softmax = llmc.register_operator(\n",
    "    \"llmc_softmax\",\n",
    "    like=lambda input: TensorProxy(like=input),\n",
    "    fn=softmax_forward,\n",
    ")\n",
    "\n",
    "def llmc_softmax_checker(x: TensorProxy, /, dim: int, *, dtype=None) -> bool:\n",
    "    if not (dim == -1 or dim == x.ndim - 1):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def llmc_softmax_execution_transform(x: TensorProxy, /, dim: int, *, dtype=None) -> TensorProxy:\n",
    "    return llmc_softmax(x)\n",
    "\n",
    "\n",
    "llmc.register_implementation(\n",
    "    thunder.torch.softmax,\n",
    "    checker=llmc_softmax_checker,\n",
    "    execution_transform=llmc_softmax_execution_transform,\n",
    ")\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa9ac149-fd06-49f3-9b28-9687c74f075d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "# Crossentropy + Softmax {\n",
    "def llmc_crossentropy_meta(\n",
    "    a: TensorProxy,\n",
    "    target: TensorProxy,\n",
    ") -> TensorProxy:\n",
    "    return TensorProxy(like=target, dtype=a.dtype)\n",
    "\n",
    "\n",
    "llmc_crossentropy = llmc.register_operator(\n",
    "    \"llmc_crossentropy\",\n",
    "    like=llmc_crossentropy_meta,\n",
    "    fn=crossentropy_forward,\n",
    ")\n",
    "\n",
    "\n",
    "llmc_crossentropy_softmax_backward = llmc.register_operator(\n",
    "    \"llmc_crossentropy_softmax_backward\",\n",
    "    like=lambda x, *args, **kwargs: TensorProxy(like=x),\n",
    "    fn=crossentropy_softmax_backward,\n",
    ")\n",
    "\n",
    "\n",
    "def llmc_crossentropy_softmax_checker(\n",
    "    a: TensorProxy,\n",
    "    /,\n",
    "    target: TensorProxy,\n",
    "    weight: None | TensorProxy = None,\n",
    "    size_average: None | Any = None,\n",
    "    ignore_index: int = -100,\n",
    "    reduce: None | Any = None,\n",
    "    reduction: str = \"mean\",\n",
    "    label_smoothing: float = 0.0,\n",
    "):\n",
    "    return (\n",
    "        thunder.dtypes.is_integer_dtype(target.dtype)\n",
    "        and weight is None\n",
    "        and size_average is None\n",
    "        and ignore_index < 0\n",
    "        and reduce is None\n",
    "        and reduction == \"mean\"\n",
    "        and label_smoothing == 0.0\n",
    "    )\n",
    "\n",
    "\n",
    "# Setting torch langctx to be able to use binary ops with torch.Tensor\n",
    "@langctx(Languages.TORCH)\n",
    "def llmc_crossentropy_softmax_execution_transform(\n",
    "    a: TensorProxy,\n",
    "    /,\n",
    "    target: TensorProxy,\n",
    "    weight: None | TensorProxy = None,\n",
    "    size_average: None | Any = None,\n",
    "    ignore_index: int = -100,\n",
    "    reduce: None | Any = None,\n",
    "    reduction: str = \"mean\",\n",
    "    label_smoothing: float = 0.0,\n",
    "):\n",
    "    probs = llmc_softmax(a)\n",
    "    loss = llmc_crossentropy(probs, target)\n",
    "    return loss.sum() / loss.numel\n",
    "\n",
    "\n",
    "# Setting torch langctx to be able to use binary ops with torch.Tensor\n",
    "@langctx(Languages.TORCH)\n",
    "def llmc_crossentropy_softmax_grad_transform(\n",
    "    logits: TensorProxy,\n",
    "    /,\n",
    "    targets: TensorProxy,\n",
    "    weight: None | TensorProxy = None,\n",
    "    size_average: None | Any = None,\n",
    "    ignore_index: int = -100,\n",
    "    reduce: None | Any = None,\n",
    "    reduction: str = \"mean\",\n",
    "    label_smoothing: float = 0.0,\n",
    "):\n",
    "    probs = llmc_softmax(logits)\n",
    "    losses = llmc_crossentropy(probs, targets)\n",
    "    loss = losses.sum() / losses.numel\n",
    "\n",
    "    loss_grad = get_grad(loss)\n",
    "\n",
    "    losses_grad = thunder.torch.ones_like(losses) / losses.numel\n",
    "    logits_grad = thunder.torch.zeros_like(logits)\n",
    "\n",
    "    logits_grad = llmc_crossentropy_softmax_backward(logits_grad, losses_grad, probs, targets)\n",
    "    put_grad(logits, logits_grad)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "llmc.register_implementation(\n",
    "    thunder.torch.cross_entropy,\n",
    "    checker=llmc_crossentropy_softmax_checker,\n",
    "    execution_transform=llmc_crossentropy_softmax_execution_transform,\n",
    "    grad_transform=llmc_crossentropy_softmax_grad_transform,\n",
    ")\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c95fb611-d37c-45df-be9b-1971847aa87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GELU {\n",
    "llmc_gelu = llmc.register_operator(\n",
    "    \"llmc_gelu\",\n",
    "    like=lambda input: TensorProxy(like=input),\n",
    "    fn=gelu_forward,\n",
    ")\n",
    "\n",
    "\n",
    "llmc_gelu_backward = llmc.register_operator(\n",
    "    \"llmc_gelu_backward\",\n",
    "    like=lambda input, grad: TensorProxy(like=input),\n",
    "    fn=gelu_backward,\n",
    ")\n",
    "\n",
    "\n",
    "llmc_module_gelu = llmc.register_operator(\n",
    "    \"llmc_module_gelu\",\n",
    "    like=lambda self, input: TensorProxy(like=input),\n",
    "    fn=NewGELU.forward,\n",
    "    replaces=NewGELU.forward,\n",
    ")\n",
    "\n",
    "\n",
    "def llmc_module_gelu_grad_transform(self, input: TensorProxy):\n",
    "    fwd = llmc_gelu(input)\n",
    "\n",
    "    fwd_grad = get_grad(fwd)\n",
    "\n",
    "    input_grad = llmc_gelu_backward(input, fwd_grad)\n",
    "\n",
    "    put_grad(input, input_grad)\n",
    "\n",
    "    return fwd\n",
    "\n",
    "\n",
    "llmc.register_implementation(\n",
    "    llmc_module_gelu,\n",
    "    checker=lambda *args, **kwargs: True,\n",
    "    execution_transform=lambda self, input: llmc_gelu(input),\n",
    "    grad_transform=llmc_module_gelu_grad_transform,\n",
    ")\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fc610a5-01b8-4ecc-83d5-512c87aef5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace LayerNorm {\n",
    "def layernorm_forward(input, weight, bias):\n",
    "    layernorm_forward_kernel = extract_cuda_kernel(\"layernorm_forward_kernel3\")\n",
    "\n",
    "    B, T, C = input.shape\n",
    "    N = B * T\n",
    "\n",
    "    out = input.new_empty(*input.shape)\n",
    "    mean = input.new_empty(B, T)\n",
    "    rstd = input.new_empty(B, T)\n",
    "\n",
    "    block_size = 512\n",
    "    grid_size = (N * 32 + block_size - 1) // block_size\n",
    "    launch_kernel(\n",
    "        layernorm_forward_kernel,\n",
    "        (grid_size,),\n",
    "        (block_size,),\n",
    "        out, mean, rstd,\n",
    "        input, weight, bias,\n",
    "        numpy.int32(N), numpy.int32(C),\n",
    "    )\n",
    "\n",
    "    return out, mean, rstd\n",
    "\n",
    "\n",
    "def layernorm_meta(input, weight, bias):\n",
    "    B, T, C = input.shape\n",
    "    return TensorProxy(like=input), TensorProxy(like=input, shape=(B, T)), TensorProxy(like=input, shape=(B, T))\n",
    "\n",
    "\n",
    "llmc_layernorm = llmc.register_operator(\n",
    "    \"llmc_layernorm\",\n",
    "    like=layernorm_meta,\n",
    "    fn=layernorm_forward,\n",
    ")\n",
    "\n",
    "\n",
    "def layernorm_backward(grad, input, weight, bias, mean, rstd):\n",
    "    layernorm_backward_kernel = extract_cuda_kernel(\"layernorm_backward_kernel1\")\n",
    "\n",
    "    B, T, C = input.shape\n",
    "    N = B * T\n",
    "\n",
    "    input_grad = torch.zeros(*input.shape, dtype=input.dtype, device=input.device)\n",
    "    weight_grad = torch.zeros(*weight.shape, dtype=weight.dtype, device=weight.device)\n",
    "    bias_grad = torch.zeros(*bias.shape, dtype=bias.dtype, device=bias.device)\n",
    "\n",
    "    block_size = 64\n",
    "    grid_size = (N + block_size - 1) // block_size\n",
    "    launch_kernel(\n",
    "        layernorm_backward_kernel,\n",
    "        (grid_size,),\n",
    "        (block_size,),\n",
    "        input_grad, weight_grad, bias_grad,\n",
    "        grad, input, weight, mean, rstd,\n",
    "        numpy.int32(B), numpy.int32(T), numpy.int32(C),\n",
    "    )\n",
    "\n",
    "    return input_grad, weight_grad, bias_grad\n",
    "\n",
    "\n",
    "def layernorm_backward_meta(grad, input, weight, bias, mean, rstd):\n",
    "    return TensorProxy(like=input), TensorProxy(like=weight), TensorProxy(like=bias)\n",
    "\n",
    "\n",
    "llmc_layernorm_backward = llmc.register_operator(\n",
    "    \"llmc_layernorm_backward\",\n",
    "    like=layernorm_backward_meta,\n",
    "    fn=layernorm_backward,\n",
    ")\n",
    "\n",
    "\n",
    "llmc_layer_norm = llmc.register_operator(\n",
    "    \"llmc_layer_norm\",\n",
    "    like=lambda input, normalzied_shape, weight, bias, eps=1e-5: TensorProxy(like=input),\n",
    "    fn=F.layer_norm,\n",
    "    replaces=F.layer_norm,\n",
    ")\n",
    "\n",
    "\n",
    "def llmc_layer_norm_execution_transform(input, normalized_shape, weight, bias, eps=1e-5):\n",
    "    return llmc_layernorm(input, weight, bias)[0]\n",
    "\n",
    "\n",
    "def llmc_layer_norm_grad_transform(input, normalized_shape, weight, bias, eps=1e-5):\n",
    "    out, mean, rstd = llmc_layernorm(input, weight, bias)\n",
    "\n",
    "    out_grad = get_grad(out)\n",
    "    mean_grad = get_grad(mean)\n",
    "    rstd_grad = get_grad(rstd)\n",
    "\n",
    "    input_grad, weight_grad, bias_grad = llmc_layernorm_backward(out_grad, input, weight, bias, mean, rstd)\n",
    "\n",
    "    put_grads((input, weight, bias), (input_grad, weight_grad, bias_grad))\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "llmc.register_implementation(\n",
    "    llmc_layer_norm,\n",
    "    checker=lambda *args, **kwargs: True,\n",
    "    execution_transform=llmc_layer_norm_execution_transform,\n",
    "    grad_transform=llmc_layer_norm_grad_transform,\n",
    ")\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7211ab-6b4b-441d-a2d4-3663197eccc1",
   "metadata": {},
   "source": [
    "## Checking whether all the pieces fit\n",
    "Now that re-mapping is complete, let's check its correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e2db2a4-239e-4310-92c4-d53c85e309dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote ./llm.c/gpt2_tokenizer.bin\n",
      "loading weights from pretrained gpt: gpt2\n",
      "compiling the model with thunder.jit...\n",
      "loss=tensor(5.2700, device='cuda:0', grad_fn=<ThunderFunctionBackward>)\n",
      "iteration 0, loss: 5.27000617980957, time: 1815.470ms\n",
      "loss=tensor(4.0597, device='cuda:0', grad_fn=<ThunderFunctionBackward>)\n",
      "iteration 1, loss: 4.059720993041992, time: 53.967ms\n",
      "loss=tensor(3.3752, device='cuda:0', grad_fn=<ThunderFunctionBackward>)\n",
      "iteration 2, loss: 3.37518310546875, time: 54.288ms\n",
      "loss=tensor(2.8008, device='cuda:0', grad_fn=<ThunderFunctionBackward>)\n",
      "iteration 3, loss: 2.800813913345337, time: 55.244ms\n",
      "loss=tensor(2.3154, device='cuda:0', grad_fn=<ThunderFunctionBackward>)\n",
      "iteration 4, loss: 2.315415143966675, time: 53.213ms\n",
      "loss=tensor(1.8490, device='cuda:0', grad_fn=<ThunderFunctionBackward>)\n",
      "iteration 5, loss: 1.8490420579910278, time: 57.130ms\n",
      "loss=tensor(1.3946, device='cuda:0', grad_fn=<ThunderFunctionBackward>)\n",
      "iteration 6, loss: 1.3946458101272583, time: 57.649ms\n",
      "loss=tensor(0.9992, device='cuda:0', grad_fn=<ThunderFunctionBackward>)\n",
      "iteration 7, loss: 0.999210000038147, time: 50.234ms\n",
      "loss=tensor(0.6241, device='cuda:0', grad_fn=<ThunderFunctionBackward>)\n",
      "iteration 8, loss: 0.6240699887275696, time: 58.997ms\n",
      "loss=tensor(0.3765, device='cuda:0', grad_fn=<ThunderFunctionBackward>)\n",
      "iteration 9, loss: 0.37648653984069824, time: 58.557ms\n",
      "final 20 iters avg: 231.475ms\n",
      "<|endoftext|>One year ago today:\n",
      "This is the first week since we last spoke.\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "custom_model = demo_model(thunder_jit=1, thunder_executors=(llmc,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f8b85f-9c1c-4cbd-aeac-4e44c63537d5",
   "metadata": {},
   "source": [
    "The outcome indicates that at least correctness is preserved! We will check the forward/backward trace to see whether all our defined symbols appear over there! Note that symbols for which execution/grad transform is defined, are not going to appear in traces. This is because execution/grad transforms define decompositions replacing these symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8dcb36da-196f-4006-b1dc-761d5dd43062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "# NOTE: \"meta\"-symbols with defined execution/grad transform are not included.\n",
    "# This is because they are being decomposed into symbols with the names from the set defined below.\n",
    "new_symbol_names = {\n",
    "    \"llmc_permute\",\n",
    "    \"llmc_unpermute\",\n",
    "    \"llmc_permute_backward\",\n",
    "    \"llmc_unpermute_backward\",\n",
    "    \"llmc_softmax\",\n",
    "    \"llmc_crossentropy\",\n",
    "    \"llmc_gelu\",\n",
    "    \"llmc_gelu_backward\",\n",
    "    \"llmc_layernorm\",\n",
    "    \"llmc_layernorm_backward\",\n",
    "}\n",
    "\n",
    "model_fwd_trace = thunder.last_traces(custom_model)[-1]\n",
    "model_bkw_trace = thunder.last_backward_traces(custom_model)[-1]\n",
    "\n",
    "seen_new_symbols = set()\n",
    "for bsym in itertools.chain(model_fwd_trace.bound_symbols, model_bkw_trace.bound_symbols):\n",
    "    seen_new_symbols.update({bsym.sym.name} & new_symbol_names)\n",
    "    \n",
    "assert seen_new_symbols == new_symbol_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e596583-8f60-40c8-aa50-e6adf157f661",
   "metadata": {},
   "source": [
    "All the forward/backward symbols that call into the custom CUDA kernels are also in the traces. So there we have it - a PyTorch implementation with parts replaced with the user's code!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d5b96a-8723-48d7-997d-1cb2fb299021",
   "metadata": {},
   "source": [
    "## Sanity check benchmarking\n",
    "Let's do some basic benchmarking and see how the custom CUDA kernels perform. We test a combination of a forward and a backward step done with the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cd05710-520f-4f16-b4c5-b0cf7e99133d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote ./llm.c/gpt2_tokenizer.bin\n",
      "loading weights from pretrained gpt: gpt2\n",
      "compiling the model with thunder.jit...\n",
      "loss=tensor(5.2700, device='cuda:0', grad_fn=<ThunderFunctionBackward>)\n",
      "iteration 0, loss: 5.270007610321045, time: 142.715ms\n",
      "loss=tensor(4.0597, device='cuda:0', grad_fn=<ThunderFunctionBackward>)\n",
      "iteration 1, loss: 4.059719562530518, time: 52.394ms\n",
      "loss=tensor(3.3752, device='cuda:0', grad_fn=<ThunderFunctionBackward>)\n",
      "iteration 2, loss: 3.375183582305908, time: 50.392ms\n",
      "loss=tensor(2.8008, device='cuda:0', grad_fn=<ThunderFunctionBackward>)\n",
      "iteration 3, loss: 2.8008131980895996, time: 57.834ms\n",
      "loss=tensor(2.3154, device='cuda:0', grad_fn=<ThunderFunctionBackward>)\n",
      "iteration 4, loss: 2.315413475036621, time: 57.975ms\n",
      "loss=tensor(1.8490, device='cuda:0', grad_fn=<ThunderFunctionBackward>)\n",
      "iteration 5, loss: 1.8490394353866577, time: 57.686ms\n",
      "loss=tensor(1.3946, device='cuda:0', grad_fn=<ThunderFunctionBackward>)\n",
      "iteration 6, loss: 1.3946452140808105, time: 57.700ms\n",
      "loss=tensor(0.9992, device='cuda:0', grad_fn=<ThunderFunctionBackward>)\n",
      "iteration 7, loss: 0.9992080926895142, time: 58.425ms\n",
      "loss=tensor(0.6241, device='cuda:0', grad_fn=<ThunderFunctionBackward>)\n",
      "iteration 8, loss: 0.6240673065185547, time: 57.662ms\n",
      "loss=tensor(0.3765, device='cuda:0', grad_fn=<ThunderFunctionBackward>)\n",
      "iteration 9, loss: 0.3764849901199341, time: 57.632ms\n",
      "final 20 iters avg: 65.041ms\n",
      "<|endoftext|>One year ago today:\n",
      "This is the first week since we last spoke.\n",
      "---------------\n",
      "wrote ./llm.c/gpt2_tokenizer.bin\n",
      "loading weights from pretrained gpt: gpt2\n",
      "loss=tensor(5.2700, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iteration 0, loss: 5.270008563995361, time: 52.918ms\n",
      "loss=tensor(4.0597, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iteration 1, loss: 4.059720993041992, time: 33.975ms\n",
      "loss=tensor(3.3752, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iteration 2, loss: 3.3751838207244873, time: 34.281ms\n",
      "loss=tensor(2.8008, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iteration 3, loss: 2.800813913345337, time: 34.204ms\n",
      "loss=tensor(2.3154, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iteration 4, loss: 2.315413475036621, time: 33.193ms\n",
      "loss=tensor(1.8490, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iteration 5, loss: 1.8490413427352905, time: 30.629ms\n",
      "loss=tensor(1.3946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iteration 6, loss: 1.3946460485458374, time: 31.495ms\n",
      "loss=tensor(0.9992, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iteration 7, loss: 0.9992104768753052, time: 31.497ms\n",
      "loss=tensor(0.6241, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iteration 8, loss: 0.6240706443786621, time: 37.860ms\n",
      "loss=tensor(0.3765, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "iteration 9, loss: 0.3764864206314087, time: 37.982ms\n",
      "final 20 iters avg: 35.803ms\n",
      "<|endoftext|>One year ago today:\n",
      "This is the first week since we last spoke.\n",
      "---------------\n",
      "\n",
      "Elapsed average time (n=30): 47.9545ms\n",
      "Elapsed average time (n=30): 47.2930ms\n",
      "Elapsed average time (n=30): 52.0403ms\n"
     ]
    }
   ],
   "source": [
    "def fwd_bkw(model):\n",
    "    B, T = 4, 64\n",
    "    device = \"cuda\"\n",
    "    \n",
    "    data_iter = get_data_iter(B, T, device)\n",
    "    x, y = next(data_iter)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    def do_fwd_bkw():\n",
    "        logits, loss = model(x, y)\n",
    "        del logits\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Warm-up runs!\n",
    "    for _ in range(2):\n",
    "        do_fwd_bkw()\n",
    "\n",
    "    return do_fwd_bkw\n",
    "\n",
    "\n",
    "custom_model_bench = fwd_bkw(custom_model)\n",
    "\n",
    "thunder_model = demo_model(thunder_jit=1)\n",
    "thunder_model_bench = fwd_bkw(thunder_model)\n",
    "\n",
    "ref_model = demo_model(thunder_jit=0);\n",
    "ref_model_bench = fwd_bkw(ref_model)\n",
    "\n",
    "def benchmark(bench_f, num_runs=30):\n",
    "    import time\n",
    "    st = time.time()\n",
    "    for i in range(num_runs):\n",
    "        bench_f()\n",
    "    torch.cuda.synchronize()\n",
    "    et = time.time()\n",
    "    avg_time_in_seconds = (et - st) / num_runs\n",
    "    print(f\"Elapsed average time (n={num_runs}): {avg_time_in_seconds * 1000:.4f}ms\")\n",
    "\n",
    "print()\n",
    "\n",
    "benchmark(custom_model_bench)\n",
    "benchmark(thunder_model_bench)\n",
    "benchmark(ref_model_bench)\n",
    "\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbced56-eddb-4a2d-a606-1f2f8ea8b3d6",
   "metadata": {},
   "source": [
    "One could see that the model with custom CUDA kernels is generally on par (or even faster!) with the reference PyTorch implementation. This is quite amazing given that not all custom CUDA kernels are necessarily optimal, and that PyTorch kernels should be quite performant and efficient in this setting. Taking into account memory layouts and fusing computation definitely helps!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfffbdac-210a-416d-80af-e501c7e680d5",
   "metadata": {},
   "source": [
    "## Gap with the C implementation\n",
    "Let us see how Python implementations compared to the C implementation. For that we will use the testing code from [llm.c](https://github.com/karpathy/llm.c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d4b77a5-f725-4c45-97c2-2e13f5b1c923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NICE Compiling with OpenMP support\n",
      "nvcc -O3 --use_fast_math test_gpt2.cu -lcublas -lcublasLt -o test_gpt2cu\n",
      "[System]\n",
      "Device 0: NVIDIA A100-SXM4-40GB\n",
      "enable_tf32: 0\n",
      "[GPT-2]\n",
      "max_seq_len: 1024\n",
      "vocab_size: 50257\n",
      "num_layers: 12\n",
      "num_heads: 12\n",
      "channels: 768\n",
      "num_parameters: 124439808\n",
      "[State]\n",
      "batch_size: 4\n",
      "seq_len: 64\n",
      "num_activations: 82760960\n",
      "-43.431671 -43.431725\n",
      "-39.836388 -39.836433\n",
      "-43.065968 -43.066017\n",
      "OK (LOGITS)\n",
      "LOSS OK: 5.270009 5.270009\n",
      "grads\n",
      "OK -0.002320 -0.002320\n",
      "OK 0.002072 0.002072\n",
      "OK 0.003717 0.003717\n",
      "OK 0.001307 0.001307\n",
      "OK 0.000632 0.000632\n",
      "TENSOR OK\n",
      "step 0: loss 5.270009 (took 34.621211 ms)\n",
      "step 1: loss 4.059718 (took 10.644219 ms)\n",
      "step 2: loss 3.375185 (took 246.578083 ms)\n",
      "step 3: loss 2.800815 (took 246.652834 ms)\n",
      "step 4: loss 2.315419 (took 247.894914 ms)\n",
      "step 5: loss 1.849051 (took 246.635729 ms)\n",
      "step 6: loss 1.394659 (took 246.620490 ms)\n",
      "step 7: loss 0.999220 (took 246.692001 ms)\n",
      "step 8: loss 0.624077 (took 246.682988 ms)\n",
      "step 9: loss 0.376495 (took 246.609135 ms)\n",
      "loss ok at step 0: 5.270009 5.270007\n",
      "loss ok at step 1: 4.059718 4.059707\n",
      "loss ok at step 2: 3.375185 3.375123\n",
      "loss ok at step 3: 2.800815 2.800783\n",
      "loss ok at step 4: 2.315419 2.315382\n",
      "loss ok at step 5: 1.849051 1.849029\n",
      "loss ok at step 6: 1.394659 1.394656\n",
      "loss ok at step 7: 0.999220 0.999147\n",
      "loss ok at step 8: 0.624077 0.624080\n",
      "loss ok at step 9: 0.376495 0.376511\n",
      "overall okay: 1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd llm.c\n",
    "make test_gpt2cu\n",
    "./test_gpt2cu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c45e6e-d981-4304-88f9-7de2a35f00ea",
   "metadata": {},
   "source": [
    "This is what we get when using [llm.c](https://github.com/karpathy/llm.c) checked out at `954077fb887d2770e4d537bafea056473d4bb4ce`. However, a more recent version `50acc125f39694ee43f285e5cf8fc123cbb911fa` is more performant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cbcf62f0-79e7-4c74-95e7-53f815ccfa33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Previous HEAD position was 954077f TRAINING WORKSgit add train_gpt2.cu! ITS SLOW BUT IT WORKS WOOT\n",
      "HEAD is now at 50acc12 Merge branch 'ngc92-split-file' Separates out common error-checking wrapper utils, that are broadly useful across all file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running pytorch 2.4.0.dev20240430+cu121\n",
      "using device: cuda\n",
      "wrote gpt2_tokenizer.bin\n",
      "loading weights from pretrained gpt: gpt2\n",
      "loading cached tokens in data/tiny_shakespeare_val.bin\n",
      "padded vocab size from 50257 to 50304\n",
      "wrote gpt2_124M.bin\n",
      "padded vocab size from 50257 to 50304\n",
      "wrote gpt2_124M_bf16.bin\n",
      "padded vocab size in reference grads from 50257 to 50304\n",
      "wrote gpt2_124M_debug_state.bin\n",
      "iteration 1, loss: 4.175999641418457, time: 42.633ms\n",
      "iteration 2, loss: 3.8168132305145264, time: 36.040ms\n",
      "iteration 3, loss: 3.706939935684204, time: 45.228ms\n",
      "iteration 4, loss: 3.9031288623809814, time: 45.948ms\n",
      "iteration 5, loss: 3.2775278091430664, time: 45.743ms\n",
      "iteration 6, loss: 2.9421181678771973, time: 45.576ms\n",
      "iteration 7, loss: 2.8467600345611572, time: 45.889ms\n",
      "iteration 8, loss: 2.6670680046081543, time: 49.916ms\n",
      "iteration 9, loss: 2.408634901046753, time: 49.969ms\n",
      "final 9 iters avg: 45.216ms\n",
      "peak memory consumption: 2341 MiB\n",
      "<|endoftext|>One of the most important issues in our political system is the threat to the stability\n",
      "---------------\n",
      "OpenMP found, compiling with OpenMP support\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Makefile:84: OpenMPI is not found, disabling multi-GPU support\n",
      "Makefile:85: On Linux you can try install OpenMPI with `sudo apt install openmpi-bin openmpi-doc libopenmpi-dev`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc found, including CUDA builds\n",
      "/usr/local/cuda-12/bin/nvcc -O3 -t=0 --use_fast_math test_gpt2_fp32.cu -lcublas -lcublasLt   -lcublas -lcublasLt -o test_gpt2fp32cu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_gpt2_fp32.cu(904): warning #177-D: variable \"V\" was declared but never referenced\n",
      "      int V = config.vocab_size;\n",
      "          ^\n",
      "\n",
      "Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[System]\n",
      "Device 0: NVIDIA A100-SXM4-40GB\n",
      "enable_tf32: 0\n",
      "[State]\n",
      "batch_size: 4\n",
      "seq_len: 64\n",
      "allocated 221 MiB for activations\n",
      "-43.431671, -43.431725\n",
      "-39.836388, -39.836433\n",
      "-43.065968, -43.066017\n",
      "-42.828091, -42.828136\n",
      "-43.529598, -43.529644\n",
      "-44.318451, -44.318501\n",
      "-41.227470, -41.227524\n",
      "-41.270821, -41.270866\n",
      "-42.541451, -42.541515\n",
      "-42.395061, -42.395103\n",
      "OK (LOGITS)\n",
      "allocated 474 MiB for parameter gradients\n",
      "allocated 4 MiB for activation gradients\n",
      "LOSS OK: 5.270009 5.270009\n",
      "grads\n",
      "OK -0.002320 -0.002320\n",
      "OK 0.002072 0.002072\n",
      "OK 0.003717 0.003717\n",
      "OK 0.001307 0.001307\n",
      "OK 0.000632 0.000632\n",
      "TENSOR OK\n",
      "allocated 474 MiB for AdamW optimizer state m\n",
      "allocated 474 MiB for AdamW optimizer state v\n",
      "step 0: loss 5.270009 (took 13.917726 ms)\n",
      "step 1: loss 4.059717 (took 12.114419 ms)\n",
      "step 2: loss 3.375185 (took 24.071226 ms)\n",
      "step 3: loss 2.800816 (took 24.149647 ms)\n",
      "step 4: loss 2.315418 (took 24.197707 ms)\n",
      "step 5: loss 1.849047 (took 24.187108 ms)\n",
      "step 6: loss 1.394655 (took 24.166976 ms)\n",
      "step 7: loss 0.999219 (took 24.185097 ms)\n",
      "step 8: loss 0.624073 (took 24.168827 ms)\n",
      "step 9: loss 0.376495 (took 24.181878 ms)\n",
      "loss ok at step 0: 5.270009 5.270007\n",
      "loss ok at step 1: 4.059717 4.059707\n",
      "loss ok at step 2: 3.375185 3.375123\n",
      "loss ok at step 3: 2.800816 2.800783\n",
      "loss ok at step 4: 2.315418 2.315382\n",
      "loss ok at step 5: 1.849047 1.849029\n",
      "loss ok at step 6: 1.394655 1.394656\n",
      "loss ok at step 7: 0.999219 0.999147\n",
      "loss ok at step 8: 0.624073 0.624080\n",
      "loss ok at step 9: 0.376495 0.376511\n",
      "overall okay: 1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd llm.c\n",
    "git checkout 50acc125f39694ee43f285e5cf8fc123cbb911fa\n",
    "python train_gpt2.py\n",
    "make test_gpt2fp32cu\n",
    "./test_gpt2fp32cu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c4ed35-eace-4219-8fef-7438dae8a77f",
   "metadata": {},
   "source": [
    "This later version also outperforms the Thunder version with custom kernels!\n",
    "To bridge this performance gap, as future work, we plan to\n",
    "* Replace older kernels with newer ones.\n",
    "* Improve memory management. The Thunder version delegates memory management to PyTorch and does so in a model-agnostic fashion which is not guaranteed to be optimal. In contrast, the C implementation is not model-agnostic and assumes full control over efficient memory allocation and management.\n",
    "\n",
    "Stay tuned and may your kernels run fast!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
