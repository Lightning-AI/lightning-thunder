{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    "In this tutorial, we will write a Trace transformation to perform CPU Offloading of intermediate tensors. CPU Offloading is a technique to decrease the peak memory usage of the training loop. This can allow us to train a larger model which would otherwise won't be possible. However, we have to trade of some performance (increased training time) to achieve the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.utils.benchmark\n",
    "\n",
    "import thunder\n",
    "from thunder import Transform\n",
    "from thunder.core.proxies import TensorProxy, variableify\n",
    "from thunder.core.pytree import tree_flatten, tree_map\n",
    "from thunder.core.trace import tracectx, from_trace\n",
    "from thunder.extend import OperatorExecutor\n",
    "from thunder.core.symbol import BoundSymbol\n",
    "from thunder.core import prims\n",
    "from thunder.core.transforms import bsym_list_to_dag, Node, toposort_bsym_dag, TOPOSORT_ORDER\n",
    "from thunder.core.vjp_utils import get_saved_for_backward_tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforms\n",
    "\n",
    "`thunder` allows you to write your own transforms which can be applied at various stages during the compilation. To write our transform, we have to inherit from `thunder.Transform` class. This class implements the interface that each transform should have. By default, it provides no-op transformations.\n",
    "\n",
    "We then provide our transform object to the `thunder.jit` via `transforms` argument.\n",
    "\n",
    "However, before writing our transform, we will make an `OperatorExecutor` with which we will create 2 operators/symbol - 1. to offload tensors to CPU 2. Load the offloaded tensors back to CUDA device\n",
    "\n",
    "You read more about adding custom operators in `adding_custom_operator.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new executor.\n",
    "offload_exec = OperatorExecutor(\"offload_exec\")\n",
    "\n",
    "\n",
    "def offload_to_cpu_impl(t):\n",
    "    # Due to https://github.com/Lightning-AI/lightning-thunder/issues/950\n",
    "    # it may receive tensor on CPU.\n",
    "    if t.device == torch.device(\"cpu\"):\n",
    "        return t\n",
    "\n",
    "    packed = torch.empty(\n",
    "        t.size(),\n",
    "        dtype=t.dtype,\n",
    "        layout=t.layout,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    packed.copy_(t, non_blocking=False)\n",
    "    return packed\n",
    "\n",
    "\n",
    "offload_to_cpu = offload_exec.register_operator(\n",
    "    \"offload_to_cpu\",\n",
    "    meta=lambda t: TensorProxy(\"offloaded_\" + t.name, like=t, device=thunder.core.devices.Device(\"cpu\")),\n",
    "    fn=offload_to_cpu_impl,\n",
    ")\n",
    "\n",
    "\n",
    "def load_to_gpu_impl(t, device):\n",
    "    return t.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "load_to_gpu = offload_exec.register_operator(\n",
    "    \"load_to_gpu\",\n",
    "    meta=lambda t, device: TensorProxy(like=t, device=thunder.core.devices.Device(device)),\n",
    "    fn=load_to_gpu_impl,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to implement our transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_symbols_to_last_or_first_used_variables(symbols, first_used=False):\n",
    "    variable_to_symbol = {}\n",
    "    symbol_to_variables = {}\n",
    "\n",
    "    def _mark_last_use(symbol, variable):\n",
    "        if not variable in variable_to_symbol:\n",
    "            variable_to_symbol[variable] = symbol\n",
    "            symbol_to_variables.setdefault(symbol, []).append(variable)\n",
    "\n",
    "    iter_symbols = symbols if first_used else reversed(symbols)\n",
    "    for symbol in iter_symbols:\n",
    "        # If this function is used in the combined nvfuser+torch executor, there are no symbols but regions.\n",
    "        # Regions do not have args, kwargs\n",
    "        if hasattr(symbol, \"inputs\"):\n",
    "            variables = tuple(symbol.inputs) + tuple(symbol.outputs)\n",
    "        else:\n",
    "            variables = (symbol.flat_variableified_proxy_args) + tuple(symbol.flat_variableified_proxy_outs)\n",
    "        tree_map(lambda x: _mark_last_use(symbol, x), variables)\n",
    "\n",
    "    return symbol_to_variables, variable_to_symbol\n",
    "\n",
    "\n",
    "def get_symbols_to_last_used_variables(symbols):\n",
    "    return _get_symbols_to_last_or_first_used_variables(symbols)\n",
    "\n",
    "\n",
    "def get_symbols_to_first_used_variables(symbols):\n",
    "    return _get_symbols_to_last_or_first_used_variables(symbols, first_used=True)\n",
    "\n",
    "\n",
    "def get_symbol_to_idx(symbols):\n",
    "    '''\n",
    "    This function returns a map from symbol to it's position in the list.\n",
    "    '''\n",
    "    return {sym: idx for idx, sym in enumerate(symbols)}\n",
    "\n",
    "\n",
    "def move_closer_to_consumer(execution_trace):\n",
    "    order_in_trace = {bsym: i for i, bsym in enumerate(execution_trace.bound_symbols)}\n",
    "\n",
    "    def prefer_ops_closer_to_consumer(eligible_nodes: list[Node]) -> int:\n",
    "        def key(node: Node) -> int:\n",
    "            return order_in_trace[node.bsym]\n",
    "\n",
    "        return min(range(len(eligible_nodes)), key=lambda i: key(eligible_nodes[i]))\n",
    "\n",
    "    # This moves all del or clear collection at the bottom (as they don't return anything)\n",
    "    bound_symbols = toposort_bsym_dag(\n",
    "        bsym_list_to_dag(execution_trace.bound_symbols)[1],\n",
    "        TOPOSORT_ORDER.BOTTOM_UP,\n",
    "        selector=prefer_ops_closer_to_consumer,\n",
    "    )\n",
    "\n",
    "    for idx, bsym in enumerate(bound_symbols):\n",
    "        if bsym.sym.id == prims.PrimIDs.DEL:\n",
    "            break\n",
    "\n",
    "    new_execution_trace = from_trace(execution_trace)\n",
    "    new_execution_trace.bound_symbols = bound_symbols[:idx]\n",
    "\n",
    "    new_execution_trace = thunder.executors.passes.del_last_used(new_execution_trace, clear_mutable_collections=True)\n",
    "    return new_execution_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to the main topic, of writing the transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPUOffloading(Transform):\n",
    "    '''\n",
    "    Transform to implement CPU Offloading.\n",
    "\n",
    "    save_tensor_policy - Users can pass a callback with signature fn(offloaded_tensors, forward_trace) to filter\n",
    "                         the offloaded_tensors based on their preference eg. biggest 20% intermediate tensors or\n",
    "                         intermediates of certain operations\n",
    "    '''\n",
    "    def __init__(self, save_tensor_policy=None):\n",
    "        self.forward_pass = None\n",
    "        self.backward_pass = None\n",
    "        self._offloaded_tensors = ()\n",
    "        self.save_tensor_policy = None\n",
    "        if save_tensor_policy is not None:\n",
    "            assert callable(save_tensor_policy)\n",
    "            self.save_tensor_policy = save_tensor_policy\n",
    "\n",
    "    def _get_tensors_to_offload(self, forward_trace):\n",
    "        '''\n",
    "        Based on the `forward_trace`, we find the symbols that we want to offload to CPU.\n",
    "        This function finds the intermediate tensors that are saved for backward i.e. ones that are not input or output of this trace.\n",
    "        '''\n",
    "        return_bsym = forward_trace.bound_symbols[-1]\n",
    "        trace_args = return_bsym.args[0][\"flat_args\"]\n",
    "        saved_tensors = get_saved_for_backward_tensors(forward_trace)\n",
    "\n",
    "        tensor_args_name = tuple(arg.name for arg in trace_args if isinstance(arg, TensorProxy))\n",
    "\n",
    "        def is_in_tensor_args(t):\n",
    "            return t.name in tensor_args_name\n",
    "\n",
    "        def is_cuda_tensor(t):\n",
    "            return t.device.type == \"cuda\"\n",
    "\n",
    "        # Tensors which are intermediate and not argument to the computation trace are\n",
    "        # the ones we are interested in offloading.\n",
    "        tensors_to_offload = tuple(t for t in saved_tensors if ((not is_in_tensor_args(t)) and is_cuda_tensor(t)))\n",
    "\n",
    "        # Apply users policy if present.\n",
    "        if self.save_tensor_policy is not None:\n",
    "            tensors_to_offload = self.save_tensor_policy(tensors_to_offload, forward_trace)\n",
    "        self.tensors_to_offload = tensors_to_offload\n",
    "        return self.tensors_to_offload\n",
    "\n",
    "    def _replace_saved_tensors(self, forward_trace, new_output_map):\n",
    "        return_bsym = forward_trace.bound_symbols[-1]\n",
    "        return_bsym_args = return_bsym.args\n",
    "        saved_tensors = return_bsym.args[1][0]\n",
    "\n",
    "        new_saved_tensors = []\n",
    "        for t in saved_tensors:\n",
    "            new_output = new_output_map.get(variableify(t), t)\n",
    "            new_saved_tensors.append(new_output)\n",
    "\n",
    "        new_return_bsym = BoundSymbol.from_bsym(\n",
    "            return_bsym, **{\"args\": (return_bsym_args[0], (tuple(new_saved_tensors), return_bsym_args[1][1]))}\n",
    "        )\n",
    "\n",
    "        # Replace the old return with our new return.\n",
    "        forward_trace.bound_symbols.pop(-1)\n",
    "        forward_trace.bound_symbols.append(new_return_bsym)\n",
    "\n",
    "    def _offload_tensors_from_forward(self, computation_trace):\n",
    "        '''\n",
    "        This function takes the forward computation trace and performs following step\n",
    "        1. Find the tensors to be offloaded using `_get_tensors_to_offload` (this also calls users `save_tensor_policy` if present).\n",
    "        2. Insert calls to the `offload_to_cpu` symbol with the tensor to offload. These calls are placed after the last computational\n",
    "           use of the tensors to be offloaded so that we free the memory as soon as possible.\n",
    "        3. Finally, we update the last symbol i.e. `return` symbol to return the offloaded tensors instead of the original tensors.\n",
    "        '''\n",
    "        # Step 1\n",
    "        # Find the tensors to offload.\n",
    "        # We offload saved tensors which are not arguments to the computation trace and are saved for backwards.\n",
    "        tensors_to_offload = self._get_tensors_to_offload(computation_trace)\n",
    "\n",
    "        # Step 2\n",
    "        # Insert the offloading calls after the last use of the saved tensor (which we want to offload).\n",
    "        _, variable_to_last_symbol = get_symbols_to_last_used_variables(\n",
    "            computation_trace.bound_symbols[:-1]\n",
    "        )  # Ignore the return statement.\n",
    "        symbol_to_idx = get_symbol_to_idx(computation_trace.bound_symbols)\n",
    "\n",
    "        # Book keeping for backward pass update.\n",
    "        new_output_map = {}\n",
    "        new_output_dev_map = {}\n",
    "\n",
    "        # Since we are inserting in the list (we need to obey increasing order) - else the insertions will be incorrect.\n",
    "        sorted_tensors_to_offload = sorted(\n",
    "            tensors_to_offload, key=lambda t: symbol_to_idx[variable_to_last_symbol[variableify(t)]]\n",
    "        )\n",
    "        for idx, t in enumerate(sorted_tensors_to_offload):\n",
    "            last_used_symbol = variable_to_last_symbol[variableify(t)]\n",
    "            last_used_symbol_idx = symbol_to_idx[last_used_symbol]\n",
    "            computation_trace.push_scope([])\n",
    "            with tracectx(computation_trace):\n",
    "                o = offload_to_cpu(t)\n",
    "                prims.python_del(t)\n",
    "            scoped_comp = computation_trace.pop_scope()\n",
    "            scoped_comp[0].header = \"Created by CPU Offloading Transform\"\n",
    "            offload_to_cpu_symbol = scoped_comp[0]\n",
    "            del_symbol = scoped_comp[1]\n",
    "\n",
    "            # This will insert `del` first and then push it down when we insert `offload_to_cpu`.\n",
    "            computation_trace.bound_symbols.insert(last_used_symbol_idx + 1 + (idx * 2), del_symbol)\n",
    "            computation_trace.bound_symbols.insert(last_used_symbol_idx + 1 + (idx * 2), offload_to_cpu_symbol)\n",
    "\n",
    "            # Update book keeping.\n",
    "            new_output_map[variableify(t)] = o\n",
    "            new_output_dev_map[variableify(t)] = t.device.device_str()\n",
    "\n",
    "        # Step 3\n",
    "        # Update the return symbol to return our offloaded tensors in saved for backward.\n",
    "        self._replace_saved_tensors(computation_trace, new_output_map)\n",
    "\n",
    "        # Book keeping for backward pass update.\n",
    "        self._offloaded_tensors = new_output_map\n",
    "        self._offloaded_tensors_dev = new_output_dev_map\n",
    "        return computation_trace\n",
    "\n",
    "    def _load_tensors_for_backward(self, computation_trace):\n",
    "        '''\n",
    "        This function takes the backward computation trace and performs following step\n",
    "        1. Finds the unpack collection symbol which unpacks the saved tensors passed to the backward trace.\n",
    "        2. Updates the unpack collection to unpack the offloaded tensors instead of the original ones.\n",
    "        3. Before the first use of the offloaded tensor in computation, we insert the `load_to_gpu` to load the tensor back on GPU.\n",
    "        '''\n",
    "        self.backward_pass = computation_trace\n",
    "        offloaded_tensors = self._offloaded_tensors\n",
    "        offloaded_tensors_dev_map = self._offloaded_tensors_dev\n",
    "\n",
    "        compute_producers, compute_consumers = thunder.core.utils.producers_and_consumers(computation_trace)\n",
    "\n",
    "        # We want to insert `loads` before the first use of offloaded_tensors.\n",
    "        _, variable_to_first_symbol = get_symbols_to_first_used_variables(computation_trace.bound_symbols)\n",
    "\n",
    "        symbol_to_idx = get_symbol_to_idx(computation_trace.bound_symbols)\n",
    "\n",
    "        # Step 1 and 2\n",
    "        # Update unpack collection so that it\n",
    "        # outputs the offloaded tensor proxies (not the original ones).\n",
    "        unpack_sym = compute_producers[list(offloaded_tensors.keys())[0].proxy]\n",
    "        unpack_idx = symbol_to_idx[unpack_sym]\n",
    "        unpack_sym_out = unpack_sym.output\n",
    "        new_out = []\n",
    "        for out in unpack_sym_out:\n",
    "            vout = variableify(out)\n",
    "            if vout in offloaded_tensors:\n",
    "                new_out.append(offloaded_tensors[vout])\n",
    "            else:\n",
    "                new_out.append(out)\n",
    "        new_unpack_bsym = BoundSymbol.from_bsym(unpack_sym, output=tuple(new_out))\n",
    "        computation_trace.bound_symbols[unpack_idx] = new_unpack_bsym\n",
    "\n",
    "        # Now we again find the first usages of offloaded tensor\n",
    "        # This will actually point us to the first consumer of the offloaded tensor.\n",
    "        offset = unpack_idx + 1\n",
    "        _, variable_to_first_symbol = get_symbols_to_first_used_variables(computation_trace.bound_symbols[offset:])\n",
    "\n",
    "        # Step 3\n",
    "        # Load the offloaded tensors to GPU before usage.\n",
    "        # Should iterate in correct order (else insertion positions will be incorrect).\n",
    "        for idx, (vt, offloaded_t) in enumerate(\n",
    "            sorted(offloaded_tensors.items(), key=lambda kv: symbol_to_idx[variable_to_first_symbol[kv[0]]])\n",
    "        ):\n",
    "            first_used_symbol = variable_to_first_symbol[vt]\n",
    "            first_used_symbol_idx = symbol_to_idx[first_used_symbol]\n",
    "            t = vt.proxy\n",
    "            device = offloaded_tensors_dev_map[vt]\n",
    "\n",
    "            with tracectx(computation_trace):\n",
    "                new_sym = load_to_gpu.bind(offloaded_t, device, output=t)\n",
    "\n",
    "            new_sym.header = \"Created by CPU Offloading Transform\"\n",
    "            computation_trace.bound_symbols.insert(first_used_symbol_idx + idx, new_sym)\n",
    "\n",
    "        return computation_trace\n",
    "\n",
    "    def transform_trace_post_optimization(self, computation_trace: thunder.TraceCtx, **kwargs):\n",
    "        if self.forward_pass is None:\n",
    "            self.forward_pass = computation_trace\n",
    "            # Processing for the forward pass (only if we are going to compute backward).\n",
    "            if \"augmented_forward\" in computation_trace.fn.__name__:\n",
    "                computation_trace = self._offload_tensors_from_forward(computation_trace)\n",
    "        else:\n",
    "            # Skip if no tensor was offloaded.\n",
    "            if len(self._offloaded_tensors) == 0:\n",
    "                return computation_trace\n",
    "\n",
    "            # We need this because in unmodified backward trace, the first consumer of saved_for_backward maybe\n",
    "            # a reshape or permute op and the actual computation occurs 50-100 (or more) lines later.\n",
    "            # Because of this we load more tensors than required eagerly (thus decreasing the memory gains from CPU Offloading).\n",
    "            # This function is currently tailored to pattern observed in Llama-2\n",
    "            # Eg. on line 92\n",
    "            #   # Created by CPU Offloading Transform\n",
    "            #   t1319 = load_to_gpu(offloaded_t1319, 'cuda:0')  # t1319: \"cuda:0 f32[8, 1024, 11008]\"\n",
    "            #   t4021 = torch.reshape(t1319, (-1, 11008))  # t4021: \"cuda:0 f32[8192, 11008]\"\n",
    "            #     # t4021 = ltorch.reshape(t1319, (-1, 11008))  # t4021: \"cuda:0 f32[8192, 11008]\"\n",
    "            #       # t4021 = prims.reshape(t1319, (8192, 11008))  # t4021: \"cuda:0 f32[8192, 11008]\"\n",
    "            #   del t1319\n",
    "            # And it's usage in computation is at 612\n",
    "            # t4022 = torch.matmul(t4020, t4021)  # t4022: \"cuda:0 f32[4096, 11008]\"\n",
    "            #   t4022 = ltorch.matmul(t4020, t4021)  # t4022: \"cuda:0 f32[4096, 11008]\"\n",
    "            #     t4022 = prims.matmul(t4020, t4021)  # t4022: \"cuda:0 f32[4096, 11008]\"\n",
    "            computation_trace = move_closer_to_consumer(computation_trace)\n",
    "\n",
    "            # Transform the backward trace to load offloaded tensors back to the device.\n",
    "            computation_trace = self._load_tensors_for_backward(computation_trace)\n",
    "\n",
    "        return computation_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_accumulated_memory_stats()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    print(f\"Allocated Memory after cleaning {torch.cuda.memory_allocated() / 1e9} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(jmodel, model, args, kwargs):\n",
    "    # NOTE - This function takes care of warm-up\n",
    "    stmt = \"\"\"\n",
    "# Use the optimized model for prediction and backward\n",
    "o = jmodel(*args, **kwargs)\n",
    "o.sum().backward()\n",
    "for param in model.parameters():  # use original model for clear grads\n",
    "    param.grad = None\n",
    "\"\"\"\n",
    "    timer = torch.utils.benchmark.Timer(\n",
    "        stmt=stmt, globals={\"jmodel\": jmodel, \"model\": model, \"args\": args, \"kwargs\": kwargs}\n",
    "    ).timeit(number=10)\n",
    "    return timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity Check the output values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated Memory after cleaning 8.192e-06 GB\n"
     ]
    }
   ],
   "source": [
    "class MySimpleModel(torch.nn.Module):\n",
    "    def __init__(self, n_layers=10):\n",
    "        super().__init__()\n",
    "        self.fcs = torch.nn.ModuleList([torch.nn.Linear(16, 16) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for fc in self.fcs:\n",
    "            x = torch.nn.functional.relu(fc(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "def get_model_and_args():\n",
    "    device = 'cuda'\n",
    "    model = MySimpleModel(n_layers=100).to(device)\n",
    "    args = (torch.randn(128, 16, device=device),)\n",
    "    kwargs = {}\n",
    "    return model, args, kwargs\n",
    "\n",
    "model, args, kwargs = get_model_and_args()\n",
    "\n",
    "jmodel = thunder.jit(model, transforms=[CPUOffloading()])\n",
    "\n",
    "actual = jmodel(*args, **kwargs)\n",
    "\n",
    "# Verify that saved tensors are on CPU.\n",
    "saved_tensor_devices = set()\n",
    "for t in actual.grad_fn.saved_tensors:\n",
    "    saved_tensor_devices.add(str(t.device))\n",
    "\n",
    "assert \"cpu\" in saved_tensor_devices  # Verify that we actually have saved tensors on CPU\n",
    "\n",
    "grad_output = torch.randn_like(actual)\n",
    "actual_grads = torch.autograd.grad(actual, jmodel.parameters(), grad_output)\n",
    "\n",
    "\n",
    "\n",
    "fw_traces = thunder.last_traces(jmodel)\n",
    "bw_traces = thunder.last_backward_traces(jmodel)\n",
    "\n",
    "with torch.no_grad():\n",
    "    actual_cpu = actual.to(\"cpu\")\n",
    "    actual_grads_cpu = tree_map(lambda t: t.to(\"cpu\"), actual_grads)\n",
    "\n",
    "# Check against the eager model\n",
    "expected = model(*args, **kwargs)\n",
    "expected_grads = torch.autograd.grad(expected, model.parameters(), grad_output)\n",
    "\n",
    "with torch.no_grad():\n",
    "    expected_cpu = expected.to(\"cpu\")\n",
    "    expected_grads_cpu = tree_map(lambda t: t.to(\"cpu\"), expected_grads)\n",
    "\n",
    "torch.testing.assert_close(actual_cpu, expected_cpu)\n",
    "torch.testing.assert_close(actual_grads_cpu, expected_grads_cpu)\n",
    "\n",
    "del jmodel, model, args, kwargs, actual, actual_grads, expected, expected_grads, grad_output  # Free memory.\n",
    "\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# Constructed by Delete Last Used (took 2 milliseconds)\n",
       "import torch\n",
       "import torch.nn.functional\n",
       "from thunder.executors.torchex import no_autocast\n",
       "\n",
       "@torch.no_grad()\n",
       "@no_autocast\n",
       "def augmented_forward_fn(x, t_fcs_0_bias, t_fcs_0_weight, t_fcs_1_bias, t_fcs_1_weight, t_fcs_2_bias, t_fcs_2_weight, t_fcs_3_bias, t_fcs_3_weight, t_fcs_4_bias, t_fcs_4_weight, t_fcs_5_bias, t_fcs_5_weight, t_fcs_6_bias, t_fcs_6_weight, t_fcs_7_bias, t_fcs_7_weight, t_fcs_8_bias, t_fcs_8_weight, t_fcs_9_bias, t_fcs_9_weight, t_fcs_10_bias, t_fcs_10_weight, t_fcs_11_bias, t_fcs_11_weight, t_fcs_12_bias, t_fcs_12_weight, t_fcs_13_bias, t_fcs_13_weight, t_fcs_14_bias, t_fcs_14_weight, t_fcs_15_bias, t_fcs_15_weight, t_fcs_16_bias, t_fcs_16_weight, t_fcs_17_bias, t_fcs_17_weight, t_fcs_18_bias, t_fcs_18_weight, t_fcs_19_bias, t_fcs_19_weight, t_fcs_20_bias, t_fcs_20_weight, t_fcs_21_bias, t_fcs_21_weight, t_fcs_22_bias, t_fcs_22_weight, t_fcs_23_bias, t_fcs_23_weight, t_fcs_24_bias, t_fcs_24_weight, t_fcs_25_bias, t_fcs_25_weight, t_fcs_26_bias, t_fcs_26_weight, t_fcs_27_bias, t_fcs_27_weight, t_fcs_28_bias, t_fcs_28_weight, t_fcs_29_bias, t_fcs_29_weight, t_fcs_30_bias, t_fcs_30_weight, t_fcs_31_bias, t_fcs_31_weight, t_fcs_32_bias, t_fcs_32_weight, t_fcs_33_bias, t_fcs_33_weight, t_fcs_34_bias, t_fcs_34_weight, t_fcs_35_bias, t_fcs_35_weight, t_fcs_36_bias, t_fcs_36_weight, t_fcs_37_bias, t_fcs_37_weight, t_fcs_38_bias, t_fcs_38_weight, t_fcs_39_bias, t_fcs_39_weight, t_fcs_40_bias, t_fcs_40_weight, t_fcs_41_bias, t_fcs_41_weight, t_fcs_42_bias, t_fcs_42_weight, t_fcs_43_bias, t_fcs_43_weight, t_fcs_44_bias, t_fcs_44_weight, t_fcs_45_bias, t_fcs_45_weight, t_fcs_46_bias, t_fcs_46_weight, t_fcs_47_bias, t_fcs_47_weight, t_fcs_48_bias, t_fcs_48_weight, t_fcs_49_bias, t_fcs_49_weight, t_fcs_50_bias, t_fcs_50_weight, t_fcs_51_bias, t_fcs_51_weight, t_fcs_52_bias, t_fcs_52_weight, t_fcs_53_bias, t_fcs_53_weight, t_fcs_54_bias, t_fcs_54_weight, t_fcs_55_bias, t_fcs_55_weight, t_fcs_56_bias, t_fcs_56_weight, t_fcs_57_bias, t_fcs_57_weight, t_fcs_58_bias, t_fcs_58_weight, t_fcs_59_bias, t_fcs_59_weight, t_fcs_60_bias, t_fcs_60_weight, t_fcs_61_bias, t_fcs_61_weight, t_fcs_62_bias, t_fcs_62_weight, t_fcs_63_bias, t_fcs_63_weight, t_fcs_64_bias, t_fcs_64_weight, t_fcs_65_bias, t_fcs_65_weight, t_fcs_66_bias, t_fcs_66_weight, t_fcs_67_bias, t_fcs_67_weight, t_fcs_68_bias, t_fcs_68_weight, t_fcs_69_bias, t_fcs_69_weight, t_fcs_70_bias, t_fcs_70_weight, t_fcs_71_bias, t_fcs_71_weight, t_fcs_72_bias, t_fcs_72_weight, t_fcs_73_bias, t_fcs_73_weight, t_fcs_74_bias, t_fcs_74_weight, t_fcs_75_bias, t_fcs_75_weight, t_fcs_76_bias, t_fcs_76_weight, t_fcs_77_bias, t_fcs_77_weight, t_fcs_78_bias, t_fcs_78_weight, t_fcs_79_bias, t_fcs_79_weight, t_fcs_80_bias, t_fcs_80_weight, t_fcs_81_bias, t_fcs_81_weight, t_fcs_82_bias, t_fcs_82_weight, t_fcs_83_bias, t_fcs_83_weight, t_fcs_84_bias, t_fcs_84_weight, t_fcs_85_bias, t_fcs_85_weight, t_fcs_86_bias, t_fcs_86_weight, t_fcs_87_bias, t_fcs_87_weight, t_fcs_88_bias, t_fcs_88_weight, t_fcs_89_bias, t_fcs_89_weight, t_fcs_90_bias, t_fcs_90_weight, t_fcs_91_bias, t_fcs_91_weight, t_fcs_92_bias, t_fcs_92_weight, t_fcs_93_bias, t_fcs_93_weight, t_fcs_94_bias, t_fcs_94_weight, t_fcs_95_bias, t_fcs_95_weight, t_fcs_96_bias, t_fcs_96_weight, t_fcs_97_bias, t_fcs_97_weight, t_fcs_98_bias, t_fcs_98_weight, t_fcs_99_bias, t_fcs_99_weight):\n",
       "  # x: \"cuda:0 f32[128, 16]\"\n",
       "  # t_fcs_0_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_0_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_1_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_1_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_2_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_2_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_3_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_3_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_4_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_4_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_5_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_5_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_6_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_6_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_7_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_7_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_8_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_8_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_9_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_9_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_10_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_10_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_11_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_11_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_12_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_12_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_13_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_13_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_14_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_14_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_15_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_15_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_16_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_16_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_17_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_17_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_18_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_18_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_19_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_19_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_20_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_20_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_21_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_21_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_22_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_22_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_23_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_23_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_24_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_24_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_25_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_25_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_26_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_26_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_27_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_27_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_28_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_28_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_29_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_29_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_30_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_30_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_31_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_31_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_32_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_32_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_33_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_33_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_34_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_34_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_35_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_35_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_36_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_36_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_37_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_37_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_38_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_38_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_39_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_39_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_40_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_40_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_41_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_41_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_42_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_42_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_43_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_43_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_44_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_44_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_45_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_45_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_46_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_46_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_47_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_47_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_48_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_48_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_49_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_49_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_50_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_50_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_51_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_51_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_52_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_52_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_53_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_53_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_54_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_54_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_55_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_55_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_56_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_56_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_57_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_57_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_58_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_58_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_59_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_59_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_60_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_60_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_61_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_61_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_62_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_62_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_63_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_63_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_64_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_64_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_65_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_65_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_66_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_66_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_67_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_67_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_68_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_68_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_69_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_69_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_70_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_70_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_71_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_71_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_72_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_72_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_73_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_73_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_74_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_74_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_75_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_75_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_76_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_76_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_77_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_77_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_78_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_78_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_79_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_79_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_80_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_80_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_81_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_81_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_82_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_82_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_83_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_83_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_84_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_84_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_85_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_85_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_86_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_86_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_87_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_87_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_88_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_88_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_89_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_89_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_90_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_90_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_91_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_91_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_92_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_92_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_93_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_93_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_94_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_94_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_95_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_95_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_96_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_96_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_97_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_97_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_98_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_98_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_99_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_99_weight: \"cuda:0 f32[16, 16]\"\n",
       "  t0 = torch.nn.functional.linear(x, t_fcs_0_weight, t_fcs_0_bias)  # t0: \"cuda:0 f32[128, 16]\"\n",
       "    # t0 = ltorch.linear(x, t_fcs_0_weight, t_fcs_0_bias)  # t0: \"cuda:0 f32[128, 16]\"\n",
       "      # t0 = prims.linear(x, t_fcs_0_weight, t_fcs_0_bias)  # t0: \"cuda:0 f32[128, 16]\"\n",
       "  [t1, t2] = nvFusion0(t0)\n",
       "    # t1 = prims.gt(t0, 0.0)  # t1: \"cuda:0 b8[128, 16]\"\n",
       "    # t2 = prims.where(t1, t0, 0.0)  # t2: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t1 = offload_to_cpu(t1)  # offloaded_t1: \"cpu b8[128, 16]\"\n",
       "  del t1\n",
       "  del t0\n",
       "  t3 = torch.nn.functional.linear(t2, t_fcs_1_weight, t_fcs_1_bias)  # t3: \"cuda:0 f32[128, 16]\"\n",
       "    # t3 = ltorch.linear(t2, t_fcs_1_weight, t_fcs_1_bias)  # t3: \"cuda:0 f32[128, 16]\"\n",
       "      # t3 = prims.linear(t2, t_fcs_1_weight, t_fcs_1_bias)  # t3: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t2 = offload_to_cpu(t2)  # offloaded_t2: \"cpu f32[128, 16]\"\n",
       "  del t2\n",
       "  [t5, t7] = nvFusion1(t3)\n",
       "    # t5 = prims.gt(t3, 0.0)  # t5: \"cuda:0 b8[128, 16]\"\n",
       "    # t7 = prims.where(t5, t3, 0.0)  # t7: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t5 = offload_to_cpu(t5)  # offloaded_t5: \"cpu b8[128, 16]\"\n",
       "  del t5\n",
       "  del t3\n",
       "  t8 = torch.nn.functional.linear(t7, t_fcs_2_weight, t_fcs_2_bias)  # t8: \"cuda:0 f32[128, 16]\"\n",
       "    # t8 = ltorch.linear(t7, t_fcs_2_weight, t_fcs_2_bias)  # t8: \"cuda:0 f32[128, 16]\"\n",
       "      # t8 = prims.linear(t7, t_fcs_2_weight, t_fcs_2_bias)  # t8: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t7 = offload_to_cpu(t7)  # offloaded_t7: \"cpu f32[128, 16]\"\n",
       "  del t7\n",
       "  [t10, t12] = nvFusion2(t8)\n",
       "    # t10 = prims.gt(t8, 0.0)  # t10: \"cuda:0 b8[128, 16]\"\n",
       "    # t12 = prims.where(t10, t8, 0.0)  # t12: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t10 = offload_to_cpu(t10)  # offloaded_t10: \"cpu b8[128, 16]\"\n",
       "  del t10\n",
       "  del t8\n",
       "  t13 = torch.nn.functional.linear(t12, t_fcs_3_weight, t_fcs_3_bias)  # t13: \"cuda:0 f32[128, 16]\"\n",
       "    # t13 = ltorch.linear(t12, t_fcs_3_weight, t_fcs_3_bias)  # t13: \"cuda:0 f32[128, 16]\"\n",
       "      # t13 = prims.linear(t12, t_fcs_3_weight, t_fcs_3_bias)  # t13: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t12 = offload_to_cpu(t12)  # offloaded_t12: \"cpu f32[128, 16]\"\n",
       "  del t12\n",
       "  [t15, t17] = nvFusion3(t13)\n",
       "    # t15 = prims.gt(t13, 0.0)  # t15: \"cuda:0 b8[128, 16]\"\n",
       "    # t17 = prims.where(t15, t13, 0.0)  # t17: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t15 = offload_to_cpu(t15)  # offloaded_t15: \"cpu b8[128, 16]\"\n",
       "  del t15\n",
       "  del t13\n",
       "  t18 = torch.nn.functional.linear(t17, t_fcs_4_weight, t_fcs_4_bias)  # t18: \"cuda:0 f32[128, 16]\"\n",
       "    # t18 = ltorch.linear(t17, t_fcs_4_weight, t_fcs_4_bias)  # t18: \"cuda:0 f32[128, 16]\"\n",
       "      # t18 = prims.linear(t17, t_fcs_4_weight, t_fcs_4_bias)  # t18: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t17 = offload_to_cpu(t17)  # offloaded_t17: \"cpu f32[128, 16]\"\n",
       "  del t17\n",
       "  [t20, t22] = nvFusion4(t18)\n",
       "    # t20 = prims.gt(t18, 0.0)  # t20: \"cuda:0 b8[128, 16]\"\n",
       "    # t22 = prims.where(t20, t18, 0.0)  # t22: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t20 = offload_to_cpu(t20)  # offloaded_t20: \"cpu b8[128, 16]\"\n",
       "  del t20\n",
       "  del t18\n",
       "  t23 = torch.nn.functional.linear(t22, t_fcs_5_weight, t_fcs_5_bias)  # t23: \"cuda:0 f32[128, 16]\"\n",
       "    # t23 = ltorch.linear(t22, t_fcs_5_weight, t_fcs_5_bias)  # t23: \"cuda:0 f32[128, 16]\"\n",
       "      # t23 = prims.linear(t22, t_fcs_5_weight, t_fcs_5_bias)  # t23: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t22 = offload_to_cpu(t22)  # offloaded_t22: \"cpu f32[128, 16]\"\n",
       "  del t22\n",
       "  [t25, t27] = nvFusion5(t23)\n",
       "    # t25 = prims.gt(t23, 0.0)  # t25: \"cuda:0 b8[128, 16]\"\n",
       "    # t27 = prims.where(t25, t23, 0.0)  # t27: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t25 = offload_to_cpu(t25)  # offloaded_t25: \"cpu b8[128, 16]\"\n",
       "  del t25\n",
       "  del t23\n",
       "  t28 = torch.nn.functional.linear(t27, t_fcs_6_weight, t_fcs_6_bias)  # t28: \"cuda:0 f32[128, 16]\"\n",
       "    # t28 = ltorch.linear(t27, t_fcs_6_weight, t_fcs_6_bias)  # t28: \"cuda:0 f32[128, 16]\"\n",
       "      # t28 = prims.linear(t27, t_fcs_6_weight, t_fcs_6_bias)  # t28: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t27 = offload_to_cpu(t27)  # offloaded_t27: \"cpu f32[128, 16]\"\n",
       "  del t27\n",
       "  [t30, t32] = nvFusion6(t28)\n",
       "    # t30 = prims.gt(t28, 0.0)  # t30: \"cuda:0 b8[128, 16]\"\n",
       "    # t32 = prims.where(t30, t28, 0.0)  # t32: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t30 = offload_to_cpu(t30)  # offloaded_t30: \"cpu b8[128, 16]\"\n",
       "  del t30\n",
       "  del t28\n",
       "  t33 = torch.nn.functional.linear(t32, t_fcs_7_weight, t_fcs_7_bias)  # t33: \"cuda:0 f32[128, 16]\"\n",
       "    # t33 = ltorch.linear(t32, t_fcs_7_weight, t_fcs_7_bias)  # t33: \"cuda:0 f32[128, 16]\"\n",
       "      # t33 = prims.linear(t32, t_fcs_7_weight, t_fcs_7_bias)  # t33: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t32 = offload_to_cpu(t32)  # offloaded_t32: \"cpu f32[128, 16]\"\n",
       "  del t32\n",
       "  [t35, t37] = nvFusion7(t33)\n",
       "    # t35 = prims.gt(t33, 0.0)  # t35: \"cuda:0 b8[128, 16]\"\n",
       "    # t37 = prims.where(t35, t33, 0.0)  # t37: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t35 = offload_to_cpu(t35)  # offloaded_t35: \"cpu b8[128, 16]\"\n",
       "  del t35\n",
       "  del t33\n",
       "  t38 = torch.nn.functional.linear(t37, t_fcs_8_weight, t_fcs_8_bias)  # t38: \"cuda:0 f32[128, 16]\"\n",
       "    # t38 = ltorch.linear(t37, t_fcs_8_weight, t_fcs_8_bias)  # t38: \"cuda:0 f32[128, 16]\"\n",
       "      # t38 = prims.linear(t37, t_fcs_8_weight, t_fcs_8_bias)  # t38: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t37 = offload_to_cpu(t37)  # offloaded_t37: \"cpu f32[128, 16]\"\n",
       "  del t37\n",
       "  [t40, t42] = nvFusion8(t38)\n",
       "    # t40 = prims.gt(t38, 0.0)  # t40: \"cuda:0 b8[128, 16]\"\n",
       "    # t42 = prims.where(t40, t38, 0.0)  # t42: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t40 = offload_to_cpu(t40)  # offloaded_t40: \"cpu b8[128, 16]\"\n",
       "  del t40\n",
       "  del t38\n",
       "  t43 = torch.nn.functional.linear(t42, t_fcs_9_weight, t_fcs_9_bias)  # t43: \"cuda:0 f32[128, 16]\"\n",
       "    # t43 = ltorch.linear(t42, t_fcs_9_weight, t_fcs_9_bias)  # t43: \"cuda:0 f32[128, 16]\"\n",
       "      # t43 = prims.linear(t42, t_fcs_9_weight, t_fcs_9_bias)  # t43: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t42 = offload_to_cpu(t42)  # offloaded_t42: \"cpu f32[128, 16]\"\n",
       "  del t42\n",
       "  [t45, t47] = nvFusion9(t43)\n",
       "    # t45 = prims.gt(t43, 0.0)  # t45: \"cuda:0 b8[128, 16]\"\n",
       "    # t47 = prims.where(t45, t43, 0.0)  # t47: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t45 = offload_to_cpu(t45)  # offloaded_t45: \"cpu b8[128, 16]\"\n",
       "  del t45\n",
       "  del t43\n",
       "  t48 = torch.nn.functional.linear(t47, t_fcs_10_weight, t_fcs_10_bias)  # t48: \"cuda:0 f32[128, 16]\"\n",
       "    # t48 = ltorch.linear(t47, t_fcs_10_weight, t_fcs_10_bias)  # t48: \"cuda:0 f32[128, 16]\"\n",
       "      # t48 = prims.linear(t47, t_fcs_10_weight, t_fcs_10_bias)  # t48: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t47 = offload_to_cpu(t47)  # offloaded_t47: \"cpu f32[128, 16]\"\n",
       "  del t47\n",
       "  [t50, t52] = nvFusion10(t48)\n",
       "    # t50 = prims.gt(t48, 0.0)  # t50: \"cuda:0 b8[128, 16]\"\n",
       "    # t52 = prims.where(t50, t48, 0.0)  # t52: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t50 = offload_to_cpu(t50)  # offloaded_t50: \"cpu b8[128, 16]\"\n",
       "  del t50\n",
       "  del t48\n",
       "  t53 = torch.nn.functional.linear(t52, t_fcs_11_weight, t_fcs_11_bias)  # t53: \"cuda:0 f32[128, 16]\"\n",
       "    # t53 = ltorch.linear(t52, t_fcs_11_weight, t_fcs_11_bias)  # t53: \"cuda:0 f32[128, 16]\"\n",
       "      # t53 = prims.linear(t52, t_fcs_11_weight, t_fcs_11_bias)  # t53: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t52 = offload_to_cpu(t52)  # offloaded_t52: \"cpu f32[128, 16]\"\n",
       "  del t52\n",
       "  [t55, t57] = nvFusion11(t53)\n",
       "    # t55 = prims.gt(t53, 0.0)  # t55: \"cuda:0 b8[128, 16]\"\n",
       "    # t57 = prims.where(t55, t53, 0.0)  # t57: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t55 = offload_to_cpu(t55)  # offloaded_t55: \"cpu b8[128, 16]\"\n",
       "  del t55\n",
       "  del t53\n",
       "  t58 = torch.nn.functional.linear(t57, t_fcs_12_weight, t_fcs_12_bias)  # t58: \"cuda:0 f32[128, 16]\"\n",
       "    # t58 = ltorch.linear(t57, t_fcs_12_weight, t_fcs_12_bias)  # t58: \"cuda:0 f32[128, 16]\"\n",
       "      # t58 = prims.linear(t57, t_fcs_12_weight, t_fcs_12_bias)  # t58: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t57 = offload_to_cpu(t57)  # offloaded_t57: \"cpu f32[128, 16]\"\n",
       "  del t57\n",
       "  [t60, t62] = nvFusion12(t58)\n",
       "    # t60 = prims.gt(t58, 0.0)  # t60: \"cuda:0 b8[128, 16]\"\n",
       "    # t62 = prims.where(t60, t58, 0.0)  # t62: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t60 = offload_to_cpu(t60)  # offloaded_t60: \"cpu b8[128, 16]\"\n",
       "  del t60\n",
       "  del t58\n",
       "  t63 = torch.nn.functional.linear(t62, t_fcs_13_weight, t_fcs_13_bias)  # t63: \"cuda:0 f32[128, 16]\"\n",
       "    # t63 = ltorch.linear(t62, t_fcs_13_weight, t_fcs_13_bias)  # t63: \"cuda:0 f32[128, 16]\"\n",
       "      # t63 = prims.linear(t62, t_fcs_13_weight, t_fcs_13_bias)  # t63: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t62 = offload_to_cpu(t62)  # offloaded_t62: \"cpu f32[128, 16]\"\n",
       "  del t62\n",
       "  [t65, t67] = nvFusion13(t63)\n",
       "    # t65 = prims.gt(t63, 0.0)  # t65: \"cuda:0 b8[128, 16]\"\n",
       "    # t67 = prims.where(t65, t63, 0.0)  # t67: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t65 = offload_to_cpu(t65)  # offloaded_t65: \"cpu b8[128, 16]\"\n",
       "  del t65\n",
       "  del t63\n",
       "  t68 = torch.nn.functional.linear(t67, t_fcs_14_weight, t_fcs_14_bias)  # t68: \"cuda:0 f32[128, 16]\"\n",
       "    # t68 = ltorch.linear(t67, t_fcs_14_weight, t_fcs_14_bias)  # t68: \"cuda:0 f32[128, 16]\"\n",
       "      # t68 = prims.linear(t67, t_fcs_14_weight, t_fcs_14_bias)  # t68: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t67 = offload_to_cpu(t67)  # offloaded_t67: \"cpu f32[128, 16]\"\n",
       "  del t67\n",
       "  [t70, t72] = nvFusion14(t68)\n",
       "    # t70 = prims.gt(t68, 0.0)  # t70: \"cuda:0 b8[128, 16]\"\n",
       "    # t72 = prims.where(t70, t68, 0.0)  # t72: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t70 = offload_to_cpu(t70)  # offloaded_t70: \"cpu b8[128, 16]\"\n",
       "  del t70\n",
       "  del t68\n",
       "  t73 = torch.nn.functional.linear(t72, t_fcs_15_weight, t_fcs_15_bias)  # t73: \"cuda:0 f32[128, 16]\"\n",
       "    # t73 = ltorch.linear(t72, t_fcs_15_weight, t_fcs_15_bias)  # t73: \"cuda:0 f32[128, 16]\"\n",
       "      # t73 = prims.linear(t72, t_fcs_15_weight, t_fcs_15_bias)  # t73: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t72 = offload_to_cpu(t72)  # offloaded_t72: \"cpu f32[128, 16]\"\n",
       "  del t72\n",
       "  [t75, t77] = nvFusion15(t73)\n",
       "    # t75 = prims.gt(t73, 0.0)  # t75: \"cuda:0 b8[128, 16]\"\n",
       "    # t77 = prims.where(t75, t73, 0.0)  # t77: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t75 = offload_to_cpu(t75)  # offloaded_t75: \"cpu b8[128, 16]\"\n",
       "  del t75\n",
       "  del t73\n",
       "  t78 = torch.nn.functional.linear(t77, t_fcs_16_weight, t_fcs_16_bias)  # t78: \"cuda:0 f32[128, 16]\"\n",
       "    # t78 = ltorch.linear(t77, t_fcs_16_weight, t_fcs_16_bias)  # t78: \"cuda:0 f32[128, 16]\"\n",
       "      # t78 = prims.linear(t77, t_fcs_16_weight, t_fcs_16_bias)  # t78: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t77 = offload_to_cpu(t77)  # offloaded_t77: \"cpu f32[128, 16]\"\n",
       "  del t77\n",
       "  [t80, t82] = nvFusion16(t78)\n",
       "    # t80 = prims.gt(t78, 0.0)  # t80: \"cuda:0 b8[128, 16]\"\n",
       "    # t82 = prims.where(t80, t78, 0.0)  # t82: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t80 = offload_to_cpu(t80)  # offloaded_t80: \"cpu b8[128, 16]\"\n",
       "  del t80\n",
       "  del t78\n",
       "  t83 = torch.nn.functional.linear(t82, t_fcs_17_weight, t_fcs_17_bias)  # t83: \"cuda:0 f32[128, 16]\"\n",
       "    # t83 = ltorch.linear(t82, t_fcs_17_weight, t_fcs_17_bias)  # t83: \"cuda:0 f32[128, 16]\"\n",
       "      # t83 = prims.linear(t82, t_fcs_17_weight, t_fcs_17_bias)  # t83: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t82 = offload_to_cpu(t82)  # offloaded_t82: \"cpu f32[128, 16]\"\n",
       "  del t82\n",
       "  [t85, t87] = nvFusion17(t83)\n",
       "    # t85 = prims.gt(t83, 0.0)  # t85: \"cuda:0 b8[128, 16]\"\n",
       "    # t87 = prims.where(t85, t83, 0.0)  # t87: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t85 = offload_to_cpu(t85)  # offloaded_t85: \"cpu b8[128, 16]\"\n",
       "  del t85\n",
       "  del t83\n",
       "  t88 = torch.nn.functional.linear(t87, t_fcs_18_weight, t_fcs_18_bias)  # t88: \"cuda:0 f32[128, 16]\"\n",
       "    # t88 = ltorch.linear(t87, t_fcs_18_weight, t_fcs_18_bias)  # t88: \"cuda:0 f32[128, 16]\"\n",
       "      # t88 = prims.linear(t87, t_fcs_18_weight, t_fcs_18_bias)  # t88: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t87 = offload_to_cpu(t87)  # offloaded_t87: \"cpu f32[128, 16]\"\n",
       "  del t87\n",
       "  [t90, t92] = nvFusion18(t88)\n",
       "    # t90 = prims.gt(t88, 0.0)  # t90: \"cuda:0 b8[128, 16]\"\n",
       "    # t92 = prims.where(t90, t88, 0.0)  # t92: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t90 = offload_to_cpu(t90)  # offloaded_t90: \"cpu b8[128, 16]\"\n",
       "  del t90\n",
       "  del t88\n",
       "  t93 = torch.nn.functional.linear(t92, t_fcs_19_weight, t_fcs_19_bias)  # t93: \"cuda:0 f32[128, 16]\"\n",
       "    # t93 = ltorch.linear(t92, t_fcs_19_weight, t_fcs_19_bias)  # t93: \"cuda:0 f32[128, 16]\"\n",
       "      # t93 = prims.linear(t92, t_fcs_19_weight, t_fcs_19_bias)  # t93: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t92 = offload_to_cpu(t92)  # offloaded_t92: \"cpu f32[128, 16]\"\n",
       "  del t92\n",
       "  [t95, t97] = nvFusion19(t93)\n",
       "    # t95 = prims.gt(t93, 0.0)  # t95: \"cuda:0 b8[128, 16]\"\n",
       "    # t97 = prims.where(t95, t93, 0.0)  # t97: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t95 = offload_to_cpu(t95)  # offloaded_t95: \"cpu b8[128, 16]\"\n",
       "  del t95\n",
       "  del t93\n",
       "  t98 = torch.nn.functional.linear(t97, t_fcs_20_weight, t_fcs_20_bias)  # t98: \"cuda:0 f32[128, 16]\"\n",
       "    # t98 = ltorch.linear(t97, t_fcs_20_weight, t_fcs_20_bias)  # t98: \"cuda:0 f32[128, 16]\"\n",
       "      # t98 = prims.linear(t97, t_fcs_20_weight, t_fcs_20_bias)  # t98: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t97 = offload_to_cpu(t97)  # offloaded_t97: \"cpu f32[128, 16]\"\n",
       "  del t97\n",
       "  [t100, t102] = nvFusion20(t98)\n",
       "    # t100 = prims.gt(t98, 0.0)  # t100: \"cuda:0 b8[128, 16]\"\n",
       "    # t102 = prims.where(t100, t98, 0.0)  # t102: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t100 = offload_to_cpu(t100)  # offloaded_t100: \"cpu b8[128, 16]\"\n",
       "  del t100\n",
       "  del t98\n",
       "  t103 = torch.nn.functional.linear(t102, t_fcs_21_weight, t_fcs_21_bias)  # t103: \"cuda:0 f32[128, 16]\"\n",
       "    # t103 = ltorch.linear(t102, t_fcs_21_weight, t_fcs_21_bias)  # t103: \"cuda:0 f32[128, 16]\"\n",
       "      # t103 = prims.linear(t102, t_fcs_21_weight, t_fcs_21_bias)  # t103: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t102 = offload_to_cpu(t102)  # offloaded_t102: \"cpu f32[128, 16]\"\n",
       "  del t102\n",
       "  [t105, t107] = nvFusion21(t103)\n",
       "    # t105 = prims.gt(t103, 0.0)  # t105: \"cuda:0 b8[128, 16]\"\n",
       "    # t107 = prims.where(t105, t103, 0.0)  # t107: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t105 = offload_to_cpu(t105)  # offloaded_t105: \"cpu b8[128, 16]\"\n",
       "  del t105\n",
       "  del t103\n",
       "  t108 = torch.nn.functional.linear(t107, t_fcs_22_weight, t_fcs_22_bias)  # t108: \"cuda:0 f32[128, 16]\"\n",
       "    # t108 = ltorch.linear(t107, t_fcs_22_weight, t_fcs_22_bias)  # t108: \"cuda:0 f32[128, 16]\"\n",
       "      # t108 = prims.linear(t107, t_fcs_22_weight, t_fcs_22_bias)  # t108: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t107 = offload_to_cpu(t107)  # offloaded_t107: \"cpu f32[128, 16]\"\n",
       "  del t107\n",
       "  [t110, t112] = nvFusion22(t108)\n",
       "    # t110 = prims.gt(t108, 0.0)  # t110: \"cuda:0 b8[128, 16]\"\n",
       "    # t112 = prims.where(t110, t108, 0.0)  # t112: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t110 = offload_to_cpu(t110)  # offloaded_t110: \"cpu b8[128, 16]\"\n",
       "  del t110\n",
       "  del t108\n",
       "  t113 = torch.nn.functional.linear(t112, t_fcs_23_weight, t_fcs_23_bias)  # t113: \"cuda:0 f32[128, 16]\"\n",
       "    # t113 = ltorch.linear(t112, t_fcs_23_weight, t_fcs_23_bias)  # t113: \"cuda:0 f32[128, 16]\"\n",
       "      # t113 = prims.linear(t112, t_fcs_23_weight, t_fcs_23_bias)  # t113: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t112 = offload_to_cpu(t112)  # offloaded_t112: \"cpu f32[128, 16]\"\n",
       "  del t112\n",
       "  [t115, t117] = nvFusion23(t113)\n",
       "    # t115 = prims.gt(t113, 0.0)  # t115: \"cuda:0 b8[128, 16]\"\n",
       "    # t117 = prims.where(t115, t113, 0.0)  # t117: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t115 = offload_to_cpu(t115)  # offloaded_t115: \"cpu b8[128, 16]\"\n",
       "  del t115\n",
       "  del t113\n",
       "  t118 = torch.nn.functional.linear(t117, t_fcs_24_weight, t_fcs_24_bias)  # t118: \"cuda:0 f32[128, 16]\"\n",
       "    # t118 = ltorch.linear(t117, t_fcs_24_weight, t_fcs_24_bias)  # t118: \"cuda:0 f32[128, 16]\"\n",
       "      # t118 = prims.linear(t117, t_fcs_24_weight, t_fcs_24_bias)  # t118: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t117 = offload_to_cpu(t117)  # offloaded_t117: \"cpu f32[128, 16]\"\n",
       "  del t117\n",
       "  [t120, t122] = nvFusion24(t118)\n",
       "    # t120 = prims.gt(t118, 0.0)  # t120: \"cuda:0 b8[128, 16]\"\n",
       "    # t122 = prims.where(t120, t118, 0.0)  # t122: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t120 = offload_to_cpu(t120)  # offloaded_t120: \"cpu b8[128, 16]\"\n",
       "  del t120\n",
       "  del t118\n",
       "  t123 = torch.nn.functional.linear(t122, t_fcs_25_weight, t_fcs_25_bias)  # t123: \"cuda:0 f32[128, 16]\"\n",
       "    # t123 = ltorch.linear(t122, t_fcs_25_weight, t_fcs_25_bias)  # t123: \"cuda:0 f32[128, 16]\"\n",
       "      # t123 = prims.linear(t122, t_fcs_25_weight, t_fcs_25_bias)  # t123: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t122 = offload_to_cpu(t122)  # offloaded_t122: \"cpu f32[128, 16]\"\n",
       "  del t122\n",
       "  [t125, t127] = nvFusion25(t123)\n",
       "    # t125 = prims.gt(t123, 0.0)  # t125: \"cuda:0 b8[128, 16]\"\n",
       "    # t127 = prims.where(t125, t123, 0.0)  # t127: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t125 = offload_to_cpu(t125)  # offloaded_t125: \"cpu b8[128, 16]\"\n",
       "  del t125\n",
       "  del t123\n",
       "  t128 = torch.nn.functional.linear(t127, t_fcs_26_weight, t_fcs_26_bias)  # t128: \"cuda:0 f32[128, 16]\"\n",
       "    # t128 = ltorch.linear(t127, t_fcs_26_weight, t_fcs_26_bias)  # t128: \"cuda:0 f32[128, 16]\"\n",
       "      # t128 = prims.linear(t127, t_fcs_26_weight, t_fcs_26_bias)  # t128: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t127 = offload_to_cpu(t127)  # offloaded_t127: \"cpu f32[128, 16]\"\n",
       "  del t127\n",
       "  [t130, t132] = nvFusion26(t128)\n",
       "    # t130 = prims.gt(t128, 0.0)  # t130: \"cuda:0 b8[128, 16]\"\n",
       "    # t132 = prims.where(t130, t128, 0.0)  # t132: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t130 = offload_to_cpu(t130)  # offloaded_t130: \"cpu b8[128, 16]\"\n",
       "  del t130\n",
       "  del t128\n",
       "  t133 = torch.nn.functional.linear(t132, t_fcs_27_weight, t_fcs_27_bias)  # t133: \"cuda:0 f32[128, 16]\"\n",
       "    # t133 = ltorch.linear(t132, t_fcs_27_weight, t_fcs_27_bias)  # t133: \"cuda:0 f32[128, 16]\"\n",
       "      # t133 = prims.linear(t132, t_fcs_27_weight, t_fcs_27_bias)  # t133: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t132 = offload_to_cpu(t132)  # offloaded_t132: \"cpu f32[128, 16]\"\n",
       "  del t132\n",
       "  [t135, t137] = nvFusion27(t133)\n",
       "    # t135 = prims.gt(t133, 0.0)  # t135: \"cuda:0 b8[128, 16]\"\n",
       "    # t137 = prims.where(t135, t133, 0.0)  # t137: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t135 = offload_to_cpu(t135)  # offloaded_t135: \"cpu b8[128, 16]\"\n",
       "  del t135\n",
       "  del t133\n",
       "  t138 = torch.nn.functional.linear(t137, t_fcs_28_weight, t_fcs_28_bias)  # t138: \"cuda:0 f32[128, 16]\"\n",
       "    # t138 = ltorch.linear(t137, t_fcs_28_weight, t_fcs_28_bias)  # t138: \"cuda:0 f32[128, 16]\"\n",
       "      # t138 = prims.linear(t137, t_fcs_28_weight, t_fcs_28_bias)  # t138: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t137 = offload_to_cpu(t137)  # offloaded_t137: \"cpu f32[128, 16]\"\n",
       "  del t137\n",
       "  [t140, t142] = nvFusion28(t138)\n",
       "    # t140 = prims.gt(t138, 0.0)  # t140: \"cuda:0 b8[128, 16]\"\n",
       "    # t142 = prims.where(t140, t138, 0.0)  # t142: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t140 = offload_to_cpu(t140)  # offloaded_t140: \"cpu b8[128, 16]\"\n",
       "  del t140\n",
       "  del t138\n",
       "  t143 = torch.nn.functional.linear(t142, t_fcs_29_weight, t_fcs_29_bias)  # t143: \"cuda:0 f32[128, 16]\"\n",
       "    # t143 = ltorch.linear(t142, t_fcs_29_weight, t_fcs_29_bias)  # t143: \"cuda:0 f32[128, 16]\"\n",
       "      # t143 = prims.linear(t142, t_fcs_29_weight, t_fcs_29_bias)  # t143: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t142 = offload_to_cpu(t142)  # offloaded_t142: \"cpu f32[128, 16]\"\n",
       "  del t142\n",
       "  [t145, t147] = nvFusion29(t143)\n",
       "    # t145 = prims.gt(t143, 0.0)  # t145: \"cuda:0 b8[128, 16]\"\n",
       "    # t147 = prims.where(t145, t143, 0.0)  # t147: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t145 = offload_to_cpu(t145)  # offloaded_t145: \"cpu b8[128, 16]\"\n",
       "  del t145\n",
       "  del t143\n",
       "  t148 = torch.nn.functional.linear(t147, t_fcs_30_weight, t_fcs_30_bias)  # t148: \"cuda:0 f32[128, 16]\"\n",
       "    # t148 = ltorch.linear(t147, t_fcs_30_weight, t_fcs_30_bias)  # t148: \"cuda:0 f32[128, 16]\"\n",
       "      # t148 = prims.linear(t147, t_fcs_30_weight, t_fcs_30_bias)  # t148: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t147 = offload_to_cpu(t147)  # offloaded_t147: \"cpu f32[128, 16]\"\n",
       "  del t147\n",
       "  [t150, t152] = nvFusion30(t148)\n",
       "    # t150 = prims.gt(t148, 0.0)  # t150: \"cuda:0 b8[128, 16]\"\n",
       "    # t152 = prims.where(t150, t148, 0.0)  # t152: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t150 = offload_to_cpu(t150)  # offloaded_t150: \"cpu b8[128, 16]\"\n",
       "  del t150\n",
       "  del t148\n",
       "  t153 = torch.nn.functional.linear(t152, t_fcs_31_weight, t_fcs_31_bias)  # t153: \"cuda:0 f32[128, 16]\"\n",
       "    # t153 = ltorch.linear(t152, t_fcs_31_weight, t_fcs_31_bias)  # t153: \"cuda:0 f32[128, 16]\"\n",
       "      # t153 = prims.linear(t152, t_fcs_31_weight, t_fcs_31_bias)  # t153: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t152 = offload_to_cpu(t152)  # offloaded_t152: \"cpu f32[128, 16]\"\n",
       "  del t152\n",
       "  [t155, t157] = nvFusion31(t153)\n",
       "    # t155 = prims.gt(t153, 0.0)  # t155: \"cuda:0 b8[128, 16]\"\n",
       "    # t157 = prims.where(t155, t153, 0.0)  # t157: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t155 = offload_to_cpu(t155)  # offloaded_t155: \"cpu b8[128, 16]\"\n",
       "  del t155\n",
       "  del t153\n",
       "  t158 = torch.nn.functional.linear(t157, t_fcs_32_weight, t_fcs_32_bias)  # t158: \"cuda:0 f32[128, 16]\"\n",
       "    # t158 = ltorch.linear(t157, t_fcs_32_weight, t_fcs_32_bias)  # t158: \"cuda:0 f32[128, 16]\"\n",
       "      # t158 = prims.linear(t157, t_fcs_32_weight, t_fcs_32_bias)  # t158: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t157 = offload_to_cpu(t157)  # offloaded_t157: \"cpu f32[128, 16]\"\n",
       "  del t157\n",
       "  [t160, t162] = nvFusion32(t158)\n",
       "    # t160 = prims.gt(t158, 0.0)  # t160: \"cuda:0 b8[128, 16]\"\n",
       "    # t162 = prims.where(t160, t158, 0.0)  # t162: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t160 = offload_to_cpu(t160)  # offloaded_t160: \"cpu b8[128, 16]\"\n",
       "  del t160\n",
       "  del t158\n",
       "  t163 = torch.nn.functional.linear(t162, t_fcs_33_weight, t_fcs_33_bias)  # t163: \"cuda:0 f32[128, 16]\"\n",
       "    # t163 = ltorch.linear(t162, t_fcs_33_weight, t_fcs_33_bias)  # t163: \"cuda:0 f32[128, 16]\"\n",
       "      # t163 = prims.linear(t162, t_fcs_33_weight, t_fcs_33_bias)  # t163: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t162 = offload_to_cpu(t162)  # offloaded_t162: \"cpu f32[128, 16]\"\n",
       "  del t162\n",
       "  [t165, t167] = nvFusion33(t163)\n",
       "    # t165 = prims.gt(t163, 0.0)  # t165: \"cuda:0 b8[128, 16]\"\n",
       "    # t167 = prims.where(t165, t163, 0.0)  # t167: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t165 = offload_to_cpu(t165)  # offloaded_t165: \"cpu b8[128, 16]\"\n",
       "  del t165\n",
       "  del t163\n",
       "  t168 = torch.nn.functional.linear(t167, t_fcs_34_weight, t_fcs_34_bias)  # t168: \"cuda:0 f32[128, 16]\"\n",
       "    # t168 = ltorch.linear(t167, t_fcs_34_weight, t_fcs_34_bias)  # t168: \"cuda:0 f32[128, 16]\"\n",
       "      # t168 = prims.linear(t167, t_fcs_34_weight, t_fcs_34_bias)  # t168: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t167 = offload_to_cpu(t167)  # offloaded_t167: \"cpu f32[128, 16]\"\n",
       "  del t167\n",
       "  [t170, t172] = nvFusion34(t168)\n",
       "    # t170 = prims.gt(t168, 0.0)  # t170: \"cuda:0 b8[128, 16]\"\n",
       "    # t172 = prims.where(t170, t168, 0.0)  # t172: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t170 = offload_to_cpu(t170)  # offloaded_t170: \"cpu b8[128, 16]\"\n",
       "  del t170\n",
       "  del t168\n",
       "  t173 = torch.nn.functional.linear(t172, t_fcs_35_weight, t_fcs_35_bias)  # t173: \"cuda:0 f32[128, 16]\"\n",
       "    # t173 = ltorch.linear(t172, t_fcs_35_weight, t_fcs_35_bias)  # t173: \"cuda:0 f32[128, 16]\"\n",
       "      # t173 = prims.linear(t172, t_fcs_35_weight, t_fcs_35_bias)  # t173: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t172 = offload_to_cpu(t172)  # offloaded_t172: \"cpu f32[128, 16]\"\n",
       "  del t172\n",
       "  [t175, t177] = nvFusion35(t173)\n",
       "    # t175 = prims.gt(t173, 0.0)  # t175: \"cuda:0 b8[128, 16]\"\n",
       "    # t177 = prims.where(t175, t173, 0.0)  # t177: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t175 = offload_to_cpu(t175)  # offloaded_t175: \"cpu b8[128, 16]\"\n",
       "  del t175\n",
       "  del t173\n",
       "  t178 = torch.nn.functional.linear(t177, t_fcs_36_weight, t_fcs_36_bias)  # t178: \"cuda:0 f32[128, 16]\"\n",
       "    # t178 = ltorch.linear(t177, t_fcs_36_weight, t_fcs_36_bias)  # t178: \"cuda:0 f32[128, 16]\"\n",
       "      # t178 = prims.linear(t177, t_fcs_36_weight, t_fcs_36_bias)  # t178: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t177 = offload_to_cpu(t177)  # offloaded_t177: \"cpu f32[128, 16]\"\n",
       "  del t177\n",
       "  [t180, t182] = nvFusion36(t178)\n",
       "    # t180 = prims.gt(t178, 0.0)  # t180: \"cuda:0 b8[128, 16]\"\n",
       "    # t182 = prims.where(t180, t178, 0.0)  # t182: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t180 = offload_to_cpu(t180)  # offloaded_t180: \"cpu b8[128, 16]\"\n",
       "  del t180\n",
       "  del t178\n",
       "  t183 = torch.nn.functional.linear(t182, t_fcs_37_weight, t_fcs_37_bias)  # t183: \"cuda:0 f32[128, 16]\"\n",
       "    # t183 = ltorch.linear(t182, t_fcs_37_weight, t_fcs_37_bias)  # t183: \"cuda:0 f32[128, 16]\"\n",
       "      # t183 = prims.linear(t182, t_fcs_37_weight, t_fcs_37_bias)  # t183: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t182 = offload_to_cpu(t182)  # offloaded_t182: \"cpu f32[128, 16]\"\n",
       "  del t182\n",
       "  [t185, t187] = nvFusion37(t183)\n",
       "    # t185 = prims.gt(t183, 0.0)  # t185: \"cuda:0 b8[128, 16]\"\n",
       "    # t187 = prims.where(t185, t183, 0.0)  # t187: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t185 = offload_to_cpu(t185)  # offloaded_t185: \"cpu b8[128, 16]\"\n",
       "  del t185\n",
       "  del t183\n",
       "  t188 = torch.nn.functional.linear(t187, t_fcs_38_weight, t_fcs_38_bias)  # t188: \"cuda:0 f32[128, 16]\"\n",
       "    # t188 = ltorch.linear(t187, t_fcs_38_weight, t_fcs_38_bias)  # t188: \"cuda:0 f32[128, 16]\"\n",
       "      # t188 = prims.linear(t187, t_fcs_38_weight, t_fcs_38_bias)  # t188: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t187 = offload_to_cpu(t187)  # offloaded_t187: \"cpu f32[128, 16]\"\n",
       "  del t187\n",
       "  [t190, t192] = nvFusion38(t188)\n",
       "    # t190 = prims.gt(t188, 0.0)  # t190: \"cuda:0 b8[128, 16]\"\n",
       "    # t192 = prims.where(t190, t188, 0.0)  # t192: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t190 = offload_to_cpu(t190)  # offloaded_t190: \"cpu b8[128, 16]\"\n",
       "  del t190\n",
       "  del t188\n",
       "  t193 = torch.nn.functional.linear(t192, t_fcs_39_weight, t_fcs_39_bias)  # t193: \"cuda:0 f32[128, 16]\"\n",
       "    # t193 = ltorch.linear(t192, t_fcs_39_weight, t_fcs_39_bias)  # t193: \"cuda:0 f32[128, 16]\"\n",
       "      # t193 = prims.linear(t192, t_fcs_39_weight, t_fcs_39_bias)  # t193: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t192 = offload_to_cpu(t192)  # offloaded_t192: \"cpu f32[128, 16]\"\n",
       "  del t192\n",
       "  [t195, t197] = nvFusion39(t193)\n",
       "    # t195 = prims.gt(t193, 0.0)  # t195: \"cuda:0 b8[128, 16]\"\n",
       "    # t197 = prims.where(t195, t193, 0.0)  # t197: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t195 = offload_to_cpu(t195)  # offloaded_t195: \"cpu b8[128, 16]\"\n",
       "  del t195\n",
       "  del t193\n",
       "  t198 = torch.nn.functional.linear(t197, t_fcs_40_weight, t_fcs_40_bias)  # t198: \"cuda:0 f32[128, 16]\"\n",
       "    # t198 = ltorch.linear(t197, t_fcs_40_weight, t_fcs_40_bias)  # t198: \"cuda:0 f32[128, 16]\"\n",
       "      # t198 = prims.linear(t197, t_fcs_40_weight, t_fcs_40_bias)  # t198: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t197 = offload_to_cpu(t197)  # offloaded_t197: \"cpu f32[128, 16]\"\n",
       "  del t197\n",
       "  [t200, t202] = nvFusion40(t198)\n",
       "    # t200 = prims.gt(t198, 0.0)  # t200: \"cuda:0 b8[128, 16]\"\n",
       "    # t202 = prims.where(t200, t198, 0.0)  # t202: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t200 = offload_to_cpu(t200)  # offloaded_t200: \"cpu b8[128, 16]\"\n",
       "  del t200\n",
       "  del t198\n",
       "  t203 = torch.nn.functional.linear(t202, t_fcs_41_weight, t_fcs_41_bias)  # t203: \"cuda:0 f32[128, 16]\"\n",
       "    # t203 = ltorch.linear(t202, t_fcs_41_weight, t_fcs_41_bias)  # t203: \"cuda:0 f32[128, 16]\"\n",
       "      # t203 = prims.linear(t202, t_fcs_41_weight, t_fcs_41_bias)  # t203: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t202 = offload_to_cpu(t202)  # offloaded_t202: \"cpu f32[128, 16]\"\n",
       "  del t202\n",
       "  [t205, t207] = nvFusion41(t203)\n",
       "    # t205 = prims.gt(t203, 0.0)  # t205: \"cuda:0 b8[128, 16]\"\n",
       "    # t207 = prims.where(t205, t203, 0.0)  # t207: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t205 = offload_to_cpu(t205)  # offloaded_t205: \"cpu b8[128, 16]\"\n",
       "  del t205\n",
       "  del t203\n",
       "  t208 = torch.nn.functional.linear(t207, t_fcs_42_weight, t_fcs_42_bias)  # t208: \"cuda:0 f32[128, 16]\"\n",
       "    # t208 = ltorch.linear(t207, t_fcs_42_weight, t_fcs_42_bias)  # t208: \"cuda:0 f32[128, 16]\"\n",
       "      # t208 = prims.linear(t207, t_fcs_42_weight, t_fcs_42_bias)  # t208: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t207 = offload_to_cpu(t207)  # offloaded_t207: \"cpu f32[128, 16]\"\n",
       "  del t207\n",
       "  [t210, t212] = nvFusion42(t208)\n",
       "    # t210 = prims.gt(t208, 0.0)  # t210: \"cuda:0 b8[128, 16]\"\n",
       "    # t212 = prims.where(t210, t208, 0.0)  # t212: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t210 = offload_to_cpu(t210)  # offloaded_t210: \"cpu b8[128, 16]\"\n",
       "  del t210\n",
       "  del t208\n",
       "  t213 = torch.nn.functional.linear(t212, t_fcs_43_weight, t_fcs_43_bias)  # t213: \"cuda:0 f32[128, 16]\"\n",
       "    # t213 = ltorch.linear(t212, t_fcs_43_weight, t_fcs_43_bias)  # t213: \"cuda:0 f32[128, 16]\"\n",
       "      # t213 = prims.linear(t212, t_fcs_43_weight, t_fcs_43_bias)  # t213: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t212 = offload_to_cpu(t212)  # offloaded_t212: \"cpu f32[128, 16]\"\n",
       "  del t212\n",
       "  [t215, t217] = nvFusion43(t213)\n",
       "    # t215 = prims.gt(t213, 0.0)  # t215: \"cuda:0 b8[128, 16]\"\n",
       "    # t217 = prims.where(t215, t213, 0.0)  # t217: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t215 = offload_to_cpu(t215)  # offloaded_t215: \"cpu b8[128, 16]\"\n",
       "  del t215\n",
       "  del t213\n",
       "  t218 = torch.nn.functional.linear(t217, t_fcs_44_weight, t_fcs_44_bias)  # t218: \"cuda:0 f32[128, 16]\"\n",
       "    # t218 = ltorch.linear(t217, t_fcs_44_weight, t_fcs_44_bias)  # t218: \"cuda:0 f32[128, 16]\"\n",
       "      # t218 = prims.linear(t217, t_fcs_44_weight, t_fcs_44_bias)  # t218: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t217 = offload_to_cpu(t217)  # offloaded_t217: \"cpu f32[128, 16]\"\n",
       "  del t217\n",
       "  [t220, t222] = nvFusion44(t218)\n",
       "    # t220 = prims.gt(t218, 0.0)  # t220: \"cuda:0 b8[128, 16]\"\n",
       "    # t222 = prims.where(t220, t218, 0.0)  # t222: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t220 = offload_to_cpu(t220)  # offloaded_t220: \"cpu b8[128, 16]\"\n",
       "  del t220\n",
       "  del t218\n",
       "  t223 = torch.nn.functional.linear(t222, t_fcs_45_weight, t_fcs_45_bias)  # t223: \"cuda:0 f32[128, 16]\"\n",
       "    # t223 = ltorch.linear(t222, t_fcs_45_weight, t_fcs_45_bias)  # t223: \"cuda:0 f32[128, 16]\"\n",
       "      # t223 = prims.linear(t222, t_fcs_45_weight, t_fcs_45_bias)  # t223: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t222 = offload_to_cpu(t222)  # offloaded_t222: \"cpu f32[128, 16]\"\n",
       "  del t222\n",
       "  [t225, t227] = nvFusion45(t223)\n",
       "    # t225 = prims.gt(t223, 0.0)  # t225: \"cuda:0 b8[128, 16]\"\n",
       "    # t227 = prims.where(t225, t223, 0.0)  # t227: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t225 = offload_to_cpu(t225)  # offloaded_t225: \"cpu b8[128, 16]\"\n",
       "  del t225\n",
       "  del t223\n",
       "  t228 = torch.nn.functional.linear(t227, t_fcs_46_weight, t_fcs_46_bias)  # t228: \"cuda:0 f32[128, 16]\"\n",
       "    # t228 = ltorch.linear(t227, t_fcs_46_weight, t_fcs_46_bias)  # t228: \"cuda:0 f32[128, 16]\"\n",
       "      # t228 = prims.linear(t227, t_fcs_46_weight, t_fcs_46_bias)  # t228: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t227 = offload_to_cpu(t227)  # offloaded_t227: \"cpu f32[128, 16]\"\n",
       "  del t227\n",
       "  [t230, t232] = nvFusion46(t228)\n",
       "    # t230 = prims.gt(t228, 0.0)  # t230: \"cuda:0 b8[128, 16]\"\n",
       "    # t232 = prims.where(t230, t228, 0.0)  # t232: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t230 = offload_to_cpu(t230)  # offloaded_t230: \"cpu b8[128, 16]\"\n",
       "  del t230\n",
       "  del t228\n",
       "  t233 = torch.nn.functional.linear(t232, t_fcs_47_weight, t_fcs_47_bias)  # t233: \"cuda:0 f32[128, 16]\"\n",
       "    # t233 = ltorch.linear(t232, t_fcs_47_weight, t_fcs_47_bias)  # t233: \"cuda:0 f32[128, 16]\"\n",
       "      # t233 = prims.linear(t232, t_fcs_47_weight, t_fcs_47_bias)  # t233: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t232 = offload_to_cpu(t232)  # offloaded_t232: \"cpu f32[128, 16]\"\n",
       "  del t232\n",
       "  [t235, t237] = nvFusion47(t233)\n",
       "    # t235 = prims.gt(t233, 0.0)  # t235: \"cuda:0 b8[128, 16]\"\n",
       "    # t237 = prims.where(t235, t233, 0.0)  # t237: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t235 = offload_to_cpu(t235)  # offloaded_t235: \"cpu b8[128, 16]\"\n",
       "  del t235\n",
       "  del t233\n",
       "  t238 = torch.nn.functional.linear(t237, t_fcs_48_weight, t_fcs_48_bias)  # t238: \"cuda:0 f32[128, 16]\"\n",
       "    # t238 = ltorch.linear(t237, t_fcs_48_weight, t_fcs_48_bias)  # t238: \"cuda:0 f32[128, 16]\"\n",
       "      # t238 = prims.linear(t237, t_fcs_48_weight, t_fcs_48_bias)  # t238: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t237 = offload_to_cpu(t237)  # offloaded_t237: \"cpu f32[128, 16]\"\n",
       "  del t237\n",
       "  [t240, t242] = nvFusion48(t238)\n",
       "    # t240 = prims.gt(t238, 0.0)  # t240: \"cuda:0 b8[128, 16]\"\n",
       "    # t242 = prims.where(t240, t238, 0.0)  # t242: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t240 = offload_to_cpu(t240)  # offloaded_t240: \"cpu b8[128, 16]\"\n",
       "  del t240\n",
       "  del t238\n",
       "  t243 = torch.nn.functional.linear(t242, t_fcs_49_weight, t_fcs_49_bias)  # t243: \"cuda:0 f32[128, 16]\"\n",
       "    # t243 = ltorch.linear(t242, t_fcs_49_weight, t_fcs_49_bias)  # t243: \"cuda:0 f32[128, 16]\"\n",
       "      # t243 = prims.linear(t242, t_fcs_49_weight, t_fcs_49_bias)  # t243: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t242 = offload_to_cpu(t242)  # offloaded_t242: \"cpu f32[128, 16]\"\n",
       "  del t242\n",
       "  [t245, t247] = nvFusion49(t243)\n",
       "    # t245 = prims.gt(t243, 0.0)  # t245: \"cuda:0 b8[128, 16]\"\n",
       "    # t247 = prims.where(t245, t243, 0.0)  # t247: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t245 = offload_to_cpu(t245)  # offloaded_t245: \"cpu b8[128, 16]\"\n",
       "  del t245\n",
       "  del t243\n",
       "  t248 = torch.nn.functional.linear(t247, t_fcs_50_weight, t_fcs_50_bias)  # t248: \"cuda:0 f32[128, 16]\"\n",
       "    # t248 = ltorch.linear(t247, t_fcs_50_weight, t_fcs_50_bias)  # t248: \"cuda:0 f32[128, 16]\"\n",
       "      # t248 = prims.linear(t247, t_fcs_50_weight, t_fcs_50_bias)  # t248: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t247 = offload_to_cpu(t247)  # offloaded_t247: \"cpu f32[128, 16]\"\n",
       "  del t247\n",
       "  [t250, t252] = nvFusion50(t248)\n",
       "    # t250 = prims.gt(t248, 0.0)  # t250: \"cuda:0 b8[128, 16]\"\n",
       "    # t252 = prims.where(t250, t248, 0.0)  # t252: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t250 = offload_to_cpu(t250)  # offloaded_t250: \"cpu b8[128, 16]\"\n",
       "  del t250\n",
       "  del t248\n",
       "  t253 = torch.nn.functional.linear(t252, t_fcs_51_weight, t_fcs_51_bias)  # t253: \"cuda:0 f32[128, 16]\"\n",
       "    # t253 = ltorch.linear(t252, t_fcs_51_weight, t_fcs_51_bias)  # t253: \"cuda:0 f32[128, 16]\"\n",
       "      # t253 = prims.linear(t252, t_fcs_51_weight, t_fcs_51_bias)  # t253: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t252 = offload_to_cpu(t252)  # offloaded_t252: \"cpu f32[128, 16]\"\n",
       "  del t252\n",
       "  [t255, t257] = nvFusion51(t253)\n",
       "    # t255 = prims.gt(t253, 0.0)  # t255: \"cuda:0 b8[128, 16]\"\n",
       "    # t257 = prims.where(t255, t253, 0.0)  # t257: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t255 = offload_to_cpu(t255)  # offloaded_t255: \"cpu b8[128, 16]\"\n",
       "  del t255\n",
       "  del t253\n",
       "  t258 = torch.nn.functional.linear(t257, t_fcs_52_weight, t_fcs_52_bias)  # t258: \"cuda:0 f32[128, 16]\"\n",
       "    # t258 = ltorch.linear(t257, t_fcs_52_weight, t_fcs_52_bias)  # t258: \"cuda:0 f32[128, 16]\"\n",
       "      # t258 = prims.linear(t257, t_fcs_52_weight, t_fcs_52_bias)  # t258: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t257 = offload_to_cpu(t257)  # offloaded_t257: \"cpu f32[128, 16]\"\n",
       "  del t257\n",
       "  [t260, t262] = nvFusion52(t258)\n",
       "    # t260 = prims.gt(t258, 0.0)  # t260: \"cuda:0 b8[128, 16]\"\n",
       "    # t262 = prims.where(t260, t258, 0.0)  # t262: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t260 = offload_to_cpu(t260)  # offloaded_t260: \"cpu b8[128, 16]\"\n",
       "  del t260\n",
       "  del t258\n",
       "  t263 = torch.nn.functional.linear(t262, t_fcs_53_weight, t_fcs_53_bias)  # t263: \"cuda:0 f32[128, 16]\"\n",
       "    # t263 = ltorch.linear(t262, t_fcs_53_weight, t_fcs_53_bias)  # t263: \"cuda:0 f32[128, 16]\"\n",
       "      # t263 = prims.linear(t262, t_fcs_53_weight, t_fcs_53_bias)  # t263: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t262 = offload_to_cpu(t262)  # offloaded_t262: \"cpu f32[128, 16]\"\n",
       "  del t262\n",
       "  [t265, t267] = nvFusion53(t263)\n",
       "    # t265 = prims.gt(t263, 0.0)  # t265: \"cuda:0 b8[128, 16]\"\n",
       "    # t267 = prims.where(t265, t263, 0.0)  # t267: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t265 = offload_to_cpu(t265)  # offloaded_t265: \"cpu b8[128, 16]\"\n",
       "  del t265\n",
       "  del t263\n",
       "  t268 = torch.nn.functional.linear(t267, t_fcs_54_weight, t_fcs_54_bias)  # t268: \"cuda:0 f32[128, 16]\"\n",
       "    # t268 = ltorch.linear(t267, t_fcs_54_weight, t_fcs_54_bias)  # t268: \"cuda:0 f32[128, 16]\"\n",
       "      # t268 = prims.linear(t267, t_fcs_54_weight, t_fcs_54_bias)  # t268: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t267 = offload_to_cpu(t267)  # offloaded_t267: \"cpu f32[128, 16]\"\n",
       "  del t267\n",
       "  [t270, t272] = nvFusion54(t268)\n",
       "    # t270 = prims.gt(t268, 0.0)  # t270: \"cuda:0 b8[128, 16]\"\n",
       "    # t272 = prims.where(t270, t268, 0.0)  # t272: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t270 = offload_to_cpu(t270)  # offloaded_t270: \"cpu b8[128, 16]\"\n",
       "  del t270\n",
       "  del t268\n",
       "  t273 = torch.nn.functional.linear(t272, t_fcs_55_weight, t_fcs_55_bias)  # t273: \"cuda:0 f32[128, 16]\"\n",
       "    # t273 = ltorch.linear(t272, t_fcs_55_weight, t_fcs_55_bias)  # t273: \"cuda:0 f32[128, 16]\"\n",
       "      # t273 = prims.linear(t272, t_fcs_55_weight, t_fcs_55_bias)  # t273: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t272 = offload_to_cpu(t272)  # offloaded_t272: \"cpu f32[128, 16]\"\n",
       "  del t272\n",
       "  [t275, t277] = nvFusion55(t273)\n",
       "    # t275 = prims.gt(t273, 0.0)  # t275: \"cuda:0 b8[128, 16]\"\n",
       "    # t277 = prims.where(t275, t273, 0.0)  # t277: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t275 = offload_to_cpu(t275)  # offloaded_t275: \"cpu b8[128, 16]\"\n",
       "  del t275\n",
       "  del t273\n",
       "  t278 = torch.nn.functional.linear(t277, t_fcs_56_weight, t_fcs_56_bias)  # t278: \"cuda:0 f32[128, 16]\"\n",
       "    # t278 = ltorch.linear(t277, t_fcs_56_weight, t_fcs_56_bias)  # t278: \"cuda:0 f32[128, 16]\"\n",
       "      # t278 = prims.linear(t277, t_fcs_56_weight, t_fcs_56_bias)  # t278: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t277 = offload_to_cpu(t277)  # offloaded_t277: \"cpu f32[128, 16]\"\n",
       "  del t277\n",
       "  [t280, t282] = nvFusion56(t278)\n",
       "    # t280 = prims.gt(t278, 0.0)  # t280: \"cuda:0 b8[128, 16]\"\n",
       "    # t282 = prims.where(t280, t278, 0.0)  # t282: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t280 = offload_to_cpu(t280)  # offloaded_t280: \"cpu b8[128, 16]\"\n",
       "  del t280\n",
       "  del t278\n",
       "  t283 = torch.nn.functional.linear(t282, t_fcs_57_weight, t_fcs_57_bias)  # t283: \"cuda:0 f32[128, 16]\"\n",
       "    # t283 = ltorch.linear(t282, t_fcs_57_weight, t_fcs_57_bias)  # t283: \"cuda:0 f32[128, 16]\"\n",
       "      # t283 = prims.linear(t282, t_fcs_57_weight, t_fcs_57_bias)  # t283: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t282 = offload_to_cpu(t282)  # offloaded_t282: \"cpu f32[128, 16]\"\n",
       "  del t282\n",
       "  [t285, t287] = nvFusion57(t283)\n",
       "    # t285 = prims.gt(t283, 0.0)  # t285: \"cuda:0 b8[128, 16]\"\n",
       "    # t287 = prims.where(t285, t283, 0.0)  # t287: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t285 = offload_to_cpu(t285)  # offloaded_t285: \"cpu b8[128, 16]\"\n",
       "  del t285\n",
       "  del t283\n",
       "  t288 = torch.nn.functional.linear(t287, t_fcs_58_weight, t_fcs_58_bias)  # t288: \"cuda:0 f32[128, 16]\"\n",
       "    # t288 = ltorch.linear(t287, t_fcs_58_weight, t_fcs_58_bias)  # t288: \"cuda:0 f32[128, 16]\"\n",
       "      # t288 = prims.linear(t287, t_fcs_58_weight, t_fcs_58_bias)  # t288: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t287 = offload_to_cpu(t287)  # offloaded_t287: \"cpu f32[128, 16]\"\n",
       "  del t287\n",
       "  [t290, t292] = nvFusion58(t288)\n",
       "    # t290 = prims.gt(t288, 0.0)  # t290: \"cuda:0 b8[128, 16]\"\n",
       "    # t292 = prims.where(t290, t288, 0.0)  # t292: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t290 = offload_to_cpu(t290)  # offloaded_t290: \"cpu b8[128, 16]\"\n",
       "  del t290\n",
       "  del t288\n",
       "  t293 = torch.nn.functional.linear(t292, t_fcs_59_weight, t_fcs_59_bias)  # t293: \"cuda:0 f32[128, 16]\"\n",
       "    # t293 = ltorch.linear(t292, t_fcs_59_weight, t_fcs_59_bias)  # t293: \"cuda:0 f32[128, 16]\"\n",
       "      # t293 = prims.linear(t292, t_fcs_59_weight, t_fcs_59_bias)  # t293: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t292 = offload_to_cpu(t292)  # offloaded_t292: \"cpu f32[128, 16]\"\n",
       "  del t292\n",
       "  [t295, t297] = nvFusion59(t293)\n",
       "    # t295 = prims.gt(t293, 0.0)  # t295: \"cuda:0 b8[128, 16]\"\n",
       "    # t297 = prims.where(t295, t293, 0.0)  # t297: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t295 = offload_to_cpu(t295)  # offloaded_t295: \"cpu b8[128, 16]\"\n",
       "  del t295\n",
       "  del t293\n",
       "  t298 = torch.nn.functional.linear(t297, t_fcs_60_weight, t_fcs_60_bias)  # t298: \"cuda:0 f32[128, 16]\"\n",
       "    # t298 = ltorch.linear(t297, t_fcs_60_weight, t_fcs_60_bias)  # t298: \"cuda:0 f32[128, 16]\"\n",
       "      # t298 = prims.linear(t297, t_fcs_60_weight, t_fcs_60_bias)  # t298: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t297 = offload_to_cpu(t297)  # offloaded_t297: \"cpu f32[128, 16]\"\n",
       "  del t297\n",
       "  [t300, t302] = nvFusion60(t298)\n",
       "    # t300 = prims.gt(t298, 0.0)  # t300: \"cuda:0 b8[128, 16]\"\n",
       "    # t302 = prims.where(t300, t298, 0.0)  # t302: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t300 = offload_to_cpu(t300)  # offloaded_t300: \"cpu b8[128, 16]\"\n",
       "  del t300\n",
       "  del t298\n",
       "  t303 = torch.nn.functional.linear(t302, t_fcs_61_weight, t_fcs_61_bias)  # t303: \"cuda:0 f32[128, 16]\"\n",
       "    # t303 = ltorch.linear(t302, t_fcs_61_weight, t_fcs_61_bias)  # t303: \"cuda:0 f32[128, 16]\"\n",
       "      # t303 = prims.linear(t302, t_fcs_61_weight, t_fcs_61_bias)  # t303: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t302 = offload_to_cpu(t302)  # offloaded_t302: \"cpu f32[128, 16]\"\n",
       "  del t302\n",
       "  [t305, t307] = nvFusion61(t303)\n",
       "    # t305 = prims.gt(t303, 0.0)  # t305: \"cuda:0 b8[128, 16]\"\n",
       "    # t307 = prims.where(t305, t303, 0.0)  # t307: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t305 = offload_to_cpu(t305)  # offloaded_t305: \"cpu b8[128, 16]\"\n",
       "  del t305\n",
       "  del t303\n",
       "  t308 = torch.nn.functional.linear(t307, t_fcs_62_weight, t_fcs_62_bias)  # t308: \"cuda:0 f32[128, 16]\"\n",
       "    # t308 = ltorch.linear(t307, t_fcs_62_weight, t_fcs_62_bias)  # t308: \"cuda:0 f32[128, 16]\"\n",
       "      # t308 = prims.linear(t307, t_fcs_62_weight, t_fcs_62_bias)  # t308: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t307 = offload_to_cpu(t307)  # offloaded_t307: \"cpu f32[128, 16]\"\n",
       "  del t307\n",
       "  [t310, t312] = nvFusion62(t308)\n",
       "    # t310 = prims.gt(t308, 0.0)  # t310: \"cuda:0 b8[128, 16]\"\n",
       "    # t312 = prims.where(t310, t308, 0.0)  # t312: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t310 = offload_to_cpu(t310)  # offloaded_t310: \"cpu b8[128, 16]\"\n",
       "  del t310\n",
       "  del t308\n",
       "  t313 = torch.nn.functional.linear(t312, t_fcs_63_weight, t_fcs_63_bias)  # t313: \"cuda:0 f32[128, 16]\"\n",
       "    # t313 = ltorch.linear(t312, t_fcs_63_weight, t_fcs_63_bias)  # t313: \"cuda:0 f32[128, 16]\"\n",
       "      # t313 = prims.linear(t312, t_fcs_63_weight, t_fcs_63_bias)  # t313: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t312 = offload_to_cpu(t312)  # offloaded_t312: \"cpu f32[128, 16]\"\n",
       "  del t312\n",
       "  [t315, t317] = nvFusion63(t313)\n",
       "    # t315 = prims.gt(t313, 0.0)  # t315: \"cuda:0 b8[128, 16]\"\n",
       "    # t317 = prims.where(t315, t313, 0.0)  # t317: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t315 = offload_to_cpu(t315)  # offloaded_t315: \"cpu b8[128, 16]\"\n",
       "  del t315\n",
       "  del t313\n",
       "  t318 = torch.nn.functional.linear(t317, t_fcs_64_weight, t_fcs_64_bias)  # t318: \"cuda:0 f32[128, 16]\"\n",
       "    # t318 = ltorch.linear(t317, t_fcs_64_weight, t_fcs_64_bias)  # t318: \"cuda:0 f32[128, 16]\"\n",
       "      # t318 = prims.linear(t317, t_fcs_64_weight, t_fcs_64_bias)  # t318: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t317 = offload_to_cpu(t317)  # offloaded_t317: \"cpu f32[128, 16]\"\n",
       "  del t317\n",
       "  [t320, t322] = nvFusion64(t318)\n",
       "    # t320 = prims.gt(t318, 0.0)  # t320: \"cuda:0 b8[128, 16]\"\n",
       "    # t322 = prims.where(t320, t318, 0.0)  # t322: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t320 = offload_to_cpu(t320)  # offloaded_t320: \"cpu b8[128, 16]\"\n",
       "  del t320\n",
       "  del t318\n",
       "  t323 = torch.nn.functional.linear(t322, t_fcs_65_weight, t_fcs_65_bias)  # t323: \"cuda:0 f32[128, 16]\"\n",
       "    # t323 = ltorch.linear(t322, t_fcs_65_weight, t_fcs_65_bias)  # t323: \"cuda:0 f32[128, 16]\"\n",
       "      # t323 = prims.linear(t322, t_fcs_65_weight, t_fcs_65_bias)  # t323: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t322 = offload_to_cpu(t322)  # offloaded_t322: \"cpu f32[128, 16]\"\n",
       "  del t322\n",
       "  [t325, t327] = nvFusion65(t323)\n",
       "    # t325 = prims.gt(t323, 0.0)  # t325: \"cuda:0 b8[128, 16]\"\n",
       "    # t327 = prims.where(t325, t323, 0.0)  # t327: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t325 = offload_to_cpu(t325)  # offloaded_t325: \"cpu b8[128, 16]\"\n",
       "  del t325\n",
       "  del t323\n",
       "  t328 = torch.nn.functional.linear(t327, t_fcs_66_weight, t_fcs_66_bias)  # t328: \"cuda:0 f32[128, 16]\"\n",
       "    # t328 = ltorch.linear(t327, t_fcs_66_weight, t_fcs_66_bias)  # t328: \"cuda:0 f32[128, 16]\"\n",
       "      # t328 = prims.linear(t327, t_fcs_66_weight, t_fcs_66_bias)  # t328: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t327 = offload_to_cpu(t327)  # offloaded_t327: \"cpu f32[128, 16]\"\n",
       "  del t327\n",
       "  [t330, t332] = nvFusion66(t328)\n",
       "    # t330 = prims.gt(t328, 0.0)  # t330: \"cuda:0 b8[128, 16]\"\n",
       "    # t332 = prims.where(t330, t328, 0.0)  # t332: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t330 = offload_to_cpu(t330)  # offloaded_t330: \"cpu b8[128, 16]\"\n",
       "  del t330\n",
       "  del t328\n",
       "  t333 = torch.nn.functional.linear(t332, t_fcs_67_weight, t_fcs_67_bias)  # t333: \"cuda:0 f32[128, 16]\"\n",
       "    # t333 = ltorch.linear(t332, t_fcs_67_weight, t_fcs_67_bias)  # t333: \"cuda:0 f32[128, 16]\"\n",
       "      # t333 = prims.linear(t332, t_fcs_67_weight, t_fcs_67_bias)  # t333: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t332 = offload_to_cpu(t332)  # offloaded_t332: \"cpu f32[128, 16]\"\n",
       "  del t332\n",
       "  [t335, t337] = nvFusion67(t333)\n",
       "    # t335 = prims.gt(t333, 0.0)  # t335: \"cuda:0 b8[128, 16]\"\n",
       "    # t337 = prims.where(t335, t333, 0.0)  # t337: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t335 = offload_to_cpu(t335)  # offloaded_t335: \"cpu b8[128, 16]\"\n",
       "  del t335\n",
       "  del t333\n",
       "  t338 = torch.nn.functional.linear(t337, t_fcs_68_weight, t_fcs_68_bias)  # t338: \"cuda:0 f32[128, 16]\"\n",
       "    # t338 = ltorch.linear(t337, t_fcs_68_weight, t_fcs_68_bias)  # t338: \"cuda:0 f32[128, 16]\"\n",
       "      # t338 = prims.linear(t337, t_fcs_68_weight, t_fcs_68_bias)  # t338: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t337 = offload_to_cpu(t337)  # offloaded_t337: \"cpu f32[128, 16]\"\n",
       "  del t337\n",
       "  [t340, t342] = nvFusion68(t338)\n",
       "    # t340 = prims.gt(t338, 0.0)  # t340: \"cuda:0 b8[128, 16]\"\n",
       "    # t342 = prims.where(t340, t338, 0.0)  # t342: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t340 = offload_to_cpu(t340)  # offloaded_t340: \"cpu b8[128, 16]\"\n",
       "  del t340\n",
       "  del t338\n",
       "  t343 = torch.nn.functional.linear(t342, t_fcs_69_weight, t_fcs_69_bias)  # t343: \"cuda:0 f32[128, 16]\"\n",
       "    # t343 = ltorch.linear(t342, t_fcs_69_weight, t_fcs_69_bias)  # t343: \"cuda:0 f32[128, 16]\"\n",
       "      # t343 = prims.linear(t342, t_fcs_69_weight, t_fcs_69_bias)  # t343: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t342 = offload_to_cpu(t342)  # offloaded_t342: \"cpu f32[128, 16]\"\n",
       "  del t342\n",
       "  [t345, t347] = nvFusion69(t343)\n",
       "    # t345 = prims.gt(t343, 0.0)  # t345: \"cuda:0 b8[128, 16]\"\n",
       "    # t347 = prims.where(t345, t343, 0.0)  # t347: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t345 = offload_to_cpu(t345)  # offloaded_t345: \"cpu b8[128, 16]\"\n",
       "  del t345\n",
       "  del t343\n",
       "  t348 = torch.nn.functional.linear(t347, t_fcs_70_weight, t_fcs_70_bias)  # t348: \"cuda:0 f32[128, 16]\"\n",
       "    # t348 = ltorch.linear(t347, t_fcs_70_weight, t_fcs_70_bias)  # t348: \"cuda:0 f32[128, 16]\"\n",
       "      # t348 = prims.linear(t347, t_fcs_70_weight, t_fcs_70_bias)  # t348: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t347 = offload_to_cpu(t347)  # offloaded_t347: \"cpu f32[128, 16]\"\n",
       "  del t347\n",
       "  [t350, t352] = nvFusion70(t348)\n",
       "    # t350 = prims.gt(t348, 0.0)  # t350: \"cuda:0 b8[128, 16]\"\n",
       "    # t352 = prims.where(t350, t348, 0.0)  # t352: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t350 = offload_to_cpu(t350)  # offloaded_t350: \"cpu b8[128, 16]\"\n",
       "  del t350\n",
       "  del t348\n",
       "  t353 = torch.nn.functional.linear(t352, t_fcs_71_weight, t_fcs_71_bias)  # t353: \"cuda:0 f32[128, 16]\"\n",
       "    # t353 = ltorch.linear(t352, t_fcs_71_weight, t_fcs_71_bias)  # t353: \"cuda:0 f32[128, 16]\"\n",
       "      # t353 = prims.linear(t352, t_fcs_71_weight, t_fcs_71_bias)  # t353: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t352 = offload_to_cpu(t352)  # offloaded_t352: \"cpu f32[128, 16]\"\n",
       "  del t352\n",
       "  [t355, t357] = nvFusion71(t353)\n",
       "    # t355 = prims.gt(t353, 0.0)  # t355: \"cuda:0 b8[128, 16]\"\n",
       "    # t357 = prims.where(t355, t353, 0.0)  # t357: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t355 = offload_to_cpu(t355)  # offloaded_t355: \"cpu b8[128, 16]\"\n",
       "  del t355\n",
       "  del t353\n",
       "  t358 = torch.nn.functional.linear(t357, t_fcs_72_weight, t_fcs_72_bias)  # t358: \"cuda:0 f32[128, 16]\"\n",
       "    # t358 = ltorch.linear(t357, t_fcs_72_weight, t_fcs_72_bias)  # t358: \"cuda:0 f32[128, 16]\"\n",
       "      # t358 = prims.linear(t357, t_fcs_72_weight, t_fcs_72_bias)  # t358: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t357 = offload_to_cpu(t357)  # offloaded_t357: \"cpu f32[128, 16]\"\n",
       "  del t357\n",
       "  [t360, t362] = nvFusion72(t358)\n",
       "    # t360 = prims.gt(t358, 0.0)  # t360: \"cuda:0 b8[128, 16]\"\n",
       "    # t362 = prims.where(t360, t358, 0.0)  # t362: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t360 = offload_to_cpu(t360)  # offloaded_t360: \"cpu b8[128, 16]\"\n",
       "  del t360\n",
       "  del t358\n",
       "  t363 = torch.nn.functional.linear(t362, t_fcs_73_weight, t_fcs_73_bias)  # t363: \"cuda:0 f32[128, 16]\"\n",
       "    # t363 = ltorch.linear(t362, t_fcs_73_weight, t_fcs_73_bias)  # t363: \"cuda:0 f32[128, 16]\"\n",
       "      # t363 = prims.linear(t362, t_fcs_73_weight, t_fcs_73_bias)  # t363: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t362 = offload_to_cpu(t362)  # offloaded_t362: \"cpu f32[128, 16]\"\n",
       "  del t362\n",
       "  [t365, t367] = nvFusion73(t363)\n",
       "    # t365 = prims.gt(t363, 0.0)  # t365: \"cuda:0 b8[128, 16]\"\n",
       "    # t367 = prims.where(t365, t363, 0.0)  # t367: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t365 = offload_to_cpu(t365)  # offloaded_t365: \"cpu b8[128, 16]\"\n",
       "  del t365\n",
       "  del t363\n",
       "  t368 = torch.nn.functional.linear(t367, t_fcs_74_weight, t_fcs_74_bias)  # t368: \"cuda:0 f32[128, 16]\"\n",
       "    # t368 = ltorch.linear(t367, t_fcs_74_weight, t_fcs_74_bias)  # t368: \"cuda:0 f32[128, 16]\"\n",
       "      # t368 = prims.linear(t367, t_fcs_74_weight, t_fcs_74_bias)  # t368: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t367 = offload_to_cpu(t367)  # offloaded_t367: \"cpu f32[128, 16]\"\n",
       "  del t367\n",
       "  [t370, t372] = nvFusion74(t368)\n",
       "    # t370 = prims.gt(t368, 0.0)  # t370: \"cuda:0 b8[128, 16]\"\n",
       "    # t372 = prims.where(t370, t368, 0.0)  # t372: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t370 = offload_to_cpu(t370)  # offloaded_t370: \"cpu b8[128, 16]\"\n",
       "  del t370\n",
       "  del t368\n",
       "  t373 = torch.nn.functional.linear(t372, t_fcs_75_weight, t_fcs_75_bias)  # t373: \"cuda:0 f32[128, 16]\"\n",
       "    # t373 = ltorch.linear(t372, t_fcs_75_weight, t_fcs_75_bias)  # t373: \"cuda:0 f32[128, 16]\"\n",
       "      # t373 = prims.linear(t372, t_fcs_75_weight, t_fcs_75_bias)  # t373: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t372 = offload_to_cpu(t372)  # offloaded_t372: \"cpu f32[128, 16]\"\n",
       "  del t372\n",
       "  [t375, t377] = nvFusion75(t373)\n",
       "    # t375 = prims.gt(t373, 0.0)  # t375: \"cuda:0 b8[128, 16]\"\n",
       "    # t377 = prims.where(t375, t373, 0.0)  # t377: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t375 = offload_to_cpu(t375)  # offloaded_t375: \"cpu b8[128, 16]\"\n",
       "  del t375\n",
       "  del t373\n",
       "  t378 = torch.nn.functional.linear(t377, t_fcs_76_weight, t_fcs_76_bias)  # t378: \"cuda:0 f32[128, 16]\"\n",
       "    # t378 = ltorch.linear(t377, t_fcs_76_weight, t_fcs_76_bias)  # t378: \"cuda:0 f32[128, 16]\"\n",
       "      # t378 = prims.linear(t377, t_fcs_76_weight, t_fcs_76_bias)  # t378: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t377 = offload_to_cpu(t377)  # offloaded_t377: \"cpu f32[128, 16]\"\n",
       "  del t377\n",
       "  [t380, t382] = nvFusion76(t378)\n",
       "    # t380 = prims.gt(t378, 0.0)  # t380: \"cuda:0 b8[128, 16]\"\n",
       "    # t382 = prims.where(t380, t378, 0.0)  # t382: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t380 = offload_to_cpu(t380)  # offloaded_t380: \"cpu b8[128, 16]\"\n",
       "  del t380\n",
       "  del t378\n",
       "  t383 = torch.nn.functional.linear(t382, t_fcs_77_weight, t_fcs_77_bias)  # t383: \"cuda:0 f32[128, 16]\"\n",
       "    # t383 = ltorch.linear(t382, t_fcs_77_weight, t_fcs_77_bias)  # t383: \"cuda:0 f32[128, 16]\"\n",
       "      # t383 = prims.linear(t382, t_fcs_77_weight, t_fcs_77_bias)  # t383: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t382 = offload_to_cpu(t382)  # offloaded_t382: \"cpu f32[128, 16]\"\n",
       "  del t382\n",
       "  [t385, t387] = nvFusion77(t383)\n",
       "    # t385 = prims.gt(t383, 0.0)  # t385: \"cuda:0 b8[128, 16]\"\n",
       "    # t387 = prims.where(t385, t383, 0.0)  # t387: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t385 = offload_to_cpu(t385)  # offloaded_t385: \"cpu b8[128, 16]\"\n",
       "  del t385\n",
       "  del t383\n",
       "  t388 = torch.nn.functional.linear(t387, t_fcs_78_weight, t_fcs_78_bias)  # t388: \"cuda:0 f32[128, 16]\"\n",
       "    # t388 = ltorch.linear(t387, t_fcs_78_weight, t_fcs_78_bias)  # t388: \"cuda:0 f32[128, 16]\"\n",
       "      # t388 = prims.linear(t387, t_fcs_78_weight, t_fcs_78_bias)  # t388: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t387 = offload_to_cpu(t387)  # offloaded_t387: \"cpu f32[128, 16]\"\n",
       "  del t387\n",
       "  [t390, t392] = nvFusion78(t388)\n",
       "    # t390 = prims.gt(t388, 0.0)  # t390: \"cuda:0 b8[128, 16]\"\n",
       "    # t392 = prims.where(t390, t388, 0.0)  # t392: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t390 = offload_to_cpu(t390)  # offloaded_t390: \"cpu b8[128, 16]\"\n",
       "  del t390\n",
       "  del t388\n",
       "  t393 = torch.nn.functional.linear(t392, t_fcs_79_weight, t_fcs_79_bias)  # t393: \"cuda:0 f32[128, 16]\"\n",
       "    # t393 = ltorch.linear(t392, t_fcs_79_weight, t_fcs_79_bias)  # t393: \"cuda:0 f32[128, 16]\"\n",
       "      # t393 = prims.linear(t392, t_fcs_79_weight, t_fcs_79_bias)  # t393: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t392 = offload_to_cpu(t392)  # offloaded_t392: \"cpu f32[128, 16]\"\n",
       "  del t392\n",
       "  [t395, t397] = nvFusion79(t393)\n",
       "    # t395 = prims.gt(t393, 0.0)  # t395: \"cuda:0 b8[128, 16]\"\n",
       "    # t397 = prims.where(t395, t393, 0.0)  # t397: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t395 = offload_to_cpu(t395)  # offloaded_t395: \"cpu b8[128, 16]\"\n",
       "  del t395\n",
       "  del t393\n",
       "  t398 = torch.nn.functional.linear(t397, t_fcs_80_weight, t_fcs_80_bias)  # t398: \"cuda:0 f32[128, 16]\"\n",
       "    # t398 = ltorch.linear(t397, t_fcs_80_weight, t_fcs_80_bias)  # t398: \"cuda:0 f32[128, 16]\"\n",
       "      # t398 = prims.linear(t397, t_fcs_80_weight, t_fcs_80_bias)  # t398: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t397 = offload_to_cpu(t397)  # offloaded_t397: \"cpu f32[128, 16]\"\n",
       "  del t397\n",
       "  [t400, t402] = nvFusion80(t398)\n",
       "    # t400 = prims.gt(t398, 0.0)  # t400: \"cuda:0 b8[128, 16]\"\n",
       "    # t402 = prims.where(t400, t398, 0.0)  # t402: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t400 = offload_to_cpu(t400)  # offloaded_t400: \"cpu b8[128, 16]\"\n",
       "  del t400\n",
       "  del t398\n",
       "  t403 = torch.nn.functional.linear(t402, t_fcs_81_weight, t_fcs_81_bias)  # t403: \"cuda:0 f32[128, 16]\"\n",
       "    # t403 = ltorch.linear(t402, t_fcs_81_weight, t_fcs_81_bias)  # t403: \"cuda:0 f32[128, 16]\"\n",
       "      # t403 = prims.linear(t402, t_fcs_81_weight, t_fcs_81_bias)  # t403: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t402 = offload_to_cpu(t402)  # offloaded_t402: \"cpu f32[128, 16]\"\n",
       "  del t402\n",
       "  [t405, t407] = nvFusion81(t403)\n",
       "    # t405 = prims.gt(t403, 0.0)  # t405: \"cuda:0 b8[128, 16]\"\n",
       "    # t407 = prims.where(t405, t403, 0.0)  # t407: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t405 = offload_to_cpu(t405)  # offloaded_t405: \"cpu b8[128, 16]\"\n",
       "  del t405\n",
       "  del t403\n",
       "  t408 = torch.nn.functional.linear(t407, t_fcs_82_weight, t_fcs_82_bias)  # t408: \"cuda:0 f32[128, 16]\"\n",
       "    # t408 = ltorch.linear(t407, t_fcs_82_weight, t_fcs_82_bias)  # t408: \"cuda:0 f32[128, 16]\"\n",
       "      # t408 = prims.linear(t407, t_fcs_82_weight, t_fcs_82_bias)  # t408: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t407 = offload_to_cpu(t407)  # offloaded_t407: \"cpu f32[128, 16]\"\n",
       "  del t407\n",
       "  [t410, t412] = nvFusion82(t408)\n",
       "    # t410 = prims.gt(t408, 0.0)  # t410: \"cuda:0 b8[128, 16]\"\n",
       "    # t412 = prims.where(t410, t408, 0.0)  # t412: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t410 = offload_to_cpu(t410)  # offloaded_t410: \"cpu b8[128, 16]\"\n",
       "  del t410\n",
       "  del t408\n",
       "  t413 = torch.nn.functional.linear(t412, t_fcs_83_weight, t_fcs_83_bias)  # t413: \"cuda:0 f32[128, 16]\"\n",
       "    # t413 = ltorch.linear(t412, t_fcs_83_weight, t_fcs_83_bias)  # t413: \"cuda:0 f32[128, 16]\"\n",
       "      # t413 = prims.linear(t412, t_fcs_83_weight, t_fcs_83_bias)  # t413: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t412 = offload_to_cpu(t412)  # offloaded_t412: \"cpu f32[128, 16]\"\n",
       "  del t412\n",
       "  [t415, t417] = nvFusion83(t413)\n",
       "    # t415 = prims.gt(t413, 0.0)  # t415: \"cuda:0 b8[128, 16]\"\n",
       "    # t417 = prims.where(t415, t413, 0.0)  # t417: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t415 = offload_to_cpu(t415)  # offloaded_t415: \"cpu b8[128, 16]\"\n",
       "  del t415\n",
       "  del t413\n",
       "  t418 = torch.nn.functional.linear(t417, t_fcs_84_weight, t_fcs_84_bias)  # t418: \"cuda:0 f32[128, 16]\"\n",
       "    # t418 = ltorch.linear(t417, t_fcs_84_weight, t_fcs_84_bias)  # t418: \"cuda:0 f32[128, 16]\"\n",
       "      # t418 = prims.linear(t417, t_fcs_84_weight, t_fcs_84_bias)  # t418: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t417 = offload_to_cpu(t417)  # offloaded_t417: \"cpu f32[128, 16]\"\n",
       "  del t417\n",
       "  [t420, t422] = nvFusion84(t418)\n",
       "    # t420 = prims.gt(t418, 0.0)  # t420: \"cuda:0 b8[128, 16]\"\n",
       "    # t422 = prims.where(t420, t418, 0.0)  # t422: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t420 = offload_to_cpu(t420)  # offloaded_t420: \"cpu b8[128, 16]\"\n",
       "  del t420\n",
       "  del t418\n",
       "  t423 = torch.nn.functional.linear(t422, t_fcs_85_weight, t_fcs_85_bias)  # t423: \"cuda:0 f32[128, 16]\"\n",
       "    # t423 = ltorch.linear(t422, t_fcs_85_weight, t_fcs_85_bias)  # t423: \"cuda:0 f32[128, 16]\"\n",
       "      # t423 = prims.linear(t422, t_fcs_85_weight, t_fcs_85_bias)  # t423: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t422 = offload_to_cpu(t422)  # offloaded_t422: \"cpu f32[128, 16]\"\n",
       "  del t422\n",
       "  [t425, t427] = nvFusion85(t423)\n",
       "    # t425 = prims.gt(t423, 0.0)  # t425: \"cuda:0 b8[128, 16]\"\n",
       "    # t427 = prims.where(t425, t423, 0.0)  # t427: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t425 = offload_to_cpu(t425)  # offloaded_t425: \"cpu b8[128, 16]\"\n",
       "  del t425\n",
       "  del t423\n",
       "  t428 = torch.nn.functional.linear(t427, t_fcs_86_weight, t_fcs_86_bias)  # t428: \"cuda:0 f32[128, 16]\"\n",
       "    # t428 = ltorch.linear(t427, t_fcs_86_weight, t_fcs_86_bias)  # t428: \"cuda:0 f32[128, 16]\"\n",
       "      # t428 = prims.linear(t427, t_fcs_86_weight, t_fcs_86_bias)  # t428: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t427 = offload_to_cpu(t427)  # offloaded_t427: \"cpu f32[128, 16]\"\n",
       "  del t427\n",
       "  [t430, t432] = nvFusion86(t428)\n",
       "    # t430 = prims.gt(t428, 0.0)  # t430: \"cuda:0 b8[128, 16]\"\n",
       "    # t432 = prims.where(t430, t428, 0.0)  # t432: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t430 = offload_to_cpu(t430)  # offloaded_t430: \"cpu b8[128, 16]\"\n",
       "  del t430\n",
       "  del t428\n",
       "  t433 = torch.nn.functional.linear(t432, t_fcs_87_weight, t_fcs_87_bias)  # t433: \"cuda:0 f32[128, 16]\"\n",
       "    # t433 = ltorch.linear(t432, t_fcs_87_weight, t_fcs_87_bias)  # t433: \"cuda:0 f32[128, 16]\"\n",
       "      # t433 = prims.linear(t432, t_fcs_87_weight, t_fcs_87_bias)  # t433: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t432 = offload_to_cpu(t432)  # offloaded_t432: \"cpu f32[128, 16]\"\n",
       "  del t432\n",
       "  [t435, t437] = nvFusion87(t433)\n",
       "    # t435 = prims.gt(t433, 0.0)  # t435: \"cuda:0 b8[128, 16]\"\n",
       "    # t437 = prims.where(t435, t433, 0.0)  # t437: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t435 = offload_to_cpu(t435)  # offloaded_t435: \"cpu b8[128, 16]\"\n",
       "  del t435\n",
       "  del t433\n",
       "  t438 = torch.nn.functional.linear(t437, t_fcs_88_weight, t_fcs_88_bias)  # t438: \"cuda:0 f32[128, 16]\"\n",
       "    # t438 = ltorch.linear(t437, t_fcs_88_weight, t_fcs_88_bias)  # t438: \"cuda:0 f32[128, 16]\"\n",
       "      # t438 = prims.linear(t437, t_fcs_88_weight, t_fcs_88_bias)  # t438: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t437 = offload_to_cpu(t437)  # offloaded_t437: \"cpu f32[128, 16]\"\n",
       "  del t437\n",
       "  [t440, t442] = nvFusion88(t438)\n",
       "    # t440 = prims.gt(t438, 0.0)  # t440: \"cuda:0 b8[128, 16]\"\n",
       "    # t442 = prims.where(t440, t438, 0.0)  # t442: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t440 = offload_to_cpu(t440)  # offloaded_t440: \"cpu b8[128, 16]\"\n",
       "  del t440\n",
       "  del t438\n",
       "  t443 = torch.nn.functional.linear(t442, t_fcs_89_weight, t_fcs_89_bias)  # t443: \"cuda:0 f32[128, 16]\"\n",
       "    # t443 = ltorch.linear(t442, t_fcs_89_weight, t_fcs_89_bias)  # t443: \"cuda:0 f32[128, 16]\"\n",
       "      # t443 = prims.linear(t442, t_fcs_89_weight, t_fcs_89_bias)  # t443: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t442 = offload_to_cpu(t442)  # offloaded_t442: \"cpu f32[128, 16]\"\n",
       "  del t442\n",
       "  [t445, t447] = nvFusion89(t443)\n",
       "    # t445 = prims.gt(t443, 0.0)  # t445: \"cuda:0 b8[128, 16]\"\n",
       "    # t447 = prims.where(t445, t443, 0.0)  # t447: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t445 = offload_to_cpu(t445)  # offloaded_t445: \"cpu b8[128, 16]\"\n",
       "  del t445\n",
       "  del t443\n",
       "  t448 = torch.nn.functional.linear(t447, t_fcs_90_weight, t_fcs_90_bias)  # t448: \"cuda:0 f32[128, 16]\"\n",
       "    # t448 = ltorch.linear(t447, t_fcs_90_weight, t_fcs_90_bias)  # t448: \"cuda:0 f32[128, 16]\"\n",
       "      # t448 = prims.linear(t447, t_fcs_90_weight, t_fcs_90_bias)  # t448: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t447 = offload_to_cpu(t447)  # offloaded_t447: \"cpu f32[128, 16]\"\n",
       "  del t447\n",
       "  [t450, t452] = nvFusion90(t448)\n",
       "    # t450 = prims.gt(t448, 0.0)  # t450: \"cuda:0 b8[128, 16]\"\n",
       "    # t452 = prims.where(t450, t448, 0.0)  # t452: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t450 = offload_to_cpu(t450)  # offloaded_t450: \"cpu b8[128, 16]\"\n",
       "  del t450\n",
       "  del t448\n",
       "  t453 = torch.nn.functional.linear(t452, t_fcs_91_weight, t_fcs_91_bias)  # t453: \"cuda:0 f32[128, 16]\"\n",
       "    # t453 = ltorch.linear(t452, t_fcs_91_weight, t_fcs_91_bias)  # t453: \"cuda:0 f32[128, 16]\"\n",
       "      # t453 = prims.linear(t452, t_fcs_91_weight, t_fcs_91_bias)  # t453: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t452 = offload_to_cpu(t452)  # offloaded_t452: \"cpu f32[128, 16]\"\n",
       "  del t452\n",
       "  [t455, t457] = nvFusion91(t453)\n",
       "    # t455 = prims.gt(t453, 0.0)  # t455: \"cuda:0 b8[128, 16]\"\n",
       "    # t457 = prims.where(t455, t453, 0.0)  # t457: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t455 = offload_to_cpu(t455)  # offloaded_t455: \"cpu b8[128, 16]\"\n",
       "  del t455\n",
       "  del t453\n",
       "  t458 = torch.nn.functional.linear(t457, t_fcs_92_weight, t_fcs_92_bias)  # t458: \"cuda:0 f32[128, 16]\"\n",
       "    # t458 = ltorch.linear(t457, t_fcs_92_weight, t_fcs_92_bias)  # t458: \"cuda:0 f32[128, 16]\"\n",
       "      # t458 = prims.linear(t457, t_fcs_92_weight, t_fcs_92_bias)  # t458: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t457 = offload_to_cpu(t457)  # offloaded_t457: \"cpu f32[128, 16]\"\n",
       "  del t457\n",
       "  [t460, t462] = nvFusion92(t458)\n",
       "    # t460 = prims.gt(t458, 0.0)  # t460: \"cuda:0 b8[128, 16]\"\n",
       "    # t462 = prims.where(t460, t458, 0.0)  # t462: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t460 = offload_to_cpu(t460)  # offloaded_t460: \"cpu b8[128, 16]\"\n",
       "  del t460\n",
       "  del t458\n",
       "  t463 = torch.nn.functional.linear(t462, t_fcs_93_weight, t_fcs_93_bias)  # t463: \"cuda:0 f32[128, 16]\"\n",
       "    # t463 = ltorch.linear(t462, t_fcs_93_weight, t_fcs_93_bias)  # t463: \"cuda:0 f32[128, 16]\"\n",
       "      # t463 = prims.linear(t462, t_fcs_93_weight, t_fcs_93_bias)  # t463: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t462 = offload_to_cpu(t462)  # offloaded_t462: \"cpu f32[128, 16]\"\n",
       "  del t462\n",
       "  [t465, t467] = nvFusion93(t463)\n",
       "    # t465 = prims.gt(t463, 0.0)  # t465: \"cuda:0 b8[128, 16]\"\n",
       "    # t467 = prims.where(t465, t463, 0.0)  # t467: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t465 = offload_to_cpu(t465)  # offloaded_t465: \"cpu b8[128, 16]\"\n",
       "  del t465\n",
       "  del t463\n",
       "  t468 = torch.nn.functional.linear(t467, t_fcs_94_weight, t_fcs_94_bias)  # t468: \"cuda:0 f32[128, 16]\"\n",
       "    # t468 = ltorch.linear(t467, t_fcs_94_weight, t_fcs_94_bias)  # t468: \"cuda:0 f32[128, 16]\"\n",
       "      # t468 = prims.linear(t467, t_fcs_94_weight, t_fcs_94_bias)  # t468: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t467 = offload_to_cpu(t467)  # offloaded_t467: \"cpu f32[128, 16]\"\n",
       "  del t467\n",
       "  [t470, t472] = nvFusion94(t468)\n",
       "    # t470 = prims.gt(t468, 0.0)  # t470: \"cuda:0 b8[128, 16]\"\n",
       "    # t472 = prims.where(t470, t468, 0.0)  # t472: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t470 = offload_to_cpu(t470)  # offloaded_t470: \"cpu b8[128, 16]\"\n",
       "  del t470\n",
       "  del t468\n",
       "  t473 = torch.nn.functional.linear(t472, t_fcs_95_weight, t_fcs_95_bias)  # t473: \"cuda:0 f32[128, 16]\"\n",
       "    # t473 = ltorch.linear(t472, t_fcs_95_weight, t_fcs_95_bias)  # t473: \"cuda:0 f32[128, 16]\"\n",
       "      # t473 = prims.linear(t472, t_fcs_95_weight, t_fcs_95_bias)  # t473: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t472 = offload_to_cpu(t472)  # offloaded_t472: \"cpu f32[128, 16]\"\n",
       "  del t472\n",
       "  [t475, t477] = nvFusion95(t473)\n",
       "    # t475 = prims.gt(t473, 0.0)  # t475: \"cuda:0 b8[128, 16]\"\n",
       "    # t477 = prims.where(t475, t473, 0.0)  # t477: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t475 = offload_to_cpu(t475)  # offloaded_t475: \"cpu b8[128, 16]\"\n",
       "  del t475\n",
       "  del t473\n",
       "  t478 = torch.nn.functional.linear(t477, t_fcs_96_weight, t_fcs_96_bias)  # t478: \"cuda:0 f32[128, 16]\"\n",
       "    # t478 = ltorch.linear(t477, t_fcs_96_weight, t_fcs_96_bias)  # t478: \"cuda:0 f32[128, 16]\"\n",
       "      # t478 = prims.linear(t477, t_fcs_96_weight, t_fcs_96_bias)  # t478: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t477 = offload_to_cpu(t477)  # offloaded_t477: \"cpu f32[128, 16]\"\n",
       "  del t477\n",
       "  [t480, t482] = nvFusion96(t478)\n",
       "    # t480 = prims.gt(t478, 0.0)  # t480: \"cuda:0 b8[128, 16]\"\n",
       "    # t482 = prims.where(t480, t478, 0.0)  # t482: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t480 = offload_to_cpu(t480)  # offloaded_t480: \"cpu b8[128, 16]\"\n",
       "  del t480\n",
       "  del t478\n",
       "  t483 = torch.nn.functional.linear(t482, t_fcs_97_weight, t_fcs_97_bias)  # t483: \"cuda:0 f32[128, 16]\"\n",
       "    # t483 = ltorch.linear(t482, t_fcs_97_weight, t_fcs_97_bias)  # t483: \"cuda:0 f32[128, 16]\"\n",
       "      # t483 = prims.linear(t482, t_fcs_97_weight, t_fcs_97_bias)  # t483: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t482 = offload_to_cpu(t482)  # offloaded_t482: \"cpu f32[128, 16]\"\n",
       "  del t482\n",
       "  [t485, t487] = nvFusion97(t483)\n",
       "    # t485 = prims.gt(t483, 0.0)  # t485: \"cuda:0 b8[128, 16]\"\n",
       "    # t487 = prims.where(t485, t483, 0.0)  # t487: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t485 = offload_to_cpu(t485)  # offloaded_t485: \"cpu b8[128, 16]\"\n",
       "  del t485\n",
       "  del t483\n",
       "  t488 = torch.nn.functional.linear(t487, t_fcs_98_weight, t_fcs_98_bias)  # t488: \"cuda:0 f32[128, 16]\"\n",
       "    # t488 = ltorch.linear(t487, t_fcs_98_weight, t_fcs_98_bias)  # t488: \"cuda:0 f32[128, 16]\"\n",
       "      # t488 = prims.linear(t487, t_fcs_98_weight, t_fcs_98_bias)  # t488: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t487 = offload_to_cpu(t487)  # offloaded_t487: \"cpu f32[128, 16]\"\n",
       "  del t487\n",
       "  [t490, t492] = nvFusion98(t488)\n",
       "    # t490 = prims.gt(t488, 0.0)  # t490: \"cuda:0 b8[128, 16]\"\n",
       "    # t492 = prims.where(t490, t488, 0.0)  # t492: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t490 = offload_to_cpu(t490)  # offloaded_t490: \"cpu b8[128, 16]\"\n",
       "  del t490\n",
       "  del t488\n",
       "  t493 = torch.nn.functional.linear(t492, t_fcs_99_weight, t_fcs_99_bias)  # t493: \"cuda:0 f32[128, 16]\"\n",
       "    # t493 = ltorch.linear(t492, t_fcs_99_weight, t_fcs_99_bias)  # t493: \"cuda:0 f32[128, 16]\"\n",
       "      # t493 = prims.linear(t492, t_fcs_99_weight, t_fcs_99_bias)  # t493: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t492 = offload_to_cpu(t492)  # offloaded_t492: \"cpu f32[128, 16]\"\n",
       "  del t492\n",
       "  [t495, t497] = nvFusion99(t493)\n",
       "    # t495 = prims.gt(t493, 0.0)  # t495: \"cuda:0 b8[128, 16]\"\n",
       "    # t497 = prims.where(t495, t493, 0.0)  # t497: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t495 = offload_to_cpu(t495)  # offloaded_t495: \"cpu b8[128, 16]\"\n",
       "  del t495\n",
       "  del t493\n",
       "  return {'output': t497, 'flat_args': [x, t_fcs_0_bias, t_fcs_0_weight, t_fcs_1_bias, t_fcs_1_weight, t_fcs_2_bias, t_fcs_2_weight, t_fcs_3_bias, t_fcs_3_weight, t_fcs_4_bias, t_fcs_4_weight, t_fcs_5_bias, t_fcs_5_weight, t_fcs_6_bias, t_fcs_6_weight, t_fcs_7_bias, t_fcs_7_weight, t_fcs_8_bias, t_fcs_8_weight, t_fcs_9_bias, t_fcs_9_weight, t_fcs_10_bias, t_fcs_10_weight, t_fcs_11_bias, t_fcs_11_weight, t_fcs_12_bias, t_fcs_12_weight, t_fcs_13_bias, t_fcs_13_weight, t_fcs_14_bias, t_fcs_14_weight, t_fcs_15_bias, t_fcs_15_weight, t_fcs_16_bias, t_fcs_16_weight, t_fcs_17_bias, t_fcs_17_weight, t_fcs_18_bias, t_fcs_18_weight, t_fcs_19_bias, t_fcs_19_weight, t_fcs_20_bias, t_fcs_20_weight, t_fcs_21_bias, t_fcs_21_weight, t_fcs_22_bias, t_fcs_22_weight, t_fcs_23_bias, t_fcs_23_weight, t_fcs_24_bias, t_fcs_24_weight, t_fcs_25_bias, t_fcs_25_weight, t_fcs_26_bias, t_fcs_26_weight, t_fcs_27_bias, t_fcs_27_weight, t_fcs_28_bias, t_fcs_28_weight, t_fcs_29_bias, t_fcs_29_weight, t_fcs_30_bias, t_fcs_30_weight, t_fcs_31_bias, t_fcs_31_weight, t_fcs_32_bias, t_fcs_32_weight, t_fcs_33_bias, t_fcs_33_weight, t_fcs_34_bias, t_fcs_34_weight, t_fcs_35_bias, t_fcs_35_weight, t_fcs_36_bias, t_fcs_36_weight, t_fcs_37_bias, t_fcs_37_weight, t_fcs_38_bias, t_fcs_38_weight, t_fcs_39_bias, t_fcs_39_weight, t_fcs_40_bias, t_fcs_40_weight, t_fcs_41_bias, t_fcs_41_weight, t_fcs_42_bias, t_fcs_42_weight, t_fcs_43_bias, t_fcs_43_weight, t_fcs_44_bias, t_fcs_44_weight, t_fcs_45_bias, t_fcs_45_weight, t_fcs_46_bias, t_fcs_46_weight, t_fcs_47_bias, t_fcs_47_weight, t_fcs_48_bias, t_fcs_48_weight, t_fcs_49_bias, t_fcs_49_weight, t_fcs_50_bias, t_fcs_50_weight, t_fcs_51_bias, t_fcs_51_weight, t_fcs_52_bias, t_fcs_52_weight, t_fcs_53_bias, t_fcs_53_weight, t_fcs_54_bias, t_fcs_54_weight, t_fcs_55_bias, t_fcs_55_weight, t_fcs_56_bias, t_fcs_56_weight, t_fcs_57_bias, t_fcs_57_weight, t_fcs_58_bias, t_fcs_58_weight, t_fcs_59_bias, t_fcs_59_weight, t_fcs_60_bias, t_fcs_60_weight, t_fcs_61_bias, t_fcs_61_weight, t_fcs_62_bias, t_fcs_62_weight, t_fcs_63_bias, t_fcs_63_weight, t_fcs_64_bias, t_fcs_64_weight, t_fcs_65_bias, t_fcs_65_weight, t_fcs_66_bias, t_fcs_66_weight, t_fcs_67_bias, t_fcs_67_weight, t_fcs_68_bias, t_fcs_68_weight, t_fcs_69_bias, t_fcs_69_weight, t_fcs_70_bias, t_fcs_70_weight, t_fcs_71_bias, t_fcs_71_weight, t_fcs_72_bias, t_fcs_72_weight, t_fcs_73_bias, t_fcs_73_weight, t_fcs_74_bias, t_fcs_74_weight, t_fcs_75_bias, t_fcs_75_weight, t_fcs_76_bias, t_fcs_76_weight, t_fcs_77_bias, t_fcs_77_weight, t_fcs_78_bias, t_fcs_78_weight, t_fcs_79_bias, t_fcs_79_weight, t_fcs_80_bias, t_fcs_80_weight, t_fcs_81_bias, t_fcs_81_weight, t_fcs_82_bias, t_fcs_82_weight, t_fcs_83_bias, t_fcs_83_weight, t_fcs_84_bias, t_fcs_84_weight, t_fcs_85_bias, t_fcs_85_weight, t_fcs_86_bias, t_fcs_86_weight, t_fcs_87_bias, t_fcs_87_weight, t_fcs_88_bias, t_fcs_88_weight, t_fcs_89_bias, t_fcs_89_weight, t_fcs_90_bias, t_fcs_90_weight, t_fcs_91_bias, t_fcs_91_weight, t_fcs_92_bias, t_fcs_92_weight, t_fcs_93_bias, t_fcs_93_weight, t_fcs_94_bias, t_fcs_94_weight, t_fcs_95_bias, t_fcs_95_weight, t_fcs_96_bias, t_fcs_96_weight, t_fcs_97_bias, t_fcs_97_weight, t_fcs_98_bias, t_fcs_98_weight, t_fcs_99_bias, t_fcs_99_weight], 'flat_output': (t497,)}, ((offloaded_t1, offloaded_t10, offloaded_t100, offloaded_t102, offloaded_t105, offloaded_t107, offloaded_t110, offloaded_t112, offloaded_t115, offloaded_t117, offloaded_t12, offloaded_t120, offloaded_t122, offloaded_t125, offloaded_t127, offloaded_t130, offloaded_t132, offloaded_t135, offloaded_t137, offloaded_t140, offloaded_t142, offloaded_t145, offloaded_t147, offloaded_t15, offloaded_t150, offloaded_t152, offloaded_t155, offloaded_t157, offloaded_t160, offloaded_t162, offloaded_t165, offloaded_t167, offloaded_t17, offloaded_t170, offloaded_t172, offloaded_t175, offloaded_t177, offloaded_t180, offloaded_t182, offloaded_t185, offloaded_t187, offloaded_t190, offloaded_t192, offloaded_t195, offloaded_t197, offloaded_t2, offloaded_t20, offloaded_t200, offloaded_t202, offloaded_t205, offloaded_t207, offloaded_t210, offloaded_t212, offloaded_t215, offloaded_t217, offloaded_t22, offloaded_t220, offloaded_t222, offloaded_t225, offloaded_t227, offloaded_t230, offloaded_t232, offloaded_t235, offloaded_t237, offloaded_t240, offloaded_t242, offloaded_t245, offloaded_t247, offloaded_t25, offloaded_t250, offloaded_t252, offloaded_t255, offloaded_t257, offloaded_t260, offloaded_t262, offloaded_t265, offloaded_t267, offloaded_t27, offloaded_t270, offloaded_t272, offloaded_t275, offloaded_t277, offloaded_t280, offloaded_t282, offloaded_t285, offloaded_t287, offloaded_t290, offloaded_t292, offloaded_t295, offloaded_t297, offloaded_t30, offloaded_t300, offloaded_t302, offloaded_t305, offloaded_t307, offloaded_t310, offloaded_t312, offloaded_t315, offloaded_t317, offloaded_t32, offloaded_t320, offloaded_t322, offloaded_t325, offloaded_t327, offloaded_t330, offloaded_t332, offloaded_t335, offloaded_t337, offloaded_t340, offloaded_t342, offloaded_t345, offloaded_t347, offloaded_t35, offloaded_t350, offloaded_t352, offloaded_t355, offloaded_t357, offloaded_t360, offloaded_t362, offloaded_t365, offloaded_t367, offloaded_t37, offloaded_t370, offloaded_t372, offloaded_t375, offloaded_t377, offloaded_t380, offloaded_t382, offloaded_t385, offloaded_t387, offloaded_t390, offloaded_t392, offloaded_t395, offloaded_t397, offloaded_t40, offloaded_t400, offloaded_t402, offloaded_t405, offloaded_t407, offloaded_t410, offloaded_t412, offloaded_t415, offloaded_t417, offloaded_t42, offloaded_t420, offloaded_t422, offloaded_t425, offloaded_t427, offloaded_t430, offloaded_t432, offloaded_t435, offloaded_t437, offloaded_t440, offloaded_t442, offloaded_t445, offloaded_t447, offloaded_t45, offloaded_t450, offloaded_t452, offloaded_t455, offloaded_t457, offloaded_t460, offloaded_t462, offloaded_t465, offloaded_t467, offloaded_t47, offloaded_t470, offloaded_t472, offloaded_t475, offloaded_t477, offloaded_t480, offloaded_t482, offloaded_t485, offloaded_t487, offloaded_t490, offloaded_t492, offloaded_t495, offloaded_t5, offloaded_t50, offloaded_t52, offloaded_t55, offloaded_t57, offloaded_t60, offloaded_t62, offloaded_t65, offloaded_t67, offloaded_t7, offloaded_t70, offloaded_t72, offloaded_t75, offloaded_t77, offloaded_t80, offloaded_t82, offloaded_t85, offloaded_t87, offloaded_t90, offloaded_t92, offloaded_t95, offloaded_t97, t_fcs_10_weight, t_fcs_11_weight, t_fcs_12_weight, t_fcs_13_weight, t_fcs_14_weight, t_fcs_15_weight, t_fcs_16_weight, t_fcs_17_weight, t_fcs_18_weight, t_fcs_19_weight, t_fcs_1_weight, t_fcs_20_weight, t_fcs_21_weight, t_fcs_22_weight, t_fcs_23_weight, t_fcs_24_weight, t_fcs_25_weight, t_fcs_26_weight, t_fcs_27_weight, t_fcs_28_weight, t_fcs_29_weight, t_fcs_2_weight, t_fcs_30_weight, t_fcs_31_weight, t_fcs_32_weight, t_fcs_33_weight, t_fcs_34_weight, t_fcs_35_weight, t_fcs_36_weight, t_fcs_37_weight, t_fcs_38_weight, t_fcs_39_weight, t_fcs_3_weight, t_fcs_40_weight, t_fcs_41_weight, t_fcs_42_weight, t_fcs_43_weight, t_fcs_44_weight, t_fcs_45_weight, t_fcs_46_weight, t_fcs_47_weight, t_fcs_48_weight, t_fcs_49_weight, t_fcs_4_weight, t_fcs_50_weight, t_fcs_51_weight, t_fcs_52_weight, t_fcs_53_weight, t_fcs_54_weight, t_fcs_55_weight, t_fcs_56_weight, t_fcs_57_weight, t_fcs_58_weight, t_fcs_59_weight, t_fcs_5_weight, t_fcs_60_weight, t_fcs_61_weight, t_fcs_62_weight, t_fcs_63_weight, t_fcs_64_weight, t_fcs_65_weight, t_fcs_66_weight, t_fcs_67_weight, t_fcs_68_weight, t_fcs_69_weight, t_fcs_6_weight, t_fcs_70_weight, t_fcs_71_weight, t_fcs_72_weight, t_fcs_73_weight, t_fcs_74_weight, t_fcs_75_weight, t_fcs_76_weight, t_fcs_77_weight, t_fcs_78_weight, t_fcs_79_weight, t_fcs_7_weight, t_fcs_80_weight, t_fcs_81_weight, t_fcs_82_weight, t_fcs_83_weight, t_fcs_84_weight, t_fcs_85_weight, t_fcs_86_weight, t_fcs_87_weight, t_fcs_88_weight, t_fcs_89_weight, t_fcs_8_weight, t_fcs_90_weight, t_fcs_91_weight, t_fcs_92_weight, t_fcs_93_weight, t_fcs_94_weight, t_fcs_95_weight, t_fcs_96_weight, t_fcs_97_weight, t_fcs_98_weight, t_fcs_99_weight, t_fcs_9_weight, x), ())"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fw_traces[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# Constructed by Delete Last Used (took 5 milliseconds)\n",
       "import torch\n",
       "from thunder.executors.torchex import no_autocast\n",
       "\n",
       "@torch.no_grad()\n",
       "@no_autocast\n",
       "def backward_fn(saved_for_backward, cotangents):\n",
       "  # saved_for_backward: \"Collection\"\n",
       "  C0, _, = saved_for_backward\n",
       "  clear_mutable_collection(saved_for_backward)\n",
       "  del saved_for_backward\n",
       "  offloaded_t1, offloaded_t10, offloaded_t100, offloaded_t102, offloaded_t105, \\\n",
       "  offloaded_t107, offloaded_t110, offloaded_t112, offloaded_t115, offloaded_t117, \\\n",
       "  offloaded_t12, offloaded_t120, offloaded_t122, offloaded_t125, offloaded_t127, \\\n",
       "  offloaded_t130, offloaded_t132, offloaded_t135, offloaded_t137, offloaded_t140, \\\n",
       "  offloaded_t142, offloaded_t145, offloaded_t147, offloaded_t15, offloaded_t150, \\\n",
       "  offloaded_t152, offloaded_t155, offloaded_t157, offloaded_t160, offloaded_t162, \\\n",
       "  offloaded_t165, offloaded_t167, offloaded_t17, offloaded_t170, offloaded_t172, \\\n",
       "  offloaded_t175, offloaded_t177, offloaded_t180, offloaded_t182, offloaded_t185, \\\n",
       "  offloaded_t187, offloaded_t190, offloaded_t192, offloaded_t195, offloaded_t197, \\\n",
       "  offloaded_t2, offloaded_t20, offloaded_t200, offloaded_t202, offloaded_t205, \\\n",
       "  offloaded_t207, offloaded_t210, offloaded_t212, offloaded_t215, offloaded_t217, \\\n",
       "  offloaded_t22, offloaded_t220, offloaded_t222, offloaded_t225, offloaded_t227, \\\n",
       "  offloaded_t230, offloaded_t232, offloaded_t235, offloaded_t237, offloaded_t240, \\\n",
       "  offloaded_t242, offloaded_t245, offloaded_t247, offloaded_t25, offloaded_t250, \\\n",
       "  offloaded_t252, offloaded_t255, offloaded_t257, offloaded_t260, offloaded_t262, \\\n",
       "  offloaded_t265, offloaded_t267, offloaded_t27, offloaded_t270, offloaded_t272, \\\n",
       "  offloaded_t275, offloaded_t277, offloaded_t280, offloaded_t282, offloaded_t285, \\\n",
       "  offloaded_t287, offloaded_t290, offloaded_t292, offloaded_t295, offloaded_t297, \\\n",
       "  offloaded_t30, offloaded_t300, offloaded_t302, offloaded_t305, offloaded_t307, \\\n",
       "  offloaded_t310, offloaded_t312, offloaded_t315, offloaded_t317, offloaded_t32, \\\n",
       "  offloaded_t320, offloaded_t322, offloaded_t325, offloaded_t327, offloaded_t330, \\\n",
       "  offloaded_t332, offloaded_t335, offloaded_t337, offloaded_t340, offloaded_t342, \\\n",
       "  offloaded_t345, offloaded_t347, offloaded_t35, offloaded_t350, offloaded_t352, \\\n",
       "  offloaded_t355, offloaded_t357, offloaded_t360, offloaded_t362, offloaded_t365, \\\n",
       "  offloaded_t367, offloaded_t37, offloaded_t370, offloaded_t372, offloaded_t375, \\\n",
       "  offloaded_t377, offloaded_t380, offloaded_t382, offloaded_t385, offloaded_t387, \\\n",
       "  offloaded_t390, offloaded_t392, offloaded_t395, offloaded_t397, offloaded_t40, \\\n",
       "  offloaded_t400, offloaded_t402, offloaded_t405, offloaded_t407, offloaded_t410, \\\n",
       "  offloaded_t412, offloaded_t415, offloaded_t417, offloaded_t42, offloaded_t420, \\\n",
       "  offloaded_t422, offloaded_t425, offloaded_t427, offloaded_t430, offloaded_t432, \\\n",
       "  offloaded_t435, offloaded_t437, offloaded_t440, offloaded_t442, offloaded_t445, \\\n",
       "  offloaded_t447, offloaded_t45, offloaded_t450, offloaded_t452, offloaded_t455, \\\n",
       "  offloaded_t457, offloaded_t460, offloaded_t462, offloaded_t465, offloaded_t467, \\\n",
       "  offloaded_t47, offloaded_t470, offloaded_t472, offloaded_t475, offloaded_t477, \\\n",
       "  offloaded_t480, offloaded_t482, offloaded_t485, offloaded_t487, offloaded_t490, \\\n",
       "  offloaded_t492, offloaded_t495, offloaded_t5, offloaded_t50, offloaded_t52, \\\n",
       "  offloaded_t55, offloaded_t57, offloaded_t60, offloaded_t62, offloaded_t65, \\\n",
       "  offloaded_t67, offloaded_t7, offloaded_t70, offloaded_t72, offloaded_t75, \\\n",
       "  offloaded_t77, offloaded_t80, offloaded_t82, offloaded_t85, offloaded_t87, \\\n",
       "  offloaded_t90, offloaded_t92, offloaded_t95, offloaded_t97, t_fcs_10_weight, \\\n",
       "  t_fcs_11_weight, t_fcs_12_weight, t_fcs_13_weight, t_fcs_14_weight, \\\n",
       "  t_fcs_15_weight, t_fcs_16_weight, t_fcs_17_weight, t_fcs_18_weight, \\\n",
       "  t_fcs_19_weight, t_fcs_1_weight, t_fcs_20_weight, t_fcs_21_weight, \\\n",
       "  t_fcs_22_weight, t_fcs_23_weight, t_fcs_24_weight, t_fcs_25_weight, \\\n",
       "  t_fcs_26_weight, t_fcs_27_weight, t_fcs_28_weight, t_fcs_29_weight, \\\n",
       "  t_fcs_2_weight, t_fcs_30_weight, t_fcs_31_weight, t_fcs_32_weight, \\\n",
       "  t_fcs_33_weight, t_fcs_34_weight, t_fcs_35_weight, t_fcs_36_weight, \\\n",
       "  t_fcs_37_weight, t_fcs_38_weight, t_fcs_39_weight, t_fcs_3_weight, \\\n",
       "  t_fcs_40_weight, t_fcs_41_weight, t_fcs_42_weight, t_fcs_43_weight, \\\n",
       "  t_fcs_44_weight, t_fcs_45_weight, t_fcs_46_weight, t_fcs_47_weight, \\\n",
       "  t_fcs_48_weight, t_fcs_49_weight, t_fcs_4_weight, t_fcs_50_weight, \\\n",
       "  t_fcs_51_weight, t_fcs_52_weight, t_fcs_53_weight, t_fcs_54_weight, \\\n",
       "  t_fcs_55_weight, t_fcs_56_weight, t_fcs_57_weight, t_fcs_58_weight, \\\n",
       "  t_fcs_59_weight, t_fcs_5_weight, t_fcs_60_weight, t_fcs_61_weight, \\\n",
       "  t_fcs_62_weight, t_fcs_63_weight, t_fcs_64_weight, t_fcs_65_weight, \\\n",
       "  t_fcs_66_weight, t_fcs_67_weight, t_fcs_68_weight, t_fcs_69_weight, \\\n",
       "  t_fcs_6_weight, t_fcs_70_weight, t_fcs_71_weight, t_fcs_72_weight, \\\n",
       "  t_fcs_73_weight, t_fcs_74_weight, t_fcs_75_weight, t_fcs_76_weight, \\\n",
       "  t_fcs_77_weight, t_fcs_78_weight, t_fcs_79_weight, t_fcs_7_weight, \\\n",
       "  t_fcs_80_weight, t_fcs_81_weight, t_fcs_82_weight, t_fcs_83_weight, \\\n",
       "  t_fcs_84_weight, t_fcs_85_weight, t_fcs_86_weight, t_fcs_87_weight, \\\n",
       "  t_fcs_88_weight, t_fcs_89_weight, t_fcs_8_weight, t_fcs_90_weight, \\\n",
       "  t_fcs_91_weight, t_fcs_92_weight, t_fcs_93_weight, t_fcs_94_weight, \\\n",
       "  t_fcs_95_weight, t_fcs_96_weight, t_fcs_97_weight, t_fcs_98_weight, \\\n",
       "  t_fcs_99_weight, t_fcs_9_weight, x, = C0\n",
       "  clear_mutable_collection(C0)\n",
       "  del C0\n",
       "  # cotangents: \"Collection\"\n",
       "  t4, = cotangents\n",
       "  clear_mutable_collection(cotangents)\n",
       "  del cotangents\n",
       "  # Created by CPU Offloading Transform\n",
       "  t495 = load_to_gpu(offloaded_t495, 'cuda:0')  # t495: \"cuda:0 b8[128, 16]\"\n",
       "  [t2001, t2008] = nvFusion0(t495, t4)\n",
       "    # t2001 = prims.where(t495, t4, 0.0)  # t2001: \"cuda:0 f32[128, 16]\"\n",
       "    # t2008 = prims.sum(t2001, (0,))  # t2008: \"cuda:0 f32[16]\"\n",
       "  del t495, t4\n",
       "  t2002 = torch.reshape(t2001, (-1, 16))  # t2002: \"cuda:0 f32[128, 16]\"\n",
       "    # t2002 = ltorch.reshape(t2001, (-1, 16))  # t2002: \"cuda:0 f32[128, 16]\"\n",
       "      # t2002 = prims.reshape(t2001, (128, 16))  # t2002: \"cuda:0 f32[128, 16]\"\n",
       "  del t2001\n",
       "  t2003 = torch.matmul(t2002, t_fcs_99_weight)  # t2003: \"cuda:0 f32[128, 16]\"\n",
       "    # t2003 = ltorch.matmul(t2002, t_fcs_99_weight)  # t2003: \"cuda:0 f32[128, 16]\"\n",
       "      # t2003 = prims.matmul(t2002, t_fcs_99_weight)  # t2003: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_99_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t490 = load_to_gpu(offloaded_t490, 'cuda:0')  # t490: \"cuda:0 b8[128, 16]\"\n",
       "  [t2010, t2017] = nvFusion1(t490, t2003)\n",
       "    # t2010 = prims.where(t490, t2003, 0.0)  # t2010: \"cuda:0 f32[128, 16]\"\n",
       "    # t2017 = prims.sum(t2010, (0,))  # t2017: \"cuda:0 f32[16]\"\n",
       "  del t490, t2003\n",
       "  t2011 = torch.reshape(t2010, (-1, 16))  # t2011: \"cuda:0 f32[128, 16]\"\n",
       "    # t2011 = ltorch.reshape(t2010, (-1, 16))  # t2011: \"cuda:0 f32[128, 16]\"\n",
       "      # t2011 = prims.reshape(t2010, (128, 16))  # t2011: \"cuda:0 f32[128, 16]\"\n",
       "  del t2010\n",
       "  t2012 = torch.matmul(t2011, t_fcs_98_weight)  # t2012: \"cuda:0 f32[128, 16]\"\n",
       "    # t2012 = ltorch.matmul(t2011, t_fcs_98_weight)  # t2012: \"cuda:0 f32[128, 16]\"\n",
       "      # t2012 = prims.matmul(t2011, t_fcs_98_weight)  # t2012: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_98_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t485 = load_to_gpu(offloaded_t485, 'cuda:0')  # t485: \"cuda:0 b8[128, 16]\"\n",
       "  [t2019, t2026] = nvFusion2(t485, t2012)\n",
       "    # t2019 = prims.where(t485, t2012, 0.0)  # t2019: \"cuda:0 f32[128, 16]\"\n",
       "    # t2026 = prims.sum(t2019, (0,))  # t2026: \"cuda:0 f32[16]\"\n",
       "  del t485, t2012\n",
       "  t2020 = torch.reshape(t2019, (-1, 16))  # t2020: \"cuda:0 f32[128, 16]\"\n",
       "    # t2020 = ltorch.reshape(t2019, (-1, 16))  # t2020: \"cuda:0 f32[128, 16]\"\n",
       "      # t2020 = prims.reshape(t2019, (128, 16))  # t2020: \"cuda:0 f32[128, 16]\"\n",
       "  del t2019\n",
       "  t2021 = torch.matmul(t2020, t_fcs_97_weight)  # t2021: \"cuda:0 f32[128, 16]\"\n",
       "    # t2021 = ltorch.matmul(t2020, t_fcs_97_weight)  # t2021: \"cuda:0 f32[128, 16]\"\n",
       "      # t2021 = prims.matmul(t2020, t_fcs_97_weight)  # t2021: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_97_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t480 = load_to_gpu(offloaded_t480, 'cuda:0')  # t480: \"cuda:0 b8[128, 16]\"\n",
       "  [t2028, t2035] = nvFusion3(t480, t2021)\n",
       "    # t2028 = prims.where(t480, t2021, 0.0)  # t2028: \"cuda:0 f32[128, 16]\"\n",
       "    # t2035 = prims.sum(t2028, (0,))  # t2035: \"cuda:0 f32[16]\"\n",
       "  del t480, t2021\n",
       "  t2029 = torch.reshape(t2028, (-1, 16))  # t2029: \"cuda:0 f32[128, 16]\"\n",
       "    # t2029 = ltorch.reshape(t2028, (-1, 16))  # t2029: \"cuda:0 f32[128, 16]\"\n",
       "      # t2029 = prims.reshape(t2028, (128, 16))  # t2029: \"cuda:0 f32[128, 16]\"\n",
       "  del t2028\n",
       "  t2030 = torch.matmul(t2029, t_fcs_96_weight)  # t2030: \"cuda:0 f32[128, 16]\"\n",
       "    # t2030 = ltorch.matmul(t2029, t_fcs_96_weight)  # t2030: \"cuda:0 f32[128, 16]\"\n",
       "      # t2030 = prims.matmul(t2029, t_fcs_96_weight)  # t2030: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_96_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t475 = load_to_gpu(offloaded_t475, 'cuda:0')  # t475: \"cuda:0 b8[128, 16]\"\n",
       "  [t2037, t2044] = nvFusion4(t475, t2030)\n",
       "    # t2037 = prims.where(t475, t2030, 0.0)  # t2037: \"cuda:0 f32[128, 16]\"\n",
       "    # t2044 = prims.sum(t2037, (0,))  # t2044: \"cuda:0 f32[16]\"\n",
       "  del t475, t2030\n",
       "  t2038 = torch.reshape(t2037, (-1, 16))  # t2038: \"cuda:0 f32[128, 16]\"\n",
       "    # t2038 = ltorch.reshape(t2037, (-1, 16))  # t2038: \"cuda:0 f32[128, 16]\"\n",
       "      # t2038 = prims.reshape(t2037, (128, 16))  # t2038: \"cuda:0 f32[128, 16]\"\n",
       "  del t2037\n",
       "  t2039 = torch.matmul(t2038, t_fcs_95_weight)  # t2039: \"cuda:0 f32[128, 16]\"\n",
       "    # t2039 = ltorch.matmul(t2038, t_fcs_95_weight)  # t2039: \"cuda:0 f32[128, 16]\"\n",
       "      # t2039 = prims.matmul(t2038, t_fcs_95_weight)  # t2039: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_95_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t470 = load_to_gpu(offloaded_t470, 'cuda:0')  # t470: \"cuda:0 b8[128, 16]\"\n",
       "  [t2046, t2053] = nvFusion5(t470, t2039)\n",
       "    # t2046 = prims.where(t470, t2039, 0.0)  # t2046: \"cuda:0 f32[128, 16]\"\n",
       "    # t2053 = prims.sum(t2046, (0,))  # t2053: \"cuda:0 f32[16]\"\n",
       "  del t470, t2039\n",
       "  t2047 = torch.reshape(t2046, (-1, 16))  # t2047: \"cuda:0 f32[128, 16]\"\n",
       "    # t2047 = ltorch.reshape(t2046, (-1, 16))  # t2047: \"cuda:0 f32[128, 16]\"\n",
       "      # t2047 = prims.reshape(t2046, (128, 16))  # t2047: \"cuda:0 f32[128, 16]\"\n",
       "  del t2046\n",
       "  t2048 = torch.matmul(t2047, t_fcs_94_weight)  # t2048: \"cuda:0 f32[128, 16]\"\n",
       "    # t2048 = ltorch.matmul(t2047, t_fcs_94_weight)  # t2048: \"cuda:0 f32[128, 16]\"\n",
       "      # t2048 = prims.matmul(t2047, t_fcs_94_weight)  # t2048: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_94_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t465 = load_to_gpu(offloaded_t465, 'cuda:0')  # t465: \"cuda:0 b8[128, 16]\"\n",
       "  [t2055, t2062] = nvFusion6(t465, t2048)\n",
       "    # t2055 = prims.where(t465, t2048, 0.0)  # t2055: \"cuda:0 f32[128, 16]\"\n",
       "    # t2062 = prims.sum(t2055, (0,))  # t2062: \"cuda:0 f32[16]\"\n",
       "  del t465, t2048\n",
       "  t2056 = torch.reshape(t2055, (-1, 16))  # t2056: \"cuda:0 f32[128, 16]\"\n",
       "    # t2056 = ltorch.reshape(t2055, (-1, 16))  # t2056: \"cuda:0 f32[128, 16]\"\n",
       "      # t2056 = prims.reshape(t2055, (128, 16))  # t2056: \"cuda:0 f32[128, 16]\"\n",
       "  del t2055\n",
       "  t2057 = torch.matmul(t2056, t_fcs_93_weight)  # t2057: \"cuda:0 f32[128, 16]\"\n",
       "    # t2057 = ltorch.matmul(t2056, t_fcs_93_weight)  # t2057: \"cuda:0 f32[128, 16]\"\n",
       "      # t2057 = prims.matmul(t2056, t_fcs_93_weight)  # t2057: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_93_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t460 = load_to_gpu(offloaded_t460, 'cuda:0')  # t460: \"cuda:0 b8[128, 16]\"\n",
       "  [t2064, t2071] = nvFusion7(t460, t2057)\n",
       "    # t2064 = prims.where(t460, t2057, 0.0)  # t2064: \"cuda:0 f32[128, 16]\"\n",
       "    # t2071 = prims.sum(t2064, (0,))  # t2071: \"cuda:0 f32[16]\"\n",
       "  del t460, t2057\n",
       "  t2065 = torch.reshape(t2064, (-1, 16))  # t2065: \"cuda:0 f32[128, 16]\"\n",
       "    # t2065 = ltorch.reshape(t2064, (-1, 16))  # t2065: \"cuda:0 f32[128, 16]\"\n",
       "      # t2065 = prims.reshape(t2064, (128, 16))  # t2065: \"cuda:0 f32[128, 16]\"\n",
       "  del t2064\n",
       "  t2066 = torch.matmul(t2065, t_fcs_92_weight)  # t2066: \"cuda:0 f32[128, 16]\"\n",
       "    # t2066 = ltorch.matmul(t2065, t_fcs_92_weight)  # t2066: \"cuda:0 f32[128, 16]\"\n",
       "      # t2066 = prims.matmul(t2065, t_fcs_92_weight)  # t2066: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_92_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t455 = load_to_gpu(offloaded_t455, 'cuda:0')  # t455: \"cuda:0 b8[128, 16]\"\n",
       "  [t2073, t2080] = nvFusion8(t455, t2066)\n",
       "    # t2073 = prims.where(t455, t2066, 0.0)  # t2073: \"cuda:0 f32[128, 16]\"\n",
       "    # t2080 = prims.sum(t2073, (0,))  # t2080: \"cuda:0 f32[16]\"\n",
       "  del t455, t2066\n",
       "  t2074 = torch.reshape(t2073, (-1, 16))  # t2074: \"cuda:0 f32[128, 16]\"\n",
       "    # t2074 = ltorch.reshape(t2073, (-1, 16))  # t2074: \"cuda:0 f32[128, 16]\"\n",
       "      # t2074 = prims.reshape(t2073, (128, 16))  # t2074: \"cuda:0 f32[128, 16]\"\n",
       "  del t2073\n",
       "  t2075 = torch.matmul(t2074, t_fcs_91_weight)  # t2075: \"cuda:0 f32[128, 16]\"\n",
       "    # t2075 = ltorch.matmul(t2074, t_fcs_91_weight)  # t2075: \"cuda:0 f32[128, 16]\"\n",
       "      # t2075 = prims.matmul(t2074, t_fcs_91_weight)  # t2075: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_91_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t450 = load_to_gpu(offloaded_t450, 'cuda:0')  # t450: \"cuda:0 b8[128, 16]\"\n",
       "  [t2082, t2089] = nvFusion9(t450, t2075)\n",
       "    # t2082 = prims.where(t450, t2075, 0.0)  # t2082: \"cuda:0 f32[128, 16]\"\n",
       "    # t2089 = prims.sum(t2082, (0,))  # t2089: \"cuda:0 f32[16]\"\n",
       "  del t450, t2075\n",
       "  t2083 = torch.reshape(t2082, (-1, 16))  # t2083: \"cuda:0 f32[128, 16]\"\n",
       "    # t2083 = ltorch.reshape(t2082, (-1, 16))  # t2083: \"cuda:0 f32[128, 16]\"\n",
       "      # t2083 = prims.reshape(t2082, (128, 16))  # t2083: \"cuda:0 f32[128, 16]\"\n",
       "  del t2082\n",
       "  t2084 = torch.matmul(t2083, t_fcs_90_weight)  # t2084: \"cuda:0 f32[128, 16]\"\n",
       "    # t2084 = ltorch.matmul(t2083, t_fcs_90_weight)  # t2084: \"cuda:0 f32[128, 16]\"\n",
       "      # t2084 = prims.matmul(t2083, t_fcs_90_weight)  # t2084: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_90_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t445 = load_to_gpu(offloaded_t445, 'cuda:0')  # t445: \"cuda:0 b8[128, 16]\"\n",
       "  [t2091, t2098] = nvFusion10(t445, t2084)\n",
       "    # t2091 = prims.where(t445, t2084, 0.0)  # t2091: \"cuda:0 f32[128, 16]\"\n",
       "    # t2098 = prims.sum(t2091, (0,))  # t2098: \"cuda:0 f32[16]\"\n",
       "  del t445, t2084\n",
       "  t2092 = torch.reshape(t2091, (-1, 16))  # t2092: \"cuda:0 f32[128, 16]\"\n",
       "    # t2092 = ltorch.reshape(t2091, (-1, 16))  # t2092: \"cuda:0 f32[128, 16]\"\n",
       "      # t2092 = prims.reshape(t2091, (128, 16))  # t2092: \"cuda:0 f32[128, 16]\"\n",
       "  del t2091\n",
       "  t2093 = torch.matmul(t2092, t_fcs_89_weight)  # t2093: \"cuda:0 f32[128, 16]\"\n",
       "    # t2093 = ltorch.matmul(t2092, t_fcs_89_weight)  # t2093: \"cuda:0 f32[128, 16]\"\n",
       "      # t2093 = prims.matmul(t2092, t_fcs_89_weight)  # t2093: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_89_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t440 = load_to_gpu(offloaded_t440, 'cuda:0')  # t440: \"cuda:0 b8[128, 16]\"\n",
       "  [t2100, t2107] = nvFusion11(t440, t2093)\n",
       "    # t2100 = prims.where(t440, t2093, 0.0)  # t2100: \"cuda:0 f32[128, 16]\"\n",
       "    # t2107 = prims.sum(t2100, (0,))  # t2107: \"cuda:0 f32[16]\"\n",
       "  del t440, t2093\n",
       "  t2101 = torch.reshape(t2100, (-1, 16))  # t2101: \"cuda:0 f32[128, 16]\"\n",
       "    # t2101 = ltorch.reshape(t2100, (-1, 16))  # t2101: \"cuda:0 f32[128, 16]\"\n",
       "      # t2101 = prims.reshape(t2100, (128, 16))  # t2101: \"cuda:0 f32[128, 16]\"\n",
       "  del t2100\n",
       "  t2102 = torch.matmul(t2101, t_fcs_88_weight)  # t2102: \"cuda:0 f32[128, 16]\"\n",
       "    # t2102 = ltorch.matmul(t2101, t_fcs_88_weight)  # t2102: \"cuda:0 f32[128, 16]\"\n",
       "      # t2102 = prims.matmul(t2101, t_fcs_88_weight)  # t2102: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_88_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t435 = load_to_gpu(offloaded_t435, 'cuda:0')  # t435: \"cuda:0 b8[128, 16]\"\n",
       "  [t2109, t2116] = nvFusion12(t435, t2102)\n",
       "    # t2109 = prims.where(t435, t2102, 0.0)  # t2109: \"cuda:0 f32[128, 16]\"\n",
       "    # t2116 = prims.sum(t2109, (0,))  # t2116: \"cuda:0 f32[16]\"\n",
       "  del t435, t2102\n",
       "  t2110 = torch.reshape(t2109, (-1, 16))  # t2110: \"cuda:0 f32[128, 16]\"\n",
       "    # t2110 = ltorch.reshape(t2109, (-1, 16))  # t2110: \"cuda:0 f32[128, 16]\"\n",
       "      # t2110 = prims.reshape(t2109, (128, 16))  # t2110: \"cuda:0 f32[128, 16]\"\n",
       "  del t2109\n",
       "  t2111 = torch.matmul(t2110, t_fcs_87_weight)  # t2111: \"cuda:0 f32[128, 16]\"\n",
       "    # t2111 = ltorch.matmul(t2110, t_fcs_87_weight)  # t2111: \"cuda:0 f32[128, 16]\"\n",
       "      # t2111 = prims.matmul(t2110, t_fcs_87_weight)  # t2111: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_87_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t430 = load_to_gpu(offloaded_t430, 'cuda:0')  # t430: \"cuda:0 b8[128, 16]\"\n",
       "  [t2118, t2125] = nvFusion13(t430, t2111)\n",
       "    # t2118 = prims.where(t430, t2111, 0.0)  # t2118: \"cuda:0 f32[128, 16]\"\n",
       "    # t2125 = prims.sum(t2118, (0,))  # t2125: \"cuda:0 f32[16]\"\n",
       "  del t430, t2111\n",
       "  t2119 = torch.reshape(t2118, (-1, 16))  # t2119: \"cuda:0 f32[128, 16]\"\n",
       "    # t2119 = ltorch.reshape(t2118, (-1, 16))  # t2119: \"cuda:0 f32[128, 16]\"\n",
       "      # t2119 = prims.reshape(t2118, (128, 16))  # t2119: \"cuda:0 f32[128, 16]\"\n",
       "  del t2118\n",
       "  t2120 = torch.matmul(t2119, t_fcs_86_weight)  # t2120: \"cuda:0 f32[128, 16]\"\n",
       "    # t2120 = ltorch.matmul(t2119, t_fcs_86_weight)  # t2120: \"cuda:0 f32[128, 16]\"\n",
       "      # t2120 = prims.matmul(t2119, t_fcs_86_weight)  # t2120: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_86_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t425 = load_to_gpu(offloaded_t425, 'cuda:0')  # t425: \"cuda:0 b8[128, 16]\"\n",
       "  [t2127, t2134] = nvFusion14(t425, t2120)\n",
       "    # t2127 = prims.where(t425, t2120, 0.0)  # t2127: \"cuda:0 f32[128, 16]\"\n",
       "    # t2134 = prims.sum(t2127, (0,))  # t2134: \"cuda:0 f32[16]\"\n",
       "  del t425, t2120\n",
       "  t2128 = torch.reshape(t2127, (-1, 16))  # t2128: \"cuda:0 f32[128, 16]\"\n",
       "    # t2128 = ltorch.reshape(t2127, (-1, 16))  # t2128: \"cuda:0 f32[128, 16]\"\n",
       "      # t2128 = prims.reshape(t2127, (128, 16))  # t2128: \"cuda:0 f32[128, 16]\"\n",
       "  del t2127\n",
       "  t2129 = torch.matmul(t2128, t_fcs_85_weight)  # t2129: \"cuda:0 f32[128, 16]\"\n",
       "    # t2129 = ltorch.matmul(t2128, t_fcs_85_weight)  # t2129: \"cuda:0 f32[128, 16]\"\n",
       "      # t2129 = prims.matmul(t2128, t_fcs_85_weight)  # t2129: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_85_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t420 = load_to_gpu(offloaded_t420, 'cuda:0')  # t420: \"cuda:0 b8[128, 16]\"\n",
       "  [t2136, t2143] = nvFusion15(t420, t2129)\n",
       "    # t2136 = prims.where(t420, t2129, 0.0)  # t2136: \"cuda:0 f32[128, 16]\"\n",
       "    # t2143 = prims.sum(t2136, (0,))  # t2143: \"cuda:0 f32[16]\"\n",
       "  del t420, t2129\n",
       "  t2137 = torch.reshape(t2136, (-1, 16))  # t2137: \"cuda:0 f32[128, 16]\"\n",
       "    # t2137 = ltorch.reshape(t2136, (-1, 16))  # t2137: \"cuda:0 f32[128, 16]\"\n",
       "      # t2137 = prims.reshape(t2136, (128, 16))  # t2137: \"cuda:0 f32[128, 16]\"\n",
       "  del t2136\n",
       "  t2138 = torch.matmul(t2137, t_fcs_84_weight)  # t2138: \"cuda:0 f32[128, 16]\"\n",
       "    # t2138 = ltorch.matmul(t2137, t_fcs_84_weight)  # t2138: \"cuda:0 f32[128, 16]\"\n",
       "      # t2138 = prims.matmul(t2137, t_fcs_84_weight)  # t2138: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_84_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t415 = load_to_gpu(offloaded_t415, 'cuda:0')  # t415: \"cuda:0 b8[128, 16]\"\n",
       "  [t2145, t2152] = nvFusion16(t415, t2138)\n",
       "    # t2145 = prims.where(t415, t2138, 0.0)  # t2145: \"cuda:0 f32[128, 16]\"\n",
       "    # t2152 = prims.sum(t2145, (0,))  # t2152: \"cuda:0 f32[16]\"\n",
       "  del t415, t2138\n",
       "  t2146 = torch.reshape(t2145, (-1, 16))  # t2146: \"cuda:0 f32[128, 16]\"\n",
       "    # t2146 = ltorch.reshape(t2145, (-1, 16))  # t2146: \"cuda:0 f32[128, 16]\"\n",
       "      # t2146 = prims.reshape(t2145, (128, 16))  # t2146: \"cuda:0 f32[128, 16]\"\n",
       "  del t2145\n",
       "  t2147 = torch.matmul(t2146, t_fcs_83_weight)  # t2147: \"cuda:0 f32[128, 16]\"\n",
       "    # t2147 = ltorch.matmul(t2146, t_fcs_83_weight)  # t2147: \"cuda:0 f32[128, 16]\"\n",
       "      # t2147 = prims.matmul(t2146, t_fcs_83_weight)  # t2147: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_83_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t410 = load_to_gpu(offloaded_t410, 'cuda:0')  # t410: \"cuda:0 b8[128, 16]\"\n",
       "  [t2154, t2161] = nvFusion17(t410, t2147)\n",
       "    # t2154 = prims.where(t410, t2147, 0.0)  # t2154: \"cuda:0 f32[128, 16]\"\n",
       "    # t2161 = prims.sum(t2154, (0,))  # t2161: \"cuda:0 f32[16]\"\n",
       "  del t410, t2147\n",
       "  t2155 = torch.reshape(t2154, (-1, 16))  # t2155: \"cuda:0 f32[128, 16]\"\n",
       "    # t2155 = ltorch.reshape(t2154, (-1, 16))  # t2155: \"cuda:0 f32[128, 16]\"\n",
       "      # t2155 = prims.reshape(t2154, (128, 16))  # t2155: \"cuda:0 f32[128, 16]\"\n",
       "  del t2154\n",
       "  t2156 = torch.matmul(t2155, t_fcs_82_weight)  # t2156: \"cuda:0 f32[128, 16]\"\n",
       "    # t2156 = ltorch.matmul(t2155, t_fcs_82_weight)  # t2156: \"cuda:0 f32[128, 16]\"\n",
       "      # t2156 = prims.matmul(t2155, t_fcs_82_weight)  # t2156: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_82_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t405 = load_to_gpu(offloaded_t405, 'cuda:0')  # t405: \"cuda:0 b8[128, 16]\"\n",
       "  [t2163, t2170] = nvFusion18(t405, t2156)\n",
       "    # t2163 = prims.where(t405, t2156, 0.0)  # t2163: \"cuda:0 f32[128, 16]\"\n",
       "    # t2170 = prims.sum(t2163, (0,))  # t2170: \"cuda:0 f32[16]\"\n",
       "  del t405, t2156\n",
       "  t2164 = torch.reshape(t2163, (-1, 16))  # t2164: \"cuda:0 f32[128, 16]\"\n",
       "    # t2164 = ltorch.reshape(t2163, (-1, 16))  # t2164: \"cuda:0 f32[128, 16]\"\n",
       "      # t2164 = prims.reshape(t2163, (128, 16))  # t2164: \"cuda:0 f32[128, 16]\"\n",
       "  del t2163\n",
       "  t2165 = torch.matmul(t2164, t_fcs_81_weight)  # t2165: \"cuda:0 f32[128, 16]\"\n",
       "    # t2165 = ltorch.matmul(t2164, t_fcs_81_weight)  # t2165: \"cuda:0 f32[128, 16]\"\n",
       "      # t2165 = prims.matmul(t2164, t_fcs_81_weight)  # t2165: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_81_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t400 = load_to_gpu(offloaded_t400, 'cuda:0')  # t400: \"cuda:0 b8[128, 16]\"\n",
       "  [t2172, t2179] = nvFusion19(t400, t2165)\n",
       "    # t2172 = prims.where(t400, t2165, 0.0)  # t2172: \"cuda:0 f32[128, 16]\"\n",
       "    # t2179 = prims.sum(t2172, (0,))  # t2179: \"cuda:0 f32[16]\"\n",
       "  del t400, t2165\n",
       "  t2173 = torch.reshape(t2172, (-1, 16))  # t2173: \"cuda:0 f32[128, 16]\"\n",
       "    # t2173 = ltorch.reshape(t2172, (-1, 16))  # t2173: \"cuda:0 f32[128, 16]\"\n",
       "      # t2173 = prims.reshape(t2172, (128, 16))  # t2173: \"cuda:0 f32[128, 16]\"\n",
       "  del t2172\n",
       "  t2174 = torch.matmul(t2173, t_fcs_80_weight)  # t2174: \"cuda:0 f32[128, 16]\"\n",
       "    # t2174 = ltorch.matmul(t2173, t_fcs_80_weight)  # t2174: \"cuda:0 f32[128, 16]\"\n",
       "      # t2174 = prims.matmul(t2173, t_fcs_80_weight)  # t2174: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_80_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t395 = load_to_gpu(offloaded_t395, 'cuda:0')  # t395: \"cuda:0 b8[128, 16]\"\n",
       "  [t2181, t2188] = nvFusion20(t395, t2174)\n",
       "    # t2181 = prims.where(t395, t2174, 0.0)  # t2181: \"cuda:0 f32[128, 16]\"\n",
       "    # t2188 = prims.sum(t2181, (0,))  # t2188: \"cuda:0 f32[16]\"\n",
       "  del t395, t2174\n",
       "  t2182 = torch.reshape(t2181, (-1, 16))  # t2182: \"cuda:0 f32[128, 16]\"\n",
       "    # t2182 = ltorch.reshape(t2181, (-1, 16))  # t2182: \"cuda:0 f32[128, 16]\"\n",
       "      # t2182 = prims.reshape(t2181, (128, 16))  # t2182: \"cuda:0 f32[128, 16]\"\n",
       "  del t2181\n",
       "  t2183 = torch.matmul(t2182, t_fcs_79_weight)  # t2183: \"cuda:0 f32[128, 16]\"\n",
       "    # t2183 = ltorch.matmul(t2182, t_fcs_79_weight)  # t2183: \"cuda:0 f32[128, 16]\"\n",
       "      # t2183 = prims.matmul(t2182, t_fcs_79_weight)  # t2183: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_79_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t390 = load_to_gpu(offloaded_t390, 'cuda:0')  # t390: \"cuda:0 b8[128, 16]\"\n",
       "  [t2190, t2197] = nvFusion21(t390, t2183)\n",
       "    # t2190 = prims.where(t390, t2183, 0.0)  # t2190: \"cuda:0 f32[128, 16]\"\n",
       "    # t2197 = prims.sum(t2190, (0,))  # t2197: \"cuda:0 f32[16]\"\n",
       "  del t390, t2183\n",
       "  t2191 = torch.reshape(t2190, (-1, 16))  # t2191: \"cuda:0 f32[128, 16]\"\n",
       "    # t2191 = ltorch.reshape(t2190, (-1, 16))  # t2191: \"cuda:0 f32[128, 16]\"\n",
       "      # t2191 = prims.reshape(t2190, (128, 16))  # t2191: \"cuda:0 f32[128, 16]\"\n",
       "  del t2190\n",
       "  t2192 = torch.matmul(t2191, t_fcs_78_weight)  # t2192: \"cuda:0 f32[128, 16]\"\n",
       "    # t2192 = ltorch.matmul(t2191, t_fcs_78_weight)  # t2192: \"cuda:0 f32[128, 16]\"\n",
       "      # t2192 = prims.matmul(t2191, t_fcs_78_weight)  # t2192: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_78_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t385 = load_to_gpu(offloaded_t385, 'cuda:0')  # t385: \"cuda:0 b8[128, 16]\"\n",
       "  [t2199, t2206] = nvFusion22(t385, t2192)\n",
       "    # t2199 = prims.where(t385, t2192, 0.0)  # t2199: \"cuda:0 f32[128, 16]\"\n",
       "    # t2206 = prims.sum(t2199, (0,))  # t2206: \"cuda:0 f32[16]\"\n",
       "  del t385, t2192\n",
       "  t2200 = torch.reshape(t2199, (-1, 16))  # t2200: \"cuda:0 f32[128, 16]\"\n",
       "    # t2200 = ltorch.reshape(t2199, (-1, 16))  # t2200: \"cuda:0 f32[128, 16]\"\n",
       "      # t2200 = prims.reshape(t2199, (128, 16))  # t2200: \"cuda:0 f32[128, 16]\"\n",
       "  del t2199\n",
       "  t2201 = torch.matmul(t2200, t_fcs_77_weight)  # t2201: \"cuda:0 f32[128, 16]\"\n",
       "    # t2201 = ltorch.matmul(t2200, t_fcs_77_weight)  # t2201: \"cuda:0 f32[128, 16]\"\n",
       "      # t2201 = prims.matmul(t2200, t_fcs_77_weight)  # t2201: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_77_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t380 = load_to_gpu(offloaded_t380, 'cuda:0')  # t380: \"cuda:0 b8[128, 16]\"\n",
       "  [t2208, t2215] = nvFusion23(t380, t2201)\n",
       "    # t2208 = prims.where(t380, t2201, 0.0)  # t2208: \"cuda:0 f32[128, 16]\"\n",
       "    # t2215 = prims.sum(t2208, (0,))  # t2215: \"cuda:0 f32[16]\"\n",
       "  del t380, t2201\n",
       "  t2209 = torch.reshape(t2208, (-1, 16))  # t2209: \"cuda:0 f32[128, 16]\"\n",
       "    # t2209 = ltorch.reshape(t2208, (-1, 16))  # t2209: \"cuda:0 f32[128, 16]\"\n",
       "      # t2209 = prims.reshape(t2208, (128, 16))  # t2209: \"cuda:0 f32[128, 16]\"\n",
       "  del t2208\n",
       "  t2210 = torch.matmul(t2209, t_fcs_76_weight)  # t2210: \"cuda:0 f32[128, 16]\"\n",
       "    # t2210 = ltorch.matmul(t2209, t_fcs_76_weight)  # t2210: \"cuda:0 f32[128, 16]\"\n",
       "      # t2210 = prims.matmul(t2209, t_fcs_76_weight)  # t2210: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_76_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t375 = load_to_gpu(offloaded_t375, 'cuda:0')  # t375: \"cuda:0 b8[128, 16]\"\n",
       "  [t2217, t2224] = nvFusion24(t375, t2210)\n",
       "    # t2217 = prims.where(t375, t2210, 0.0)  # t2217: \"cuda:0 f32[128, 16]\"\n",
       "    # t2224 = prims.sum(t2217, (0,))  # t2224: \"cuda:0 f32[16]\"\n",
       "  del t375, t2210\n",
       "  t2218 = torch.reshape(t2217, (-1, 16))  # t2218: \"cuda:0 f32[128, 16]\"\n",
       "    # t2218 = ltorch.reshape(t2217, (-1, 16))  # t2218: \"cuda:0 f32[128, 16]\"\n",
       "      # t2218 = prims.reshape(t2217, (128, 16))  # t2218: \"cuda:0 f32[128, 16]\"\n",
       "  del t2217\n",
       "  t2219 = torch.matmul(t2218, t_fcs_75_weight)  # t2219: \"cuda:0 f32[128, 16]\"\n",
       "    # t2219 = ltorch.matmul(t2218, t_fcs_75_weight)  # t2219: \"cuda:0 f32[128, 16]\"\n",
       "      # t2219 = prims.matmul(t2218, t_fcs_75_weight)  # t2219: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_75_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t370 = load_to_gpu(offloaded_t370, 'cuda:0')  # t370: \"cuda:0 b8[128, 16]\"\n",
       "  [t2226, t2233] = nvFusion25(t370, t2219)\n",
       "    # t2226 = prims.where(t370, t2219, 0.0)  # t2226: \"cuda:0 f32[128, 16]\"\n",
       "    # t2233 = prims.sum(t2226, (0,))  # t2233: \"cuda:0 f32[16]\"\n",
       "  del t370, t2219\n",
       "  t2227 = torch.reshape(t2226, (-1, 16))  # t2227: \"cuda:0 f32[128, 16]\"\n",
       "    # t2227 = ltorch.reshape(t2226, (-1, 16))  # t2227: \"cuda:0 f32[128, 16]\"\n",
       "      # t2227 = prims.reshape(t2226, (128, 16))  # t2227: \"cuda:0 f32[128, 16]\"\n",
       "  del t2226\n",
       "  t2228 = torch.matmul(t2227, t_fcs_74_weight)  # t2228: \"cuda:0 f32[128, 16]\"\n",
       "    # t2228 = ltorch.matmul(t2227, t_fcs_74_weight)  # t2228: \"cuda:0 f32[128, 16]\"\n",
       "      # t2228 = prims.matmul(t2227, t_fcs_74_weight)  # t2228: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_74_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t365 = load_to_gpu(offloaded_t365, 'cuda:0')  # t365: \"cuda:0 b8[128, 16]\"\n",
       "  [t2235, t2242] = nvFusion26(t365, t2228)\n",
       "    # t2235 = prims.where(t365, t2228, 0.0)  # t2235: \"cuda:0 f32[128, 16]\"\n",
       "    # t2242 = prims.sum(t2235, (0,))  # t2242: \"cuda:0 f32[16]\"\n",
       "  del t365, t2228\n",
       "  t2236 = torch.reshape(t2235, (-1, 16))  # t2236: \"cuda:0 f32[128, 16]\"\n",
       "    # t2236 = ltorch.reshape(t2235, (-1, 16))  # t2236: \"cuda:0 f32[128, 16]\"\n",
       "      # t2236 = prims.reshape(t2235, (128, 16))  # t2236: \"cuda:0 f32[128, 16]\"\n",
       "  del t2235\n",
       "  t2237 = torch.matmul(t2236, t_fcs_73_weight)  # t2237: \"cuda:0 f32[128, 16]\"\n",
       "    # t2237 = ltorch.matmul(t2236, t_fcs_73_weight)  # t2237: \"cuda:0 f32[128, 16]\"\n",
       "      # t2237 = prims.matmul(t2236, t_fcs_73_weight)  # t2237: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_73_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t360 = load_to_gpu(offloaded_t360, 'cuda:0')  # t360: \"cuda:0 b8[128, 16]\"\n",
       "  [t2244, t2251] = nvFusion27(t360, t2237)\n",
       "    # t2244 = prims.where(t360, t2237, 0.0)  # t2244: \"cuda:0 f32[128, 16]\"\n",
       "    # t2251 = prims.sum(t2244, (0,))  # t2251: \"cuda:0 f32[16]\"\n",
       "  del t360, t2237\n",
       "  t2245 = torch.reshape(t2244, (-1, 16))  # t2245: \"cuda:0 f32[128, 16]\"\n",
       "    # t2245 = ltorch.reshape(t2244, (-1, 16))  # t2245: \"cuda:0 f32[128, 16]\"\n",
       "      # t2245 = prims.reshape(t2244, (128, 16))  # t2245: \"cuda:0 f32[128, 16]\"\n",
       "  del t2244\n",
       "  t2246 = torch.matmul(t2245, t_fcs_72_weight)  # t2246: \"cuda:0 f32[128, 16]\"\n",
       "    # t2246 = ltorch.matmul(t2245, t_fcs_72_weight)  # t2246: \"cuda:0 f32[128, 16]\"\n",
       "      # t2246 = prims.matmul(t2245, t_fcs_72_weight)  # t2246: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_72_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t355 = load_to_gpu(offloaded_t355, 'cuda:0')  # t355: \"cuda:0 b8[128, 16]\"\n",
       "  [t2253, t2260] = nvFusion28(t355, t2246)\n",
       "    # t2253 = prims.where(t355, t2246, 0.0)  # t2253: \"cuda:0 f32[128, 16]\"\n",
       "    # t2260 = prims.sum(t2253, (0,))  # t2260: \"cuda:0 f32[16]\"\n",
       "  del t355, t2246\n",
       "  t2254 = torch.reshape(t2253, (-1, 16))  # t2254: \"cuda:0 f32[128, 16]\"\n",
       "    # t2254 = ltorch.reshape(t2253, (-1, 16))  # t2254: \"cuda:0 f32[128, 16]\"\n",
       "      # t2254 = prims.reshape(t2253, (128, 16))  # t2254: \"cuda:0 f32[128, 16]\"\n",
       "  del t2253\n",
       "  t2255 = torch.matmul(t2254, t_fcs_71_weight)  # t2255: \"cuda:0 f32[128, 16]\"\n",
       "    # t2255 = ltorch.matmul(t2254, t_fcs_71_weight)  # t2255: \"cuda:0 f32[128, 16]\"\n",
       "      # t2255 = prims.matmul(t2254, t_fcs_71_weight)  # t2255: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_71_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t350 = load_to_gpu(offloaded_t350, 'cuda:0')  # t350: \"cuda:0 b8[128, 16]\"\n",
       "  [t2262, t2269] = nvFusion29(t350, t2255)\n",
       "    # t2262 = prims.where(t350, t2255, 0.0)  # t2262: \"cuda:0 f32[128, 16]\"\n",
       "    # t2269 = prims.sum(t2262, (0,))  # t2269: \"cuda:0 f32[16]\"\n",
       "  del t350, t2255\n",
       "  t2263 = torch.reshape(t2262, (-1, 16))  # t2263: \"cuda:0 f32[128, 16]\"\n",
       "    # t2263 = ltorch.reshape(t2262, (-1, 16))  # t2263: \"cuda:0 f32[128, 16]\"\n",
       "      # t2263 = prims.reshape(t2262, (128, 16))  # t2263: \"cuda:0 f32[128, 16]\"\n",
       "  del t2262\n",
       "  t2264 = torch.matmul(t2263, t_fcs_70_weight)  # t2264: \"cuda:0 f32[128, 16]\"\n",
       "    # t2264 = ltorch.matmul(t2263, t_fcs_70_weight)  # t2264: \"cuda:0 f32[128, 16]\"\n",
       "      # t2264 = prims.matmul(t2263, t_fcs_70_weight)  # t2264: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_70_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t345 = load_to_gpu(offloaded_t345, 'cuda:0')  # t345: \"cuda:0 b8[128, 16]\"\n",
       "  [t2271, t2278] = nvFusion30(t345, t2264)\n",
       "    # t2271 = prims.where(t345, t2264, 0.0)  # t2271: \"cuda:0 f32[128, 16]\"\n",
       "    # t2278 = prims.sum(t2271, (0,))  # t2278: \"cuda:0 f32[16]\"\n",
       "  del t345, t2264\n",
       "  t2272 = torch.reshape(t2271, (-1, 16))  # t2272: \"cuda:0 f32[128, 16]\"\n",
       "    # t2272 = ltorch.reshape(t2271, (-1, 16))  # t2272: \"cuda:0 f32[128, 16]\"\n",
       "      # t2272 = prims.reshape(t2271, (128, 16))  # t2272: \"cuda:0 f32[128, 16]\"\n",
       "  del t2271\n",
       "  t2273 = torch.matmul(t2272, t_fcs_69_weight)  # t2273: \"cuda:0 f32[128, 16]\"\n",
       "    # t2273 = ltorch.matmul(t2272, t_fcs_69_weight)  # t2273: \"cuda:0 f32[128, 16]\"\n",
       "      # t2273 = prims.matmul(t2272, t_fcs_69_weight)  # t2273: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_69_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t340 = load_to_gpu(offloaded_t340, 'cuda:0')  # t340: \"cuda:0 b8[128, 16]\"\n",
       "  [t2280, t2287] = nvFusion31(t340, t2273)\n",
       "    # t2280 = prims.where(t340, t2273, 0.0)  # t2280: \"cuda:0 f32[128, 16]\"\n",
       "    # t2287 = prims.sum(t2280, (0,))  # t2287: \"cuda:0 f32[16]\"\n",
       "  del t340, t2273\n",
       "  t2281 = torch.reshape(t2280, (-1, 16))  # t2281: \"cuda:0 f32[128, 16]\"\n",
       "    # t2281 = ltorch.reshape(t2280, (-1, 16))  # t2281: \"cuda:0 f32[128, 16]\"\n",
       "      # t2281 = prims.reshape(t2280, (128, 16))  # t2281: \"cuda:0 f32[128, 16]\"\n",
       "  del t2280\n",
       "  t2282 = torch.matmul(t2281, t_fcs_68_weight)  # t2282: \"cuda:0 f32[128, 16]\"\n",
       "    # t2282 = ltorch.matmul(t2281, t_fcs_68_weight)  # t2282: \"cuda:0 f32[128, 16]\"\n",
       "      # t2282 = prims.matmul(t2281, t_fcs_68_weight)  # t2282: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_68_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t335 = load_to_gpu(offloaded_t335, 'cuda:0')  # t335: \"cuda:0 b8[128, 16]\"\n",
       "  [t2289, t2296] = nvFusion32(t335, t2282)\n",
       "    # t2289 = prims.where(t335, t2282, 0.0)  # t2289: \"cuda:0 f32[128, 16]\"\n",
       "    # t2296 = prims.sum(t2289, (0,))  # t2296: \"cuda:0 f32[16]\"\n",
       "  del t335, t2282\n",
       "  t2290 = torch.reshape(t2289, (-1, 16))  # t2290: \"cuda:0 f32[128, 16]\"\n",
       "    # t2290 = ltorch.reshape(t2289, (-1, 16))  # t2290: \"cuda:0 f32[128, 16]\"\n",
       "      # t2290 = prims.reshape(t2289, (128, 16))  # t2290: \"cuda:0 f32[128, 16]\"\n",
       "  del t2289\n",
       "  t2291 = torch.matmul(t2290, t_fcs_67_weight)  # t2291: \"cuda:0 f32[128, 16]\"\n",
       "    # t2291 = ltorch.matmul(t2290, t_fcs_67_weight)  # t2291: \"cuda:0 f32[128, 16]\"\n",
       "      # t2291 = prims.matmul(t2290, t_fcs_67_weight)  # t2291: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_67_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t330 = load_to_gpu(offloaded_t330, 'cuda:0')  # t330: \"cuda:0 b8[128, 16]\"\n",
       "  [t2298, t2305] = nvFusion33(t330, t2291)\n",
       "    # t2298 = prims.where(t330, t2291, 0.0)  # t2298: \"cuda:0 f32[128, 16]\"\n",
       "    # t2305 = prims.sum(t2298, (0,))  # t2305: \"cuda:0 f32[16]\"\n",
       "  del t330, t2291\n",
       "  t2299 = torch.reshape(t2298, (-1, 16))  # t2299: \"cuda:0 f32[128, 16]\"\n",
       "    # t2299 = ltorch.reshape(t2298, (-1, 16))  # t2299: \"cuda:0 f32[128, 16]\"\n",
       "      # t2299 = prims.reshape(t2298, (128, 16))  # t2299: \"cuda:0 f32[128, 16]\"\n",
       "  del t2298\n",
       "  t2300 = torch.matmul(t2299, t_fcs_66_weight)  # t2300: \"cuda:0 f32[128, 16]\"\n",
       "    # t2300 = ltorch.matmul(t2299, t_fcs_66_weight)  # t2300: \"cuda:0 f32[128, 16]\"\n",
       "      # t2300 = prims.matmul(t2299, t_fcs_66_weight)  # t2300: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_66_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t325 = load_to_gpu(offloaded_t325, 'cuda:0')  # t325: \"cuda:0 b8[128, 16]\"\n",
       "  [t2307, t2314] = nvFusion34(t325, t2300)\n",
       "    # t2307 = prims.where(t325, t2300, 0.0)  # t2307: \"cuda:0 f32[128, 16]\"\n",
       "    # t2314 = prims.sum(t2307, (0,))  # t2314: \"cuda:0 f32[16]\"\n",
       "  del t325, t2300\n",
       "  t2308 = torch.reshape(t2307, (-1, 16))  # t2308: \"cuda:0 f32[128, 16]\"\n",
       "    # t2308 = ltorch.reshape(t2307, (-1, 16))  # t2308: \"cuda:0 f32[128, 16]\"\n",
       "      # t2308 = prims.reshape(t2307, (128, 16))  # t2308: \"cuda:0 f32[128, 16]\"\n",
       "  del t2307\n",
       "  t2309 = torch.matmul(t2308, t_fcs_65_weight)  # t2309: \"cuda:0 f32[128, 16]\"\n",
       "    # t2309 = ltorch.matmul(t2308, t_fcs_65_weight)  # t2309: \"cuda:0 f32[128, 16]\"\n",
       "      # t2309 = prims.matmul(t2308, t_fcs_65_weight)  # t2309: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_65_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t320 = load_to_gpu(offloaded_t320, 'cuda:0')  # t320: \"cuda:0 b8[128, 16]\"\n",
       "  [t2316, t2323] = nvFusion35(t320, t2309)\n",
       "    # t2316 = prims.where(t320, t2309, 0.0)  # t2316: \"cuda:0 f32[128, 16]\"\n",
       "    # t2323 = prims.sum(t2316, (0,))  # t2323: \"cuda:0 f32[16]\"\n",
       "  del t320, t2309\n",
       "  t2317 = torch.reshape(t2316, (-1, 16))  # t2317: \"cuda:0 f32[128, 16]\"\n",
       "    # t2317 = ltorch.reshape(t2316, (-1, 16))  # t2317: \"cuda:0 f32[128, 16]\"\n",
       "      # t2317 = prims.reshape(t2316, (128, 16))  # t2317: \"cuda:0 f32[128, 16]\"\n",
       "  del t2316\n",
       "  t2318 = torch.matmul(t2317, t_fcs_64_weight)  # t2318: \"cuda:0 f32[128, 16]\"\n",
       "    # t2318 = ltorch.matmul(t2317, t_fcs_64_weight)  # t2318: \"cuda:0 f32[128, 16]\"\n",
       "      # t2318 = prims.matmul(t2317, t_fcs_64_weight)  # t2318: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_64_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t315 = load_to_gpu(offloaded_t315, 'cuda:0')  # t315: \"cuda:0 b8[128, 16]\"\n",
       "  [t2325, t2332] = nvFusion36(t315, t2318)\n",
       "    # t2325 = prims.where(t315, t2318, 0.0)  # t2325: \"cuda:0 f32[128, 16]\"\n",
       "    # t2332 = prims.sum(t2325, (0,))  # t2332: \"cuda:0 f32[16]\"\n",
       "  del t315, t2318\n",
       "  t2326 = torch.reshape(t2325, (-1, 16))  # t2326: \"cuda:0 f32[128, 16]\"\n",
       "    # t2326 = ltorch.reshape(t2325, (-1, 16))  # t2326: \"cuda:0 f32[128, 16]\"\n",
       "      # t2326 = prims.reshape(t2325, (128, 16))  # t2326: \"cuda:0 f32[128, 16]\"\n",
       "  del t2325\n",
       "  t2327 = torch.matmul(t2326, t_fcs_63_weight)  # t2327: \"cuda:0 f32[128, 16]\"\n",
       "    # t2327 = ltorch.matmul(t2326, t_fcs_63_weight)  # t2327: \"cuda:0 f32[128, 16]\"\n",
       "      # t2327 = prims.matmul(t2326, t_fcs_63_weight)  # t2327: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_63_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t310 = load_to_gpu(offloaded_t310, 'cuda:0')  # t310: \"cuda:0 b8[128, 16]\"\n",
       "  [t2334, t2341] = nvFusion37(t310, t2327)\n",
       "    # t2334 = prims.where(t310, t2327, 0.0)  # t2334: \"cuda:0 f32[128, 16]\"\n",
       "    # t2341 = prims.sum(t2334, (0,))  # t2341: \"cuda:0 f32[16]\"\n",
       "  del t310, t2327\n",
       "  t2335 = torch.reshape(t2334, (-1, 16))  # t2335: \"cuda:0 f32[128, 16]\"\n",
       "    # t2335 = ltorch.reshape(t2334, (-1, 16))  # t2335: \"cuda:0 f32[128, 16]\"\n",
       "      # t2335 = prims.reshape(t2334, (128, 16))  # t2335: \"cuda:0 f32[128, 16]\"\n",
       "  del t2334\n",
       "  t2336 = torch.matmul(t2335, t_fcs_62_weight)  # t2336: \"cuda:0 f32[128, 16]\"\n",
       "    # t2336 = ltorch.matmul(t2335, t_fcs_62_weight)  # t2336: \"cuda:0 f32[128, 16]\"\n",
       "      # t2336 = prims.matmul(t2335, t_fcs_62_weight)  # t2336: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_62_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t305 = load_to_gpu(offloaded_t305, 'cuda:0')  # t305: \"cuda:0 b8[128, 16]\"\n",
       "  [t2343, t2350] = nvFusion38(t305, t2336)\n",
       "    # t2343 = prims.where(t305, t2336, 0.0)  # t2343: \"cuda:0 f32[128, 16]\"\n",
       "    # t2350 = prims.sum(t2343, (0,))  # t2350: \"cuda:0 f32[16]\"\n",
       "  del t305, t2336\n",
       "  t2344 = torch.reshape(t2343, (-1, 16))  # t2344: \"cuda:0 f32[128, 16]\"\n",
       "    # t2344 = ltorch.reshape(t2343, (-1, 16))  # t2344: \"cuda:0 f32[128, 16]\"\n",
       "      # t2344 = prims.reshape(t2343, (128, 16))  # t2344: \"cuda:0 f32[128, 16]\"\n",
       "  del t2343\n",
       "  t2345 = torch.matmul(t2344, t_fcs_61_weight)  # t2345: \"cuda:0 f32[128, 16]\"\n",
       "    # t2345 = ltorch.matmul(t2344, t_fcs_61_weight)  # t2345: \"cuda:0 f32[128, 16]\"\n",
       "      # t2345 = prims.matmul(t2344, t_fcs_61_weight)  # t2345: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_61_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t300 = load_to_gpu(offloaded_t300, 'cuda:0')  # t300: \"cuda:0 b8[128, 16]\"\n",
       "  [t2352, t2359] = nvFusion39(t300, t2345)\n",
       "    # t2352 = prims.where(t300, t2345, 0.0)  # t2352: \"cuda:0 f32[128, 16]\"\n",
       "    # t2359 = prims.sum(t2352, (0,))  # t2359: \"cuda:0 f32[16]\"\n",
       "  del t300, t2345\n",
       "  t2353 = torch.reshape(t2352, (-1, 16))  # t2353: \"cuda:0 f32[128, 16]\"\n",
       "    # t2353 = ltorch.reshape(t2352, (-1, 16))  # t2353: \"cuda:0 f32[128, 16]\"\n",
       "      # t2353 = prims.reshape(t2352, (128, 16))  # t2353: \"cuda:0 f32[128, 16]\"\n",
       "  del t2352\n",
       "  t2354 = torch.matmul(t2353, t_fcs_60_weight)  # t2354: \"cuda:0 f32[128, 16]\"\n",
       "    # t2354 = ltorch.matmul(t2353, t_fcs_60_weight)  # t2354: \"cuda:0 f32[128, 16]\"\n",
       "      # t2354 = prims.matmul(t2353, t_fcs_60_weight)  # t2354: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_60_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t295 = load_to_gpu(offloaded_t295, 'cuda:0')  # t295: \"cuda:0 b8[128, 16]\"\n",
       "  [t2361, t2368] = nvFusion40(t295, t2354)\n",
       "    # t2361 = prims.where(t295, t2354, 0.0)  # t2361: \"cuda:0 f32[128, 16]\"\n",
       "    # t2368 = prims.sum(t2361, (0,))  # t2368: \"cuda:0 f32[16]\"\n",
       "  del t295, t2354\n",
       "  t2362 = torch.reshape(t2361, (-1, 16))  # t2362: \"cuda:0 f32[128, 16]\"\n",
       "    # t2362 = ltorch.reshape(t2361, (-1, 16))  # t2362: \"cuda:0 f32[128, 16]\"\n",
       "      # t2362 = prims.reshape(t2361, (128, 16))  # t2362: \"cuda:0 f32[128, 16]\"\n",
       "  del t2361\n",
       "  t2363 = torch.matmul(t2362, t_fcs_59_weight)  # t2363: \"cuda:0 f32[128, 16]\"\n",
       "    # t2363 = ltorch.matmul(t2362, t_fcs_59_weight)  # t2363: \"cuda:0 f32[128, 16]\"\n",
       "      # t2363 = prims.matmul(t2362, t_fcs_59_weight)  # t2363: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_59_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t290 = load_to_gpu(offloaded_t290, 'cuda:0')  # t290: \"cuda:0 b8[128, 16]\"\n",
       "  [t2370, t2377] = nvFusion41(t290, t2363)\n",
       "    # t2370 = prims.where(t290, t2363, 0.0)  # t2370: \"cuda:0 f32[128, 16]\"\n",
       "    # t2377 = prims.sum(t2370, (0,))  # t2377: \"cuda:0 f32[16]\"\n",
       "  del t290, t2363\n",
       "  t2371 = torch.reshape(t2370, (-1, 16))  # t2371: \"cuda:0 f32[128, 16]\"\n",
       "    # t2371 = ltorch.reshape(t2370, (-1, 16))  # t2371: \"cuda:0 f32[128, 16]\"\n",
       "      # t2371 = prims.reshape(t2370, (128, 16))  # t2371: \"cuda:0 f32[128, 16]\"\n",
       "  del t2370\n",
       "  t2372 = torch.matmul(t2371, t_fcs_58_weight)  # t2372: \"cuda:0 f32[128, 16]\"\n",
       "    # t2372 = ltorch.matmul(t2371, t_fcs_58_weight)  # t2372: \"cuda:0 f32[128, 16]\"\n",
       "      # t2372 = prims.matmul(t2371, t_fcs_58_weight)  # t2372: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_58_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t285 = load_to_gpu(offloaded_t285, 'cuda:0')  # t285: \"cuda:0 b8[128, 16]\"\n",
       "  [t2379, t2386] = nvFusion42(t285, t2372)\n",
       "    # t2379 = prims.where(t285, t2372, 0.0)  # t2379: \"cuda:0 f32[128, 16]\"\n",
       "    # t2386 = prims.sum(t2379, (0,))  # t2386: \"cuda:0 f32[16]\"\n",
       "  del t285, t2372\n",
       "  t2380 = torch.reshape(t2379, (-1, 16))  # t2380: \"cuda:0 f32[128, 16]\"\n",
       "    # t2380 = ltorch.reshape(t2379, (-1, 16))  # t2380: \"cuda:0 f32[128, 16]\"\n",
       "      # t2380 = prims.reshape(t2379, (128, 16))  # t2380: \"cuda:0 f32[128, 16]\"\n",
       "  del t2379\n",
       "  t2381 = torch.matmul(t2380, t_fcs_57_weight)  # t2381: \"cuda:0 f32[128, 16]\"\n",
       "    # t2381 = ltorch.matmul(t2380, t_fcs_57_weight)  # t2381: \"cuda:0 f32[128, 16]\"\n",
       "      # t2381 = prims.matmul(t2380, t_fcs_57_weight)  # t2381: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_57_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t280 = load_to_gpu(offloaded_t280, 'cuda:0')  # t280: \"cuda:0 b8[128, 16]\"\n",
       "  [t2388, t2395] = nvFusion43(t280, t2381)\n",
       "    # t2388 = prims.where(t280, t2381, 0.0)  # t2388: \"cuda:0 f32[128, 16]\"\n",
       "    # t2395 = prims.sum(t2388, (0,))  # t2395: \"cuda:0 f32[16]\"\n",
       "  del t280, t2381\n",
       "  t2389 = torch.reshape(t2388, (-1, 16))  # t2389: \"cuda:0 f32[128, 16]\"\n",
       "    # t2389 = ltorch.reshape(t2388, (-1, 16))  # t2389: \"cuda:0 f32[128, 16]\"\n",
       "      # t2389 = prims.reshape(t2388, (128, 16))  # t2389: \"cuda:0 f32[128, 16]\"\n",
       "  del t2388\n",
       "  t2390 = torch.matmul(t2389, t_fcs_56_weight)  # t2390: \"cuda:0 f32[128, 16]\"\n",
       "    # t2390 = ltorch.matmul(t2389, t_fcs_56_weight)  # t2390: \"cuda:0 f32[128, 16]\"\n",
       "      # t2390 = prims.matmul(t2389, t_fcs_56_weight)  # t2390: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_56_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t275 = load_to_gpu(offloaded_t275, 'cuda:0')  # t275: \"cuda:0 b8[128, 16]\"\n",
       "  [t2397, t2404] = nvFusion44(t275, t2390)\n",
       "    # t2397 = prims.where(t275, t2390, 0.0)  # t2397: \"cuda:0 f32[128, 16]\"\n",
       "    # t2404 = prims.sum(t2397, (0,))  # t2404: \"cuda:0 f32[16]\"\n",
       "  del t275, t2390\n",
       "  t2398 = torch.reshape(t2397, (-1, 16))  # t2398: \"cuda:0 f32[128, 16]\"\n",
       "    # t2398 = ltorch.reshape(t2397, (-1, 16))  # t2398: \"cuda:0 f32[128, 16]\"\n",
       "      # t2398 = prims.reshape(t2397, (128, 16))  # t2398: \"cuda:0 f32[128, 16]\"\n",
       "  del t2397\n",
       "  t2399 = torch.matmul(t2398, t_fcs_55_weight)  # t2399: \"cuda:0 f32[128, 16]\"\n",
       "    # t2399 = ltorch.matmul(t2398, t_fcs_55_weight)  # t2399: \"cuda:0 f32[128, 16]\"\n",
       "      # t2399 = prims.matmul(t2398, t_fcs_55_weight)  # t2399: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_55_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t270 = load_to_gpu(offloaded_t270, 'cuda:0')  # t270: \"cuda:0 b8[128, 16]\"\n",
       "  [t2406, t2413] = nvFusion45(t270, t2399)\n",
       "    # t2406 = prims.where(t270, t2399, 0.0)  # t2406: \"cuda:0 f32[128, 16]\"\n",
       "    # t2413 = prims.sum(t2406, (0,))  # t2413: \"cuda:0 f32[16]\"\n",
       "  del t270, t2399\n",
       "  t2407 = torch.reshape(t2406, (-1, 16))  # t2407: \"cuda:0 f32[128, 16]\"\n",
       "    # t2407 = ltorch.reshape(t2406, (-1, 16))  # t2407: \"cuda:0 f32[128, 16]\"\n",
       "      # t2407 = prims.reshape(t2406, (128, 16))  # t2407: \"cuda:0 f32[128, 16]\"\n",
       "  del t2406\n",
       "  t2408 = torch.matmul(t2407, t_fcs_54_weight)  # t2408: \"cuda:0 f32[128, 16]\"\n",
       "    # t2408 = ltorch.matmul(t2407, t_fcs_54_weight)  # t2408: \"cuda:0 f32[128, 16]\"\n",
       "      # t2408 = prims.matmul(t2407, t_fcs_54_weight)  # t2408: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_54_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t265 = load_to_gpu(offloaded_t265, 'cuda:0')  # t265: \"cuda:0 b8[128, 16]\"\n",
       "  [t2415, t2422] = nvFusion46(t265, t2408)\n",
       "    # t2415 = prims.where(t265, t2408, 0.0)  # t2415: \"cuda:0 f32[128, 16]\"\n",
       "    # t2422 = prims.sum(t2415, (0,))  # t2422: \"cuda:0 f32[16]\"\n",
       "  del t265, t2408\n",
       "  t2416 = torch.reshape(t2415, (-1, 16))  # t2416: \"cuda:0 f32[128, 16]\"\n",
       "    # t2416 = ltorch.reshape(t2415, (-1, 16))  # t2416: \"cuda:0 f32[128, 16]\"\n",
       "      # t2416 = prims.reshape(t2415, (128, 16))  # t2416: \"cuda:0 f32[128, 16]\"\n",
       "  del t2415\n",
       "  t2417 = torch.matmul(t2416, t_fcs_53_weight)  # t2417: \"cuda:0 f32[128, 16]\"\n",
       "    # t2417 = ltorch.matmul(t2416, t_fcs_53_weight)  # t2417: \"cuda:0 f32[128, 16]\"\n",
       "      # t2417 = prims.matmul(t2416, t_fcs_53_weight)  # t2417: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_53_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t260 = load_to_gpu(offloaded_t260, 'cuda:0')  # t260: \"cuda:0 b8[128, 16]\"\n",
       "  [t2424, t2431] = nvFusion47(t260, t2417)\n",
       "    # t2424 = prims.where(t260, t2417, 0.0)  # t2424: \"cuda:0 f32[128, 16]\"\n",
       "    # t2431 = prims.sum(t2424, (0,))  # t2431: \"cuda:0 f32[16]\"\n",
       "  del t260, t2417\n",
       "  t2425 = torch.reshape(t2424, (-1, 16))  # t2425: \"cuda:0 f32[128, 16]\"\n",
       "    # t2425 = ltorch.reshape(t2424, (-1, 16))  # t2425: \"cuda:0 f32[128, 16]\"\n",
       "      # t2425 = prims.reshape(t2424, (128, 16))  # t2425: \"cuda:0 f32[128, 16]\"\n",
       "  del t2424\n",
       "  t2426 = torch.matmul(t2425, t_fcs_52_weight)  # t2426: \"cuda:0 f32[128, 16]\"\n",
       "    # t2426 = ltorch.matmul(t2425, t_fcs_52_weight)  # t2426: \"cuda:0 f32[128, 16]\"\n",
       "      # t2426 = prims.matmul(t2425, t_fcs_52_weight)  # t2426: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_52_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t255 = load_to_gpu(offloaded_t255, 'cuda:0')  # t255: \"cuda:0 b8[128, 16]\"\n",
       "  [t2433, t2440] = nvFusion48(t255, t2426)\n",
       "    # t2433 = prims.where(t255, t2426, 0.0)  # t2433: \"cuda:0 f32[128, 16]\"\n",
       "    # t2440 = prims.sum(t2433, (0,))  # t2440: \"cuda:0 f32[16]\"\n",
       "  del t255, t2426\n",
       "  t2434 = torch.reshape(t2433, (-1, 16))  # t2434: \"cuda:0 f32[128, 16]\"\n",
       "    # t2434 = ltorch.reshape(t2433, (-1, 16))  # t2434: \"cuda:0 f32[128, 16]\"\n",
       "      # t2434 = prims.reshape(t2433, (128, 16))  # t2434: \"cuda:0 f32[128, 16]\"\n",
       "  del t2433\n",
       "  t2435 = torch.matmul(t2434, t_fcs_51_weight)  # t2435: \"cuda:0 f32[128, 16]\"\n",
       "    # t2435 = ltorch.matmul(t2434, t_fcs_51_weight)  # t2435: \"cuda:0 f32[128, 16]\"\n",
       "      # t2435 = prims.matmul(t2434, t_fcs_51_weight)  # t2435: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_51_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t250 = load_to_gpu(offloaded_t250, 'cuda:0')  # t250: \"cuda:0 b8[128, 16]\"\n",
       "  [t2442, t2449] = nvFusion49(t250, t2435)\n",
       "    # t2442 = prims.where(t250, t2435, 0.0)  # t2442: \"cuda:0 f32[128, 16]\"\n",
       "    # t2449 = prims.sum(t2442, (0,))  # t2449: \"cuda:0 f32[16]\"\n",
       "  del t250, t2435\n",
       "  t2443 = torch.reshape(t2442, (-1, 16))  # t2443: \"cuda:0 f32[128, 16]\"\n",
       "    # t2443 = ltorch.reshape(t2442, (-1, 16))  # t2443: \"cuda:0 f32[128, 16]\"\n",
       "      # t2443 = prims.reshape(t2442, (128, 16))  # t2443: \"cuda:0 f32[128, 16]\"\n",
       "  del t2442\n",
       "  t2444 = torch.matmul(t2443, t_fcs_50_weight)  # t2444: \"cuda:0 f32[128, 16]\"\n",
       "    # t2444 = ltorch.matmul(t2443, t_fcs_50_weight)  # t2444: \"cuda:0 f32[128, 16]\"\n",
       "      # t2444 = prims.matmul(t2443, t_fcs_50_weight)  # t2444: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_50_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t245 = load_to_gpu(offloaded_t245, 'cuda:0')  # t245: \"cuda:0 b8[128, 16]\"\n",
       "  [t2451, t2458] = nvFusion50(t245, t2444)\n",
       "    # t2451 = prims.where(t245, t2444, 0.0)  # t2451: \"cuda:0 f32[128, 16]\"\n",
       "    # t2458 = prims.sum(t2451, (0,))  # t2458: \"cuda:0 f32[16]\"\n",
       "  del t245, t2444\n",
       "  t2452 = torch.reshape(t2451, (-1, 16))  # t2452: \"cuda:0 f32[128, 16]\"\n",
       "    # t2452 = ltorch.reshape(t2451, (-1, 16))  # t2452: \"cuda:0 f32[128, 16]\"\n",
       "      # t2452 = prims.reshape(t2451, (128, 16))  # t2452: \"cuda:0 f32[128, 16]\"\n",
       "  del t2451\n",
       "  t2453 = torch.matmul(t2452, t_fcs_49_weight)  # t2453: \"cuda:0 f32[128, 16]\"\n",
       "    # t2453 = ltorch.matmul(t2452, t_fcs_49_weight)  # t2453: \"cuda:0 f32[128, 16]\"\n",
       "      # t2453 = prims.matmul(t2452, t_fcs_49_weight)  # t2453: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_49_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t240 = load_to_gpu(offloaded_t240, 'cuda:0')  # t240: \"cuda:0 b8[128, 16]\"\n",
       "  [t2460, t2467] = nvFusion51(t240, t2453)\n",
       "    # t2460 = prims.where(t240, t2453, 0.0)  # t2460: \"cuda:0 f32[128, 16]\"\n",
       "    # t2467 = prims.sum(t2460, (0,))  # t2467: \"cuda:0 f32[16]\"\n",
       "  del t240, t2453\n",
       "  t2461 = torch.reshape(t2460, (-1, 16))  # t2461: \"cuda:0 f32[128, 16]\"\n",
       "    # t2461 = ltorch.reshape(t2460, (-1, 16))  # t2461: \"cuda:0 f32[128, 16]\"\n",
       "      # t2461 = prims.reshape(t2460, (128, 16))  # t2461: \"cuda:0 f32[128, 16]\"\n",
       "  del t2460\n",
       "  t2462 = torch.matmul(t2461, t_fcs_48_weight)  # t2462: \"cuda:0 f32[128, 16]\"\n",
       "    # t2462 = ltorch.matmul(t2461, t_fcs_48_weight)  # t2462: \"cuda:0 f32[128, 16]\"\n",
       "      # t2462 = prims.matmul(t2461, t_fcs_48_weight)  # t2462: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_48_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t235 = load_to_gpu(offloaded_t235, 'cuda:0')  # t235: \"cuda:0 b8[128, 16]\"\n",
       "  [t2469, t2476] = nvFusion52(t235, t2462)\n",
       "    # t2469 = prims.where(t235, t2462, 0.0)  # t2469: \"cuda:0 f32[128, 16]\"\n",
       "    # t2476 = prims.sum(t2469, (0,))  # t2476: \"cuda:0 f32[16]\"\n",
       "  del t235, t2462\n",
       "  t2470 = torch.reshape(t2469, (-1, 16))  # t2470: \"cuda:0 f32[128, 16]\"\n",
       "    # t2470 = ltorch.reshape(t2469, (-1, 16))  # t2470: \"cuda:0 f32[128, 16]\"\n",
       "      # t2470 = prims.reshape(t2469, (128, 16))  # t2470: \"cuda:0 f32[128, 16]\"\n",
       "  del t2469\n",
       "  t2471 = torch.matmul(t2470, t_fcs_47_weight)  # t2471: \"cuda:0 f32[128, 16]\"\n",
       "    # t2471 = ltorch.matmul(t2470, t_fcs_47_weight)  # t2471: \"cuda:0 f32[128, 16]\"\n",
       "      # t2471 = prims.matmul(t2470, t_fcs_47_weight)  # t2471: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_47_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t230 = load_to_gpu(offloaded_t230, 'cuda:0')  # t230: \"cuda:0 b8[128, 16]\"\n",
       "  [t2478, t2485] = nvFusion53(t230, t2471)\n",
       "    # t2478 = prims.where(t230, t2471, 0.0)  # t2478: \"cuda:0 f32[128, 16]\"\n",
       "    # t2485 = prims.sum(t2478, (0,))  # t2485: \"cuda:0 f32[16]\"\n",
       "  del t230, t2471\n",
       "  t2479 = torch.reshape(t2478, (-1, 16))  # t2479: \"cuda:0 f32[128, 16]\"\n",
       "    # t2479 = ltorch.reshape(t2478, (-1, 16))  # t2479: \"cuda:0 f32[128, 16]\"\n",
       "      # t2479 = prims.reshape(t2478, (128, 16))  # t2479: \"cuda:0 f32[128, 16]\"\n",
       "  del t2478\n",
       "  t2480 = torch.matmul(t2479, t_fcs_46_weight)  # t2480: \"cuda:0 f32[128, 16]\"\n",
       "    # t2480 = ltorch.matmul(t2479, t_fcs_46_weight)  # t2480: \"cuda:0 f32[128, 16]\"\n",
       "      # t2480 = prims.matmul(t2479, t_fcs_46_weight)  # t2480: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_46_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t225 = load_to_gpu(offloaded_t225, 'cuda:0')  # t225: \"cuda:0 b8[128, 16]\"\n",
       "  [t2487, t2494] = nvFusion54(t225, t2480)\n",
       "    # t2487 = prims.where(t225, t2480, 0.0)  # t2487: \"cuda:0 f32[128, 16]\"\n",
       "    # t2494 = prims.sum(t2487, (0,))  # t2494: \"cuda:0 f32[16]\"\n",
       "  del t225, t2480\n",
       "  t2488 = torch.reshape(t2487, (-1, 16))  # t2488: \"cuda:0 f32[128, 16]\"\n",
       "    # t2488 = ltorch.reshape(t2487, (-1, 16))  # t2488: \"cuda:0 f32[128, 16]\"\n",
       "      # t2488 = prims.reshape(t2487, (128, 16))  # t2488: \"cuda:0 f32[128, 16]\"\n",
       "  del t2487\n",
       "  t2489 = torch.matmul(t2488, t_fcs_45_weight)  # t2489: \"cuda:0 f32[128, 16]\"\n",
       "    # t2489 = ltorch.matmul(t2488, t_fcs_45_weight)  # t2489: \"cuda:0 f32[128, 16]\"\n",
       "      # t2489 = prims.matmul(t2488, t_fcs_45_weight)  # t2489: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_45_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t220 = load_to_gpu(offloaded_t220, 'cuda:0')  # t220: \"cuda:0 b8[128, 16]\"\n",
       "  [t2496, t2503] = nvFusion55(t220, t2489)\n",
       "    # t2496 = prims.where(t220, t2489, 0.0)  # t2496: \"cuda:0 f32[128, 16]\"\n",
       "    # t2503 = prims.sum(t2496, (0,))  # t2503: \"cuda:0 f32[16]\"\n",
       "  del t220, t2489\n",
       "  t2497 = torch.reshape(t2496, (-1, 16))  # t2497: \"cuda:0 f32[128, 16]\"\n",
       "    # t2497 = ltorch.reshape(t2496, (-1, 16))  # t2497: \"cuda:0 f32[128, 16]\"\n",
       "      # t2497 = prims.reshape(t2496, (128, 16))  # t2497: \"cuda:0 f32[128, 16]\"\n",
       "  del t2496\n",
       "  t2498 = torch.matmul(t2497, t_fcs_44_weight)  # t2498: \"cuda:0 f32[128, 16]\"\n",
       "    # t2498 = ltorch.matmul(t2497, t_fcs_44_weight)  # t2498: \"cuda:0 f32[128, 16]\"\n",
       "      # t2498 = prims.matmul(t2497, t_fcs_44_weight)  # t2498: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_44_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t215 = load_to_gpu(offloaded_t215, 'cuda:0')  # t215: \"cuda:0 b8[128, 16]\"\n",
       "  [t2505, t2512] = nvFusion56(t215, t2498)\n",
       "    # t2505 = prims.where(t215, t2498, 0.0)  # t2505: \"cuda:0 f32[128, 16]\"\n",
       "    # t2512 = prims.sum(t2505, (0,))  # t2512: \"cuda:0 f32[16]\"\n",
       "  del t215, t2498\n",
       "  t2506 = torch.reshape(t2505, (-1, 16))  # t2506: \"cuda:0 f32[128, 16]\"\n",
       "    # t2506 = ltorch.reshape(t2505, (-1, 16))  # t2506: \"cuda:0 f32[128, 16]\"\n",
       "      # t2506 = prims.reshape(t2505, (128, 16))  # t2506: \"cuda:0 f32[128, 16]\"\n",
       "  del t2505\n",
       "  t2507 = torch.matmul(t2506, t_fcs_43_weight)  # t2507: \"cuda:0 f32[128, 16]\"\n",
       "    # t2507 = ltorch.matmul(t2506, t_fcs_43_weight)  # t2507: \"cuda:0 f32[128, 16]\"\n",
       "      # t2507 = prims.matmul(t2506, t_fcs_43_weight)  # t2507: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_43_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t210 = load_to_gpu(offloaded_t210, 'cuda:0')  # t210: \"cuda:0 b8[128, 16]\"\n",
       "  [t2514, t2521] = nvFusion57(t210, t2507)\n",
       "    # t2514 = prims.where(t210, t2507, 0.0)  # t2514: \"cuda:0 f32[128, 16]\"\n",
       "    # t2521 = prims.sum(t2514, (0,))  # t2521: \"cuda:0 f32[16]\"\n",
       "  del t210, t2507\n",
       "  t2515 = torch.reshape(t2514, (-1, 16))  # t2515: \"cuda:0 f32[128, 16]\"\n",
       "    # t2515 = ltorch.reshape(t2514, (-1, 16))  # t2515: \"cuda:0 f32[128, 16]\"\n",
       "      # t2515 = prims.reshape(t2514, (128, 16))  # t2515: \"cuda:0 f32[128, 16]\"\n",
       "  del t2514\n",
       "  t2516 = torch.matmul(t2515, t_fcs_42_weight)  # t2516: \"cuda:0 f32[128, 16]\"\n",
       "    # t2516 = ltorch.matmul(t2515, t_fcs_42_weight)  # t2516: \"cuda:0 f32[128, 16]\"\n",
       "      # t2516 = prims.matmul(t2515, t_fcs_42_weight)  # t2516: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_42_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t205 = load_to_gpu(offloaded_t205, 'cuda:0')  # t205: \"cuda:0 b8[128, 16]\"\n",
       "  [t2523, t2530] = nvFusion58(t205, t2516)\n",
       "    # t2523 = prims.where(t205, t2516, 0.0)  # t2523: \"cuda:0 f32[128, 16]\"\n",
       "    # t2530 = prims.sum(t2523, (0,))  # t2530: \"cuda:0 f32[16]\"\n",
       "  del t205, t2516\n",
       "  t2524 = torch.reshape(t2523, (-1, 16))  # t2524: \"cuda:0 f32[128, 16]\"\n",
       "    # t2524 = ltorch.reshape(t2523, (-1, 16))  # t2524: \"cuda:0 f32[128, 16]\"\n",
       "      # t2524 = prims.reshape(t2523, (128, 16))  # t2524: \"cuda:0 f32[128, 16]\"\n",
       "  del t2523\n",
       "  t2525 = torch.matmul(t2524, t_fcs_41_weight)  # t2525: \"cuda:0 f32[128, 16]\"\n",
       "    # t2525 = ltorch.matmul(t2524, t_fcs_41_weight)  # t2525: \"cuda:0 f32[128, 16]\"\n",
       "      # t2525 = prims.matmul(t2524, t_fcs_41_weight)  # t2525: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_41_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t200 = load_to_gpu(offloaded_t200, 'cuda:0')  # t200: \"cuda:0 b8[128, 16]\"\n",
       "  [t2532, t2539] = nvFusion59(t200, t2525)\n",
       "    # t2532 = prims.where(t200, t2525, 0.0)  # t2532: \"cuda:0 f32[128, 16]\"\n",
       "    # t2539 = prims.sum(t2532, (0,))  # t2539: \"cuda:0 f32[16]\"\n",
       "  del t200, t2525\n",
       "  t2533 = torch.reshape(t2532, (-1, 16))  # t2533: \"cuda:0 f32[128, 16]\"\n",
       "    # t2533 = ltorch.reshape(t2532, (-1, 16))  # t2533: \"cuda:0 f32[128, 16]\"\n",
       "      # t2533 = prims.reshape(t2532, (128, 16))  # t2533: \"cuda:0 f32[128, 16]\"\n",
       "  del t2532\n",
       "  t2534 = torch.matmul(t2533, t_fcs_40_weight)  # t2534: \"cuda:0 f32[128, 16]\"\n",
       "    # t2534 = ltorch.matmul(t2533, t_fcs_40_weight)  # t2534: \"cuda:0 f32[128, 16]\"\n",
       "      # t2534 = prims.matmul(t2533, t_fcs_40_weight)  # t2534: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_40_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t195 = load_to_gpu(offloaded_t195, 'cuda:0')  # t195: \"cuda:0 b8[128, 16]\"\n",
       "  [t2541, t2548] = nvFusion60(t195, t2534)\n",
       "    # t2541 = prims.where(t195, t2534, 0.0)  # t2541: \"cuda:0 f32[128, 16]\"\n",
       "    # t2548 = prims.sum(t2541, (0,))  # t2548: \"cuda:0 f32[16]\"\n",
       "  del t195, t2534\n",
       "  t2542 = torch.reshape(t2541, (-1, 16))  # t2542: \"cuda:0 f32[128, 16]\"\n",
       "    # t2542 = ltorch.reshape(t2541, (-1, 16))  # t2542: \"cuda:0 f32[128, 16]\"\n",
       "      # t2542 = prims.reshape(t2541, (128, 16))  # t2542: \"cuda:0 f32[128, 16]\"\n",
       "  del t2541\n",
       "  t2543 = torch.matmul(t2542, t_fcs_39_weight)  # t2543: \"cuda:0 f32[128, 16]\"\n",
       "    # t2543 = ltorch.matmul(t2542, t_fcs_39_weight)  # t2543: \"cuda:0 f32[128, 16]\"\n",
       "      # t2543 = prims.matmul(t2542, t_fcs_39_weight)  # t2543: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_39_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t190 = load_to_gpu(offloaded_t190, 'cuda:0')  # t190: \"cuda:0 b8[128, 16]\"\n",
       "  [t2550, t2557] = nvFusion61(t190, t2543)\n",
       "    # t2550 = prims.where(t190, t2543, 0.0)  # t2550: \"cuda:0 f32[128, 16]\"\n",
       "    # t2557 = prims.sum(t2550, (0,))  # t2557: \"cuda:0 f32[16]\"\n",
       "  del t190, t2543\n",
       "  t2551 = torch.reshape(t2550, (-1, 16))  # t2551: \"cuda:0 f32[128, 16]\"\n",
       "    # t2551 = ltorch.reshape(t2550, (-1, 16))  # t2551: \"cuda:0 f32[128, 16]\"\n",
       "      # t2551 = prims.reshape(t2550, (128, 16))  # t2551: \"cuda:0 f32[128, 16]\"\n",
       "  del t2550\n",
       "  t2552 = torch.matmul(t2551, t_fcs_38_weight)  # t2552: \"cuda:0 f32[128, 16]\"\n",
       "    # t2552 = ltorch.matmul(t2551, t_fcs_38_weight)  # t2552: \"cuda:0 f32[128, 16]\"\n",
       "      # t2552 = prims.matmul(t2551, t_fcs_38_weight)  # t2552: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_38_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t185 = load_to_gpu(offloaded_t185, 'cuda:0')  # t185: \"cuda:0 b8[128, 16]\"\n",
       "  [t2559, t2566] = nvFusion62(t185, t2552)\n",
       "    # t2559 = prims.where(t185, t2552, 0.0)  # t2559: \"cuda:0 f32[128, 16]\"\n",
       "    # t2566 = prims.sum(t2559, (0,))  # t2566: \"cuda:0 f32[16]\"\n",
       "  del t185, t2552\n",
       "  t2560 = torch.reshape(t2559, (-1, 16))  # t2560: \"cuda:0 f32[128, 16]\"\n",
       "    # t2560 = ltorch.reshape(t2559, (-1, 16))  # t2560: \"cuda:0 f32[128, 16]\"\n",
       "      # t2560 = prims.reshape(t2559, (128, 16))  # t2560: \"cuda:0 f32[128, 16]\"\n",
       "  del t2559\n",
       "  t2561 = torch.matmul(t2560, t_fcs_37_weight)  # t2561: \"cuda:0 f32[128, 16]\"\n",
       "    # t2561 = ltorch.matmul(t2560, t_fcs_37_weight)  # t2561: \"cuda:0 f32[128, 16]\"\n",
       "      # t2561 = prims.matmul(t2560, t_fcs_37_weight)  # t2561: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_37_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t180 = load_to_gpu(offloaded_t180, 'cuda:0')  # t180: \"cuda:0 b8[128, 16]\"\n",
       "  [t2568, t2575] = nvFusion63(t180, t2561)\n",
       "    # t2568 = prims.where(t180, t2561, 0.0)  # t2568: \"cuda:0 f32[128, 16]\"\n",
       "    # t2575 = prims.sum(t2568, (0,))  # t2575: \"cuda:0 f32[16]\"\n",
       "  del t180, t2561\n",
       "  t2569 = torch.reshape(t2568, (-1, 16))  # t2569: \"cuda:0 f32[128, 16]\"\n",
       "    # t2569 = ltorch.reshape(t2568, (-1, 16))  # t2569: \"cuda:0 f32[128, 16]\"\n",
       "      # t2569 = prims.reshape(t2568, (128, 16))  # t2569: \"cuda:0 f32[128, 16]\"\n",
       "  del t2568\n",
       "  t2570 = torch.matmul(t2569, t_fcs_36_weight)  # t2570: \"cuda:0 f32[128, 16]\"\n",
       "    # t2570 = ltorch.matmul(t2569, t_fcs_36_weight)  # t2570: \"cuda:0 f32[128, 16]\"\n",
       "      # t2570 = prims.matmul(t2569, t_fcs_36_weight)  # t2570: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_36_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t175 = load_to_gpu(offloaded_t175, 'cuda:0')  # t175: \"cuda:0 b8[128, 16]\"\n",
       "  [t2577, t2584] = nvFusion64(t175, t2570)\n",
       "    # t2577 = prims.where(t175, t2570, 0.0)  # t2577: \"cuda:0 f32[128, 16]\"\n",
       "    # t2584 = prims.sum(t2577, (0,))  # t2584: \"cuda:0 f32[16]\"\n",
       "  del t175, t2570\n",
       "  t2578 = torch.reshape(t2577, (-1, 16))  # t2578: \"cuda:0 f32[128, 16]\"\n",
       "    # t2578 = ltorch.reshape(t2577, (-1, 16))  # t2578: \"cuda:0 f32[128, 16]\"\n",
       "      # t2578 = prims.reshape(t2577, (128, 16))  # t2578: \"cuda:0 f32[128, 16]\"\n",
       "  del t2577\n",
       "  t2579 = torch.matmul(t2578, t_fcs_35_weight)  # t2579: \"cuda:0 f32[128, 16]\"\n",
       "    # t2579 = ltorch.matmul(t2578, t_fcs_35_weight)  # t2579: \"cuda:0 f32[128, 16]\"\n",
       "      # t2579 = prims.matmul(t2578, t_fcs_35_weight)  # t2579: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_35_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t170 = load_to_gpu(offloaded_t170, 'cuda:0')  # t170: \"cuda:0 b8[128, 16]\"\n",
       "  [t2586, t2593] = nvFusion65(t170, t2579)\n",
       "    # t2586 = prims.where(t170, t2579, 0.0)  # t2586: \"cuda:0 f32[128, 16]\"\n",
       "    # t2593 = prims.sum(t2586, (0,))  # t2593: \"cuda:0 f32[16]\"\n",
       "  del t170, t2579\n",
       "  t2587 = torch.reshape(t2586, (-1, 16))  # t2587: \"cuda:0 f32[128, 16]\"\n",
       "    # t2587 = ltorch.reshape(t2586, (-1, 16))  # t2587: \"cuda:0 f32[128, 16]\"\n",
       "      # t2587 = prims.reshape(t2586, (128, 16))  # t2587: \"cuda:0 f32[128, 16]\"\n",
       "  del t2586\n",
       "  t2588 = torch.matmul(t2587, t_fcs_34_weight)  # t2588: \"cuda:0 f32[128, 16]\"\n",
       "    # t2588 = ltorch.matmul(t2587, t_fcs_34_weight)  # t2588: \"cuda:0 f32[128, 16]\"\n",
       "      # t2588 = prims.matmul(t2587, t_fcs_34_weight)  # t2588: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_34_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t165 = load_to_gpu(offloaded_t165, 'cuda:0')  # t165: \"cuda:0 b8[128, 16]\"\n",
       "  [t2595, t2602] = nvFusion66(t165, t2588)\n",
       "    # t2595 = prims.where(t165, t2588, 0.0)  # t2595: \"cuda:0 f32[128, 16]\"\n",
       "    # t2602 = prims.sum(t2595, (0,))  # t2602: \"cuda:0 f32[16]\"\n",
       "  del t165, t2588\n",
       "  t2596 = torch.reshape(t2595, (-1, 16))  # t2596: \"cuda:0 f32[128, 16]\"\n",
       "    # t2596 = ltorch.reshape(t2595, (-1, 16))  # t2596: \"cuda:0 f32[128, 16]\"\n",
       "      # t2596 = prims.reshape(t2595, (128, 16))  # t2596: \"cuda:0 f32[128, 16]\"\n",
       "  del t2595\n",
       "  t2597 = torch.matmul(t2596, t_fcs_33_weight)  # t2597: \"cuda:0 f32[128, 16]\"\n",
       "    # t2597 = ltorch.matmul(t2596, t_fcs_33_weight)  # t2597: \"cuda:0 f32[128, 16]\"\n",
       "      # t2597 = prims.matmul(t2596, t_fcs_33_weight)  # t2597: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_33_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t160 = load_to_gpu(offloaded_t160, 'cuda:0')  # t160: \"cuda:0 b8[128, 16]\"\n",
       "  [t2604, t2611] = nvFusion67(t160, t2597)\n",
       "    # t2604 = prims.where(t160, t2597, 0.0)  # t2604: \"cuda:0 f32[128, 16]\"\n",
       "    # t2611 = prims.sum(t2604, (0,))  # t2611: \"cuda:0 f32[16]\"\n",
       "  del t160, t2597\n",
       "  t2605 = torch.reshape(t2604, (-1, 16))  # t2605: \"cuda:0 f32[128, 16]\"\n",
       "    # t2605 = ltorch.reshape(t2604, (-1, 16))  # t2605: \"cuda:0 f32[128, 16]\"\n",
       "      # t2605 = prims.reshape(t2604, (128, 16))  # t2605: \"cuda:0 f32[128, 16]\"\n",
       "  del t2604\n",
       "  t2606 = torch.matmul(t2605, t_fcs_32_weight)  # t2606: \"cuda:0 f32[128, 16]\"\n",
       "    # t2606 = ltorch.matmul(t2605, t_fcs_32_weight)  # t2606: \"cuda:0 f32[128, 16]\"\n",
       "      # t2606 = prims.matmul(t2605, t_fcs_32_weight)  # t2606: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_32_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t155 = load_to_gpu(offloaded_t155, 'cuda:0')  # t155: \"cuda:0 b8[128, 16]\"\n",
       "  [t2613, t2620] = nvFusion68(t155, t2606)\n",
       "    # t2613 = prims.where(t155, t2606, 0.0)  # t2613: \"cuda:0 f32[128, 16]\"\n",
       "    # t2620 = prims.sum(t2613, (0,))  # t2620: \"cuda:0 f32[16]\"\n",
       "  del t155, t2606\n",
       "  t2614 = torch.reshape(t2613, (-1, 16))  # t2614: \"cuda:0 f32[128, 16]\"\n",
       "    # t2614 = ltorch.reshape(t2613, (-1, 16))  # t2614: \"cuda:0 f32[128, 16]\"\n",
       "      # t2614 = prims.reshape(t2613, (128, 16))  # t2614: \"cuda:0 f32[128, 16]\"\n",
       "  del t2613\n",
       "  t2615 = torch.matmul(t2614, t_fcs_31_weight)  # t2615: \"cuda:0 f32[128, 16]\"\n",
       "    # t2615 = ltorch.matmul(t2614, t_fcs_31_weight)  # t2615: \"cuda:0 f32[128, 16]\"\n",
       "      # t2615 = prims.matmul(t2614, t_fcs_31_weight)  # t2615: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_31_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t150 = load_to_gpu(offloaded_t150, 'cuda:0')  # t150: \"cuda:0 b8[128, 16]\"\n",
       "  [t2622, t2629] = nvFusion69(t150, t2615)\n",
       "    # t2622 = prims.where(t150, t2615, 0.0)  # t2622: \"cuda:0 f32[128, 16]\"\n",
       "    # t2629 = prims.sum(t2622, (0,))  # t2629: \"cuda:0 f32[16]\"\n",
       "  del t150, t2615\n",
       "  t2623 = torch.reshape(t2622, (-1, 16))  # t2623: \"cuda:0 f32[128, 16]\"\n",
       "    # t2623 = ltorch.reshape(t2622, (-1, 16))  # t2623: \"cuda:0 f32[128, 16]\"\n",
       "      # t2623 = prims.reshape(t2622, (128, 16))  # t2623: \"cuda:0 f32[128, 16]\"\n",
       "  del t2622\n",
       "  t2624 = torch.matmul(t2623, t_fcs_30_weight)  # t2624: \"cuda:0 f32[128, 16]\"\n",
       "    # t2624 = ltorch.matmul(t2623, t_fcs_30_weight)  # t2624: \"cuda:0 f32[128, 16]\"\n",
       "      # t2624 = prims.matmul(t2623, t_fcs_30_weight)  # t2624: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_30_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t145 = load_to_gpu(offloaded_t145, 'cuda:0')  # t145: \"cuda:0 b8[128, 16]\"\n",
       "  [t2631, t2638] = nvFusion70(t145, t2624)\n",
       "    # t2631 = prims.where(t145, t2624, 0.0)  # t2631: \"cuda:0 f32[128, 16]\"\n",
       "    # t2638 = prims.sum(t2631, (0,))  # t2638: \"cuda:0 f32[16]\"\n",
       "  del t145, t2624\n",
       "  t2632 = torch.reshape(t2631, (-1, 16))  # t2632: \"cuda:0 f32[128, 16]\"\n",
       "    # t2632 = ltorch.reshape(t2631, (-1, 16))  # t2632: \"cuda:0 f32[128, 16]\"\n",
       "      # t2632 = prims.reshape(t2631, (128, 16))  # t2632: \"cuda:0 f32[128, 16]\"\n",
       "  del t2631\n",
       "  t2633 = torch.matmul(t2632, t_fcs_29_weight)  # t2633: \"cuda:0 f32[128, 16]\"\n",
       "    # t2633 = ltorch.matmul(t2632, t_fcs_29_weight)  # t2633: \"cuda:0 f32[128, 16]\"\n",
       "      # t2633 = prims.matmul(t2632, t_fcs_29_weight)  # t2633: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_29_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t140 = load_to_gpu(offloaded_t140, 'cuda:0')  # t140: \"cuda:0 b8[128, 16]\"\n",
       "  [t2640, t2647] = nvFusion71(t140, t2633)\n",
       "    # t2640 = prims.where(t140, t2633, 0.0)  # t2640: \"cuda:0 f32[128, 16]\"\n",
       "    # t2647 = prims.sum(t2640, (0,))  # t2647: \"cuda:0 f32[16]\"\n",
       "  del t140, t2633\n",
       "  t2641 = torch.reshape(t2640, (-1, 16))  # t2641: \"cuda:0 f32[128, 16]\"\n",
       "    # t2641 = ltorch.reshape(t2640, (-1, 16))  # t2641: \"cuda:0 f32[128, 16]\"\n",
       "      # t2641 = prims.reshape(t2640, (128, 16))  # t2641: \"cuda:0 f32[128, 16]\"\n",
       "  del t2640\n",
       "  t2642 = torch.matmul(t2641, t_fcs_28_weight)  # t2642: \"cuda:0 f32[128, 16]\"\n",
       "    # t2642 = ltorch.matmul(t2641, t_fcs_28_weight)  # t2642: \"cuda:0 f32[128, 16]\"\n",
       "      # t2642 = prims.matmul(t2641, t_fcs_28_weight)  # t2642: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_28_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t135 = load_to_gpu(offloaded_t135, 'cuda:0')  # t135: \"cuda:0 b8[128, 16]\"\n",
       "  [t2649, t2656] = nvFusion72(t135, t2642)\n",
       "    # t2649 = prims.where(t135, t2642, 0.0)  # t2649: \"cuda:0 f32[128, 16]\"\n",
       "    # t2656 = prims.sum(t2649, (0,))  # t2656: \"cuda:0 f32[16]\"\n",
       "  del t135, t2642\n",
       "  t2650 = torch.reshape(t2649, (-1, 16))  # t2650: \"cuda:0 f32[128, 16]\"\n",
       "    # t2650 = ltorch.reshape(t2649, (-1, 16))  # t2650: \"cuda:0 f32[128, 16]\"\n",
       "      # t2650 = prims.reshape(t2649, (128, 16))  # t2650: \"cuda:0 f32[128, 16]\"\n",
       "  del t2649\n",
       "  t2651 = torch.matmul(t2650, t_fcs_27_weight)  # t2651: \"cuda:0 f32[128, 16]\"\n",
       "    # t2651 = ltorch.matmul(t2650, t_fcs_27_weight)  # t2651: \"cuda:0 f32[128, 16]\"\n",
       "      # t2651 = prims.matmul(t2650, t_fcs_27_weight)  # t2651: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_27_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t130 = load_to_gpu(offloaded_t130, 'cuda:0')  # t130: \"cuda:0 b8[128, 16]\"\n",
       "  [t2658, t2665] = nvFusion73(t130, t2651)\n",
       "    # t2658 = prims.where(t130, t2651, 0.0)  # t2658: \"cuda:0 f32[128, 16]\"\n",
       "    # t2665 = prims.sum(t2658, (0,))  # t2665: \"cuda:0 f32[16]\"\n",
       "  del t130, t2651\n",
       "  t2659 = torch.reshape(t2658, (-1, 16))  # t2659: \"cuda:0 f32[128, 16]\"\n",
       "    # t2659 = ltorch.reshape(t2658, (-1, 16))  # t2659: \"cuda:0 f32[128, 16]\"\n",
       "      # t2659 = prims.reshape(t2658, (128, 16))  # t2659: \"cuda:0 f32[128, 16]\"\n",
       "  del t2658\n",
       "  t2660 = torch.matmul(t2659, t_fcs_26_weight)  # t2660: \"cuda:0 f32[128, 16]\"\n",
       "    # t2660 = ltorch.matmul(t2659, t_fcs_26_weight)  # t2660: \"cuda:0 f32[128, 16]\"\n",
       "      # t2660 = prims.matmul(t2659, t_fcs_26_weight)  # t2660: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_26_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t125 = load_to_gpu(offloaded_t125, 'cuda:0')  # t125: \"cuda:0 b8[128, 16]\"\n",
       "  [t2667, t2674] = nvFusion74(t125, t2660)\n",
       "    # t2667 = prims.where(t125, t2660, 0.0)  # t2667: \"cuda:0 f32[128, 16]\"\n",
       "    # t2674 = prims.sum(t2667, (0,))  # t2674: \"cuda:0 f32[16]\"\n",
       "  del t125, t2660\n",
       "  t2668 = torch.reshape(t2667, (-1, 16))  # t2668: \"cuda:0 f32[128, 16]\"\n",
       "    # t2668 = ltorch.reshape(t2667, (-1, 16))  # t2668: \"cuda:0 f32[128, 16]\"\n",
       "      # t2668 = prims.reshape(t2667, (128, 16))  # t2668: \"cuda:0 f32[128, 16]\"\n",
       "  del t2667\n",
       "  t2669 = torch.matmul(t2668, t_fcs_25_weight)  # t2669: \"cuda:0 f32[128, 16]\"\n",
       "    # t2669 = ltorch.matmul(t2668, t_fcs_25_weight)  # t2669: \"cuda:0 f32[128, 16]\"\n",
       "      # t2669 = prims.matmul(t2668, t_fcs_25_weight)  # t2669: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_25_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t120 = load_to_gpu(offloaded_t120, 'cuda:0')  # t120: \"cuda:0 b8[128, 16]\"\n",
       "  [t2676, t2683] = nvFusion75(t120, t2669)\n",
       "    # t2676 = prims.where(t120, t2669, 0.0)  # t2676: \"cuda:0 f32[128, 16]\"\n",
       "    # t2683 = prims.sum(t2676, (0,))  # t2683: \"cuda:0 f32[16]\"\n",
       "  del t120, t2669\n",
       "  t2677 = torch.reshape(t2676, (-1, 16))  # t2677: \"cuda:0 f32[128, 16]\"\n",
       "    # t2677 = ltorch.reshape(t2676, (-1, 16))  # t2677: \"cuda:0 f32[128, 16]\"\n",
       "      # t2677 = prims.reshape(t2676, (128, 16))  # t2677: \"cuda:0 f32[128, 16]\"\n",
       "  del t2676\n",
       "  t2678 = torch.matmul(t2677, t_fcs_24_weight)  # t2678: \"cuda:0 f32[128, 16]\"\n",
       "    # t2678 = ltorch.matmul(t2677, t_fcs_24_weight)  # t2678: \"cuda:0 f32[128, 16]\"\n",
       "      # t2678 = prims.matmul(t2677, t_fcs_24_weight)  # t2678: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_24_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t115 = load_to_gpu(offloaded_t115, 'cuda:0')  # t115: \"cuda:0 b8[128, 16]\"\n",
       "  [t2685, t2692] = nvFusion76(t115, t2678)\n",
       "    # t2685 = prims.where(t115, t2678, 0.0)  # t2685: \"cuda:0 f32[128, 16]\"\n",
       "    # t2692 = prims.sum(t2685, (0,))  # t2692: \"cuda:0 f32[16]\"\n",
       "  del t115, t2678\n",
       "  t2686 = torch.reshape(t2685, (-1, 16))  # t2686: \"cuda:0 f32[128, 16]\"\n",
       "    # t2686 = ltorch.reshape(t2685, (-1, 16))  # t2686: \"cuda:0 f32[128, 16]\"\n",
       "      # t2686 = prims.reshape(t2685, (128, 16))  # t2686: \"cuda:0 f32[128, 16]\"\n",
       "  del t2685\n",
       "  t2687 = torch.matmul(t2686, t_fcs_23_weight)  # t2687: \"cuda:0 f32[128, 16]\"\n",
       "    # t2687 = ltorch.matmul(t2686, t_fcs_23_weight)  # t2687: \"cuda:0 f32[128, 16]\"\n",
       "      # t2687 = prims.matmul(t2686, t_fcs_23_weight)  # t2687: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_23_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t110 = load_to_gpu(offloaded_t110, 'cuda:0')  # t110: \"cuda:0 b8[128, 16]\"\n",
       "  [t2694, t2701] = nvFusion77(t110, t2687)\n",
       "    # t2694 = prims.where(t110, t2687, 0.0)  # t2694: \"cuda:0 f32[128, 16]\"\n",
       "    # t2701 = prims.sum(t2694, (0,))  # t2701: \"cuda:0 f32[16]\"\n",
       "  del t110, t2687\n",
       "  t2695 = torch.reshape(t2694, (-1, 16))  # t2695: \"cuda:0 f32[128, 16]\"\n",
       "    # t2695 = ltorch.reshape(t2694, (-1, 16))  # t2695: \"cuda:0 f32[128, 16]\"\n",
       "      # t2695 = prims.reshape(t2694, (128, 16))  # t2695: \"cuda:0 f32[128, 16]\"\n",
       "  del t2694\n",
       "  t2696 = torch.matmul(t2695, t_fcs_22_weight)  # t2696: \"cuda:0 f32[128, 16]\"\n",
       "    # t2696 = ltorch.matmul(t2695, t_fcs_22_weight)  # t2696: \"cuda:0 f32[128, 16]\"\n",
       "      # t2696 = prims.matmul(t2695, t_fcs_22_weight)  # t2696: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_22_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t105 = load_to_gpu(offloaded_t105, 'cuda:0')  # t105: \"cuda:0 b8[128, 16]\"\n",
       "  [t2703, t2710] = nvFusion78(t105, t2696)\n",
       "    # t2703 = prims.where(t105, t2696, 0.0)  # t2703: \"cuda:0 f32[128, 16]\"\n",
       "    # t2710 = prims.sum(t2703, (0,))  # t2710: \"cuda:0 f32[16]\"\n",
       "  del t105, t2696\n",
       "  t2704 = torch.reshape(t2703, (-1, 16))  # t2704: \"cuda:0 f32[128, 16]\"\n",
       "    # t2704 = ltorch.reshape(t2703, (-1, 16))  # t2704: \"cuda:0 f32[128, 16]\"\n",
       "      # t2704 = prims.reshape(t2703, (128, 16))  # t2704: \"cuda:0 f32[128, 16]\"\n",
       "  del t2703\n",
       "  t2705 = torch.matmul(t2704, t_fcs_21_weight)  # t2705: \"cuda:0 f32[128, 16]\"\n",
       "    # t2705 = ltorch.matmul(t2704, t_fcs_21_weight)  # t2705: \"cuda:0 f32[128, 16]\"\n",
       "      # t2705 = prims.matmul(t2704, t_fcs_21_weight)  # t2705: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_21_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t100 = load_to_gpu(offloaded_t100, 'cuda:0')  # t100: \"cuda:0 b8[128, 16]\"\n",
       "  [t2712, t2719] = nvFusion79(t100, t2705)\n",
       "    # t2712 = prims.where(t100, t2705, 0.0)  # t2712: \"cuda:0 f32[128, 16]\"\n",
       "    # t2719 = prims.sum(t2712, (0,))  # t2719: \"cuda:0 f32[16]\"\n",
       "  del t100, t2705\n",
       "  t2713 = torch.reshape(t2712, (-1, 16))  # t2713: \"cuda:0 f32[128, 16]\"\n",
       "    # t2713 = ltorch.reshape(t2712, (-1, 16))  # t2713: \"cuda:0 f32[128, 16]\"\n",
       "      # t2713 = prims.reshape(t2712, (128, 16))  # t2713: \"cuda:0 f32[128, 16]\"\n",
       "  del t2712\n",
       "  t2714 = torch.matmul(t2713, t_fcs_20_weight)  # t2714: \"cuda:0 f32[128, 16]\"\n",
       "    # t2714 = ltorch.matmul(t2713, t_fcs_20_weight)  # t2714: \"cuda:0 f32[128, 16]\"\n",
       "      # t2714 = prims.matmul(t2713, t_fcs_20_weight)  # t2714: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_20_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t95 = load_to_gpu(offloaded_t95, 'cuda:0')  # t95: \"cuda:0 b8[128, 16]\"\n",
       "  [t2721, t2728] = nvFusion80(t95, t2714)\n",
       "    # t2721 = prims.where(t95, t2714, 0.0)  # t2721: \"cuda:0 f32[128, 16]\"\n",
       "    # t2728 = prims.sum(t2721, (0,))  # t2728: \"cuda:0 f32[16]\"\n",
       "  del t95, t2714\n",
       "  t2722 = torch.reshape(t2721, (-1, 16))  # t2722: \"cuda:0 f32[128, 16]\"\n",
       "    # t2722 = ltorch.reshape(t2721, (-1, 16))  # t2722: \"cuda:0 f32[128, 16]\"\n",
       "      # t2722 = prims.reshape(t2721, (128, 16))  # t2722: \"cuda:0 f32[128, 16]\"\n",
       "  del t2721\n",
       "  t2723 = torch.matmul(t2722, t_fcs_19_weight)  # t2723: \"cuda:0 f32[128, 16]\"\n",
       "    # t2723 = ltorch.matmul(t2722, t_fcs_19_weight)  # t2723: \"cuda:0 f32[128, 16]\"\n",
       "      # t2723 = prims.matmul(t2722, t_fcs_19_weight)  # t2723: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_19_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t90 = load_to_gpu(offloaded_t90, 'cuda:0')  # t90: \"cuda:0 b8[128, 16]\"\n",
       "  [t2730, t2737] = nvFusion81(t90, t2723)\n",
       "    # t2730 = prims.where(t90, t2723, 0.0)  # t2730: \"cuda:0 f32[128, 16]\"\n",
       "    # t2737 = prims.sum(t2730, (0,))  # t2737: \"cuda:0 f32[16]\"\n",
       "  del t90, t2723\n",
       "  t2731 = torch.reshape(t2730, (-1, 16))  # t2731: \"cuda:0 f32[128, 16]\"\n",
       "    # t2731 = ltorch.reshape(t2730, (-1, 16))  # t2731: \"cuda:0 f32[128, 16]\"\n",
       "      # t2731 = prims.reshape(t2730, (128, 16))  # t2731: \"cuda:0 f32[128, 16]\"\n",
       "  del t2730\n",
       "  t2732 = torch.matmul(t2731, t_fcs_18_weight)  # t2732: \"cuda:0 f32[128, 16]\"\n",
       "    # t2732 = ltorch.matmul(t2731, t_fcs_18_weight)  # t2732: \"cuda:0 f32[128, 16]\"\n",
       "      # t2732 = prims.matmul(t2731, t_fcs_18_weight)  # t2732: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_18_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t85 = load_to_gpu(offloaded_t85, 'cuda:0')  # t85: \"cuda:0 b8[128, 16]\"\n",
       "  [t2739, t2746] = nvFusion82(t85, t2732)\n",
       "    # t2739 = prims.where(t85, t2732, 0.0)  # t2739: \"cuda:0 f32[128, 16]\"\n",
       "    # t2746 = prims.sum(t2739, (0,))  # t2746: \"cuda:0 f32[16]\"\n",
       "  del t85, t2732\n",
       "  t2740 = torch.reshape(t2739, (-1, 16))  # t2740: \"cuda:0 f32[128, 16]\"\n",
       "    # t2740 = ltorch.reshape(t2739, (-1, 16))  # t2740: \"cuda:0 f32[128, 16]\"\n",
       "      # t2740 = prims.reshape(t2739, (128, 16))  # t2740: \"cuda:0 f32[128, 16]\"\n",
       "  del t2739\n",
       "  t2741 = torch.matmul(t2740, t_fcs_17_weight)  # t2741: \"cuda:0 f32[128, 16]\"\n",
       "    # t2741 = ltorch.matmul(t2740, t_fcs_17_weight)  # t2741: \"cuda:0 f32[128, 16]\"\n",
       "      # t2741 = prims.matmul(t2740, t_fcs_17_weight)  # t2741: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_17_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t80 = load_to_gpu(offloaded_t80, 'cuda:0')  # t80: \"cuda:0 b8[128, 16]\"\n",
       "  [t2748, t2755] = nvFusion83(t80, t2741)\n",
       "    # t2748 = prims.where(t80, t2741, 0.0)  # t2748: \"cuda:0 f32[128, 16]\"\n",
       "    # t2755 = prims.sum(t2748, (0,))  # t2755: \"cuda:0 f32[16]\"\n",
       "  del t80, t2741\n",
       "  t2749 = torch.reshape(t2748, (-1, 16))  # t2749: \"cuda:0 f32[128, 16]\"\n",
       "    # t2749 = ltorch.reshape(t2748, (-1, 16))  # t2749: \"cuda:0 f32[128, 16]\"\n",
       "      # t2749 = prims.reshape(t2748, (128, 16))  # t2749: \"cuda:0 f32[128, 16]\"\n",
       "  del t2748\n",
       "  t2750 = torch.matmul(t2749, t_fcs_16_weight)  # t2750: \"cuda:0 f32[128, 16]\"\n",
       "    # t2750 = ltorch.matmul(t2749, t_fcs_16_weight)  # t2750: \"cuda:0 f32[128, 16]\"\n",
       "      # t2750 = prims.matmul(t2749, t_fcs_16_weight)  # t2750: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_16_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t75 = load_to_gpu(offloaded_t75, 'cuda:0')  # t75: \"cuda:0 b8[128, 16]\"\n",
       "  [t2757, t2764] = nvFusion84(t75, t2750)\n",
       "    # t2757 = prims.where(t75, t2750, 0.0)  # t2757: \"cuda:0 f32[128, 16]\"\n",
       "    # t2764 = prims.sum(t2757, (0,))  # t2764: \"cuda:0 f32[16]\"\n",
       "  del t75, t2750\n",
       "  t2758 = torch.reshape(t2757, (-1, 16))  # t2758: \"cuda:0 f32[128, 16]\"\n",
       "    # t2758 = ltorch.reshape(t2757, (-1, 16))  # t2758: \"cuda:0 f32[128, 16]\"\n",
       "      # t2758 = prims.reshape(t2757, (128, 16))  # t2758: \"cuda:0 f32[128, 16]\"\n",
       "  del t2757\n",
       "  t2759 = torch.matmul(t2758, t_fcs_15_weight)  # t2759: \"cuda:0 f32[128, 16]\"\n",
       "    # t2759 = ltorch.matmul(t2758, t_fcs_15_weight)  # t2759: \"cuda:0 f32[128, 16]\"\n",
       "      # t2759 = prims.matmul(t2758, t_fcs_15_weight)  # t2759: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_15_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t70 = load_to_gpu(offloaded_t70, 'cuda:0')  # t70: \"cuda:0 b8[128, 16]\"\n",
       "  [t2766, t2773] = nvFusion85(t70, t2759)\n",
       "    # t2766 = prims.where(t70, t2759, 0.0)  # t2766: \"cuda:0 f32[128, 16]\"\n",
       "    # t2773 = prims.sum(t2766, (0,))  # t2773: \"cuda:0 f32[16]\"\n",
       "  del t70, t2759\n",
       "  t2767 = torch.reshape(t2766, (-1, 16))  # t2767: \"cuda:0 f32[128, 16]\"\n",
       "    # t2767 = ltorch.reshape(t2766, (-1, 16))  # t2767: \"cuda:0 f32[128, 16]\"\n",
       "      # t2767 = prims.reshape(t2766, (128, 16))  # t2767: \"cuda:0 f32[128, 16]\"\n",
       "  del t2766\n",
       "  t2768 = torch.matmul(t2767, t_fcs_14_weight)  # t2768: \"cuda:0 f32[128, 16]\"\n",
       "    # t2768 = ltorch.matmul(t2767, t_fcs_14_weight)  # t2768: \"cuda:0 f32[128, 16]\"\n",
       "      # t2768 = prims.matmul(t2767, t_fcs_14_weight)  # t2768: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_14_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t65 = load_to_gpu(offloaded_t65, 'cuda:0')  # t65: \"cuda:0 b8[128, 16]\"\n",
       "  [t2775, t2782] = nvFusion86(t65, t2768)\n",
       "    # t2775 = prims.where(t65, t2768, 0.0)  # t2775: \"cuda:0 f32[128, 16]\"\n",
       "    # t2782 = prims.sum(t2775, (0,))  # t2782: \"cuda:0 f32[16]\"\n",
       "  del t65, t2768\n",
       "  t2776 = torch.reshape(t2775, (-1, 16))  # t2776: \"cuda:0 f32[128, 16]\"\n",
       "    # t2776 = ltorch.reshape(t2775, (-1, 16))  # t2776: \"cuda:0 f32[128, 16]\"\n",
       "      # t2776 = prims.reshape(t2775, (128, 16))  # t2776: \"cuda:0 f32[128, 16]\"\n",
       "  del t2775\n",
       "  t2777 = torch.matmul(t2776, t_fcs_13_weight)  # t2777: \"cuda:0 f32[128, 16]\"\n",
       "    # t2777 = ltorch.matmul(t2776, t_fcs_13_weight)  # t2777: \"cuda:0 f32[128, 16]\"\n",
       "      # t2777 = prims.matmul(t2776, t_fcs_13_weight)  # t2777: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_13_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t60 = load_to_gpu(offloaded_t60, 'cuda:0')  # t60: \"cuda:0 b8[128, 16]\"\n",
       "  [t2784, t2791] = nvFusion87(t60, t2777)\n",
       "    # t2784 = prims.where(t60, t2777, 0.0)  # t2784: \"cuda:0 f32[128, 16]\"\n",
       "    # t2791 = prims.sum(t2784, (0,))  # t2791: \"cuda:0 f32[16]\"\n",
       "  del t60, t2777\n",
       "  t2785 = torch.reshape(t2784, (-1, 16))  # t2785: \"cuda:0 f32[128, 16]\"\n",
       "    # t2785 = ltorch.reshape(t2784, (-1, 16))  # t2785: \"cuda:0 f32[128, 16]\"\n",
       "      # t2785 = prims.reshape(t2784, (128, 16))  # t2785: \"cuda:0 f32[128, 16]\"\n",
       "  del t2784\n",
       "  t2786 = torch.matmul(t2785, t_fcs_12_weight)  # t2786: \"cuda:0 f32[128, 16]\"\n",
       "    # t2786 = ltorch.matmul(t2785, t_fcs_12_weight)  # t2786: \"cuda:0 f32[128, 16]\"\n",
       "      # t2786 = prims.matmul(t2785, t_fcs_12_weight)  # t2786: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_12_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t55 = load_to_gpu(offloaded_t55, 'cuda:0')  # t55: \"cuda:0 b8[128, 16]\"\n",
       "  [t2793, t2800] = nvFusion88(t55, t2786)\n",
       "    # t2793 = prims.where(t55, t2786, 0.0)  # t2793: \"cuda:0 f32[128, 16]\"\n",
       "    # t2800 = prims.sum(t2793, (0,))  # t2800: \"cuda:0 f32[16]\"\n",
       "  del t55, t2786\n",
       "  t2794 = torch.reshape(t2793, (-1, 16))  # t2794: \"cuda:0 f32[128, 16]\"\n",
       "    # t2794 = ltorch.reshape(t2793, (-1, 16))  # t2794: \"cuda:0 f32[128, 16]\"\n",
       "      # t2794 = prims.reshape(t2793, (128, 16))  # t2794: \"cuda:0 f32[128, 16]\"\n",
       "  del t2793\n",
       "  t2795 = torch.matmul(t2794, t_fcs_11_weight)  # t2795: \"cuda:0 f32[128, 16]\"\n",
       "    # t2795 = ltorch.matmul(t2794, t_fcs_11_weight)  # t2795: \"cuda:0 f32[128, 16]\"\n",
       "      # t2795 = prims.matmul(t2794, t_fcs_11_weight)  # t2795: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_11_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t50 = load_to_gpu(offloaded_t50, 'cuda:0')  # t50: \"cuda:0 b8[128, 16]\"\n",
       "  [t2802, t2809] = nvFusion89(t50, t2795)\n",
       "    # t2802 = prims.where(t50, t2795, 0.0)  # t2802: \"cuda:0 f32[128, 16]\"\n",
       "    # t2809 = prims.sum(t2802, (0,))  # t2809: \"cuda:0 f32[16]\"\n",
       "  del t50, t2795\n",
       "  t2803 = torch.reshape(t2802, (-1, 16))  # t2803: \"cuda:0 f32[128, 16]\"\n",
       "    # t2803 = ltorch.reshape(t2802, (-1, 16))  # t2803: \"cuda:0 f32[128, 16]\"\n",
       "      # t2803 = prims.reshape(t2802, (128, 16))  # t2803: \"cuda:0 f32[128, 16]\"\n",
       "  del t2802\n",
       "  t2804 = torch.matmul(t2803, t_fcs_10_weight)  # t2804: \"cuda:0 f32[128, 16]\"\n",
       "    # t2804 = ltorch.matmul(t2803, t_fcs_10_weight)  # t2804: \"cuda:0 f32[128, 16]\"\n",
       "      # t2804 = prims.matmul(t2803, t_fcs_10_weight)  # t2804: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_10_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t45 = load_to_gpu(offloaded_t45, 'cuda:0')  # t45: \"cuda:0 b8[128, 16]\"\n",
       "  [t2811, t2818] = nvFusion90(t45, t2804)\n",
       "    # t2811 = prims.where(t45, t2804, 0.0)  # t2811: \"cuda:0 f32[128, 16]\"\n",
       "    # t2818 = prims.sum(t2811, (0,))  # t2818: \"cuda:0 f32[16]\"\n",
       "  del t45, t2804\n",
       "  t2812 = torch.reshape(t2811, (-1, 16))  # t2812: \"cuda:0 f32[128, 16]\"\n",
       "    # t2812 = ltorch.reshape(t2811, (-1, 16))  # t2812: \"cuda:0 f32[128, 16]\"\n",
       "      # t2812 = prims.reshape(t2811, (128, 16))  # t2812: \"cuda:0 f32[128, 16]\"\n",
       "  del t2811\n",
       "  t2813 = torch.matmul(t2812, t_fcs_9_weight)  # t2813: \"cuda:0 f32[128, 16]\"\n",
       "    # t2813 = ltorch.matmul(t2812, t_fcs_9_weight)  # t2813: \"cuda:0 f32[128, 16]\"\n",
       "      # t2813 = prims.matmul(t2812, t_fcs_9_weight)  # t2813: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_9_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t40 = load_to_gpu(offloaded_t40, 'cuda:0')  # t40: \"cuda:0 b8[128, 16]\"\n",
       "  [t2820, t2827] = nvFusion91(t40, t2813)\n",
       "    # t2820 = prims.where(t40, t2813, 0.0)  # t2820: \"cuda:0 f32[128, 16]\"\n",
       "    # t2827 = prims.sum(t2820, (0,))  # t2827: \"cuda:0 f32[16]\"\n",
       "  del t40, t2813\n",
       "  t2821 = torch.reshape(t2820, (-1, 16))  # t2821: \"cuda:0 f32[128, 16]\"\n",
       "    # t2821 = ltorch.reshape(t2820, (-1, 16))  # t2821: \"cuda:0 f32[128, 16]\"\n",
       "      # t2821 = prims.reshape(t2820, (128, 16))  # t2821: \"cuda:0 f32[128, 16]\"\n",
       "  del t2820\n",
       "  t2822 = torch.matmul(t2821, t_fcs_8_weight)  # t2822: \"cuda:0 f32[128, 16]\"\n",
       "    # t2822 = ltorch.matmul(t2821, t_fcs_8_weight)  # t2822: \"cuda:0 f32[128, 16]\"\n",
       "      # t2822 = prims.matmul(t2821, t_fcs_8_weight)  # t2822: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_8_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t35 = load_to_gpu(offloaded_t35, 'cuda:0')  # t35: \"cuda:0 b8[128, 16]\"\n",
       "  [t2829, t2836] = nvFusion92(t35, t2822)\n",
       "    # t2829 = prims.where(t35, t2822, 0.0)  # t2829: \"cuda:0 f32[128, 16]\"\n",
       "    # t2836 = prims.sum(t2829, (0,))  # t2836: \"cuda:0 f32[16]\"\n",
       "  del t35, t2822\n",
       "  t2830 = torch.reshape(t2829, (-1, 16))  # t2830: \"cuda:0 f32[128, 16]\"\n",
       "    # t2830 = ltorch.reshape(t2829, (-1, 16))  # t2830: \"cuda:0 f32[128, 16]\"\n",
       "      # t2830 = prims.reshape(t2829, (128, 16))  # t2830: \"cuda:0 f32[128, 16]\"\n",
       "  del t2829\n",
       "  t2831 = torch.matmul(t2830, t_fcs_7_weight)  # t2831: \"cuda:0 f32[128, 16]\"\n",
       "    # t2831 = ltorch.matmul(t2830, t_fcs_7_weight)  # t2831: \"cuda:0 f32[128, 16]\"\n",
       "      # t2831 = prims.matmul(t2830, t_fcs_7_weight)  # t2831: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_7_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t30 = load_to_gpu(offloaded_t30, 'cuda:0')  # t30: \"cuda:0 b8[128, 16]\"\n",
       "  [t2838, t2845] = nvFusion93(t30, t2831)\n",
       "    # t2838 = prims.where(t30, t2831, 0.0)  # t2838: \"cuda:0 f32[128, 16]\"\n",
       "    # t2845 = prims.sum(t2838, (0,))  # t2845: \"cuda:0 f32[16]\"\n",
       "  del t30, t2831\n",
       "  t2839 = torch.reshape(t2838, (-1, 16))  # t2839: \"cuda:0 f32[128, 16]\"\n",
       "    # t2839 = ltorch.reshape(t2838, (-1, 16))  # t2839: \"cuda:0 f32[128, 16]\"\n",
       "      # t2839 = prims.reshape(t2838, (128, 16))  # t2839: \"cuda:0 f32[128, 16]\"\n",
       "  del t2838\n",
       "  t2840 = torch.matmul(t2839, t_fcs_6_weight)  # t2840: \"cuda:0 f32[128, 16]\"\n",
       "    # t2840 = ltorch.matmul(t2839, t_fcs_6_weight)  # t2840: \"cuda:0 f32[128, 16]\"\n",
       "      # t2840 = prims.matmul(t2839, t_fcs_6_weight)  # t2840: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_6_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t25 = load_to_gpu(offloaded_t25, 'cuda:0')  # t25: \"cuda:0 b8[128, 16]\"\n",
       "  [t2847, t2854] = nvFusion94(t25, t2840)\n",
       "    # t2847 = prims.where(t25, t2840, 0.0)  # t2847: \"cuda:0 f32[128, 16]\"\n",
       "    # t2854 = prims.sum(t2847, (0,))  # t2854: \"cuda:0 f32[16]\"\n",
       "  del t25, t2840\n",
       "  t2848 = torch.reshape(t2847, (-1, 16))  # t2848: \"cuda:0 f32[128, 16]\"\n",
       "    # t2848 = ltorch.reshape(t2847, (-1, 16))  # t2848: \"cuda:0 f32[128, 16]\"\n",
       "      # t2848 = prims.reshape(t2847, (128, 16))  # t2848: \"cuda:0 f32[128, 16]\"\n",
       "  del t2847\n",
       "  t2849 = torch.matmul(t2848, t_fcs_5_weight)  # t2849: \"cuda:0 f32[128, 16]\"\n",
       "    # t2849 = ltorch.matmul(t2848, t_fcs_5_weight)  # t2849: \"cuda:0 f32[128, 16]\"\n",
       "      # t2849 = prims.matmul(t2848, t_fcs_5_weight)  # t2849: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_5_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t20 = load_to_gpu(offloaded_t20, 'cuda:0')  # t20: \"cuda:0 b8[128, 16]\"\n",
       "  [t2856, t2863] = nvFusion95(t20, t2849)\n",
       "    # t2856 = prims.where(t20, t2849, 0.0)  # t2856: \"cuda:0 f32[128, 16]\"\n",
       "    # t2863 = prims.sum(t2856, (0,))  # t2863: \"cuda:0 f32[16]\"\n",
       "  del t20, t2849\n",
       "  t2857 = torch.reshape(t2856, (-1, 16))  # t2857: \"cuda:0 f32[128, 16]\"\n",
       "    # t2857 = ltorch.reshape(t2856, (-1, 16))  # t2857: \"cuda:0 f32[128, 16]\"\n",
       "      # t2857 = prims.reshape(t2856, (128, 16))  # t2857: \"cuda:0 f32[128, 16]\"\n",
       "  del t2856\n",
       "  t2858 = torch.matmul(t2857, t_fcs_4_weight)  # t2858: \"cuda:0 f32[128, 16]\"\n",
       "    # t2858 = ltorch.matmul(t2857, t_fcs_4_weight)  # t2858: \"cuda:0 f32[128, 16]\"\n",
       "      # t2858 = prims.matmul(t2857, t_fcs_4_weight)  # t2858: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_4_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t15 = load_to_gpu(offloaded_t15, 'cuda:0')  # t15: \"cuda:0 b8[128, 16]\"\n",
       "  [t2865, t2872] = nvFusion96(t15, t2858)\n",
       "    # t2865 = prims.where(t15, t2858, 0.0)  # t2865: \"cuda:0 f32[128, 16]\"\n",
       "    # t2872 = prims.sum(t2865, (0,))  # t2872: \"cuda:0 f32[16]\"\n",
       "  del t15, t2858\n",
       "  t2866 = torch.reshape(t2865, (-1, 16))  # t2866: \"cuda:0 f32[128, 16]\"\n",
       "    # t2866 = ltorch.reshape(t2865, (-1, 16))  # t2866: \"cuda:0 f32[128, 16]\"\n",
       "      # t2866 = prims.reshape(t2865, (128, 16))  # t2866: \"cuda:0 f32[128, 16]\"\n",
       "  del t2865\n",
       "  t2867 = torch.matmul(t2866, t_fcs_3_weight)  # t2867: \"cuda:0 f32[128, 16]\"\n",
       "    # t2867 = ltorch.matmul(t2866, t_fcs_3_weight)  # t2867: \"cuda:0 f32[128, 16]\"\n",
       "      # t2867 = prims.matmul(t2866, t_fcs_3_weight)  # t2867: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_3_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t10 = load_to_gpu(offloaded_t10, 'cuda:0')  # t10: \"cuda:0 b8[128, 16]\"\n",
       "  [t2874, t2881] = nvFusion97(t10, t2867)\n",
       "    # t2874 = prims.where(t10, t2867, 0.0)  # t2874: \"cuda:0 f32[128, 16]\"\n",
       "    # t2881 = prims.sum(t2874, (0,))  # t2881: \"cuda:0 f32[16]\"\n",
       "  del t10, t2867\n",
       "  t2875 = torch.reshape(t2874, (-1, 16))  # t2875: \"cuda:0 f32[128, 16]\"\n",
       "    # t2875 = ltorch.reshape(t2874, (-1, 16))  # t2875: \"cuda:0 f32[128, 16]\"\n",
       "      # t2875 = prims.reshape(t2874, (128, 16))  # t2875: \"cuda:0 f32[128, 16]\"\n",
       "  del t2874\n",
       "  t2876 = torch.matmul(t2875, t_fcs_2_weight)  # t2876: \"cuda:0 f32[128, 16]\"\n",
       "    # t2876 = ltorch.matmul(t2875, t_fcs_2_weight)  # t2876: \"cuda:0 f32[128, 16]\"\n",
       "      # t2876 = prims.matmul(t2875, t_fcs_2_weight)  # t2876: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_2_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t5 = load_to_gpu(offloaded_t5, 'cuda:0')  # t5: \"cuda:0 b8[128, 16]\"\n",
       "  [t2883, t2890] = nvFusion98(t5, t2876)\n",
       "    # t2883 = prims.where(t5, t2876, 0.0)  # t2883: \"cuda:0 f32[128, 16]\"\n",
       "    # t2890 = prims.sum(t2883, (0,))  # t2890: \"cuda:0 f32[16]\"\n",
       "  del t5, t2876\n",
       "  t2884 = torch.reshape(t2883, (-1, 16))  # t2884: \"cuda:0 f32[128, 16]\"\n",
       "    # t2884 = ltorch.reshape(t2883, (-1, 16))  # t2884: \"cuda:0 f32[128, 16]\"\n",
       "      # t2884 = prims.reshape(t2883, (128, 16))  # t2884: \"cuda:0 f32[128, 16]\"\n",
       "  del t2883\n",
       "  t2885 = torch.matmul(t2884, t_fcs_1_weight)  # t2885: \"cuda:0 f32[128, 16]\"\n",
       "    # t2885 = ltorch.matmul(t2884, t_fcs_1_weight)  # t2885: \"cuda:0 f32[128, 16]\"\n",
       "      # t2885 = prims.matmul(t2884, t_fcs_1_weight)  # t2885: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_1_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t1 = load_to_gpu(offloaded_t1, 'cuda:0')  # t1: \"cuda:0 b8[128, 16]\"\n",
       "  [t2892, t2899] = nvFusion99(t1, t2885)\n",
       "    # t2892 = prims.where(t1, t2885, 0.0)  # t2892: \"cuda:0 f32[128, 16]\"\n",
       "    # t2899 = prims.sum(t2892, (0,))  # t2899: \"cuda:0 f32[16]\"\n",
       "  del t1, t2885\n",
       "  t2895 = torch.reshape(t2892, (-1, 16))  # t2895: \"cuda:0 f32[128, 16]\"\n",
       "    # t2895 = ltorch.reshape(t2892, (-1, 16))  # t2895: \"cuda:0 f32[128, 16]\"\n",
       "      # t2895 = prims.reshape(t2892, (128, 16))  # t2895: \"cuda:0 f32[128, 16]\"\n",
       "  del t2892\n",
       "  t2896 = torch.permute(t2895, (1, 0))  # t2896: \"cuda:0 f32[16, 128]\"\n",
       "    # t2896 = ltorch.permute(t2895, (1, 0))  # t2896: \"cuda:0 f32[16, 128]\"\n",
       "      # t2896 = prims.transpose(t2895, (1, 0))  # t2896: \"cuda:0 f32[16, 128]\"\n",
       "  del t2895\n",
       "  t2897 = torch.reshape(x, (-1, 16))  # t2897: \"cuda:0 f32[128, 16]\"\n",
       "    # t2897 = ltorch.reshape(x, (-1, 16))  # t2897: \"cuda:0 f32[128, 16]\"\n",
       "      # t2897 = prims.reshape(x, (128, 16))  # t2897: \"cuda:0 f32[128, 16]\"\n",
       "  del x\n",
       "  t2898 = torch.matmul(t2896, t2897)  # t2898: \"cuda:0 f32[16, 16]\"\n",
       "    # t2898 = ltorch.matmul(t2896, t2897)  # t2898: \"cuda:0 f32[16, 16]\"\n",
       "      # t2898 = prims.matmul(t2896, t2897)  # t2898: \"cuda:0 f32[16, 16]\"\n",
       "  del t2896, t2897\n",
       "  t2887 = torch.permute(t2884, (1, 0))  # t2887: \"cuda:0 f32[16, 128]\"\n",
       "    # t2887 = ltorch.permute(t2884, (1, 0))  # t2887: \"cuda:0 f32[16, 128]\"\n",
       "      # t2887 = prims.transpose(t2884, (1, 0))  # t2887: \"cuda:0 f32[16, 128]\"\n",
       "  del t2884\n",
       "  # Created by CPU Offloading Transform\n",
       "  t2 = load_to_gpu(offloaded_t2, 'cuda:0')  # t2: \"cuda:0 f32[128, 16]\"\n",
       "  t2888 = torch.reshape(t2, (-1, 16))  # t2888: \"cuda:0 f32[128, 16]\"\n",
       "    # t2888 = ltorch.reshape(t2, (-1, 16))  # t2888: \"cuda:0 f32[128, 16]\"\n",
       "      # t2888 = prims.reshape(t2, (128, 16))  # t2888: \"cuda:0 f32[128, 16]\"\n",
       "  del t2\n",
       "  t2889 = torch.matmul(t2887, t2888)  # t2889: \"cuda:0 f32[16, 16]\"\n",
       "    # t2889 = ltorch.matmul(t2887, t2888)  # t2889: \"cuda:0 f32[16, 16]\"\n",
       "      # t2889 = prims.matmul(t2887, t2888)  # t2889: \"cuda:0 f32[16, 16]\"\n",
       "  del t2887, t2888\n",
       "  t2878 = torch.permute(t2875, (1, 0))  # t2878: \"cuda:0 f32[16, 128]\"\n",
       "    # t2878 = ltorch.permute(t2875, (1, 0))  # t2878: \"cuda:0 f32[16, 128]\"\n",
       "      # t2878 = prims.transpose(t2875, (1, 0))  # t2878: \"cuda:0 f32[16, 128]\"\n",
       "  del t2875\n",
       "  # Created by CPU Offloading Transform\n",
       "  t7 = load_to_gpu(offloaded_t7, 'cuda:0')  # t7: \"cuda:0 f32[128, 16]\"\n",
       "  t2879 = torch.reshape(t7, (-1, 16))  # t2879: \"cuda:0 f32[128, 16]\"\n",
       "    # t2879 = ltorch.reshape(t7, (-1, 16))  # t2879: \"cuda:0 f32[128, 16]\"\n",
       "      # t2879 = prims.reshape(t7, (128, 16))  # t2879: \"cuda:0 f32[128, 16]\"\n",
       "  del t7\n",
       "  t2880 = torch.matmul(t2878, t2879)  # t2880: \"cuda:0 f32[16, 16]\"\n",
       "    # t2880 = ltorch.matmul(t2878, t2879)  # t2880: \"cuda:0 f32[16, 16]\"\n",
       "      # t2880 = prims.matmul(t2878, t2879)  # t2880: \"cuda:0 f32[16, 16]\"\n",
       "  del t2878, t2879\n",
       "  t2869 = torch.permute(t2866, (1, 0))  # t2869: \"cuda:0 f32[16, 128]\"\n",
       "    # t2869 = ltorch.permute(t2866, (1, 0))  # t2869: \"cuda:0 f32[16, 128]\"\n",
       "      # t2869 = prims.transpose(t2866, (1, 0))  # t2869: \"cuda:0 f32[16, 128]\"\n",
       "  del t2866\n",
       "  # Created by CPU Offloading Transform\n",
       "  t12 = load_to_gpu(offloaded_t12, 'cuda:0')  # t12: \"cuda:0 f32[128, 16]\"\n",
       "  t2870 = torch.reshape(t12, (-1, 16))  # t2870: \"cuda:0 f32[128, 16]\"\n",
       "    # t2870 = ltorch.reshape(t12, (-1, 16))  # t2870: \"cuda:0 f32[128, 16]\"\n",
       "      # t2870 = prims.reshape(t12, (128, 16))  # t2870: \"cuda:0 f32[128, 16]\"\n",
       "  del t12\n",
       "  t2871 = torch.matmul(t2869, t2870)  # t2871: \"cuda:0 f32[16, 16]\"\n",
       "    # t2871 = ltorch.matmul(t2869, t2870)  # t2871: \"cuda:0 f32[16, 16]\"\n",
       "      # t2871 = prims.matmul(t2869, t2870)  # t2871: \"cuda:0 f32[16, 16]\"\n",
       "  del t2869, t2870\n",
       "  t2860 = torch.permute(t2857, (1, 0))  # t2860: \"cuda:0 f32[16, 128]\"\n",
       "    # t2860 = ltorch.permute(t2857, (1, 0))  # t2860: \"cuda:0 f32[16, 128]\"\n",
       "      # t2860 = prims.transpose(t2857, (1, 0))  # t2860: \"cuda:0 f32[16, 128]\"\n",
       "  del t2857\n",
       "  # Created by CPU Offloading Transform\n",
       "  t17 = load_to_gpu(offloaded_t17, 'cuda:0')  # t17: \"cuda:0 f32[128, 16]\"\n",
       "  t2861 = torch.reshape(t17, (-1, 16))  # t2861: \"cuda:0 f32[128, 16]\"\n",
       "    # t2861 = ltorch.reshape(t17, (-1, 16))  # t2861: \"cuda:0 f32[128, 16]\"\n",
       "      # t2861 = prims.reshape(t17, (128, 16))  # t2861: \"cuda:0 f32[128, 16]\"\n",
       "  del t17\n",
       "  t2862 = torch.matmul(t2860, t2861)  # t2862: \"cuda:0 f32[16, 16]\"\n",
       "    # t2862 = ltorch.matmul(t2860, t2861)  # t2862: \"cuda:0 f32[16, 16]\"\n",
       "      # t2862 = prims.matmul(t2860, t2861)  # t2862: \"cuda:0 f32[16, 16]\"\n",
       "  del t2860, t2861\n",
       "  t2851 = torch.permute(t2848, (1, 0))  # t2851: \"cuda:0 f32[16, 128]\"\n",
       "    # t2851 = ltorch.permute(t2848, (1, 0))  # t2851: \"cuda:0 f32[16, 128]\"\n",
       "      # t2851 = prims.transpose(t2848, (1, 0))  # t2851: \"cuda:0 f32[16, 128]\"\n",
       "  del t2848\n",
       "  # Created by CPU Offloading Transform\n",
       "  t22 = load_to_gpu(offloaded_t22, 'cuda:0')  # t22: \"cuda:0 f32[128, 16]\"\n",
       "  t2852 = torch.reshape(t22, (-1, 16))  # t2852: \"cuda:0 f32[128, 16]\"\n",
       "    # t2852 = ltorch.reshape(t22, (-1, 16))  # t2852: \"cuda:0 f32[128, 16]\"\n",
       "      # t2852 = prims.reshape(t22, (128, 16))  # t2852: \"cuda:0 f32[128, 16]\"\n",
       "  del t22\n",
       "  t2853 = torch.matmul(t2851, t2852)  # t2853: \"cuda:0 f32[16, 16]\"\n",
       "    # t2853 = ltorch.matmul(t2851, t2852)  # t2853: \"cuda:0 f32[16, 16]\"\n",
       "      # t2853 = prims.matmul(t2851, t2852)  # t2853: \"cuda:0 f32[16, 16]\"\n",
       "  del t2851, t2852\n",
       "  t2842 = torch.permute(t2839, (1, 0))  # t2842: \"cuda:0 f32[16, 128]\"\n",
       "    # t2842 = ltorch.permute(t2839, (1, 0))  # t2842: \"cuda:0 f32[16, 128]\"\n",
       "      # t2842 = prims.transpose(t2839, (1, 0))  # t2842: \"cuda:0 f32[16, 128]\"\n",
       "  del t2839\n",
       "  # Created by CPU Offloading Transform\n",
       "  t27 = load_to_gpu(offloaded_t27, 'cuda:0')  # t27: \"cuda:0 f32[128, 16]\"\n",
       "  t2843 = torch.reshape(t27, (-1, 16))  # t2843: \"cuda:0 f32[128, 16]\"\n",
       "    # t2843 = ltorch.reshape(t27, (-1, 16))  # t2843: \"cuda:0 f32[128, 16]\"\n",
       "      # t2843 = prims.reshape(t27, (128, 16))  # t2843: \"cuda:0 f32[128, 16]\"\n",
       "  del t27\n",
       "  t2844 = torch.matmul(t2842, t2843)  # t2844: \"cuda:0 f32[16, 16]\"\n",
       "    # t2844 = ltorch.matmul(t2842, t2843)  # t2844: \"cuda:0 f32[16, 16]\"\n",
       "      # t2844 = prims.matmul(t2842, t2843)  # t2844: \"cuda:0 f32[16, 16]\"\n",
       "  del t2842, t2843\n",
       "  t2833 = torch.permute(t2830, (1, 0))  # t2833: \"cuda:0 f32[16, 128]\"\n",
       "    # t2833 = ltorch.permute(t2830, (1, 0))  # t2833: \"cuda:0 f32[16, 128]\"\n",
       "      # t2833 = prims.transpose(t2830, (1, 0))  # t2833: \"cuda:0 f32[16, 128]\"\n",
       "  del t2830\n",
       "  # Created by CPU Offloading Transform\n",
       "  t32 = load_to_gpu(offloaded_t32, 'cuda:0')  # t32: \"cuda:0 f32[128, 16]\"\n",
       "  t2834 = torch.reshape(t32, (-1, 16))  # t2834: \"cuda:0 f32[128, 16]\"\n",
       "    # t2834 = ltorch.reshape(t32, (-1, 16))  # t2834: \"cuda:0 f32[128, 16]\"\n",
       "      # t2834 = prims.reshape(t32, (128, 16))  # t2834: \"cuda:0 f32[128, 16]\"\n",
       "  del t32\n",
       "  t2835 = torch.matmul(t2833, t2834)  # t2835: \"cuda:0 f32[16, 16]\"\n",
       "    # t2835 = ltorch.matmul(t2833, t2834)  # t2835: \"cuda:0 f32[16, 16]\"\n",
       "      # t2835 = prims.matmul(t2833, t2834)  # t2835: \"cuda:0 f32[16, 16]\"\n",
       "  del t2833, t2834\n",
       "  t2824 = torch.permute(t2821, (1, 0))  # t2824: \"cuda:0 f32[16, 128]\"\n",
       "    # t2824 = ltorch.permute(t2821, (1, 0))  # t2824: \"cuda:0 f32[16, 128]\"\n",
       "      # t2824 = prims.transpose(t2821, (1, 0))  # t2824: \"cuda:0 f32[16, 128]\"\n",
       "  del t2821\n",
       "  # Created by CPU Offloading Transform\n",
       "  t37 = load_to_gpu(offloaded_t37, 'cuda:0')  # t37: \"cuda:0 f32[128, 16]\"\n",
       "  t2825 = torch.reshape(t37, (-1, 16))  # t2825: \"cuda:0 f32[128, 16]\"\n",
       "    # t2825 = ltorch.reshape(t37, (-1, 16))  # t2825: \"cuda:0 f32[128, 16]\"\n",
       "      # t2825 = prims.reshape(t37, (128, 16))  # t2825: \"cuda:0 f32[128, 16]\"\n",
       "  del t37\n",
       "  t2826 = torch.matmul(t2824, t2825)  # t2826: \"cuda:0 f32[16, 16]\"\n",
       "    # t2826 = ltorch.matmul(t2824, t2825)  # t2826: \"cuda:0 f32[16, 16]\"\n",
       "      # t2826 = prims.matmul(t2824, t2825)  # t2826: \"cuda:0 f32[16, 16]\"\n",
       "  del t2824, t2825\n",
       "  t2815 = torch.permute(t2812, (1, 0))  # t2815: \"cuda:0 f32[16, 128]\"\n",
       "    # t2815 = ltorch.permute(t2812, (1, 0))  # t2815: \"cuda:0 f32[16, 128]\"\n",
       "      # t2815 = prims.transpose(t2812, (1, 0))  # t2815: \"cuda:0 f32[16, 128]\"\n",
       "  del t2812\n",
       "  # Created by CPU Offloading Transform\n",
       "  t42 = load_to_gpu(offloaded_t42, 'cuda:0')  # t42: \"cuda:0 f32[128, 16]\"\n",
       "  t2816 = torch.reshape(t42, (-1, 16))  # t2816: \"cuda:0 f32[128, 16]\"\n",
       "    # t2816 = ltorch.reshape(t42, (-1, 16))  # t2816: \"cuda:0 f32[128, 16]\"\n",
       "      # t2816 = prims.reshape(t42, (128, 16))  # t2816: \"cuda:0 f32[128, 16]\"\n",
       "  del t42\n",
       "  t2817 = torch.matmul(t2815, t2816)  # t2817: \"cuda:0 f32[16, 16]\"\n",
       "    # t2817 = ltorch.matmul(t2815, t2816)  # t2817: \"cuda:0 f32[16, 16]\"\n",
       "      # t2817 = prims.matmul(t2815, t2816)  # t2817: \"cuda:0 f32[16, 16]\"\n",
       "  del t2815, t2816\n",
       "  t2806 = torch.permute(t2803, (1, 0))  # t2806: \"cuda:0 f32[16, 128]\"\n",
       "    # t2806 = ltorch.permute(t2803, (1, 0))  # t2806: \"cuda:0 f32[16, 128]\"\n",
       "      # t2806 = prims.transpose(t2803, (1, 0))  # t2806: \"cuda:0 f32[16, 128]\"\n",
       "  del t2803\n",
       "  # Created by CPU Offloading Transform\n",
       "  t47 = load_to_gpu(offloaded_t47, 'cuda:0')  # t47: \"cuda:0 f32[128, 16]\"\n",
       "  t2807 = torch.reshape(t47, (-1, 16))  # t2807: \"cuda:0 f32[128, 16]\"\n",
       "    # t2807 = ltorch.reshape(t47, (-1, 16))  # t2807: \"cuda:0 f32[128, 16]\"\n",
       "      # t2807 = prims.reshape(t47, (128, 16))  # t2807: \"cuda:0 f32[128, 16]\"\n",
       "  del t47\n",
       "  t2808 = torch.matmul(t2806, t2807)  # t2808: \"cuda:0 f32[16, 16]\"\n",
       "    # t2808 = ltorch.matmul(t2806, t2807)  # t2808: \"cuda:0 f32[16, 16]\"\n",
       "      # t2808 = prims.matmul(t2806, t2807)  # t2808: \"cuda:0 f32[16, 16]\"\n",
       "  del t2806, t2807\n",
       "  t2797 = torch.permute(t2794, (1, 0))  # t2797: \"cuda:0 f32[16, 128]\"\n",
       "    # t2797 = ltorch.permute(t2794, (1, 0))  # t2797: \"cuda:0 f32[16, 128]\"\n",
       "      # t2797 = prims.transpose(t2794, (1, 0))  # t2797: \"cuda:0 f32[16, 128]\"\n",
       "  del t2794\n",
       "  # Created by CPU Offloading Transform\n",
       "  t52 = load_to_gpu(offloaded_t52, 'cuda:0')  # t52: \"cuda:0 f32[128, 16]\"\n",
       "  t2798 = torch.reshape(t52, (-1, 16))  # t2798: \"cuda:0 f32[128, 16]\"\n",
       "    # t2798 = ltorch.reshape(t52, (-1, 16))  # t2798: \"cuda:0 f32[128, 16]\"\n",
       "      # t2798 = prims.reshape(t52, (128, 16))  # t2798: \"cuda:0 f32[128, 16]\"\n",
       "  del t52\n",
       "  t2799 = torch.matmul(t2797, t2798)  # t2799: \"cuda:0 f32[16, 16]\"\n",
       "    # t2799 = ltorch.matmul(t2797, t2798)  # t2799: \"cuda:0 f32[16, 16]\"\n",
       "      # t2799 = prims.matmul(t2797, t2798)  # t2799: \"cuda:0 f32[16, 16]\"\n",
       "  del t2797, t2798\n",
       "  t2788 = torch.permute(t2785, (1, 0))  # t2788: \"cuda:0 f32[16, 128]\"\n",
       "    # t2788 = ltorch.permute(t2785, (1, 0))  # t2788: \"cuda:0 f32[16, 128]\"\n",
       "      # t2788 = prims.transpose(t2785, (1, 0))  # t2788: \"cuda:0 f32[16, 128]\"\n",
       "  del t2785\n",
       "  # Created by CPU Offloading Transform\n",
       "  t57 = load_to_gpu(offloaded_t57, 'cuda:0')  # t57: \"cuda:0 f32[128, 16]\"\n",
       "  t2789 = torch.reshape(t57, (-1, 16))  # t2789: \"cuda:0 f32[128, 16]\"\n",
       "    # t2789 = ltorch.reshape(t57, (-1, 16))  # t2789: \"cuda:0 f32[128, 16]\"\n",
       "      # t2789 = prims.reshape(t57, (128, 16))  # t2789: \"cuda:0 f32[128, 16]\"\n",
       "  del t57\n",
       "  t2790 = torch.matmul(t2788, t2789)  # t2790: \"cuda:0 f32[16, 16]\"\n",
       "    # t2790 = ltorch.matmul(t2788, t2789)  # t2790: \"cuda:0 f32[16, 16]\"\n",
       "      # t2790 = prims.matmul(t2788, t2789)  # t2790: \"cuda:0 f32[16, 16]\"\n",
       "  del t2788, t2789\n",
       "  t2779 = torch.permute(t2776, (1, 0))  # t2779: \"cuda:0 f32[16, 128]\"\n",
       "    # t2779 = ltorch.permute(t2776, (1, 0))  # t2779: \"cuda:0 f32[16, 128]\"\n",
       "      # t2779 = prims.transpose(t2776, (1, 0))  # t2779: \"cuda:0 f32[16, 128]\"\n",
       "  del t2776\n",
       "  # Created by CPU Offloading Transform\n",
       "  t62 = load_to_gpu(offloaded_t62, 'cuda:0')  # t62: \"cuda:0 f32[128, 16]\"\n",
       "  t2780 = torch.reshape(t62, (-1, 16))  # t2780: \"cuda:0 f32[128, 16]\"\n",
       "    # t2780 = ltorch.reshape(t62, (-1, 16))  # t2780: \"cuda:0 f32[128, 16]\"\n",
       "      # t2780 = prims.reshape(t62, (128, 16))  # t2780: \"cuda:0 f32[128, 16]\"\n",
       "  del t62\n",
       "  t2781 = torch.matmul(t2779, t2780)  # t2781: \"cuda:0 f32[16, 16]\"\n",
       "    # t2781 = ltorch.matmul(t2779, t2780)  # t2781: \"cuda:0 f32[16, 16]\"\n",
       "      # t2781 = prims.matmul(t2779, t2780)  # t2781: \"cuda:0 f32[16, 16]\"\n",
       "  del t2779, t2780\n",
       "  t2770 = torch.permute(t2767, (1, 0))  # t2770: \"cuda:0 f32[16, 128]\"\n",
       "    # t2770 = ltorch.permute(t2767, (1, 0))  # t2770: \"cuda:0 f32[16, 128]\"\n",
       "      # t2770 = prims.transpose(t2767, (1, 0))  # t2770: \"cuda:0 f32[16, 128]\"\n",
       "  del t2767\n",
       "  # Created by CPU Offloading Transform\n",
       "  t67 = load_to_gpu(offloaded_t67, 'cuda:0')  # t67: \"cuda:0 f32[128, 16]\"\n",
       "  t2771 = torch.reshape(t67, (-1, 16))  # t2771: \"cuda:0 f32[128, 16]\"\n",
       "    # t2771 = ltorch.reshape(t67, (-1, 16))  # t2771: \"cuda:0 f32[128, 16]\"\n",
       "      # t2771 = prims.reshape(t67, (128, 16))  # t2771: \"cuda:0 f32[128, 16]\"\n",
       "  del t67\n",
       "  t2772 = torch.matmul(t2770, t2771)  # t2772: \"cuda:0 f32[16, 16]\"\n",
       "    # t2772 = ltorch.matmul(t2770, t2771)  # t2772: \"cuda:0 f32[16, 16]\"\n",
       "      # t2772 = prims.matmul(t2770, t2771)  # t2772: \"cuda:0 f32[16, 16]\"\n",
       "  del t2770, t2771\n",
       "  t2761 = torch.permute(t2758, (1, 0))  # t2761: \"cuda:0 f32[16, 128]\"\n",
       "    # t2761 = ltorch.permute(t2758, (1, 0))  # t2761: \"cuda:0 f32[16, 128]\"\n",
       "      # t2761 = prims.transpose(t2758, (1, 0))  # t2761: \"cuda:0 f32[16, 128]\"\n",
       "  del t2758\n",
       "  # Created by CPU Offloading Transform\n",
       "  t72 = load_to_gpu(offloaded_t72, 'cuda:0')  # t72: \"cuda:0 f32[128, 16]\"\n",
       "  t2762 = torch.reshape(t72, (-1, 16))  # t2762: \"cuda:0 f32[128, 16]\"\n",
       "    # t2762 = ltorch.reshape(t72, (-1, 16))  # t2762: \"cuda:0 f32[128, 16]\"\n",
       "      # t2762 = prims.reshape(t72, (128, 16))  # t2762: \"cuda:0 f32[128, 16]\"\n",
       "  del t72\n",
       "  t2763 = torch.matmul(t2761, t2762)  # t2763: \"cuda:0 f32[16, 16]\"\n",
       "    # t2763 = ltorch.matmul(t2761, t2762)  # t2763: \"cuda:0 f32[16, 16]\"\n",
       "      # t2763 = prims.matmul(t2761, t2762)  # t2763: \"cuda:0 f32[16, 16]\"\n",
       "  del t2761, t2762\n",
       "  t2752 = torch.permute(t2749, (1, 0))  # t2752: \"cuda:0 f32[16, 128]\"\n",
       "    # t2752 = ltorch.permute(t2749, (1, 0))  # t2752: \"cuda:0 f32[16, 128]\"\n",
       "      # t2752 = prims.transpose(t2749, (1, 0))  # t2752: \"cuda:0 f32[16, 128]\"\n",
       "  del t2749\n",
       "  # Created by CPU Offloading Transform\n",
       "  t77 = load_to_gpu(offloaded_t77, 'cuda:0')  # t77: \"cuda:0 f32[128, 16]\"\n",
       "  t2753 = torch.reshape(t77, (-1, 16))  # t2753: \"cuda:0 f32[128, 16]\"\n",
       "    # t2753 = ltorch.reshape(t77, (-1, 16))  # t2753: \"cuda:0 f32[128, 16]\"\n",
       "      # t2753 = prims.reshape(t77, (128, 16))  # t2753: \"cuda:0 f32[128, 16]\"\n",
       "  del t77\n",
       "  t2754 = torch.matmul(t2752, t2753)  # t2754: \"cuda:0 f32[16, 16]\"\n",
       "    # t2754 = ltorch.matmul(t2752, t2753)  # t2754: \"cuda:0 f32[16, 16]\"\n",
       "      # t2754 = prims.matmul(t2752, t2753)  # t2754: \"cuda:0 f32[16, 16]\"\n",
       "  del t2752, t2753\n",
       "  t2743 = torch.permute(t2740, (1, 0))  # t2743: \"cuda:0 f32[16, 128]\"\n",
       "    # t2743 = ltorch.permute(t2740, (1, 0))  # t2743: \"cuda:0 f32[16, 128]\"\n",
       "      # t2743 = prims.transpose(t2740, (1, 0))  # t2743: \"cuda:0 f32[16, 128]\"\n",
       "  del t2740\n",
       "  # Created by CPU Offloading Transform\n",
       "  t82 = load_to_gpu(offloaded_t82, 'cuda:0')  # t82: \"cuda:0 f32[128, 16]\"\n",
       "  t2744 = torch.reshape(t82, (-1, 16))  # t2744: \"cuda:0 f32[128, 16]\"\n",
       "    # t2744 = ltorch.reshape(t82, (-1, 16))  # t2744: \"cuda:0 f32[128, 16]\"\n",
       "      # t2744 = prims.reshape(t82, (128, 16))  # t2744: \"cuda:0 f32[128, 16]\"\n",
       "  del t82\n",
       "  t2745 = torch.matmul(t2743, t2744)  # t2745: \"cuda:0 f32[16, 16]\"\n",
       "    # t2745 = ltorch.matmul(t2743, t2744)  # t2745: \"cuda:0 f32[16, 16]\"\n",
       "      # t2745 = prims.matmul(t2743, t2744)  # t2745: \"cuda:0 f32[16, 16]\"\n",
       "  del t2743, t2744\n",
       "  t2734 = torch.permute(t2731, (1, 0))  # t2734: \"cuda:0 f32[16, 128]\"\n",
       "    # t2734 = ltorch.permute(t2731, (1, 0))  # t2734: \"cuda:0 f32[16, 128]\"\n",
       "      # t2734 = prims.transpose(t2731, (1, 0))  # t2734: \"cuda:0 f32[16, 128]\"\n",
       "  del t2731\n",
       "  # Created by CPU Offloading Transform\n",
       "  t87 = load_to_gpu(offloaded_t87, 'cuda:0')  # t87: \"cuda:0 f32[128, 16]\"\n",
       "  t2735 = torch.reshape(t87, (-1, 16))  # t2735: \"cuda:0 f32[128, 16]\"\n",
       "    # t2735 = ltorch.reshape(t87, (-1, 16))  # t2735: \"cuda:0 f32[128, 16]\"\n",
       "      # t2735 = prims.reshape(t87, (128, 16))  # t2735: \"cuda:0 f32[128, 16]\"\n",
       "  del t87\n",
       "  t2736 = torch.matmul(t2734, t2735)  # t2736: \"cuda:0 f32[16, 16]\"\n",
       "    # t2736 = ltorch.matmul(t2734, t2735)  # t2736: \"cuda:0 f32[16, 16]\"\n",
       "      # t2736 = prims.matmul(t2734, t2735)  # t2736: \"cuda:0 f32[16, 16]\"\n",
       "  del t2734, t2735\n",
       "  t2725 = torch.permute(t2722, (1, 0))  # t2725: \"cuda:0 f32[16, 128]\"\n",
       "    # t2725 = ltorch.permute(t2722, (1, 0))  # t2725: \"cuda:0 f32[16, 128]\"\n",
       "      # t2725 = prims.transpose(t2722, (1, 0))  # t2725: \"cuda:0 f32[16, 128]\"\n",
       "  del t2722\n",
       "  # Created by CPU Offloading Transform\n",
       "  t92 = load_to_gpu(offloaded_t92, 'cuda:0')  # t92: \"cuda:0 f32[128, 16]\"\n",
       "  t2726 = torch.reshape(t92, (-1, 16))  # t2726: \"cuda:0 f32[128, 16]\"\n",
       "    # t2726 = ltorch.reshape(t92, (-1, 16))  # t2726: \"cuda:0 f32[128, 16]\"\n",
       "      # t2726 = prims.reshape(t92, (128, 16))  # t2726: \"cuda:0 f32[128, 16]\"\n",
       "  del t92\n",
       "  t2727 = torch.matmul(t2725, t2726)  # t2727: \"cuda:0 f32[16, 16]\"\n",
       "    # t2727 = ltorch.matmul(t2725, t2726)  # t2727: \"cuda:0 f32[16, 16]\"\n",
       "      # t2727 = prims.matmul(t2725, t2726)  # t2727: \"cuda:0 f32[16, 16]\"\n",
       "  del t2725, t2726\n",
       "  t2716 = torch.permute(t2713, (1, 0))  # t2716: \"cuda:0 f32[16, 128]\"\n",
       "    # t2716 = ltorch.permute(t2713, (1, 0))  # t2716: \"cuda:0 f32[16, 128]\"\n",
       "      # t2716 = prims.transpose(t2713, (1, 0))  # t2716: \"cuda:0 f32[16, 128]\"\n",
       "  del t2713\n",
       "  # Created by CPU Offloading Transform\n",
       "  t97 = load_to_gpu(offloaded_t97, 'cuda:0')  # t97: \"cuda:0 f32[128, 16]\"\n",
       "  t2717 = torch.reshape(t97, (-1, 16))  # t2717: \"cuda:0 f32[128, 16]\"\n",
       "    # t2717 = ltorch.reshape(t97, (-1, 16))  # t2717: \"cuda:0 f32[128, 16]\"\n",
       "      # t2717 = prims.reshape(t97, (128, 16))  # t2717: \"cuda:0 f32[128, 16]\"\n",
       "  del t97\n",
       "  t2718 = torch.matmul(t2716, t2717)  # t2718: \"cuda:0 f32[16, 16]\"\n",
       "    # t2718 = ltorch.matmul(t2716, t2717)  # t2718: \"cuda:0 f32[16, 16]\"\n",
       "      # t2718 = prims.matmul(t2716, t2717)  # t2718: \"cuda:0 f32[16, 16]\"\n",
       "  del t2716, t2717\n",
       "  t2707 = torch.permute(t2704, (1, 0))  # t2707: \"cuda:0 f32[16, 128]\"\n",
       "    # t2707 = ltorch.permute(t2704, (1, 0))  # t2707: \"cuda:0 f32[16, 128]\"\n",
       "      # t2707 = prims.transpose(t2704, (1, 0))  # t2707: \"cuda:0 f32[16, 128]\"\n",
       "  del t2704\n",
       "  # Created by CPU Offloading Transform\n",
       "  t102 = load_to_gpu(offloaded_t102, 'cuda:0')  # t102: \"cuda:0 f32[128, 16]\"\n",
       "  t2708 = torch.reshape(t102, (-1, 16))  # t2708: \"cuda:0 f32[128, 16]\"\n",
       "    # t2708 = ltorch.reshape(t102, (-1, 16))  # t2708: \"cuda:0 f32[128, 16]\"\n",
       "      # t2708 = prims.reshape(t102, (128, 16))  # t2708: \"cuda:0 f32[128, 16]\"\n",
       "  del t102\n",
       "  t2709 = torch.matmul(t2707, t2708)  # t2709: \"cuda:0 f32[16, 16]\"\n",
       "    # t2709 = ltorch.matmul(t2707, t2708)  # t2709: \"cuda:0 f32[16, 16]\"\n",
       "      # t2709 = prims.matmul(t2707, t2708)  # t2709: \"cuda:0 f32[16, 16]\"\n",
       "  del t2707, t2708\n",
       "  t2698 = torch.permute(t2695, (1, 0))  # t2698: \"cuda:0 f32[16, 128]\"\n",
       "    # t2698 = ltorch.permute(t2695, (1, 0))  # t2698: \"cuda:0 f32[16, 128]\"\n",
       "      # t2698 = prims.transpose(t2695, (1, 0))  # t2698: \"cuda:0 f32[16, 128]\"\n",
       "  del t2695\n",
       "  # Created by CPU Offloading Transform\n",
       "  t107 = load_to_gpu(offloaded_t107, 'cuda:0')  # t107: \"cuda:0 f32[128, 16]\"\n",
       "  t2699 = torch.reshape(t107, (-1, 16))  # t2699: \"cuda:0 f32[128, 16]\"\n",
       "    # t2699 = ltorch.reshape(t107, (-1, 16))  # t2699: \"cuda:0 f32[128, 16]\"\n",
       "      # t2699 = prims.reshape(t107, (128, 16))  # t2699: \"cuda:0 f32[128, 16]\"\n",
       "  del t107\n",
       "  t2700 = torch.matmul(t2698, t2699)  # t2700: \"cuda:0 f32[16, 16]\"\n",
       "    # t2700 = ltorch.matmul(t2698, t2699)  # t2700: \"cuda:0 f32[16, 16]\"\n",
       "      # t2700 = prims.matmul(t2698, t2699)  # t2700: \"cuda:0 f32[16, 16]\"\n",
       "  del t2698, t2699\n",
       "  t2689 = torch.permute(t2686, (1, 0))  # t2689: \"cuda:0 f32[16, 128]\"\n",
       "    # t2689 = ltorch.permute(t2686, (1, 0))  # t2689: \"cuda:0 f32[16, 128]\"\n",
       "      # t2689 = prims.transpose(t2686, (1, 0))  # t2689: \"cuda:0 f32[16, 128]\"\n",
       "  del t2686\n",
       "  # Created by CPU Offloading Transform\n",
       "  t112 = load_to_gpu(offloaded_t112, 'cuda:0')  # t112: \"cuda:0 f32[128, 16]\"\n",
       "  t2690 = torch.reshape(t112, (-1, 16))  # t2690: \"cuda:0 f32[128, 16]\"\n",
       "    # t2690 = ltorch.reshape(t112, (-1, 16))  # t2690: \"cuda:0 f32[128, 16]\"\n",
       "      # t2690 = prims.reshape(t112, (128, 16))  # t2690: \"cuda:0 f32[128, 16]\"\n",
       "  del t112\n",
       "  t2691 = torch.matmul(t2689, t2690)  # t2691: \"cuda:0 f32[16, 16]\"\n",
       "    # t2691 = ltorch.matmul(t2689, t2690)  # t2691: \"cuda:0 f32[16, 16]\"\n",
       "      # t2691 = prims.matmul(t2689, t2690)  # t2691: \"cuda:0 f32[16, 16]\"\n",
       "  del t2689, t2690\n",
       "  t2680 = torch.permute(t2677, (1, 0))  # t2680: \"cuda:0 f32[16, 128]\"\n",
       "    # t2680 = ltorch.permute(t2677, (1, 0))  # t2680: \"cuda:0 f32[16, 128]\"\n",
       "      # t2680 = prims.transpose(t2677, (1, 0))  # t2680: \"cuda:0 f32[16, 128]\"\n",
       "  del t2677\n",
       "  # Created by CPU Offloading Transform\n",
       "  t117 = load_to_gpu(offloaded_t117, 'cuda:0')  # t117: \"cuda:0 f32[128, 16]\"\n",
       "  t2681 = torch.reshape(t117, (-1, 16))  # t2681: \"cuda:0 f32[128, 16]\"\n",
       "    # t2681 = ltorch.reshape(t117, (-1, 16))  # t2681: \"cuda:0 f32[128, 16]\"\n",
       "      # t2681 = prims.reshape(t117, (128, 16))  # t2681: \"cuda:0 f32[128, 16]\"\n",
       "  del t117\n",
       "  t2682 = torch.matmul(t2680, t2681)  # t2682: \"cuda:0 f32[16, 16]\"\n",
       "    # t2682 = ltorch.matmul(t2680, t2681)  # t2682: \"cuda:0 f32[16, 16]\"\n",
       "      # t2682 = prims.matmul(t2680, t2681)  # t2682: \"cuda:0 f32[16, 16]\"\n",
       "  del t2680, t2681\n",
       "  t2671 = torch.permute(t2668, (1, 0))  # t2671: \"cuda:0 f32[16, 128]\"\n",
       "    # t2671 = ltorch.permute(t2668, (1, 0))  # t2671: \"cuda:0 f32[16, 128]\"\n",
       "      # t2671 = prims.transpose(t2668, (1, 0))  # t2671: \"cuda:0 f32[16, 128]\"\n",
       "  del t2668\n",
       "  # Created by CPU Offloading Transform\n",
       "  t122 = load_to_gpu(offloaded_t122, 'cuda:0')  # t122: \"cuda:0 f32[128, 16]\"\n",
       "  t2672 = torch.reshape(t122, (-1, 16))  # t2672: \"cuda:0 f32[128, 16]\"\n",
       "    # t2672 = ltorch.reshape(t122, (-1, 16))  # t2672: \"cuda:0 f32[128, 16]\"\n",
       "      # t2672 = prims.reshape(t122, (128, 16))  # t2672: \"cuda:0 f32[128, 16]\"\n",
       "  del t122\n",
       "  t2673 = torch.matmul(t2671, t2672)  # t2673: \"cuda:0 f32[16, 16]\"\n",
       "    # t2673 = ltorch.matmul(t2671, t2672)  # t2673: \"cuda:0 f32[16, 16]\"\n",
       "      # t2673 = prims.matmul(t2671, t2672)  # t2673: \"cuda:0 f32[16, 16]\"\n",
       "  del t2671, t2672\n",
       "  t2662 = torch.permute(t2659, (1, 0))  # t2662: \"cuda:0 f32[16, 128]\"\n",
       "    # t2662 = ltorch.permute(t2659, (1, 0))  # t2662: \"cuda:0 f32[16, 128]\"\n",
       "      # t2662 = prims.transpose(t2659, (1, 0))  # t2662: \"cuda:0 f32[16, 128]\"\n",
       "  del t2659\n",
       "  # Created by CPU Offloading Transform\n",
       "  t127 = load_to_gpu(offloaded_t127, 'cuda:0')  # t127: \"cuda:0 f32[128, 16]\"\n",
       "  t2663 = torch.reshape(t127, (-1, 16))  # t2663: \"cuda:0 f32[128, 16]\"\n",
       "    # t2663 = ltorch.reshape(t127, (-1, 16))  # t2663: \"cuda:0 f32[128, 16]\"\n",
       "      # t2663 = prims.reshape(t127, (128, 16))  # t2663: \"cuda:0 f32[128, 16]\"\n",
       "  del t127\n",
       "  t2664 = torch.matmul(t2662, t2663)  # t2664: \"cuda:0 f32[16, 16]\"\n",
       "    # t2664 = ltorch.matmul(t2662, t2663)  # t2664: \"cuda:0 f32[16, 16]\"\n",
       "      # t2664 = prims.matmul(t2662, t2663)  # t2664: \"cuda:0 f32[16, 16]\"\n",
       "  del t2662, t2663\n",
       "  t2653 = torch.permute(t2650, (1, 0))  # t2653: \"cuda:0 f32[16, 128]\"\n",
       "    # t2653 = ltorch.permute(t2650, (1, 0))  # t2653: \"cuda:0 f32[16, 128]\"\n",
       "      # t2653 = prims.transpose(t2650, (1, 0))  # t2653: \"cuda:0 f32[16, 128]\"\n",
       "  del t2650\n",
       "  # Created by CPU Offloading Transform\n",
       "  t132 = load_to_gpu(offloaded_t132, 'cuda:0')  # t132: \"cuda:0 f32[128, 16]\"\n",
       "  t2654 = torch.reshape(t132, (-1, 16))  # t2654: \"cuda:0 f32[128, 16]\"\n",
       "    # t2654 = ltorch.reshape(t132, (-1, 16))  # t2654: \"cuda:0 f32[128, 16]\"\n",
       "      # t2654 = prims.reshape(t132, (128, 16))  # t2654: \"cuda:0 f32[128, 16]\"\n",
       "  del t132\n",
       "  t2655 = torch.matmul(t2653, t2654)  # t2655: \"cuda:0 f32[16, 16]\"\n",
       "    # t2655 = ltorch.matmul(t2653, t2654)  # t2655: \"cuda:0 f32[16, 16]\"\n",
       "      # t2655 = prims.matmul(t2653, t2654)  # t2655: \"cuda:0 f32[16, 16]\"\n",
       "  del t2653, t2654\n",
       "  t2644 = torch.permute(t2641, (1, 0))  # t2644: \"cuda:0 f32[16, 128]\"\n",
       "    # t2644 = ltorch.permute(t2641, (1, 0))  # t2644: \"cuda:0 f32[16, 128]\"\n",
       "      # t2644 = prims.transpose(t2641, (1, 0))  # t2644: \"cuda:0 f32[16, 128]\"\n",
       "  del t2641\n",
       "  # Created by CPU Offloading Transform\n",
       "  t137 = load_to_gpu(offloaded_t137, 'cuda:0')  # t137: \"cuda:0 f32[128, 16]\"\n",
       "  t2645 = torch.reshape(t137, (-1, 16))  # t2645: \"cuda:0 f32[128, 16]\"\n",
       "    # t2645 = ltorch.reshape(t137, (-1, 16))  # t2645: \"cuda:0 f32[128, 16]\"\n",
       "      # t2645 = prims.reshape(t137, (128, 16))  # t2645: \"cuda:0 f32[128, 16]\"\n",
       "  del t137\n",
       "  t2646 = torch.matmul(t2644, t2645)  # t2646: \"cuda:0 f32[16, 16]\"\n",
       "    # t2646 = ltorch.matmul(t2644, t2645)  # t2646: \"cuda:0 f32[16, 16]\"\n",
       "      # t2646 = prims.matmul(t2644, t2645)  # t2646: \"cuda:0 f32[16, 16]\"\n",
       "  del t2644, t2645\n",
       "  t2635 = torch.permute(t2632, (1, 0))  # t2635: \"cuda:0 f32[16, 128]\"\n",
       "    # t2635 = ltorch.permute(t2632, (1, 0))  # t2635: \"cuda:0 f32[16, 128]\"\n",
       "      # t2635 = prims.transpose(t2632, (1, 0))  # t2635: \"cuda:0 f32[16, 128]\"\n",
       "  del t2632\n",
       "  # Created by CPU Offloading Transform\n",
       "  t142 = load_to_gpu(offloaded_t142, 'cuda:0')  # t142: \"cuda:0 f32[128, 16]\"\n",
       "  t2636 = torch.reshape(t142, (-1, 16))  # t2636: \"cuda:0 f32[128, 16]\"\n",
       "    # t2636 = ltorch.reshape(t142, (-1, 16))  # t2636: \"cuda:0 f32[128, 16]\"\n",
       "      # t2636 = prims.reshape(t142, (128, 16))  # t2636: \"cuda:0 f32[128, 16]\"\n",
       "  del t142\n",
       "  t2637 = torch.matmul(t2635, t2636)  # t2637: \"cuda:0 f32[16, 16]\"\n",
       "    # t2637 = ltorch.matmul(t2635, t2636)  # t2637: \"cuda:0 f32[16, 16]\"\n",
       "      # t2637 = prims.matmul(t2635, t2636)  # t2637: \"cuda:0 f32[16, 16]\"\n",
       "  del t2635, t2636\n",
       "  t2626 = torch.permute(t2623, (1, 0))  # t2626: \"cuda:0 f32[16, 128]\"\n",
       "    # t2626 = ltorch.permute(t2623, (1, 0))  # t2626: \"cuda:0 f32[16, 128]\"\n",
       "      # t2626 = prims.transpose(t2623, (1, 0))  # t2626: \"cuda:0 f32[16, 128]\"\n",
       "  del t2623\n",
       "  # Created by CPU Offloading Transform\n",
       "  t147 = load_to_gpu(offloaded_t147, 'cuda:0')  # t147: \"cuda:0 f32[128, 16]\"\n",
       "  t2627 = torch.reshape(t147, (-1, 16))  # t2627: \"cuda:0 f32[128, 16]\"\n",
       "    # t2627 = ltorch.reshape(t147, (-1, 16))  # t2627: \"cuda:0 f32[128, 16]\"\n",
       "      # t2627 = prims.reshape(t147, (128, 16))  # t2627: \"cuda:0 f32[128, 16]\"\n",
       "  del t147\n",
       "  t2628 = torch.matmul(t2626, t2627)  # t2628: \"cuda:0 f32[16, 16]\"\n",
       "    # t2628 = ltorch.matmul(t2626, t2627)  # t2628: \"cuda:0 f32[16, 16]\"\n",
       "      # t2628 = prims.matmul(t2626, t2627)  # t2628: \"cuda:0 f32[16, 16]\"\n",
       "  del t2626, t2627\n",
       "  t2617 = torch.permute(t2614, (1, 0))  # t2617: \"cuda:0 f32[16, 128]\"\n",
       "    # t2617 = ltorch.permute(t2614, (1, 0))  # t2617: \"cuda:0 f32[16, 128]\"\n",
       "      # t2617 = prims.transpose(t2614, (1, 0))  # t2617: \"cuda:0 f32[16, 128]\"\n",
       "  del t2614\n",
       "  # Created by CPU Offloading Transform\n",
       "  t152 = load_to_gpu(offloaded_t152, 'cuda:0')  # t152: \"cuda:0 f32[128, 16]\"\n",
       "  t2618 = torch.reshape(t152, (-1, 16))  # t2618: \"cuda:0 f32[128, 16]\"\n",
       "    # t2618 = ltorch.reshape(t152, (-1, 16))  # t2618: \"cuda:0 f32[128, 16]\"\n",
       "      # t2618 = prims.reshape(t152, (128, 16))  # t2618: \"cuda:0 f32[128, 16]\"\n",
       "  del t152\n",
       "  t2619 = torch.matmul(t2617, t2618)  # t2619: \"cuda:0 f32[16, 16]\"\n",
       "    # t2619 = ltorch.matmul(t2617, t2618)  # t2619: \"cuda:0 f32[16, 16]\"\n",
       "      # t2619 = prims.matmul(t2617, t2618)  # t2619: \"cuda:0 f32[16, 16]\"\n",
       "  del t2617, t2618\n",
       "  t2608 = torch.permute(t2605, (1, 0))  # t2608: \"cuda:0 f32[16, 128]\"\n",
       "    # t2608 = ltorch.permute(t2605, (1, 0))  # t2608: \"cuda:0 f32[16, 128]\"\n",
       "      # t2608 = prims.transpose(t2605, (1, 0))  # t2608: \"cuda:0 f32[16, 128]\"\n",
       "  del t2605\n",
       "  # Created by CPU Offloading Transform\n",
       "  t157 = load_to_gpu(offloaded_t157, 'cuda:0')  # t157: \"cuda:0 f32[128, 16]\"\n",
       "  t2609 = torch.reshape(t157, (-1, 16))  # t2609: \"cuda:0 f32[128, 16]\"\n",
       "    # t2609 = ltorch.reshape(t157, (-1, 16))  # t2609: \"cuda:0 f32[128, 16]\"\n",
       "      # t2609 = prims.reshape(t157, (128, 16))  # t2609: \"cuda:0 f32[128, 16]\"\n",
       "  del t157\n",
       "  t2610 = torch.matmul(t2608, t2609)  # t2610: \"cuda:0 f32[16, 16]\"\n",
       "    # t2610 = ltorch.matmul(t2608, t2609)  # t2610: \"cuda:0 f32[16, 16]\"\n",
       "      # t2610 = prims.matmul(t2608, t2609)  # t2610: \"cuda:0 f32[16, 16]\"\n",
       "  del t2608, t2609\n",
       "  t2599 = torch.permute(t2596, (1, 0))  # t2599: \"cuda:0 f32[16, 128]\"\n",
       "    # t2599 = ltorch.permute(t2596, (1, 0))  # t2599: \"cuda:0 f32[16, 128]\"\n",
       "      # t2599 = prims.transpose(t2596, (1, 0))  # t2599: \"cuda:0 f32[16, 128]\"\n",
       "  del t2596\n",
       "  # Created by CPU Offloading Transform\n",
       "  t162 = load_to_gpu(offloaded_t162, 'cuda:0')  # t162: \"cuda:0 f32[128, 16]\"\n",
       "  t2600 = torch.reshape(t162, (-1, 16))  # t2600: \"cuda:0 f32[128, 16]\"\n",
       "    # t2600 = ltorch.reshape(t162, (-1, 16))  # t2600: \"cuda:0 f32[128, 16]\"\n",
       "      # t2600 = prims.reshape(t162, (128, 16))  # t2600: \"cuda:0 f32[128, 16]\"\n",
       "  del t162\n",
       "  t2601 = torch.matmul(t2599, t2600)  # t2601: \"cuda:0 f32[16, 16]\"\n",
       "    # t2601 = ltorch.matmul(t2599, t2600)  # t2601: \"cuda:0 f32[16, 16]\"\n",
       "      # t2601 = prims.matmul(t2599, t2600)  # t2601: \"cuda:0 f32[16, 16]\"\n",
       "  del t2599, t2600\n",
       "  t2590 = torch.permute(t2587, (1, 0))  # t2590: \"cuda:0 f32[16, 128]\"\n",
       "    # t2590 = ltorch.permute(t2587, (1, 0))  # t2590: \"cuda:0 f32[16, 128]\"\n",
       "      # t2590 = prims.transpose(t2587, (1, 0))  # t2590: \"cuda:0 f32[16, 128]\"\n",
       "  del t2587\n",
       "  # Created by CPU Offloading Transform\n",
       "  t167 = load_to_gpu(offloaded_t167, 'cuda:0')  # t167: \"cuda:0 f32[128, 16]\"\n",
       "  t2591 = torch.reshape(t167, (-1, 16))  # t2591: \"cuda:0 f32[128, 16]\"\n",
       "    # t2591 = ltorch.reshape(t167, (-1, 16))  # t2591: \"cuda:0 f32[128, 16]\"\n",
       "      # t2591 = prims.reshape(t167, (128, 16))  # t2591: \"cuda:0 f32[128, 16]\"\n",
       "  del t167\n",
       "  t2592 = torch.matmul(t2590, t2591)  # t2592: \"cuda:0 f32[16, 16]\"\n",
       "    # t2592 = ltorch.matmul(t2590, t2591)  # t2592: \"cuda:0 f32[16, 16]\"\n",
       "      # t2592 = prims.matmul(t2590, t2591)  # t2592: \"cuda:0 f32[16, 16]\"\n",
       "  del t2590, t2591\n",
       "  t2581 = torch.permute(t2578, (1, 0))  # t2581: \"cuda:0 f32[16, 128]\"\n",
       "    # t2581 = ltorch.permute(t2578, (1, 0))  # t2581: \"cuda:0 f32[16, 128]\"\n",
       "      # t2581 = prims.transpose(t2578, (1, 0))  # t2581: \"cuda:0 f32[16, 128]\"\n",
       "  del t2578\n",
       "  # Created by CPU Offloading Transform\n",
       "  t172 = load_to_gpu(offloaded_t172, 'cuda:0')  # t172: \"cuda:0 f32[128, 16]\"\n",
       "  t2582 = torch.reshape(t172, (-1, 16))  # t2582: \"cuda:0 f32[128, 16]\"\n",
       "    # t2582 = ltorch.reshape(t172, (-1, 16))  # t2582: \"cuda:0 f32[128, 16]\"\n",
       "      # t2582 = prims.reshape(t172, (128, 16))  # t2582: \"cuda:0 f32[128, 16]\"\n",
       "  del t172\n",
       "  t2583 = torch.matmul(t2581, t2582)  # t2583: \"cuda:0 f32[16, 16]\"\n",
       "    # t2583 = ltorch.matmul(t2581, t2582)  # t2583: \"cuda:0 f32[16, 16]\"\n",
       "      # t2583 = prims.matmul(t2581, t2582)  # t2583: \"cuda:0 f32[16, 16]\"\n",
       "  del t2581, t2582\n",
       "  t2572 = torch.permute(t2569, (1, 0))  # t2572: \"cuda:0 f32[16, 128]\"\n",
       "    # t2572 = ltorch.permute(t2569, (1, 0))  # t2572: \"cuda:0 f32[16, 128]\"\n",
       "      # t2572 = prims.transpose(t2569, (1, 0))  # t2572: \"cuda:0 f32[16, 128]\"\n",
       "  del t2569\n",
       "  # Created by CPU Offloading Transform\n",
       "  t177 = load_to_gpu(offloaded_t177, 'cuda:0')  # t177: \"cuda:0 f32[128, 16]\"\n",
       "  t2573 = torch.reshape(t177, (-1, 16))  # t2573: \"cuda:0 f32[128, 16]\"\n",
       "    # t2573 = ltorch.reshape(t177, (-1, 16))  # t2573: \"cuda:0 f32[128, 16]\"\n",
       "      # t2573 = prims.reshape(t177, (128, 16))  # t2573: \"cuda:0 f32[128, 16]\"\n",
       "  del t177\n",
       "  t2574 = torch.matmul(t2572, t2573)  # t2574: \"cuda:0 f32[16, 16]\"\n",
       "    # t2574 = ltorch.matmul(t2572, t2573)  # t2574: \"cuda:0 f32[16, 16]\"\n",
       "      # t2574 = prims.matmul(t2572, t2573)  # t2574: \"cuda:0 f32[16, 16]\"\n",
       "  del t2572, t2573\n",
       "  t2563 = torch.permute(t2560, (1, 0))  # t2563: \"cuda:0 f32[16, 128]\"\n",
       "    # t2563 = ltorch.permute(t2560, (1, 0))  # t2563: \"cuda:0 f32[16, 128]\"\n",
       "      # t2563 = prims.transpose(t2560, (1, 0))  # t2563: \"cuda:0 f32[16, 128]\"\n",
       "  del t2560\n",
       "  # Created by CPU Offloading Transform\n",
       "  t182 = load_to_gpu(offloaded_t182, 'cuda:0')  # t182: \"cuda:0 f32[128, 16]\"\n",
       "  t2564 = torch.reshape(t182, (-1, 16))  # t2564: \"cuda:0 f32[128, 16]\"\n",
       "    # t2564 = ltorch.reshape(t182, (-1, 16))  # t2564: \"cuda:0 f32[128, 16]\"\n",
       "      # t2564 = prims.reshape(t182, (128, 16))  # t2564: \"cuda:0 f32[128, 16]\"\n",
       "  del t182\n",
       "  t2565 = torch.matmul(t2563, t2564)  # t2565: \"cuda:0 f32[16, 16]\"\n",
       "    # t2565 = ltorch.matmul(t2563, t2564)  # t2565: \"cuda:0 f32[16, 16]\"\n",
       "      # t2565 = prims.matmul(t2563, t2564)  # t2565: \"cuda:0 f32[16, 16]\"\n",
       "  del t2563, t2564\n",
       "  t2554 = torch.permute(t2551, (1, 0))  # t2554: \"cuda:0 f32[16, 128]\"\n",
       "    # t2554 = ltorch.permute(t2551, (1, 0))  # t2554: \"cuda:0 f32[16, 128]\"\n",
       "      # t2554 = prims.transpose(t2551, (1, 0))  # t2554: \"cuda:0 f32[16, 128]\"\n",
       "  del t2551\n",
       "  # Created by CPU Offloading Transform\n",
       "  t187 = load_to_gpu(offloaded_t187, 'cuda:0')  # t187: \"cuda:0 f32[128, 16]\"\n",
       "  t2555 = torch.reshape(t187, (-1, 16))  # t2555: \"cuda:0 f32[128, 16]\"\n",
       "    # t2555 = ltorch.reshape(t187, (-1, 16))  # t2555: \"cuda:0 f32[128, 16]\"\n",
       "      # t2555 = prims.reshape(t187, (128, 16))  # t2555: \"cuda:0 f32[128, 16]\"\n",
       "  del t187\n",
       "  t2556 = torch.matmul(t2554, t2555)  # t2556: \"cuda:0 f32[16, 16]\"\n",
       "    # t2556 = ltorch.matmul(t2554, t2555)  # t2556: \"cuda:0 f32[16, 16]\"\n",
       "      # t2556 = prims.matmul(t2554, t2555)  # t2556: \"cuda:0 f32[16, 16]\"\n",
       "  del t2554, t2555\n",
       "  t2545 = torch.permute(t2542, (1, 0))  # t2545: \"cuda:0 f32[16, 128]\"\n",
       "    # t2545 = ltorch.permute(t2542, (1, 0))  # t2545: \"cuda:0 f32[16, 128]\"\n",
       "      # t2545 = prims.transpose(t2542, (1, 0))  # t2545: \"cuda:0 f32[16, 128]\"\n",
       "  del t2542\n",
       "  # Created by CPU Offloading Transform\n",
       "  t192 = load_to_gpu(offloaded_t192, 'cuda:0')  # t192: \"cuda:0 f32[128, 16]\"\n",
       "  t2546 = torch.reshape(t192, (-1, 16))  # t2546: \"cuda:0 f32[128, 16]\"\n",
       "    # t2546 = ltorch.reshape(t192, (-1, 16))  # t2546: \"cuda:0 f32[128, 16]\"\n",
       "      # t2546 = prims.reshape(t192, (128, 16))  # t2546: \"cuda:0 f32[128, 16]\"\n",
       "  del t192\n",
       "  t2547 = torch.matmul(t2545, t2546)  # t2547: \"cuda:0 f32[16, 16]\"\n",
       "    # t2547 = ltorch.matmul(t2545, t2546)  # t2547: \"cuda:0 f32[16, 16]\"\n",
       "      # t2547 = prims.matmul(t2545, t2546)  # t2547: \"cuda:0 f32[16, 16]\"\n",
       "  del t2545, t2546\n",
       "  t2536 = torch.permute(t2533, (1, 0))  # t2536: \"cuda:0 f32[16, 128]\"\n",
       "    # t2536 = ltorch.permute(t2533, (1, 0))  # t2536: \"cuda:0 f32[16, 128]\"\n",
       "      # t2536 = prims.transpose(t2533, (1, 0))  # t2536: \"cuda:0 f32[16, 128]\"\n",
       "  del t2533\n",
       "  # Created by CPU Offloading Transform\n",
       "  t197 = load_to_gpu(offloaded_t197, 'cuda:0')  # t197: \"cuda:0 f32[128, 16]\"\n",
       "  t2537 = torch.reshape(t197, (-1, 16))  # t2537: \"cuda:0 f32[128, 16]\"\n",
       "    # t2537 = ltorch.reshape(t197, (-1, 16))  # t2537: \"cuda:0 f32[128, 16]\"\n",
       "      # t2537 = prims.reshape(t197, (128, 16))  # t2537: \"cuda:0 f32[128, 16]\"\n",
       "  del t197\n",
       "  t2538 = torch.matmul(t2536, t2537)  # t2538: \"cuda:0 f32[16, 16]\"\n",
       "    # t2538 = ltorch.matmul(t2536, t2537)  # t2538: \"cuda:0 f32[16, 16]\"\n",
       "      # t2538 = prims.matmul(t2536, t2537)  # t2538: \"cuda:0 f32[16, 16]\"\n",
       "  del t2536, t2537\n",
       "  t2527 = torch.permute(t2524, (1, 0))  # t2527: \"cuda:0 f32[16, 128]\"\n",
       "    # t2527 = ltorch.permute(t2524, (1, 0))  # t2527: \"cuda:0 f32[16, 128]\"\n",
       "      # t2527 = prims.transpose(t2524, (1, 0))  # t2527: \"cuda:0 f32[16, 128]\"\n",
       "  del t2524\n",
       "  # Created by CPU Offloading Transform\n",
       "  t202 = load_to_gpu(offloaded_t202, 'cuda:0')  # t202: \"cuda:0 f32[128, 16]\"\n",
       "  t2528 = torch.reshape(t202, (-1, 16))  # t2528: \"cuda:0 f32[128, 16]\"\n",
       "    # t2528 = ltorch.reshape(t202, (-1, 16))  # t2528: \"cuda:0 f32[128, 16]\"\n",
       "      # t2528 = prims.reshape(t202, (128, 16))  # t2528: \"cuda:0 f32[128, 16]\"\n",
       "  del t202\n",
       "  t2529 = torch.matmul(t2527, t2528)  # t2529: \"cuda:0 f32[16, 16]\"\n",
       "    # t2529 = ltorch.matmul(t2527, t2528)  # t2529: \"cuda:0 f32[16, 16]\"\n",
       "      # t2529 = prims.matmul(t2527, t2528)  # t2529: \"cuda:0 f32[16, 16]\"\n",
       "  del t2527, t2528\n",
       "  t2518 = torch.permute(t2515, (1, 0))  # t2518: \"cuda:0 f32[16, 128]\"\n",
       "    # t2518 = ltorch.permute(t2515, (1, 0))  # t2518: \"cuda:0 f32[16, 128]\"\n",
       "      # t2518 = prims.transpose(t2515, (1, 0))  # t2518: \"cuda:0 f32[16, 128]\"\n",
       "  del t2515\n",
       "  # Created by CPU Offloading Transform\n",
       "  t207 = load_to_gpu(offloaded_t207, 'cuda:0')  # t207: \"cuda:0 f32[128, 16]\"\n",
       "  t2519 = torch.reshape(t207, (-1, 16))  # t2519: \"cuda:0 f32[128, 16]\"\n",
       "    # t2519 = ltorch.reshape(t207, (-1, 16))  # t2519: \"cuda:0 f32[128, 16]\"\n",
       "      # t2519 = prims.reshape(t207, (128, 16))  # t2519: \"cuda:0 f32[128, 16]\"\n",
       "  del t207\n",
       "  t2520 = torch.matmul(t2518, t2519)  # t2520: \"cuda:0 f32[16, 16]\"\n",
       "    # t2520 = ltorch.matmul(t2518, t2519)  # t2520: \"cuda:0 f32[16, 16]\"\n",
       "      # t2520 = prims.matmul(t2518, t2519)  # t2520: \"cuda:0 f32[16, 16]\"\n",
       "  del t2518, t2519\n",
       "  t2509 = torch.permute(t2506, (1, 0))  # t2509: \"cuda:0 f32[16, 128]\"\n",
       "    # t2509 = ltorch.permute(t2506, (1, 0))  # t2509: \"cuda:0 f32[16, 128]\"\n",
       "      # t2509 = prims.transpose(t2506, (1, 0))  # t2509: \"cuda:0 f32[16, 128]\"\n",
       "  del t2506\n",
       "  # Created by CPU Offloading Transform\n",
       "  t212 = load_to_gpu(offloaded_t212, 'cuda:0')  # t212: \"cuda:0 f32[128, 16]\"\n",
       "  t2510 = torch.reshape(t212, (-1, 16))  # t2510: \"cuda:0 f32[128, 16]\"\n",
       "    # t2510 = ltorch.reshape(t212, (-1, 16))  # t2510: \"cuda:0 f32[128, 16]\"\n",
       "      # t2510 = prims.reshape(t212, (128, 16))  # t2510: \"cuda:0 f32[128, 16]\"\n",
       "  del t212\n",
       "  t2511 = torch.matmul(t2509, t2510)  # t2511: \"cuda:0 f32[16, 16]\"\n",
       "    # t2511 = ltorch.matmul(t2509, t2510)  # t2511: \"cuda:0 f32[16, 16]\"\n",
       "      # t2511 = prims.matmul(t2509, t2510)  # t2511: \"cuda:0 f32[16, 16]\"\n",
       "  del t2509, t2510\n",
       "  t2500 = torch.permute(t2497, (1, 0))  # t2500: \"cuda:0 f32[16, 128]\"\n",
       "    # t2500 = ltorch.permute(t2497, (1, 0))  # t2500: \"cuda:0 f32[16, 128]\"\n",
       "      # t2500 = prims.transpose(t2497, (1, 0))  # t2500: \"cuda:0 f32[16, 128]\"\n",
       "  del t2497\n",
       "  # Created by CPU Offloading Transform\n",
       "  t217 = load_to_gpu(offloaded_t217, 'cuda:0')  # t217: \"cuda:0 f32[128, 16]\"\n",
       "  t2501 = torch.reshape(t217, (-1, 16))  # t2501: \"cuda:0 f32[128, 16]\"\n",
       "    # t2501 = ltorch.reshape(t217, (-1, 16))  # t2501: \"cuda:0 f32[128, 16]\"\n",
       "      # t2501 = prims.reshape(t217, (128, 16))  # t2501: \"cuda:0 f32[128, 16]\"\n",
       "  del t217\n",
       "  t2502 = torch.matmul(t2500, t2501)  # t2502: \"cuda:0 f32[16, 16]\"\n",
       "    # t2502 = ltorch.matmul(t2500, t2501)  # t2502: \"cuda:0 f32[16, 16]\"\n",
       "      # t2502 = prims.matmul(t2500, t2501)  # t2502: \"cuda:0 f32[16, 16]\"\n",
       "  del t2500, t2501\n",
       "  t2491 = torch.permute(t2488, (1, 0))  # t2491: \"cuda:0 f32[16, 128]\"\n",
       "    # t2491 = ltorch.permute(t2488, (1, 0))  # t2491: \"cuda:0 f32[16, 128]\"\n",
       "      # t2491 = prims.transpose(t2488, (1, 0))  # t2491: \"cuda:0 f32[16, 128]\"\n",
       "  del t2488\n",
       "  # Created by CPU Offloading Transform\n",
       "  t222 = load_to_gpu(offloaded_t222, 'cuda:0')  # t222: \"cuda:0 f32[128, 16]\"\n",
       "  t2492 = torch.reshape(t222, (-1, 16))  # t2492: \"cuda:0 f32[128, 16]\"\n",
       "    # t2492 = ltorch.reshape(t222, (-1, 16))  # t2492: \"cuda:0 f32[128, 16]\"\n",
       "      # t2492 = prims.reshape(t222, (128, 16))  # t2492: \"cuda:0 f32[128, 16]\"\n",
       "  del t222\n",
       "  t2493 = torch.matmul(t2491, t2492)  # t2493: \"cuda:0 f32[16, 16]\"\n",
       "    # t2493 = ltorch.matmul(t2491, t2492)  # t2493: \"cuda:0 f32[16, 16]\"\n",
       "      # t2493 = prims.matmul(t2491, t2492)  # t2493: \"cuda:0 f32[16, 16]\"\n",
       "  del t2491, t2492\n",
       "  t2482 = torch.permute(t2479, (1, 0))  # t2482: \"cuda:0 f32[16, 128]\"\n",
       "    # t2482 = ltorch.permute(t2479, (1, 0))  # t2482: \"cuda:0 f32[16, 128]\"\n",
       "      # t2482 = prims.transpose(t2479, (1, 0))  # t2482: \"cuda:0 f32[16, 128]\"\n",
       "  del t2479\n",
       "  # Created by CPU Offloading Transform\n",
       "  t227 = load_to_gpu(offloaded_t227, 'cuda:0')  # t227: \"cuda:0 f32[128, 16]\"\n",
       "  t2483 = torch.reshape(t227, (-1, 16))  # t2483: \"cuda:0 f32[128, 16]\"\n",
       "    # t2483 = ltorch.reshape(t227, (-1, 16))  # t2483: \"cuda:0 f32[128, 16]\"\n",
       "      # t2483 = prims.reshape(t227, (128, 16))  # t2483: \"cuda:0 f32[128, 16]\"\n",
       "  del t227\n",
       "  t2484 = torch.matmul(t2482, t2483)  # t2484: \"cuda:0 f32[16, 16]\"\n",
       "    # t2484 = ltorch.matmul(t2482, t2483)  # t2484: \"cuda:0 f32[16, 16]\"\n",
       "      # t2484 = prims.matmul(t2482, t2483)  # t2484: \"cuda:0 f32[16, 16]\"\n",
       "  del t2482, t2483\n",
       "  t2473 = torch.permute(t2470, (1, 0))  # t2473: \"cuda:0 f32[16, 128]\"\n",
       "    # t2473 = ltorch.permute(t2470, (1, 0))  # t2473: \"cuda:0 f32[16, 128]\"\n",
       "      # t2473 = prims.transpose(t2470, (1, 0))  # t2473: \"cuda:0 f32[16, 128]\"\n",
       "  del t2470\n",
       "  # Created by CPU Offloading Transform\n",
       "  t232 = load_to_gpu(offloaded_t232, 'cuda:0')  # t232: \"cuda:0 f32[128, 16]\"\n",
       "  t2474 = torch.reshape(t232, (-1, 16))  # t2474: \"cuda:0 f32[128, 16]\"\n",
       "    # t2474 = ltorch.reshape(t232, (-1, 16))  # t2474: \"cuda:0 f32[128, 16]\"\n",
       "      # t2474 = prims.reshape(t232, (128, 16))  # t2474: \"cuda:0 f32[128, 16]\"\n",
       "  del t232\n",
       "  t2475 = torch.matmul(t2473, t2474)  # t2475: \"cuda:0 f32[16, 16]\"\n",
       "    # t2475 = ltorch.matmul(t2473, t2474)  # t2475: \"cuda:0 f32[16, 16]\"\n",
       "      # t2475 = prims.matmul(t2473, t2474)  # t2475: \"cuda:0 f32[16, 16]\"\n",
       "  del t2473, t2474\n",
       "  t2464 = torch.permute(t2461, (1, 0))  # t2464: \"cuda:0 f32[16, 128]\"\n",
       "    # t2464 = ltorch.permute(t2461, (1, 0))  # t2464: \"cuda:0 f32[16, 128]\"\n",
       "      # t2464 = prims.transpose(t2461, (1, 0))  # t2464: \"cuda:0 f32[16, 128]\"\n",
       "  del t2461\n",
       "  # Created by CPU Offloading Transform\n",
       "  t237 = load_to_gpu(offloaded_t237, 'cuda:0')  # t237: \"cuda:0 f32[128, 16]\"\n",
       "  t2465 = torch.reshape(t237, (-1, 16))  # t2465: \"cuda:0 f32[128, 16]\"\n",
       "    # t2465 = ltorch.reshape(t237, (-1, 16))  # t2465: \"cuda:0 f32[128, 16]\"\n",
       "      # t2465 = prims.reshape(t237, (128, 16))  # t2465: \"cuda:0 f32[128, 16]\"\n",
       "  del t237\n",
       "  t2466 = torch.matmul(t2464, t2465)  # t2466: \"cuda:0 f32[16, 16]\"\n",
       "    # t2466 = ltorch.matmul(t2464, t2465)  # t2466: \"cuda:0 f32[16, 16]\"\n",
       "      # t2466 = prims.matmul(t2464, t2465)  # t2466: \"cuda:0 f32[16, 16]\"\n",
       "  del t2464, t2465\n",
       "  t2455 = torch.permute(t2452, (1, 0))  # t2455: \"cuda:0 f32[16, 128]\"\n",
       "    # t2455 = ltorch.permute(t2452, (1, 0))  # t2455: \"cuda:0 f32[16, 128]\"\n",
       "      # t2455 = prims.transpose(t2452, (1, 0))  # t2455: \"cuda:0 f32[16, 128]\"\n",
       "  del t2452\n",
       "  # Created by CPU Offloading Transform\n",
       "  t242 = load_to_gpu(offloaded_t242, 'cuda:0')  # t242: \"cuda:0 f32[128, 16]\"\n",
       "  t2456 = torch.reshape(t242, (-1, 16))  # t2456: \"cuda:0 f32[128, 16]\"\n",
       "    # t2456 = ltorch.reshape(t242, (-1, 16))  # t2456: \"cuda:0 f32[128, 16]\"\n",
       "      # t2456 = prims.reshape(t242, (128, 16))  # t2456: \"cuda:0 f32[128, 16]\"\n",
       "  del t242\n",
       "  t2457 = torch.matmul(t2455, t2456)  # t2457: \"cuda:0 f32[16, 16]\"\n",
       "    # t2457 = ltorch.matmul(t2455, t2456)  # t2457: \"cuda:0 f32[16, 16]\"\n",
       "      # t2457 = prims.matmul(t2455, t2456)  # t2457: \"cuda:0 f32[16, 16]\"\n",
       "  del t2455, t2456\n",
       "  t2446 = torch.permute(t2443, (1, 0))  # t2446: \"cuda:0 f32[16, 128]\"\n",
       "    # t2446 = ltorch.permute(t2443, (1, 0))  # t2446: \"cuda:0 f32[16, 128]\"\n",
       "      # t2446 = prims.transpose(t2443, (1, 0))  # t2446: \"cuda:0 f32[16, 128]\"\n",
       "  del t2443\n",
       "  # Created by CPU Offloading Transform\n",
       "  t247 = load_to_gpu(offloaded_t247, 'cuda:0')  # t247: \"cuda:0 f32[128, 16]\"\n",
       "  t2447 = torch.reshape(t247, (-1, 16))  # t2447: \"cuda:0 f32[128, 16]\"\n",
       "    # t2447 = ltorch.reshape(t247, (-1, 16))  # t2447: \"cuda:0 f32[128, 16]\"\n",
       "      # t2447 = prims.reshape(t247, (128, 16))  # t2447: \"cuda:0 f32[128, 16]\"\n",
       "  del t247\n",
       "  t2448 = torch.matmul(t2446, t2447)  # t2448: \"cuda:0 f32[16, 16]\"\n",
       "    # t2448 = ltorch.matmul(t2446, t2447)  # t2448: \"cuda:0 f32[16, 16]\"\n",
       "      # t2448 = prims.matmul(t2446, t2447)  # t2448: \"cuda:0 f32[16, 16]\"\n",
       "  del t2446, t2447\n",
       "  t2437 = torch.permute(t2434, (1, 0))  # t2437: \"cuda:0 f32[16, 128]\"\n",
       "    # t2437 = ltorch.permute(t2434, (1, 0))  # t2437: \"cuda:0 f32[16, 128]\"\n",
       "      # t2437 = prims.transpose(t2434, (1, 0))  # t2437: \"cuda:0 f32[16, 128]\"\n",
       "  del t2434\n",
       "  # Created by CPU Offloading Transform\n",
       "  t252 = load_to_gpu(offloaded_t252, 'cuda:0')  # t252: \"cuda:0 f32[128, 16]\"\n",
       "  t2438 = torch.reshape(t252, (-1, 16))  # t2438: \"cuda:0 f32[128, 16]\"\n",
       "    # t2438 = ltorch.reshape(t252, (-1, 16))  # t2438: \"cuda:0 f32[128, 16]\"\n",
       "      # t2438 = prims.reshape(t252, (128, 16))  # t2438: \"cuda:0 f32[128, 16]\"\n",
       "  del t252\n",
       "  t2439 = torch.matmul(t2437, t2438)  # t2439: \"cuda:0 f32[16, 16]\"\n",
       "    # t2439 = ltorch.matmul(t2437, t2438)  # t2439: \"cuda:0 f32[16, 16]\"\n",
       "      # t2439 = prims.matmul(t2437, t2438)  # t2439: \"cuda:0 f32[16, 16]\"\n",
       "  del t2437, t2438\n",
       "  t2428 = torch.permute(t2425, (1, 0))  # t2428: \"cuda:0 f32[16, 128]\"\n",
       "    # t2428 = ltorch.permute(t2425, (1, 0))  # t2428: \"cuda:0 f32[16, 128]\"\n",
       "      # t2428 = prims.transpose(t2425, (1, 0))  # t2428: \"cuda:0 f32[16, 128]\"\n",
       "  del t2425\n",
       "  # Created by CPU Offloading Transform\n",
       "  t257 = load_to_gpu(offloaded_t257, 'cuda:0')  # t257: \"cuda:0 f32[128, 16]\"\n",
       "  t2429 = torch.reshape(t257, (-1, 16))  # t2429: \"cuda:0 f32[128, 16]\"\n",
       "    # t2429 = ltorch.reshape(t257, (-1, 16))  # t2429: \"cuda:0 f32[128, 16]\"\n",
       "      # t2429 = prims.reshape(t257, (128, 16))  # t2429: \"cuda:0 f32[128, 16]\"\n",
       "  del t257\n",
       "  t2430 = torch.matmul(t2428, t2429)  # t2430: \"cuda:0 f32[16, 16]\"\n",
       "    # t2430 = ltorch.matmul(t2428, t2429)  # t2430: \"cuda:0 f32[16, 16]\"\n",
       "      # t2430 = prims.matmul(t2428, t2429)  # t2430: \"cuda:0 f32[16, 16]\"\n",
       "  del t2428, t2429\n",
       "  t2419 = torch.permute(t2416, (1, 0))  # t2419: \"cuda:0 f32[16, 128]\"\n",
       "    # t2419 = ltorch.permute(t2416, (1, 0))  # t2419: \"cuda:0 f32[16, 128]\"\n",
       "      # t2419 = prims.transpose(t2416, (1, 0))  # t2419: \"cuda:0 f32[16, 128]\"\n",
       "  del t2416\n",
       "  # Created by CPU Offloading Transform\n",
       "  t262 = load_to_gpu(offloaded_t262, 'cuda:0')  # t262: \"cuda:0 f32[128, 16]\"\n",
       "  t2420 = torch.reshape(t262, (-1, 16))  # t2420: \"cuda:0 f32[128, 16]\"\n",
       "    # t2420 = ltorch.reshape(t262, (-1, 16))  # t2420: \"cuda:0 f32[128, 16]\"\n",
       "      # t2420 = prims.reshape(t262, (128, 16))  # t2420: \"cuda:0 f32[128, 16]\"\n",
       "  del t262\n",
       "  t2421 = torch.matmul(t2419, t2420)  # t2421: \"cuda:0 f32[16, 16]\"\n",
       "    # t2421 = ltorch.matmul(t2419, t2420)  # t2421: \"cuda:0 f32[16, 16]\"\n",
       "      # t2421 = prims.matmul(t2419, t2420)  # t2421: \"cuda:0 f32[16, 16]\"\n",
       "  del t2419, t2420\n",
       "  t2410 = torch.permute(t2407, (1, 0))  # t2410: \"cuda:0 f32[16, 128]\"\n",
       "    # t2410 = ltorch.permute(t2407, (1, 0))  # t2410: \"cuda:0 f32[16, 128]\"\n",
       "      # t2410 = prims.transpose(t2407, (1, 0))  # t2410: \"cuda:0 f32[16, 128]\"\n",
       "  del t2407\n",
       "  # Created by CPU Offloading Transform\n",
       "  t267 = load_to_gpu(offloaded_t267, 'cuda:0')  # t267: \"cuda:0 f32[128, 16]\"\n",
       "  t2411 = torch.reshape(t267, (-1, 16))  # t2411: \"cuda:0 f32[128, 16]\"\n",
       "    # t2411 = ltorch.reshape(t267, (-1, 16))  # t2411: \"cuda:0 f32[128, 16]\"\n",
       "      # t2411 = prims.reshape(t267, (128, 16))  # t2411: \"cuda:0 f32[128, 16]\"\n",
       "  del t267\n",
       "  t2412 = torch.matmul(t2410, t2411)  # t2412: \"cuda:0 f32[16, 16]\"\n",
       "    # t2412 = ltorch.matmul(t2410, t2411)  # t2412: \"cuda:0 f32[16, 16]\"\n",
       "      # t2412 = prims.matmul(t2410, t2411)  # t2412: \"cuda:0 f32[16, 16]\"\n",
       "  del t2410, t2411\n",
       "  t2401 = torch.permute(t2398, (1, 0))  # t2401: \"cuda:0 f32[16, 128]\"\n",
       "    # t2401 = ltorch.permute(t2398, (1, 0))  # t2401: \"cuda:0 f32[16, 128]\"\n",
       "      # t2401 = prims.transpose(t2398, (1, 0))  # t2401: \"cuda:0 f32[16, 128]\"\n",
       "  del t2398\n",
       "  # Created by CPU Offloading Transform\n",
       "  t272 = load_to_gpu(offloaded_t272, 'cuda:0')  # t272: \"cuda:0 f32[128, 16]\"\n",
       "  t2402 = torch.reshape(t272, (-1, 16))  # t2402: \"cuda:0 f32[128, 16]\"\n",
       "    # t2402 = ltorch.reshape(t272, (-1, 16))  # t2402: \"cuda:0 f32[128, 16]\"\n",
       "      # t2402 = prims.reshape(t272, (128, 16))  # t2402: \"cuda:0 f32[128, 16]\"\n",
       "  del t272\n",
       "  t2403 = torch.matmul(t2401, t2402)  # t2403: \"cuda:0 f32[16, 16]\"\n",
       "    # t2403 = ltorch.matmul(t2401, t2402)  # t2403: \"cuda:0 f32[16, 16]\"\n",
       "      # t2403 = prims.matmul(t2401, t2402)  # t2403: \"cuda:0 f32[16, 16]\"\n",
       "  del t2401, t2402\n",
       "  t2392 = torch.permute(t2389, (1, 0))  # t2392: \"cuda:0 f32[16, 128]\"\n",
       "    # t2392 = ltorch.permute(t2389, (1, 0))  # t2392: \"cuda:0 f32[16, 128]\"\n",
       "      # t2392 = prims.transpose(t2389, (1, 0))  # t2392: \"cuda:0 f32[16, 128]\"\n",
       "  del t2389\n",
       "  # Created by CPU Offloading Transform\n",
       "  t277 = load_to_gpu(offloaded_t277, 'cuda:0')  # t277: \"cuda:0 f32[128, 16]\"\n",
       "  t2393 = torch.reshape(t277, (-1, 16))  # t2393: \"cuda:0 f32[128, 16]\"\n",
       "    # t2393 = ltorch.reshape(t277, (-1, 16))  # t2393: \"cuda:0 f32[128, 16]\"\n",
       "      # t2393 = prims.reshape(t277, (128, 16))  # t2393: \"cuda:0 f32[128, 16]\"\n",
       "  del t277\n",
       "  t2394 = torch.matmul(t2392, t2393)  # t2394: \"cuda:0 f32[16, 16]\"\n",
       "    # t2394 = ltorch.matmul(t2392, t2393)  # t2394: \"cuda:0 f32[16, 16]\"\n",
       "      # t2394 = prims.matmul(t2392, t2393)  # t2394: \"cuda:0 f32[16, 16]\"\n",
       "  del t2392, t2393\n",
       "  t2383 = torch.permute(t2380, (1, 0))  # t2383: \"cuda:0 f32[16, 128]\"\n",
       "    # t2383 = ltorch.permute(t2380, (1, 0))  # t2383: \"cuda:0 f32[16, 128]\"\n",
       "      # t2383 = prims.transpose(t2380, (1, 0))  # t2383: \"cuda:0 f32[16, 128]\"\n",
       "  del t2380\n",
       "  # Created by CPU Offloading Transform\n",
       "  t282 = load_to_gpu(offloaded_t282, 'cuda:0')  # t282: \"cuda:0 f32[128, 16]\"\n",
       "  t2384 = torch.reshape(t282, (-1, 16))  # t2384: \"cuda:0 f32[128, 16]\"\n",
       "    # t2384 = ltorch.reshape(t282, (-1, 16))  # t2384: \"cuda:0 f32[128, 16]\"\n",
       "      # t2384 = prims.reshape(t282, (128, 16))  # t2384: \"cuda:0 f32[128, 16]\"\n",
       "  del t282\n",
       "  t2385 = torch.matmul(t2383, t2384)  # t2385: \"cuda:0 f32[16, 16]\"\n",
       "    # t2385 = ltorch.matmul(t2383, t2384)  # t2385: \"cuda:0 f32[16, 16]\"\n",
       "      # t2385 = prims.matmul(t2383, t2384)  # t2385: \"cuda:0 f32[16, 16]\"\n",
       "  del t2383, t2384\n",
       "  t2374 = torch.permute(t2371, (1, 0))  # t2374: \"cuda:0 f32[16, 128]\"\n",
       "    # t2374 = ltorch.permute(t2371, (1, 0))  # t2374: \"cuda:0 f32[16, 128]\"\n",
       "      # t2374 = prims.transpose(t2371, (1, 0))  # t2374: \"cuda:0 f32[16, 128]\"\n",
       "  del t2371\n",
       "  # Created by CPU Offloading Transform\n",
       "  t287 = load_to_gpu(offloaded_t287, 'cuda:0')  # t287: \"cuda:0 f32[128, 16]\"\n",
       "  t2375 = torch.reshape(t287, (-1, 16))  # t2375: \"cuda:0 f32[128, 16]\"\n",
       "    # t2375 = ltorch.reshape(t287, (-1, 16))  # t2375: \"cuda:0 f32[128, 16]\"\n",
       "      # t2375 = prims.reshape(t287, (128, 16))  # t2375: \"cuda:0 f32[128, 16]\"\n",
       "  del t287\n",
       "  t2376 = torch.matmul(t2374, t2375)  # t2376: \"cuda:0 f32[16, 16]\"\n",
       "    # t2376 = ltorch.matmul(t2374, t2375)  # t2376: \"cuda:0 f32[16, 16]\"\n",
       "      # t2376 = prims.matmul(t2374, t2375)  # t2376: \"cuda:0 f32[16, 16]\"\n",
       "  del t2374, t2375\n",
       "  t2365 = torch.permute(t2362, (1, 0))  # t2365: \"cuda:0 f32[16, 128]\"\n",
       "    # t2365 = ltorch.permute(t2362, (1, 0))  # t2365: \"cuda:0 f32[16, 128]\"\n",
       "      # t2365 = prims.transpose(t2362, (1, 0))  # t2365: \"cuda:0 f32[16, 128]\"\n",
       "  del t2362\n",
       "  # Created by CPU Offloading Transform\n",
       "  t292 = load_to_gpu(offloaded_t292, 'cuda:0')  # t292: \"cuda:0 f32[128, 16]\"\n",
       "  t2366 = torch.reshape(t292, (-1, 16))  # t2366: \"cuda:0 f32[128, 16]\"\n",
       "    # t2366 = ltorch.reshape(t292, (-1, 16))  # t2366: \"cuda:0 f32[128, 16]\"\n",
       "      # t2366 = prims.reshape(t292, (128, 16))  # t2366: \"cuda:0 f32[128, 16]\"\n",
       "  del t292\n",
       "  t2367 = torch.matmul(t2365, t2366)  # t2367: \"cuda:0 f32[16, 16]\"\n",
       "    # t2367 = ltorch.matmul(t2365, t2366)  # t2367: \"cuda:0 f32[16, 16]\"\n",
       "      # t2367 = prims.matmul(t2365, t2366)  # t2367: \"cuda:0 f32[16, 16]\"\n",
       "  del t2365, t2366\n",
       "  t2356 = torch.permute(t2353, (1, 0))  # t2356: \"cuda:0 f32[16, 128]\"\n",
       "    # t2356 = ltorch.permute(t2353, (1, 0))  # t2356: \"cuda:0 f32[16, 128]\"\n",
       "      # t2356 = prims.transpose(t2353, (1, 0))  # t2356: \"cuda:0 f32[16, 128]\"\n",
       "  del t2353\n",
       "  # Created by CPU Offloading Transform\n",
       "  t297 = load_to_gpu(offloaded_t297, 'cuda:0')  # t297: \"cuda:0 f32[128, 16]\"\n",
       "  t2357 = torch.reshape(t297, (-1, 16))  # t2357: \"cuda:0 f32[128, 16]\"\n",
       "    # t2357 = ltorch.reshape(t297, (-1, 16))  # t2357: \"cuda:0 f32[128, 16]\"\n",
       "      # t2357 = prims.reshape(t297, (128, 16))  # t2357: \"cuda:0 f32[128, 16]\"\n",
       "  del t297\n",
       "  t2358 = torch.matmul(t2356, t2357)  # t2358: \"cuda:0 f32[16, 16]\"\n",
       "    # t2358 = ltorch.matmul(t2356, t2357)  # t2358: \"cuda:0 f32[16, 16]\"\n",
       "      # t2358 = prims.matmul(t2356, t2357)  # t2358: \"cuda:0 f32[16, 16]\"\n",
       "  del t2356, t2357\n",
       "  t2347 = torch.permute(t2344, (1, 0))  # t2347: \"cuda:0 f32[16, 128]\"\n",
       "    # t2347 = ltorch.permute(t2344, (1, 0))  # t2347: \"cuda:0 f32[16, 128]\"\n",
       "      # t2347 = prims.transpose(t2344, (1, 0))  # t2347: \"cuda:0 f32[16, 128]\"\n",
       "  del t2344\n",
       "  # Created by CPU Offloading Transform\n",
       "  t302 = load_to_gpu(offloaded_t302, 'cuda:0')  # t302: \"cuda:0 f32[128, 16]\"\n",
       "  t2348 = torch.reshape(t302, (-1, 16))  # t2348: \"cuda:0 f32[128, 16]\"\n",
       "    # t2348 = ltorch.reshape(t302, (-1, 16))  # t2348: \"cuda:0 f32[128, 16]\"\n",
       "      # t2348 = prims.reshape(t302, (128, 16))  # t2348: \"cuda:0 f32[128, 16]\"\n",
       "  del t302\n",
       "  t2349 = torch.matmul(t2347, t2348)  # t2349: \"cuda:0 f32[16, 16]\"\n",
       "    # t2349 = ltorch.matmul(t2347, t2348)  # t2349: \"cuda:0 f32[16, 16]\"\n",
       "      # t2349 = prims.matmul(t2347, t2348)  # t2349: \"cuda:0 f32[16, 16]\"\n",
       "  del t2347, t2348\n",
       "  t2338 = torch.permute(t2335, (1, 0))  # t2338: \"cuda:0 f32[16, 128]\"\n",
       "    # t2338 = ltorch.permute(t2335, (1, 0))  # t2338: \"cuda:0 f32[16, 128]\"\n",
       "      # t2338 = prims.transpose(t2335, (1, 0))  # t2338: \"cuda:0 f32[16, 128]\"\n",
       "  del t2335\n",
       "  # Created by CPU Offloading Transform\n",
       "  t307 = load_to_gpu(offloaded_t307, 'cuda:0')  # t307: \"cuda:0 f32[128, 16]\"\n",
       "  t2339 = torch.reshape(t307, (-1, 16))  # t2339: \"cuda:0 f32[128, 16]\"\n",
       "    # t2339 = ltorch.reshape(t307, (-1, 16))  # t2339: \"cuda:0 f32[128, 16]\"\n",
       "      # t2339 = prims.reshape(t307, (128, 16))  # t2339: \"cuda:0 f32[128, 16]\"\n",
       "  del t307\n",
       "  t2340 = torch.matmul(t2338, t2339)  # t2340: \"cuda:0 f32[16, 16]\"\n",
       "    # t2340 = ltorch.matmul(t2338, t2339)  # t2340: \"cuda:0 f32[16, 16]\"\n",
       "      # t2340 = prims.matmul(t2338, t2339)  # t2340: \"cuda:0 f32[16, 16]\"\n",
       "  del t2338, t2339\n",
       "  t2329 = torch.permute(t2326, (1, 0))  # t2329: \"cuda:0 f32[16, 128]\"\n",
       "    # t2329 = ltorch.permute(t2326, (1, 0))  # t2329: \"cuda:0 f32[16, 128]\"\n",
       "      # t2329 = prims.transpose(t2326, (1, 0))  # t2329: \"cuda:0 f32[16, 128]\"\n",
       "  del t2326\n",
       "  # Created by CPU Offloading Transform\n",
       "  t312 = load_to_gpu(offloaded_t312, 'cuda:0')  # t312: \"cuda:0 f32[128, 16]\"\n",
       "  t2330 = torch.reshape(t312, (-1, 16))  # t2330: \"cuda:0 f32[128, 16]\"\n",
       "    # t2330 = ltorch.reshape(t312, (-1, 16))  # t2330: \"cuda:0 f32[128, 16]\"\n",
       "      # t2330 = prims.reshape(t312, (128, 16))  # t2330: \"cuda:0 f32[128, 16]\"\n",
       "  del t312\n",
       "  t2331 = torch.matmul(t2329, t2330)  # t2331: \"cuda:0 f32[16, 16]\"\n",
       "    # t2331 = ltorch.matmul(t2329, t2330)  # t2331: \"cuda:0 f32[16, 16]\"\n",
       "      # t2331 = prims.matmul(t2329, t2330)  # t2331: \"cuda:0 f32[16, 16]\"\n",
       "  del t2329, t2330\n",
       "  t2320 = torch.permute(t2317, (1, 0))  # t2320: \"cuda:0 f32[16, 128]\"\n",
       "    # t2320 = ltorch.permute(t2317, (1, 0))  # t2320: \"cuda:0 f32[16, 128]\"\n",
       "      # t2320 = prims.transpose(t2317, (1, 0))  # t2320: \"cuda:0 f32[16, 128]\"\n",
       "  del t2317\n",
       "  # Created by CPU Offloading Transform\n",
       "  t317 = load_to_gpu(offloaded_t317, 'cuda:0')  # t317: \"cuda:0 f32[128, 16]\"\n",
       "  t2321 = torch.reshape(t317, (-1, 16))  # t2321: \"cuda:0 f32[128, 16]\"\n",
       "    # t2321 = ltorch.reshape(t317, (-1, 16))  # t2321: \"cuda:0 f32[128, 16]\"\n",
       "      # t2321 = prims.reshape(t317, (128, 16))  # t2321: \"cuda:0 f32[128, 16]\"\n",
       "  del t317\n",
       "  t2322 = torch.matmul(t2320, t2321)  # t2322: \"cuda:0 f32[16, 16]\"\n",
       "    # t2322 = ltorch.matmul(t2320, t2321)  # t2322: \"cuda:0 f32[16, 16]\"\n",
       "      # t2322 = prims.matmul(t2320, t2321)  # t2322: \"cuda:0 f32[16, 16]\"\n",
       "  del t2320, t2321\n",
       "  t2311 = torch.permute(t2308, (1, 0))  # t2311: \"cuda:0 f32[16, 128]\"\n",
       "    # t2311 = ltorch.permute(t2308, (1, 0))  # t2311: \"cuda:0 f32[16, 128]\"\n",
       "      # t2311 = prims.transpose(t2308, (1, 0))  # t2311: \"cuda:0 f32[16, 128]\"\n",
       "  del t2308\n",
       "  # Created by CPU Offloading Transform\n",
       "  t322 = load_to_gpu(offloaded_t322, 'cuda:0')  # t322: \"cuda:0 f32[128, 16]\"\n",
       "  t2312 = torch.reshape(t322, (-1, 16))  # t2312: \"cuda:0 f32[128, 16]\"\n",
       "    # t2312 = ltorch.reshape(t322, (-1, 16))  # t2312: \"cuda:0 f32[128, 16]\"\n",
       "      # t2312 = prims.reshape(t322, (128, 16))  # t2312: \"cuda:0 f32[128, 16]\"\n",
       "  del t322\n",
       "  t2313 = torch.matmul(t2311, t2312)  # t2313: \"cuda:0 f32[16, 16]\"\n",
       "    # t2313 = ltorch.matmul(t2311, t2312)  # t2313: \"cuda:0 f32[16, 16]\"\n",
       "      # t2313 = prims.matmul(t2311, t2312)  # t2313: \"cuda:0 f32[16, 16]\"\n",
       "  del t2311, t2312\n",
       "  t2302 = torch.permute(t2299, (1, 0))  # t2302: \"cuda:0 f32[16, 128]\"\n",
       "    # t2302 = ltorch.permute(t2299, (1, 0))  # t2302: \"cuda:0 f32[16, 128]\"\n",
       "      # t2302 = prims.transpose(t2299, (1, 0))  # t2302: \"cuda:0 f32[16, 128]\"\n",
       "  del t2299\n",
       "  # Created by CPU Offloading Transform\n",
       "  t327 = load_to_gpu(offloaded_t327, 'cuda:0')  # t327: \"cuda:0 f32[128, 16]\"\n",
       "  t2303 = torch.reshape(t327, (-1, 16))  # t2303: \"cuda:0 f32[128, 16]\"\n",
       "    # t2303 = ltorch.reshape(t327, (-1, 16))  # t2303: \"cuda:0 f32[128, 16]\"\n",
       "      # t2303 = prims.reshape(t327, (128, 16))  # t2303: \"cuda:0 f32[128, 16]\"\n",
       "  del t327\n",
       "  t2304 = torch.matmul(t2302, t2303)  # t2304: \"cuda:0 f32[16, 16]\"\n",
       "    # t2304 = ltorch.matmul(t2302, t2303)  # t2304: \"cuda:0 f32[16, 16]\"\n",
       "      # t2304 = prims.matmul(t2302, t2303)  # t2304: \"cuda:0 f32[16, 16]\"\n",
       "  del t2302, t2303\n",
       "  t2293 = torch.permute(t2290, (1, 0))  # t2293: \"cuda:0 f32[16, 128]\"\n",
       "    # t2293 = ltorch.permute(t2290, (1, 0))  # t2293: \"cuda:0 f32[16, 128]\"\n",
       "      # t2293 = prims.transpose(t2290, (1, 0))  # t2293: \"cuda:0 f32[16, 128]\"\n",
       "  del t2290\n",
       "  # Created by CPU Offloading Transform\n",
       "  t332 = load_to_gpu(offloaded_t332, 'cuda:0')  # t332: \"cuda:0 f32[128, 16]\"\n",
       "  t2294 = torch.reshape(t332, (-1, 16))  # t2294: \"cuda:0 f32[128, 16]\"\n",
       "    # t2294 = ltorch.reshape(t332, (-1, 16))  # t2294: \"cuda:0 f32[128, 16]\"\n",
       "      # t2294 = prims.reshape(t332, (128, 16))  # t2294: \"cuda:0 f32[128, 16]\"\n",
       "  del t332\n",
       "  t2295 = torch.matmul(t2293, t2294)  # t2295: \"cuda:0 f32[16, 16]\"\n",
       "    # t2295 = ltorch.matmul(t2293, t2294)  # t2295: \"cuda:0 f32[16, 16]\"\n",
       "      # t2295 = prims.matmul(t2293, t2294)  # t2295: \"cuda:0 f32[16, 16]\"\n",
       "  del t2293, t2294\n",
       "  t2284 = torch.permute(t2281, (1, 0))  # t2284: \"cuda:0 f32[16, 128]\"\n",
       "    # t2284 = ltorch.permute(t2281, (1, 0))  # t2284: \"cuda:0 f32[16, 128]\"\n",
       "      # t2284 = prims.transpose(t2281, (1, 0))  # t2284: \"cuda:0 f32[16, 128]\"\n",
       "  del t2281\n",
       "  # Created by CPU Offloading Transform\n",
       "  t337 = load_to_gpu(offloaded_t337, 'cuda:0')  # t337: \"cuda:0 f32[128, 16]\"\n",
       "  t2285 = torch.reshape(t337, (-1, 16))  # t2285: \"cuda:0 f32[128, 16]\"\n",
       "    # t2285 = ltorch.reshape(t337, (-1, 16))  # t2285: \"cuda:0 f32[128, 16]\"\n",
       "      # t2285 = prims.reshape(t337, (128, 16))  # t2285: \"cuda:0 f32[128, 16]\"\n",
       "  del t337\n",
       "  t2286 = torch.matmul(t2284, t2285)  # t2286: \"cuda:0 f32[16, 16]\"\n",
       "    # t2286 = ltorch.matmul(t2284, t2285)  # t2286: \"cuda:0 f32[16, 16]\"\n",
       "      # t2286 = prims.matmul(t2284, t2285)  # t2286: \"cuda:0 f32[16, 16]\"\n",
       "  del t2284, t2285\n",
       "  t2275 = torch.permute(t2272, (1, 0))  # t2275: \"cuda:0 f32[16, 128]\"\n",
       "    # t2275 = ltorch.permute(t2272, (1, 0))  # t2275: \"cuda:0 f32[16, 128]\"\n",
       "      # t2275 = prims.transpose(t2272, (1, 0))  # t2275: \"cuda:0 f32[16, 128]\"\n",
       "  del t2272\n",
       "  # Created by CPU Offloading Transform\n",
       "  t342 = load_to_gpu(offloaded_t342, 'cuda:0')  # t342: \"cuda:0 f32[128, 16]\"\n",
       "  t2276 = torch.reshape(t342, (-1, 16))  # t2276: \"cuda:0 f32[128, 16]\"\n",
       "    # t2276 = ltorch.reshape(t342, (-1, 16))  # t2276: \"cuda:0 f32[128, 16]\"\n",
       "      # t2276 = prims.reshape(t342, (128, 16))  # t2276: \"cuda:0 f32[128, 16]\"\n",
       "  del t342\n",
       "  t2277 = torch.matmul(t2275, t2276)  # t2277: \"cuda:0 f32[16, 16]\"\n",
       "    # t2277 = ltorch.matmul(t2275, t2276)  # t2277: \"cuda:0 f32[16, 16]\"\n",
       "      # t2277 = prims.matmul(t2275, t2276)  # t2277: \"cuda:0 f32[16, 16]\"\n",
       "  del t2275, t2276\n",
       "  t2266 = torch.permute(t2263, (1, 0))  # t2266: \"cuda:0 f32[16, 128]\"\n",
       "    # t2266 = ltorch.permute(t2263, (1, 0))  # t2266: \"cuda:0 f32[16, 128]\"\n",
       "      # t2266 = prims.transpose(t2263, (1, 0))  # t2266: \"cuda:0 f32[16, 128]\"\n",
       "  del t2263\n",
       "  # Created by CPU Offloading Transform\n",
       "  t347 = load_to_gpu(offloaded_t347, 'cuda:0')  # t347: \"cuda:0 f32[128, 16]\"\n",
       "  t2267 = torch.reshape(t347, (-1, 16))  # t2267: \"cuda:0 f32[128, 16]\"\n",
       "    # t2267 = ltorch.reshape(t347, (-1, 16))  # t2267: \"cuda:0 f32[128, 16]\"\n",
       "      # t2267 = prims.reshape(t347, (128, 16))  # t2267: \"cuda:0 f32[128, 16]\"\n",
       "  del t347\n",
       "  t2268 = torch.matmul(t2266, t2267)  # t2268: \"cuda:0 f32[16, 16]\"\n",
       "    # t2268 = ltorch.matmul(t2266, t2267)  # t2268: \"cuda:0 f32[16, 16]\"\n",
       "      # t2268 = prims.matmul(t2266, t2267)  # t2268: \"cuda:0 f32[16, 16]\"\n",
       "  del t2266, t2267\n",
       "  t2257 = torch.permute(t2254, (1, 0))  # t2257: \"cuda:0 f32[16, 128]\"\n",
       "    # t2257 = ltorch.permute(t2254, (1, 0))  # t2257: \"cuda:0 f32[16, 128]\"\n",
       "      # t2257 = prims.transpose(t2254, (1, 0))  # t2257: \"cuda:0 f32[16, 128]\"\n",
       "  del t2254\n",
       "  # Created by CPU Offloading Transform\n",
       "  t352 = load_to_gpu(offloaded_t352, 'cuda:0')  # t352: \"cuda:0 f32[128, 16]\"\n",
       "  t2258 = torch.reshape(t352, (-1, 16))  # t2258: \"cuda:0 f32[128, 16]\"\n",
       "    # t2258 = ltorch.reshape(t352, (-1, 16))  # t2258: \"cuda:0 f32[128, 16]\"\n",
       "      # t2258 = prims.reshape(t352, (128, 16))  # t2258: \"cuda:0 f32[128, 16]\"\n",
       "  del t352\n",
       "  t2259 = torch.matmul(t2257, t2258)  # t2259: \"cuda:0 f32[16, 16]\"\n",
       "    # t2259 = ltorch.matmul(t2257, t2258)  # t2259: \"cuda:0 f32[16, 16]\"\n",
       "      # t2259 = prims.matmul(t2257, t2258)  # t2259: \"cuda:0 f32[16, 16]\"\n",
       "  del t2257, t2258\n",
       "  t2248 = torch.permute(t2245, (1, 0))  # t2248: \"cuda:0 f32[16, 128]\"\n",
       "    # t2248 = ltorch.permute(t2245, (1, 0))  # t2248: \"cuda:0 f32[16, 128]\"\n",
       "      # t2248 = prims.transpose(t2245, (1, 0))  # t2248: \"cuda:0 f32[16, 128]\"\n",
       "  del t2245\n",
       "  # Created by CPU Offloading Transform\n",
       "  t357 = load_to_gpu(offloaded_t357, 'cuda:0')  # t357: \"cuda:0 f32[128, 16]\"\n",
       "  t2249 = torch.reshape(t357, (-1, 16))  # t2249: \"cuda:0 f32[128, 16]\"\n",
       "    # t2249 = ltorch.reshape(t357, (-1, 16))  # t2249: \"cuda:0 f32[128, 16]\"\n",
       "      # t2249 = prims.reshape(t357, (128, 16))  # t2249: \"cuda:0 f32[128, 16]\"\n",
       "  del t357\n",
       "  t2250 = torch.matmul(t2248, t2249)  # t2250: \"cuda:0 f32[16, 16]\"\n",
       "    # t2250 = ltorch.matmul(t2248, t2249)  # t2250: \"cuda:0 f32[16, 16]\"\n",
       "      # t2250 = prims.matmul(t2248, t2249)  # t2250: \"cuda:0 f32[16, 16]\"\n",
       "  del t2248, t2249\n",
       "  t2239 = torch.permute(t2236, (1, 0))  # t2239: \"cuda:0 f32[16, 128]\"\n",
       "    # t2239 = ltorch.permute(t2236, (1, 0))  # t2239: \"cuda:0 f32[16, 128]\"\n",
       "      # t2239 = prims.transpose(t2236, (1, 0))  # t2239: \"cuda:0 f32[16, 128]\"\n",
       "  del t2236\n",
       "  # Created by CPU Offloading Transform\n",
       "  t362 = load_to_gpu(offloaded_t362, 'cuda:0')  # t362: \"cuda:0 f32[128, 16]\"\n",
       "  t2240 = torch.reshape(t362, (-1, 16))  # t2240: \"cuda:0 f32[128, 16]\"\n",
       "    # t2240 = ltorch.reshape(t362, (-1, 16))  # t2240: \"cuda:0 f32[128, 16]\"\n",
       "      # t2240 = prims.reshape(t362, (128, 16))  # t2240: \"cuda:0 f32[128, 16]\"\n",
       "  del t362\n",
       "  t2241 = torch.matmul(t2239, t2240)  # t2241: \"cuda:0 f32[16, 16]\"\n",
       "    # t2241 = ltorch.matmul(t2239, t2240)  # t2241: \"cuda:0 f32[16, 16]\"\n",
       "      # t2241 = prims.matmul(t2239, t2240)  # t2241: \"cuda:0 f32[16, 16]\"\n",
       "  del t2239, t2240\n",
       "  t2230 = torch.permute(t2227, (1, 0))  # t2230: \"cuda:0 f32[16, 128]\"\n",
       "    # t2230 = ltorch.permute(t2227, (1, 0))  # t2230: \"cuda:0 f32[16, 128]\"\n",
       "      # t2230 = prims.transpose(t2227, (1, 0))  # t2230: \"cuda:0 f32[16, 128]\"\n",
       "  del t2227\n",
       "  # Created by CPU Offloading Transform\n",
       "  t367 = load_to_gpu(offloaded_t367, 'cuda:0')  # t367: \"cuda:0 f32[128, 16]\"\n",
       "  t2231 = torch.reshape(t367, (-1, 16))  # t2231: \"cuda:0 f32[128, 16]\"\n",
       "    # t2231 = ltorch.reshape(t367, (-1, 16))  # t2231: \"cuda:0 f32[128, 16]\"\n",
       "      # t2231 = prims.reshape(t367, (128, 16))  # t2231: \"cuda:0 f32[128, 16]\"\n",
       "  del t367\n",
       "  t2232 = torch.matmul(t2230, t2231)  # t2232: \"cuda:0 f32[16, 16]\"\n",
       "    # t2232 = ltorch.matmul(t2230, t2231)  # t2232: \"cuda:0 f32[16, 16]\"\n",
       "      # t2232 = prims.matmul(t2230, t2231)  # t2232: \"cuda:0 f32[16, 16]\"\n",
       "  del t2230, t2231\n",
       "  t2221 = torch.permute(t2218, (1, 0))  # t2221: \"cuda:0 f32[16, 128]\"\n",
       "    # t2221 = ltorch.permute(t2218, (1, 0))  # t2221: \"cuda:0 f32[16, 128]\"\n",
       "      # t2221 = prims.transpose(t2218, (1, 0))  # t2221: \"cuda:0 f32[16, 128]\"\n",
       "  del t2218\n",
       "  # Created by CPU Offloading Transform\n",
       "  t372 = load_to_gpu(offloaded_t372, 'cuda:0')  # t372: \"cuda:0 f32[128, 16]\"\n",
       "  t2222 = torch.reshape(t372, (-1, 16))  # t2222: \"cuda:0 f32[128, 16]\"\n",
       "    # t2222 = ltorch.reshape(t372, (-1, 16))  # t2222: \"cuda:0 f32[128, 16]\"\n",
       "      # t2222 = prims.reshape(t372, (128, 16))  # t2222: \"cuda:0 f32[128, 16]\"\n",
       "  del t372\n",
       "  t2223 = torch.matmul(t2221, t2222)  # t2223: \"cuda:0 f32[16, 16]\"\n",
       "    # t2223 = ltorch.matmul(t2221, t2222)  # t2223: \"cuda:0 f32[16, 16]\"\n",
       "      # t2223 = prims.matmul(t2221, t2222)  # t2223: \"cuda:0 f32[16, 16]\"\n",
       "  del t2221, t2222\n",
       "  t2212 = torch.permute(t2209, (1, 0))  # t2212: \"cuda:0 f32[16, 128]\"\n",
       "    # t2212 = ltorch.permute(t2209, (1, 0))  # t2212: \"cuda:0 f32[16, 128]\"\n",
       "      # t2212 = prims.transpose(t2209, (1, 0))  # t2212: \"cuda:0 f32[16, 128]\"\n",
       "  del t2209\n",
       "  # Created by CPU Offloading Transform\n",
       "  t377 = load_to_gpu(offloaded_t377, 'cuda:0')  # t377: \"cuda:0 f32[128, 16]\"\n",
       "  t2213 = torch.reshape(t377, (-1, 16))  # t2213: \"cuda:0 f32[128, 16]\"\n",
       "    # t2213 = ltorch.reshape(t377, (-1, 16))  # t2213: \"cuda:0 f32[128, 16]\"\n",
       "      # t2213 = prims.reshape(t377, (128, 16))  # t2213: \"cuda:0 f32[128, 16]\"\n",
       "  del t377\n",
       "  t2214 = torch.matmul(t2212, t2213)  # t2214: \"cuda:0 f32[16, 16]\"\n",
       "    # t2214 = ltorch.matmul(t2212, t2213)  # t2214: \"cuda:0 f32[16, 16]\"\n",
       "      # t2214 = prims.matmul(t2212, t2213)  # t2214: \"cuda:0 f32[16, 16]\"\n",
       "  del t2212, t2213\n",
       "  t2203 = torch.permute(t2200, (1, 0))  # t2203: \"cuda:0 f32[16, 128]\"\n",
       "    # t2203 = ltorch.permute(t2200, (1, 0))  # t2203: \"cuda:0 f32[16, 128]\"\n",
       "      # t2203 = prims.transpose(t2200, (1, 0))  # t2203: \"cuda:0 f32[16, 128]\"\n",
       "  del t2200\n",
       "  # Created by CPU Offloading Transform\n",
       "  t382 = load_to_gpu(offloaded_t382, 'cuda:0')  # t382: \"cuda:0 f32[128, 16]\"\n",
       "  t2204 = torch.reshape(t382, (-1, 16))  # t2204: \"cuda:0 f32[128, 16]\"\n",
       "    # t2204 = ltorch.reshape(t382, (-1, 16))  # t2204: \"cuda:0 f32[128, 16]\"\n",
       "      # t2204 = prims.reshape(t382, (128, 16))  # t2204: \"cuda:0 f32[128, 16]\"\n",
       "  del t382\n",
       "  t2205 = torch.matmul(t2203, t2204)  # t2205: \"cuda:0 f32[16, 16]\"\n",
       "    # t2205 = ltorch.matmul(t2203, t2204)  # t2205: \"cuda:0 f32[16, 16]\"\n",
       "      # t2205 = prims.matmul(t2203, t2204)  # t2205: \"cuda:0 f32[16, 16]\"\n",
       "  del t2203, t2204\n",
       "  t2194 = torch.permute(t2191, (1, 0))  # t2194: \"cuda:0 f32[16, 128]\"\n",
       "    # t2194 = ltorch.permute(t2191, (1, 0))  # t2194: \"cuda:0 f32[16, 128]\"\n",
       "      # t2194 = prims.transpose(t2191, (1, 0))  # t2194: \"cuda:0 f32[16, 128]\"\n",
       "  del t2191\n",
       "  # Created by CPU Offloading Transform\n",
       "  t387 = load_to_gpu(offloaded_t387, 'cuda:0')  # t387: \"cuda:0 f32[128, 16]\"\n",
       "  t2195 = torch.reshape(t387, (-1, 16))  # t2195: \"cuda:0 f32[128, 16]\"\n",
       "    # t2195 = ltorch.reshape(t387, (-1, 16))  # t2195: \"cuda:0 f32[128, 16]\"\n",
       "      # t2195 = prims.reshape(t387, (128, 16))  # t2195: \"cuda:0 f32[128, 16]\"\n",
       "  del t387\n",
       "  t2196 = torch.matmul(t2194, t2195)  # t2196: \"cuda:0 f32[16, 16]\"\n",
       "    # t2196 = ltorch.matmul(t2194, t2195)  # t2196: \"cuda:0 f32[16, 16]\"\n",
       "      # t2196 = prims.matmul(t2194, t2195)  # t2196: \"cuda:0 f32[16, 16]\"\n",
       "  del t2194, t2195\n",
       "  t2185 = torch.permute(t2182, (1, 0))  # t2185: \"cuda:0 f32[16, 128]\"\n",
       "    # t2185 = ltorch.permute(t2182, (1, 0))  # t2185: \"cuda:0 f32[16, 128]\"\n",
       "      # t2185 = prims.transpose(t2182, (1, 0))  # t2185: \"cuda:0 f32[16, 128]\"\n",
       "  del t2182\n",
       "  # Created by CPU Offloading Transform\n",
       "  t392 = load_to_gpu(offloaded_t392, 'cuda:0')  # t392: \"cuda:0 f32[128, 16]\"\n",
       "  t2186 = torch.reshape(t392, (-1, 16))  # t2186: \"cuda:0 f32[128, 16]\"\n",
       "    # t2186 = ltorch.reshape(t392, (-1, 16))  # t2186: \"cuda:0 f32[128, 16]\"\n",
       "      # t2186 = prims.reshape(t392, (128, 16))  # t2186: \"cuda:0 f32[128, 16]\"\n",
       "  del t392\n",
       "  t2187 = torch.matmul(t2185, t2186)  # t2187: \"cuda:0 f32[16, 16]\"\n",
       "    # t2187 = ltorch.matmul(t2185, t2186)  # t2187: \"cuda:0 f32[16, 16]\"\n",
       "      # t2187 = prims.matmul(t2185, t2186)  # t2187: \"cuda:0 f32[16, 16]\"\n",
       "  del t2185, t2186\n",
       "  t2176 = torch.permute(t2173, (1, 0))  # t2176: \"cuda:0 f32[16, 128]\"\n",
       "    # t2176 = ltorch.permute(t2173, (1, 0))  # t2176: \"cuda:0 f32[16, 128]\"\n",
       "      # t2176 = prims.transpose(t2173, (1, 0))  # t2176: \"cuda:0 f32[16, 128]\"\n",
       "  del t2173\n",
       "  # Created by CPU Offloading Transform\n",
       "  t397 = load_to_gpu(offloaded_t397, 'cuda:0')  # t397: \"cuda:0 f32[128, 16]\"\n",
       "  t2177 = torch.reshape(t397, (-1, 16))  # t2177: \"cuda:0 f32[128, 16]\"\n",
       "    # t2177 = ltorch.reshape(t397, (-1, 16))  # t2177: \"cuda:0 f32[128, 16]\"\n",
       "      # t2177 = prims.reshape(t397, (128, 16))  # t2177: \"cuda:0 f32[128, 16]\"\n",
       "  del t397\n",
       "  t2178 = torch.matmul(t2176, t2177)  # t2178: \"cuda:0 f32[16, 16]\"\n",
       "    # t2178 = ltorch.matmul(t2176, t2177)  # t2178: \"cuda:0 f32[16, 16]\"\n",
       "      # t2178 = prims.matmul(t2176, t2177)  # t2178: \"cuda:0 f32[16, 16]\"\n",
       "  del t2176, t2177\n",
       "  t2167 = torch.permute(t2164, (1, 0))  # t2167: \"cuda:0 f32[16, 128]\"\n",
       "    # t2167 = ltorch.permute(t2164, (1, 0))  # t2167: \"cuda:0 f32[16, 128]\"\n",
       "      # t2167 = prims.transpose(t2164, (1, 0))  # t2167: \"cuda:0 f32[16, 128]\"\n",
       "  del t2164\n",
       "  # Created by CPU Offloading Transform\n",
       "  t402 = load_to_gpu(offloaded_t402, 'cuda:0')  # t402: \"cuda:0 f32[128, 16]\"\n",
       "  t2168 = torch.reshape(t402, (-1, 16))  # t2168: \"cuda:0 f32[128, 16]\"\n",
       "    # t2168 = ltorch.reshape(t402, (-1, 16))  # t2168: \"cuda:0 f32[128, 16]\"\n",
       "      # t2168 = prims.reshape(t402, (128, 16))  # t2168: \"cuda:0 f32[128, 16]\"\n",
       "  del t402\n",
       "  t2169 = torch.matmul(t2167, t2168)  # t2169: \"cuda:0 f32[16, 16]\"\n",
       "    # t2169 = ltorch.matmul(t2167, t2168)  # t2169: \"cuda:0 f32[16, 16]\"\n",
       "      # t2169 = prims.matmul(t2167, t2168)  # t2169: \"cuda:0 f32[16, 16]\"\n",
       "  del t2167, t2168\n",
       "  t2158 = torch.permute(t2155, (1, 0))  # t2158: \"cuda:0 f32[16, 128]\"\n",
       "    # t2158 = ltorch.permute(t2155, (1, 0))  # t2158: \"cuda:0 f32[16, 128]\"\n",
       "      # t2158 = prims.transpose(t2155, (1, 0))  # t2158: \"cuda:0 f32[16, 128]\"\n",
       "  del t2155\n",
       "  # Created by CPU Offloading Transform\n",
       "  t407 = load_to_gpu(offloaded_t407, 'cuda:0')  # t407: \"cuda:0 f32[128, 16]\"\n",
       "  t2159 = torch.reshape(t407, (-1, 16))  # t2159: \"cuda:0 f32[128, 16]\"\n",
       "    # t2159 = ltorch.reshape(t407, (-1, 16))  # t2159: \"cuda:0 f32[128, 16]\"\n",
       "      # t2159 = prims.reshape(t407, (128, 16))  # t2159: \"cuda:0 f32[128, 16]\"\n",
       "  del t407\n",
       "  t2160 = torch.matmul(t2158, t2159)  # t2160: \"cuda:0 f32[16, 16]\"\n",
       "    # t2160 = ltorch.matmul(t2158, t2159)  # t2160: \"cuda:0 f32[16, 16]\"\n",
       "      # t2160 = prims.matmul(t2158, t2159)  # t2160: \"cuda:0 f32[16, 16]\"\n",
       "  del t2158, t2159\n",
       "  t2149 = torch.permute(t2146, (1, 0))  # t2149: \"cuda:0 f32[16, 128]\"\n",
       "    # t2149 = ltorch.permute(t2146, (1, 0))  # t2149: \"cuda:0 f32[16, 128]\"\n",
       "      # t2149 = prims.transpose(t2146, (1, 0))  # t2149: \"cuda:0 f32[16, 128]\"\n",
       "  del t2146\n",
       "  # Created by CPU Offloading Transform\n",
       "  t412 = load_to_gpu(offloaded_t412, 'cuda:0')  # t412: \"cuda:0 f32[128, 16]\"\n",
       "  t2150 = torch.reshape(t412, (-1, 16))  # t2150: \"cuda:0 f32[128, 16]\"\n",
       "    # t2150 = ltorch.reshape(t412, (-1, 16))  # t2150: \"cuda:0 f32[128, 16]\"\n",
       "      # t2150 = prims.reshape(t412, (128, 16))  # t2150: \"cuda:0 f32[128, 16]\"\n",
       "  del t412\n",
       "  t2151 = torch.matmul(t2149, t2150)  # t2151: \"cuda:0 f32[16, 16]\"\n",
       "    # t2151 = ltorch.matmul(t2149, t2150)  # t2151: \"cuda:0 f32[16, 16]\"\n",
       "      # t2151 = prims.matmul(t2149, t2150)  # t2151: \"cuda:0 f32[16, 16]\"\n",
       "  del t2149, t2150\n",
       "  t2140 = torch.permute(t2137, (1, 0))  # t2140: \"cuda:0 f32[16, 128]\"\n",
       "    # t2140 = ltorch.permute(t2137, (1, 0))  # t2140: \"cuda:0 f32[16, 128]\"\n",
       "      # t2140 = prims.transpose(t2137, (1, 0))  # t2140: \"cuda:0 f32[16, 128]\"\n",
       "  del t2137\n",
       "  # Created by CPU Offloading Transform\n",
       "  t417 = load_to_gpu(offloaded_t417, 'cuda:0')  # t417: \"cuda:0 f32[128, 16]\"\n",
       "  t2141 = torch.reshape(t417, (-1, 16))  # t2141: \"cuda:0 f32[128, 16]\"\n",
       "    # t2141 = ltorch.reshape(t417, (-1, 16))  # t2141: \"cuda:0 f32[128, 16]\"\n",
       "      # t2141 = prims.reshape(t417, (128, 16))  # t2141: \"cuda:0 f32[128, 16]\"\n",
       "  del t417\n",
       "  t2142 = torch.matmul(t2140, t2141)  # t2142: \"cuda:0 f32[16, 16]\"\n",
       "    # t2142 = ltorch.matmul(t2140, t2141)  # t2142: \"cuda:0 f32[16, 16]\"\n",
       "      # t2142 = prims.matmul(t2140, t2141)  # t2142: \"cuda:0 f32[16, 16]\"\n",
       "  del t2140, t2141\n",
       "  t2131 = torch.permute(t2128, (1, 0))  # t2131: \"cuda:0 f32[16, 128]\"\n",
       "    # t2131 = ltorch.permute(t2128, (1, 0))  # t2131: \"cuda:0 f32[16, 128]\"\n",
       "      # t2131 = prims.transpose(t2128, (1, 0))  # t2131: \"cuda:0 f32[16, 128]\"\n",
       "  del t2128\n",
       "  # Created by CPU Offloading Transform\n",
       "  t422 = load_to_gpu(offloaded_t422, 'cuda:0')  # t422: \"cuda:0 f32[128, 16]\"\n",
       "  t2132 = torch.reshape(t422, (-1, 16))  # t2132: \"cuda:0 f32[128, 16]\"\n",
       "    # t2132 = ltorch.reshape(t422, (-1, 16))  # t2132: \"cuda:0 f32[128, 16]\"\n",
       "      # t2132 = prims.reshape(t422, (128, 16))  # t2132: \"cuda:0 f32[128, 16]\"\n",
       "  del t422\n",
       "  t2133 = torch.matmul(t2131, t2132)  # t2133: \"cuda:0 f32[16, 16]\"\n",
       "    # t2133 = ltorch.matmul(t2131, t2132)  # t2133: \"cuda:0 f32[16, 16]\"\n",
       "      # t2133 = prims.matmul(t2131, t2132)  # t2133: \"cuda:0 f32[16, 16]\"\n",
       "  del t2131, t2132\n",
       "  t2122 = torch.permute(t2119, (1, 0))  # t2122: \"cuda:0 f32[16, 128]\"\n",
       "    # t2122 = ltorch.permute(t2119, (1, 0))  # t2122: \"cuda:0 f32[16, 128]\"\n",
       "      # t2122 = prims.transpose(t2119, (1, 0))  # t2122: \"cuda:0 f32[16, 128]\"\n",
       "  del t2119\n",
       "  # Created by CPU Offloading Transform\n",
       "  t427 = load_to_gpu(offloaded_t427, 'cuda:0')  # t427: \"cuda:0 f32[128, 16]\"\n",
       "  t2123 = torch.reshape(t427, (-1, 16))  # t2123: \"cuda:0 f32[128, 16]\"\n",
       "    # t2123 = ltorch.reshape(t427, (-1, 16))  # t2123: \"cuda:0 f32[128, 16]\"\n",
       "      # t2123 = prims.reshape(t427, (128, 16))  # t2123: \"cuda:0 f32[128, 16]\"\n",
       "  del t427\n",
       "  t2124 = torch.matmul(t2122, t2123)  # t2124: \"cuda:0 f32[16, 16]\"\n",
       "    # t2124 = ltorch.matmul(t2122, t2123)  # t2124: \"cuda:0 f32[16, 16]\"\n",
       "      # t2124 = prims.matmul(t2122, t2123)  # t2124: \"cuda:0 f32[16, 16]\"\n",
       "  del t2122, t2123\n",
       "  t2113 = torch.permute(t2110, (1, 0))  # t2113: \"cuda:0 f32[16, 128]\"\n",
       "    # t2113 = ltorch.permute(t2110, (1, 0))  # t2113: \"cuda:0 f32[16, 128]\"\n",
       "      # t2113 = prims.transpose(t2110, (1, 0))  # t2113: \"cuda:0 f32[16, 128]\"\n",
       "  del t2110\n",
       "  # Created by CPU Offloading Transform\n",
       "  t432 = load_to_gpu(offloaded_t432, 'cuda:0')  # t432: \"cuda:0 f32[128, 16]\"\n",
       "  t2114 = torch.reshape(t432, (-1, 16))  # t2114: \"cuda:0 f32[128, 16]\"\n",
       "    # t2114 = ltorch.reshape(t432, (-1, 16))  # t2114: \"cuda:0 f32[128, 16]\"\n",
       "      # t2114 = prims.reshape(t432, (128, 16))  # t2114: \"cuda:0 f32[128, 16]\"\n",
       "  del t432\n",
       "  t2115 = torch.matmul(t2113, t2114)  # t2115: \"cuda:0 f32[16, 16]\"\n",
       "    # t2115 = ltorch.matmul(t2113, t2114)  # t2115: \"cuda:0 f32[16, 16]\"\n",
       "      # t2115 = prims.matmul(t2113, t2114)  # t2115: \"cuda:0 f32[16, 16]\"\n",
       "  del t2113, t2114\n",
       "  t2104 = torch.permute(t2101, (1, 0))  # t2104: \"cuda:0 f32[16, 128]\"\n",
       "    # t2104 = ltorch.permute(t2101, (1, 0))  # t2104: \"cuda:0 f32[16, 128]\"\n",
       "      # t2104 = prims.transpose(t2101, (1, 0))  # t2104: \"cuda:0 f32[16, 128]\"\n",
       "  del t2101\n",
       "  # Created by CPU Offloading Transform\n",
       "  t437 = load_to_gpu(offloaded_t437, 'cuda:0')  # t437: \"cuda:0 f32[128, 16]\"\n",
       "  t2105 = torch.reshape(t437, (-1, 16))  # t2105: \"cuda:0 f32[128, 16]\"\n",
       "    # t2105 = ltorch.reshape(t437, (-1, 16))  # t2105: \"cuda:0 f32[128, 16]\"\n",
       "      # t2105 = prims.reshape(t437, (128, 16))  # t2105: \"cuda:0 f32[128, 16]\"\n",
       "  del t437\n",
       "  t2106 = torch.matmul(t2104, t2105)  # t2106: \"cuda:0 f32[16, 16]\"\n",
       "    # t2106 = ltorch.matmul(t2104, t2105)  # t2106: \"cuda:0 f32[16, 16]\"\n",
       "      # t2106 = prims.matmul(t2104, t2105)  # t2106: \"cuda:0 f32[16, 16]\"\n",
       "  del t2104, t2105\n",
       "  t2095 = torch.permute(t2092, (1, 0))  # t2095: \"cuda:0 f32[16, 128]\"\n",
       "    # t2095 = ltorch.permute(t2092, (1, 0))  # t2095: \"cuda:0 f32[16, 128]\"\n",
       "      # t2095 = prims.transpose(t2092, (1, 0))  # t2095: \"cuda:0 f32[16, 128]\"\n",
       "  del t2092\n",
       "  # Created by CPU Offloading Transform\n",
       "  t442 = load_to_gpu(offloaded_t442, 'cuda:0')  # t442: \"cuda:0 f32[128, 16]\"\n",
       "  t2096 = torch.reshape(t442, (-1, 16))  # t2096: \"cuda:0 f32[128, 16]\"\n",
       "    # t2096 = ltorch.reshape(t442, (-1, 16))  # t2096: \"cuda:0 f32[128, 16]\"\n",
       "      # t2096 = prims.reshape(t442, (128, 16))  # t2096: \"cuda:0 f32[128, 16]\"\n",
       "  del t442\n",
       "  t2097 = torch.matmul(t2095, t2096)  # t2097: \"cuda:0 f32[16, 16]\"\n",
       "    # t2097 = ltorch.matmul(t2095, t2096)  # t2097: \"cuda:0 f32[16, 16]\"\n",
       "      # t2097 = prims.matmul(t2095, t2096)  # t2097: \"cuda:0 f32[16, 16]\"\n",
       "  del t2095, t2096\n",
       "  t2086 = torch.permute(t2083, (1, 0))  # t2086: \"cuda:0 f32[16, 128]\"\n",
       "    # t2086 = ltorch.permute(t2083, (1, 0))  # t2086: \"cuda:0 f32[16, 128]\"\n",
       "      # t2086 = prims.transpose(t2083, (1, 0))  # t2086: \"cuda:0 f32[16, 128]\"\n",
       "  del t2083\n",
       "  # Created by CPU Offloading Transform\n",
       "  t447 = load_to_gpu(offloaded_t447, 'cuda:0')  # t447: \"cuda:0 f32[128, 16]\"\n",
       "  t2087 = torch.reshape(t447, (-1, 16))  # t2087: \"cuda:0 f32[128, 16]\"\n",
       "    # t2087 = ltorch.reshape(t447, (-1, 16))  # t2087: \"cuda:0 f32[128, 16]\"\n",
       "      # t2087 = prims.reshape(t447, (128, 16))  # t2087: \"cuda:0 f32[128, 16]\"\n",
       "  del t447\n",
       "  t2088 = torch.matmul(t2086, t2087)  # t2088: \"cuda:0 f32[16, 16]\"\n",
       "    # t2088 = ltorch.matmul(t2086, t2087)  # t2088: \"cuda:0 f32[16, 16]\"\n",
       "      # t2088 = prims.matmul(t2086, t2087)  # t2088: \"cuda:0 f32[16, 16]\"\n",
       "  del t2086, t2087\n",
       "  t2077 = torch.permute(t2074, (1, 0))  # t2077: \"cuda:0 f32[16, 128]\"\n",
       "    # t2077 = ltorch.permute(t2074, (1, 0))  # t2077: \"cuda:0 f32[16, 128]\"\n",
       "      # t2077 = prims.transpose(t2074, (1, 0))  # t2077: \"cuda:0 f32[16, 128]\"\n",
       "  del t2074\n",
       "  # Created by CPU Offloading Transform\n",
       "  t452 = load_to_gpu(offloaded_t452, 'cuda:0')  # t452: \"cuda:0 f32[128, 16]\"\n",
       "  t2078 = torch.reshape(t452, (-1, 16))  # t2078: \"cuda:0 f32[128, 16]\"\n",
       "    # t2078 = ltorch.reshape(t452, (-1, 16))  # t2078: \"cuda:0 f32[128, 16]\"\n",
       "      # t2078 = prims.reshape(t452, (128, 16))  # t2078: \"cuda:0 f32[128, 16]\"\n",
       "  del t452\n",
       "  t2079 = torch.matmul(t2077, t2078)  # t2079: \"cuda:0 f32[16, 16]\"\n",
       "    # t2079 = ltorch.matmul(t2077, t2078)  # t2079: \"cuda:0 f32[16, 16]\"\n",
       "      # t2079 = prims.matmul(t2077, t2078)  # t2079: \"cuda:0 f32[16, 16]\"\n",
       "  del t2077, t2078\n",
       "  t2068 = torch.permute(t2065, (1, 0))  # t2068: \"cuda:0 f32[16, 128]\"\n",
       "    # t2068 = ltorch.permute(t2065, (1, 0))  # t2068: \"cuda:0 f32[16, 128]\"\n",
       "      # t2068 = prims.transpose(t2065, (1, 0))  # t2068: \"cuda:0 f32[16, 128]\"\n",
       "  del t2065\n",
       "  # Created by CPU Offloading Transform\n",
       "  t457 = load_to_gpu(offloaded_t457, 'cuda:0')  # t457: \"cuda:0 f32[128, 16]\"\n",
       "  t2069 = torch.reshape(t457, (-1, 16))  # t2069: \"cuda:0 f32[128, 16]\"\n",
       "    # t2069 = ltorch.reshape(t457, (-1, 16))  # t2069: \"cuda:0 f32[128, 16]\"\n",
       "      # t2069 = prims.reshape(t457, (128, 16))  # t2069: \"cuda:0 f32[128, 16]\"\n",
       "  del t457\n",
       "  t2070 = torch.matmul(t2068, t2069)  # t2070: \"cuda:0 f32[16, 16]\"\n",
       "    # t2070 = ltorch.matmul(t2068, t2069)  # t2070: \"cuda:0 f32[16, 16]\"\n",
       "      # t2070 = prims.matmul(t2068, t2069)  # t2070: \"cuda:0 f32[16, 16]\"\n",
       "  del t2068, t2069\n",
       "  t2059 = torch.permute(t2056, (1, 0))  # t2059: \"cuda:0 f32[16, 128]\"\n",
       "    # t2059 = ltorch.permute(t2056, (1, 0))  # t2059: \"cuda:0 f32[16, 128]\"\n",
       "      # t2059 = prims.transpose(t2056, (1, 0))  # t2059: \"cuda:0 f32[16, 128]\"\n",
       "  del t2056\n",
       "  # Created by CPU Offloading Transform\n",
       "  t462 = load_to_gpu(offloaded_t462, 'cuda:0')  # t462: \"cuda:0 f32[128, 16]\"\n",
       "  t2060 = torch.reshape(t462, (-1, 16))  # t2060: \"cuda:0 f32[128, 16]\"\n",
       "    # t2060 = ltorch.reshape(t462, (-1, 16))  # t2060: \"cuda:0 f32[128, 16]\"\n",
       "      # t2060 = prims.reshape(t462, (128, 16))  # t2060: \"cuda:0 f32[128, 16]\"\n",
       "  del t462\n",
       "  t2061 = torch.matmul(t2059, t2060)  # t2061: \"cuda:0 f32[16, 16]\"\n",
       "    # t2061 = ltorch.matmul(t2059, t2060)  # t2061: \"cuda:0 f32[16, 16]\"\n",
       "      # t2061 = prims.matmul(t2059, t2060)  # t2061: \"cuda:0 f32[16, 16]\"\n",
       "  del t2059, t2060\n",
       "  t2050 = torch.permute(t2047, (1, 0))  # t2050: \"cuda:0 f32[16, 128]\"\n",
       "    # t2050 = ltorch.permute(t2047, (1, 0))  # t2050: \"cuda:0 f32[16, 128]\"\n",
       "      # t2050 = prims.transpose(t2047, (1, 0))  # t2050: \"cuda:0 f32[16, 128]\"\n",
       "  del t2047\n",
       "  # Created by CPU Offloading Transform\n",
       "  t467 = load_to_gpu(offloaded_t467, 'cuda:0')  # t467: \"cuda:0 f32[128, 16]\"\n",
       "  t2051 = torch.reshape(t467, (-1, 16))  # t2051: \"cuda:0 f32[128, 16]\"\n",
       "    # t2051 = ltorch.reshape(t467, (-1, 16))  # t2051: \"cuda:0 f32[128, 16]\"\n",
       "      # t2051 = prims.reshape(t467, (128, 16))  # t2051: \"cuda:0 f32[128, 16]\"\n",
       "  del t467\n",
       "  t2052 = torch.matmul(t2050, t2051)  # t2052: \"cuda:0 f32[16, 16]\"\n",
       "    # t2052 = ltorch.matmul(t2050, t2051)  # t2052: \"cuda:0 f32[16, 16]\"\n",
       "      # t2052 = prims.matmul(t2050, t2051)  # t2052: \"cuda:0 f32[16, 16]\"\n",
       "  del t2050, t2051\n",
       "  t2041 = torch.permute(t2038, (1, 0))  # t2041: \"cuda:0 f32[16, 128]\"\n",
       "    # t2041 = ltorch.permute(t2038, (1, 0))  # t2041: \"cuda:0 f32[16, 128]\"\n",
       "      # t2041 = prims.transpose(t2038, (1, 0))  # t2041: \"cuda:0 f32[16, 128]\"\n",
       "  del t2038\n",
       "  # Created by CPU Offloading Transform\n",
       "  t472 = load_to_gpu(offloaded_t472, 'cuda:0')  # t472: \"cuda:0 f32[128, 16]\"\n",
       "  t2042 = torch.reshape(t472, (-1, 16))  # t2042: \"cuda:0 f32[128, 16]\"\n",
       "    # t2042 = ltorch.reshape(t472, (-1, 16))  # t2042: \"cuda:0 f32[128, 16]\"\n",
       "      # t2042 = prims.reshape(t472, (128, 16))  # t2042: \"cuda:0 f32[128, 16]\"\n",
       "  del t472\n",
       "  t2043 = torch.matmul(t2041, t2042)  # t2043: \"cuda:0 f32[16, 16]\"\n",
       "    # t2043 = ltorch.matmul(t2041, t2042)  # t2043: \"cuda:0 f32[16, 16]\"\n",
       "      # t2043 = prims.matmul(t2041, t2042)  # t2043: \"cuda:0 f32[16, 16]\"\n",
       "  del t2041, t2042\n",
       "  t2032 = torch.permute(t2029, (1, 0))  # t2032: \"cuda:0 f32[16, 128]\"\n",
       "    # t2032 = ltorch.permute(t2029, (1, 0))  # t2032: \"cuda:0 f32[16, 128]\"\n",
       "      # t2032 = prims.transpose(t2029, (1, 0))  # t2032: \"cuda:0 f32[16, 128]\"\n",
       "  del t2029\n",
       "  # Created by CPU Offloading Transform\n",
       "  t477 = load_to_gpu(offloaded_t477, 'cuda:0')  # t477: \"cuda:0 f32[128, 16]\"\n",
       "  t2033 = torch.reshape(t477, (-1, 16))  # t2033: \"cuda:0 f32[128, 16]\"\n",
       "    # t2033 = ltorch.reshape(t477, (-1, 16))  # t2033: \"cuda:0 f32[128, 16]\"\n",
       "      # t2033 = prims.reshape(t477, (128, 16))  # t2033: \"cuda:0 f32[128, 16]\"\n",
       "  del t477\n",
       "  t2034 = torch.matmul(t2032, t2033)  # t2034: \"cuda:0 f32[16, 16]\"\n",
       "    # t2034 = ltorch.matmul(t2032, t2033)  # t2034: \"cuda:0 f32[16, 16]\"\n",
       "      # t2034 = prims.matmul(t2032, t2033)  # t2034: \"cuda:0 f32[16, 16]\"\n",
       "  del t2032, t2033\n",
       "  t2023 = torch.permute(t2020, (1, 0))  # t2023: \"cuda:0 f32[16, 128]\"\n",
       "    # t2023 = ltorch.permute(t2020, (1, 0))  # t2023: \"cuda:0 f32[16, 128]\"\n",
       "      # t2023 = prims.transpose(t2020, (1, 0))  # t2023: \"cuda:0 f32[16, 128]\"\n",
       "  del t2020\n",
       "  # Created by CPU Offloading Transform\n",
       "  t482 = load_to_gpu(offloaded_t482, 'cuda:0')  # t482: \"cuda:0 f32[128, 16]\"\n",
       "  t2024 = torch.reshape(t482, (-1, 16))  # t2024: \"cuda:0 f32[128, 16]\"\n",
       "    # t2024 = ltorch.reshape(t482, (-1, 16))  # t2024: \"cuda:0 f32[128, 16]\"\n",
       "      # t2024 = prims.reshape(t482, (128, 16))  # t2024: \"cuda:0 f32[128, 16]\"\n",
       "  del t482\n",
       "  t2025 = torch.matmul(t2023, t2024)  # t2025: \"cuda:0 f32[16, 16]\"\n",
       "    # t2025 = ltorch.matmul(t2023, t2024)  # t2025: \"cuda:0 f32[16, 16]\"\n",
       "      # t2025 = prims.matmul(t2023, t2024)  # t2025: \"cuda:0 f32[16, 16]\"\n",
       "  del t2023, t2024\n",
       "  t2014 = torch.permute(t2011, (1, 0))  # t2014: \"cuda:0 f32[16, 128]\"\n",
       "    # t2014 = ltorch.permute(t2011, (1, 0))  # t2014: \"cuda:0 f32[16, 128]\"\n",
       "      # t2014 = prims.transpose(t2011, (1, 0))  # t2014: \"cuda:0 f32[16, 128]\"\n",
       "  del t2011\n",
       "  # Created by CPU Offloading Transform\n",
       "  t487 = load_to_gpu(offloaded_t487, 'cuda:0')  # t487: \"cuda:0 f32[128, 16]\"\n",
       "  t2015 = torch.reshape(t487, (-1, 16))  # t2015: \"cuda:0 f32[128, 16]\"\n",
       "    # t2015 = ltorch.reshape(t487, (-1, 16))  # t2015: \"cuda:0 f32[128, 16]\"\n",
       "      # t2015 = prims.reshape(t487, (128, 16))  # t2015: \"cuda:0 f32[128, 16]\"\n",
       "  del t487\n",
       "  t2016 = torch.matmul(t2014, t2015)  # t2016: \"cuda:0 f32[16, 16]\"\n",
       "    # t2016 = ltorch.matmul(t2014, t2015)  # t2016: \"cuda:0 f32[16, 16]\"\n",
       "      # t2016 = prims.matmul(t2014, t2015)  # t2016: \"cuda:0 f32[16, 16]\"\n",
       "  del t2014, t2015\n",
       "  t2005 = torch.permute(t2002, (1, 0))  # t2005: \"cuda:0 f32[16, 128]\"\n",
       "    # t2005 = ltorch.permute(t2002, (1, 0))  # t2005: \"cuda:0 f32[16, 128]\"\n",
       "      # t2005 = prims.transpose(t2002, (1, 0))  # t2005: \"cuda:0 f32[16, 128]\"\n",
       "  del t2002\n",
       "  # Created by CPU Offloading Transform\n",
       "  t492 = load_to_gpu(offloaded_t492, 'cuda:0')  # t492: \"cuda:0 f32[128, 16]\"\n",
       "  t2006 = torch.reshape(t492, (-1, 16))  # t2006: \"cuda:0 f32[128, 16]\"\n",
       "    # t2006 = ltorch.reshape(t492, (-1, 16))  # t2006: \"cuda:0 f32[128, 16]\"\n",
       "      # t2006 = prims.reshape(t492, (128, 16))  # t2006: \"cuda:0 f32[128, 16]\"\n",
       "  del t492\n",
       "  t2007 = torch.matmul(t2005, t2006)  # t2007: \"cuda:0 f32[16, 16]\"\n",
       "    # t2007 = ltorch.matmul(t2005, t2006)  # t2007: \"cuda:0 f32[16, 16]\"\n",
       "      # t2007 = prims.matmul(t2005, t2006)  # t2007: \"cuda:0 f32[16, 16]\"\n",
       "  del t2005, t2006\n",
       "  return (None, t2899, t2898, t2890, t2889, t2881, t2880, t2872, t2871, t2863, t2862, t2854, t2853, t2845, t2844, t2836, t2835, t2827, t2826, t2818, t2817, t2809, t2808, t2800, t2799, t2791, t2790, t2782, t2781, t2773, t2772, t2764, t2763, t2755, t2754, t2746, t2745, t2737, t2736, t2728, t2727, t2719, t2718, t2710, t2709, t2701, t2700, t2692, t2691, t2683, t2682, t2674, t2673, t2665, t2664, t2656, t2655, t2647, t2646, t2638, t2637, t2629, t2628, t2620, t2619, t2611, t2610, t2602, t2601, t2593, t2592, t2584, t2583, t2575, t2574, t2566, t2565, t2557, t2556, t2548, t2547, t2539, t2538, t2530, t2529, t2521, t2520, t2512, t2511, t2503, t2502, t2494, t2493, t2485, t2484, t2476, t2475, t2467, t2466, t2458, t2457, t2449, t2448, t2440, t2439, t2431, t2430, t2422, t2421, t2413, t2412, t2404, t2403, t2395, t2394, t2386, t2385, t2377, t2376, t2368, t2367, t2359, t2358, t2350, t2349, t2341, t2340, t2332, t2331, t2323, t2322, t2314, t2313, t2305, t2304, t2296, t2295, t2287, t2286, t2278, t2277, t2269, t2268, t2260, t2259, t2251, t2250, t2242, t2241, t2233, t2232, t2224, t2223, t2215, t2214, t2206, t2205, t2197, t2196, t2188, t2187, t2179, t2178, t2170, t2169, t2161, t2160, t2152, t2151, t2143, t2142, t2134, t2133, t2125, t2124, t2116, t2115, t2107, t2106, t2098, t2097, t2089, t2088, t2080, t2079, t2071, t2070, t2062, t2061, t2053, t2052, t2044, t2043, t2035, t2034, t2026, t2025, t2017, t2016, t2008, t2007)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bw_traces[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated Memory after cleaning 0.0 GB\n"
     ]
    }
   ],
   "source": [
    "model, args, kwargs = get_model_and_args()\n",
    "\n",
    "measurement_thunder = benchmark(thunder.jit(model), model, args, kwargs)\n",
    "measurement_thunder_offload = benchmark(thunder.jit(model, transforms=[CPUOffloading()]), model, args, kwargs)\n",
    "\n",
    "del model, args, kwargs\n",
    "\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.benchmark.utils.common.Measurement object at 0x787d2014f970>\n",
       "stmt:\n",
       "  # Use the optimized model for prediction and backward\n",
       "  o = jmodel(*args, **kwargs)\n",
       "  o.sum().backward()\n",
       "  for param in model.parameters():  # use original model for clear grads\n",
       "      param.grad = None\n",
       "\n",
       "  8.79 ms\n",
       "  1 measurement, 10 runs , 1 thread"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measurement_thunder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.benchmark.utils.common.Measurement object at 0x787d191b2f80>\n",
       "stmt:\n",
       "  # Use the optimized model for prediction and backward\n",
       "  o = jmodel(*args, **kwargs)\n",
       "  o.sum().backward()\n",
       "  for param in model.parameters():  # use original model for clear grads\n",
       "      param.grad = None\n",
       "\n",
       "  12.79 ms\n",
       "  1 measurement, 10 runs , 1 thread"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measurement_thunder_offload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try it on a real-life model Llama-3. We will run it on a smaller Llama-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kkalambarkar/git/torch-vision/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from litgpt import Config, GPT\n",
    "from functools import partial\n",
    "from torch.testing import make_tensor\n",
    "\n",
    "N_LAYER = 7\n",
    "BLOCK_SIZE = 1024\n",
    "\n",
    "def get_model_and_args(batchdims=8):\n",
    "    with torch.device(\"cuda\"):\n",
    "        cfg: Config = Config.from_name(\"Llama-3-8B\")\n",
    "        # Smaller configuration\n",
    "        cfg.n_layer = N_LAYER\n",
    "        cfg.block_size = BLOCK_SIZE\n",
    "\n",
    "        model = GPT(cfg)\n",
    "        make = partial(make_tensor, low=0, high=255, device='cuda', dtype=torch.int64, requires_grad=False)\n",
    "        shape = (batchdims,) + (cfg.block_size,)\n",
    "\n",
    "        x = make(shape)\n",
    "        args, kwargs = (x,), {}\n",
    "\n",
    "    return model, args, kwargs, cfg\n",
    "\n",
    "def print_memory_usage_and_benchmark(name):\n",
    "    print(f\"{name} took -\")\n",
    "    model, args, kwargs, cfg = get_model_and_args()\n",
    "\n",
    "    if name == 'thunder':\n",
    "        jmodel = thunder.jit(model)\n",
    "    elif name == 'thunder_offload':\n",
    "        jmodel = thunder.jit(model, transforms=[CPUOffloading()])\n",
    "    else:\n",
    "        raise RuntimeError(\"Received invalid value for `name` - try `thunder` or `thunder_offload`.\")\n",
    "\n",
    "    memory_after_model_load = torch.cuda.max_memory_allocated() / 1e9\n",
    "    print(f\"Peak memory after loading the model : {memory_after_model_load} GB\")\n",
    "\n",
    "    a = jmodel(*args, **kwargs)\n",
    "\n",
    "    memory_after_forward = torch.cuda.max_memory_allocated() / 1e9\n",
    "    print(f\"Peak memory after forward the model : {memory_after_forward} GB\")\n",
    "\n",
    "    g = torch.rand_like(a)\n",
    "    actual_grads = torch.autograd.grad(a, model.parameters(), g)\n",
    "\n",
    "    memory_after_backward = torch.cuda.max_memory_allocated() / 1e9\n",
    "    print(f\"Peak memory after backward the model : {memory_after_backward} GB\")\n",
    "\n",
    "    del a, g, actual_grads  # Clear data which is not required for benchmark to free some memory.\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    measurement = benchmark(jmodel, model, args, kwargs)\n",
    "    print(f\"Benchmark Timings - mean : {measurement.mean} - median {measurement.median}\")\n",
    "\n",
    "    del jmodel, model, cfg, args, kwargs\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.reset_accumulated_memory_stats()\n",
    "    print(\"CUDA Memory has been cleared and currently allocated memory is \", torch.cuda.memory_allocated() / 1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thunder took -\n",
      "Peak memory after loading the model : 10.311422464 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W814 12:14:19.087144933 sdp_utils.cpp:455] Warning: 1Torch was not compiled with flash attention. (function operator())\n",
      "[W814 12:14:19.087159023 sdp_utils.cpp:504] Warning: 1Torch was not compiled with memory efficient attention. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak memory after forward the model : 39.679967232 GB\n",
      "Peak memory after backward the model : 47.024177152 GB\n",
      "Benchmark Timings - mean : 3.944030920599471 - median 3.944030920599471\n",
      "CUDA Memory has been cleared and currently allocated memory is  0.0\n"
     ]
    }
   ],
   "source": [
    "print_memory_usage_and_benchmark(\"thunder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thunder_offload took -\n",
      "Peak memory after loading the model : 10.311422464 GB\n",
      "Peak memory after forward the model : 14.64786944 GB\n",
      "Peak memory after backward the model : 32.004145152 GB\n",
      "Benchmark Timings - mean : 5.2520787032000955 - median 5.2520787032000955\n",
      "CUDA Memory has been cleared and currently allocated memory is  0.0\n"
     ]
    }
   ],
   "source": [
    "print_memory_usage_and_benchmark(\"thunder_offload\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
