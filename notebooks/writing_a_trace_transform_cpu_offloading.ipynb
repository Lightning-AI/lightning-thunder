{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    "In this tutorial, we will write a Trace transformation to perform CPU Offloading of intermediate tensors. CPU Offloading is a technique to decrease the peak memory usage of the training loop. This can allow us to train a larger model which would otherwise won't be possible. However, we have to trade of some performance (increased training time) to achieve the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.utils.benchmark\n",
    "\n",
    "import thunder\n",
    "from thunder import Transform\n",
    "from thunder.core.proxies import TensorProxy, variableify\n",
    "from thunder.core.pytree import tree_flatten, tree_map\n",
    "from thunder.core.trace import tracectx, from_trace\n",
    "from thunder.extend import OperatorExecutor\n",
    "from thunder.core.symbol import BoundSymbol\n",
    "from thunder.core import prims\n",
    "from thunder.core.transforms import bsym_list_to_dag, Node, toposort_bsym_dag, TOPOSORT_ORDER\n",
    "from thunder.core.vjp_utils import get_saved_for_backward_tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforms\n",
    "\n",
    "`thunder` allows you to write your own transforms which can be applied at various stages during the compilation. To write our transform, we have to inherit from `thunder.Transform` class. This class implements the interface that each transform should have. By default, it provides no-op transformations.\n",
    "\n",
    "We then provide our transform object to the `thunder.jit` via `transforms` argument.\n",
    "\n",
    "However, before writing our transform, we will make an `OperatorExecutor` with which we will create 2 operators/symbol - 1. to offload tensors to CPU 2. Load the offloaded tensors back to CUDA device\n",
    "\n",
    "You read more about adding custom operators in `adding_custom_operator.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new executor.\n",
    "offload_exec = OperatorExecutor(\"offload_exec\")\n",
    "\n",
    "\n",
    "def offload_to_cpu_impl(t):\n",
    "    # Due to https://github.com/Lightning-AI/lightning-thunder/issues/950\n",
    "    # it may receive tensor on CPU.\n",
    "    if t.device == torch.device(\"cpu\"):\n",
    "        return t\n",
    "\n",
    "    packed = torch.empty(\n",
    "        t.size(),\n",
    "        dtype=t.dtype,\n",
    "        layout=t.layout,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    packed.copy_(t, non_blocking=False)\n",
    "    return packed\n",
    "\n",
    "\n",
    "offload_to_cpu = offload_exec.register_operator(\n",
    "    \"offload_to_cpu\",\n",
    "    meta=lambda t: TensorProxy(\"offloaded_\" + t.name, like=t, device=thunder.core.devices.Device(\"cpu\")),\n",
    "    fn=offload_to_cpu_impl,\n",
    ")\n",
    "\n",
    "\n",
    "def load_to_gpu_impl(t, device):\n",
    "    return t.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "load_to_gpu = offload_exec.register_operator(\n",
    "    \"load_to_gpu\",\n",
    "    meta=lambda t, device: TensorProxy(like=t, device=thunder.core.devices.Device(device)),\n",
    "    fn=load_to_gpu_impl,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to implement our transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_symbols_to_last_or_first_used_variables(symbols, first_used=False):\n",
    "    variable_to_symbol = {}\n",
    "    symbol_to_variables = {}\n",
    "\n",
    "    def _mark_last_use(symbol, variable):\n",
    "        if not variable in variable_to_symbol:\n",
    "            variable_to_symbol[variable] = symbol\n",
    "            symbol_to_variables.setdefault(symbol, []).append(variable)\n",
    "\n",
    "    iter_symbols = symbols if first_used else reversed(symbols)\n",
    "    for symbol in iter_symbols:\n",
    "        # If this function is used in the combined nvfuser+torch executor, there are no symbols but regions.\n",
    "        # Regions do not have args, kwargs\n",
    "        if hasattr(symbol, \"inputs\"):\n",
    "            variables = tuple(symbol.inputs) + tuple(symbol.outputs)\n",
    "        else:\n",
    "            variables = (symbol.flat_variableified_proxy_args) + tuple(symbol.flat_variableified_proxy_outs)\n",
    "        tree_map(lambda x: _mark_last_use(symbol, x), variables)\n",
    "\n",
    "    return symbol_to_variables, variable_to_symbol\n",
    "\n",
    "\n",
    "def get_symbols_to_last_used_variables(symbols):\n",
    "    return _get_symbols_to_last_or_first_used_variables(symbols)\n",
    "\n",
    "\n",
    "def get_symbols_to_first_used_variables(symbols):\n",
    "    return _get_symbols_to_last_or_first_used_variables(symbols, first_used=True)\n",
    "\n",
    "\n",
    "def get_symbol_to_idx(symbols):\n",
    "    '''\n",
    "    This function returns a map from symbol to it's position in the list.\n",
    "    '''\n",
    "    return {sym: idx for idx, sym in enumerate(symbols)}\n",
    "\n",
    "\n",
    "def move_closer_to_consumer(execution_trace):\n",
    "    order_in_trace = {bsym: i for i, bsym in enumerate(execution_trace.bound_symbols)}\n",
    "\n",
    "    def prefer_ops_closer_to_consumer(eligible_nodes: list[Node]) -> int:\n",
    "        def key(node: Node) -> int:\n",
    "            return order_in_trace[node.bsym]\n",
    "\n",
    "        return min(range(len(eligible_nodes)), key=lambda i: key(eligible_nodes[i]))\n",
    "\n",
    "    # This moves all del or clear collection at the bottom (as they don't return anything)\n",
    "    bound_symbols = toposort_bsym_dag(\n",
    "        bsym_list_to_dag(execution_trace.bound_symbols)[1],\n",
    "        TOPOSORT_ORDER.BOTTOM_UP,\n",
    "        selector=prefer_ops_closer_to_consumer,\n",
    "    )\n",
    "\n",
    "    for idx, bsym in enumerate(bound_symbols):\n",
    "        if bsym.sym.id == prims.PrimIDs.DEL:\n",
    "            break\n",
    "\n",
    "    new_execution_trace = from_trace(execution_trace)\n",
    "    new_execution_trace.bound_symbols = bound_symbols[:idx]\n",
    "\n",
    "    new_execution_trace = thunder.executors.passes.del_last_used(new_execution_trace, clear_mutable_collections=True)\n",
    "    return new_execution_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to the main topic, of writing the transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPUOffloading(Transform):\n",
    "    '''\n",
    "    Transform to implement CPU Offloading.\n",
    "\n",
    "    save_tensor_policy - Users can pass a callback with signature fn(offloaded_tensors, forward_trace) to filter\n",
    "                         the offloaded_tensors based on their preference eg. biggest 20% intermediate tensors or\n",
    "                         intermediates of certain operations\n",
    "    '''\n",
    "    def __init__(self, save_tensor_policy=None):\n",
    "        self.forward_pass = None\n",
    "        self.backward_pass = None\n",
    "        self._offloaded_tensors = ()\n",
    "        self.save_tensor_policy = None\n",
    "        if save_tensor_policy is not None:\n",
    "            assert callable(save_tensor_policy)\n",
    "            self.save_tensor_policy = save_tensor_policy\n",
    "\n",
    "    def _get_tensors_to_offload(self, forward_trace):\n",
    "        '''\n",
    "        Based on the `forward_trace`, we find the symbols that we want to offload to CPU.\n",
    "        This function finds the intermediate tensors that are saved for backward i.e. ones that are not input or output of this trace.\n",
    "        '''\n",
    "        return_bsym = forward_trace.bound_symbols[-1]\n",
    "        trace_args = return_bsym.args[0][\"flat_args\"]\n",
    "        saved_tensors = get_saved_for_backward_tensors(forward_trace)\n",
    "\n",
    "        tensor_args_name = tuple(arg.name for arg in trace_args if isinstance(arg, TensorProxy))\n",
    "\n",
    "        def is_in_tensor_args(t):\n",
    "            return t.name in tensor_args_name\n",
    "\n",
    "        def is_cuda_tensor(t):\n",
    "            return t.device.type == \"cuda\"\n",
    "\n",
    "        # Tensors which are intermediate and not argument to the computation trace are\n",
    "        # the ones we are interested in offloading.\n",
    "        tensors_to_offload = tuple(t for t in saved_tensors if ((not is_in_tensor_args(t)) and is_cuda_tensor(t)))\n",
    "\n",
    "        # Apply users policy if present.\n",
    "        if self.save_tensor_policy is not None:\n",
    "            tensors_to_offload = self.save_tensor_policy(tensors_to_offload, forward_trace)\n",
    "        self.tensors_to_offload = tensors_to_offload\n",
    "        return self.tensors_to_offload\n",
    "\n",
    "    def _replace_saved_tensors(self, forward_trace, new_output_map):\n",
    "        return_bsym = forward_trace.bound_symbols[-1]\n",
    "        return_bsym_args = return_bsym.args\n",
    "        saved_tensors = return_bsym.args[1][0]\n",
    "\n",
    "        new_saved_tensors = []\n",
    "        for t in saved_tensors:\n",
    "            new_output = new_output_map.get(variableify(t), t)\n",
    "            new_saved_tensors.append(new_output)\n",
    "\n",
    "        new_return_bsym = BoundSymbol.from_bsym(\n",
    "            return_bsym, **{\"args\": (return_bsym_args[0], (tuple(new_saved_tensors), return_bsym_args[1][1]))}\n",
    "        )\n",
    "\n",
    "        # Replace the old return with our new return.\n",
    "        forward_trace.bound_symbols.pop(-1)\n",
    "        forward_trace.bound_symbols.append(new_return_bsym)\n",
    "\n",
    "    def _offload_tensors_from_forward(self, computation_trace):\n",
    "        '''\n",
    "        This function takes the forward computation trace and performs following step\n",
    "        1. Find the tensors to be offloaded using `_get_tensors_to_offload` (this also calls users `save_tensor_policy` if present).\n",
    "        2. Insert calls to the `offload_to_cpu` symbol with the tensor to offload. These calls are placed after the last computational\n",
    "           use of the tensors to be offloaded so that we free the memory as soon as possible.\n",
    "        3. Finally, we update the last symbol i.e. `return` symbol to return the offloaded tensors instead of the original tensors.\n",
    "        '''\n",
    "        # Step 1\n",
    "        # Find the tensors to offload.\n",
    "        # We offload saved tensors which are not arguments to the computation trace and are saved for backwards.\n",
    "        tensors_to_offload = self._get_tensors_to_offload(computation_trace)\n",
    "\n",
    "        # Step 2\n",
    "        # Insert the offloading calls after the last use of the saved tensor (which we want to offload).\n",
    "        _, variable_to_last_symbol = get_symbols_to_last_used_variables(\n",
    "            computation_trace.bound_symbols[:-1]\n",
    "        )  # Ignore the return statement.\n",
    "        symbol_to_idx = get_symbol_to_idx(computation_trace.bound_symbols)\n",
    "\n",
    "        # Book keeping for backward pass update.\n",
    "        new_output_map = {}\n",
    "        new_output_dev_map = {}\n",
    "\n",
    "        # Since we are inserting in the list (we need to obey increasing order) - else the insertions will be incorrect.\n",
    "        sorted_tensors_to_offload = sorted(\n",
    "            tensors_to_offload, key=lambda t: symbol_to_idx[variable_to_last_symbol[variableify(t)]]\n",
    "        )\n",
    "        for idx, t in enumerate(sorted_tensors_to_offload):\n",
    "            last_used_symbol = variable_to_last_symbol[variableify(t)]\n",
    "            last_used_symbol_idx = symbol_to_idx[last_used_symbol]\n",
    "            computation_trace.push_scope([])\n",
    "            with tracectx(computation_trace):\n",
    "                o = offload_to_cpu(t)\n",
    "                prims.python_del(t)\n",
    "            scoped_comp = computation_trace.pop_scope()\n",
    "            scoped_comp[0].header = \"Created by CPU Offloading Transform\"\n",
    "            offload_to_cpu_symbol = scoped_comp[0]\n",
    "            del_symbol = scoped_comp[1]\n",
    "\n",
    "            # This will insert `del` first and then push it down when we insert `offload_to_cpu`.\n",
    "            computation_trace.bound_symbols.insert(last_used_symbol_idx + 1 + (idx * 2), del_symbol)\n",
    "            computation_trace.bound_symbols.insert(last_used_symbol_idx + 1 + (idx * 2), offload_to_cpu_symbol)\n",
    "\n",
    "            # Update book keeping.\n",
    "            new_output_map[variableify(t)] = o\n",
    "            new_output_dev_map[variableify(t)] = t.device.device_str()\n",
    "\n",
    "        # Step 3\n",
    "        # Update the return symbol to return our offloaded tensors in saved for backward.\n",
    "        self._replace_saved_tensors(computation_trace, new_output_map)\n",
    "\n",
    "        # Book keeping for backward pass update.\n",
    "        self._offloaded_tensors = new_output_map\n",
    "        self._offloaded_tensors_dev = new_output_dev_map\n",
    "        return computation_trace\n",
    "\n",
    "    def _load_tensors_for_backward(self, computation_trace):\n",
    "        '''\n",
    "        This function takes the backward computation trace and performs following step\n",
    "        1. Finds the unpack collection symbol which unpacks the saved tensors passed to the backward trace.\n",
    "        2. Updates the unpack collection to unpack the offloaded tensors instead of the original ones.\n",
    "        3. Before the first use of the offloaded tensor in computation, we insert the `load_to_gpu` to load the tensor back on GPU.\n",
    "        '''\n",
    "        self.backward_pass = computation_trace\n",
    "        offloaded_tensors = self._offloaded_tensors\n",
    "        offloaded_tensors_dev_map = self._offloaded_tensors_dev\n",
    "\n",
    "        compute_producers, compute_consumers = thunder.core.utils.producers_and_consumers(computation_trace)\n",
    "\n",
    "        # We want to insert `loads` before the first use of offloaded_tensors.\n",
    "        _, variable_to_first_symbol = get_symbols_to_first_used_variables(computation_trace.bound_symbols)\n",
    "\n",
    "        symbol_to_idx = get_symbol_to_idx(computation_trace.bound_symbols)\n",
    "\n",
    "        # Step 1 and 2\n",
    "        # Update unpack collection so that it\n",
    "        # outputs the offloaded tensor proxies (not the original ones).\n",
    "        unpack_sym = compute_producers[list(offloaded_tensors.keys())[0].proxy]\n",
    "        unpack_idx = symbol_to_idx[unpack_sym]\n",
    "        unpack_sym_out = unpack_sym.output\n",
    "        new_out = []\n",
    "        for out in unpack_sym_out:\n",
    "            vout = variableify(out)\n",
    "            if vout in offloaded_tensors:\n",
    "                new_out.append(offloaded_tensors[vout])\n",
    "            else:\n",
    "                new_out.append(out)\n",
    "        new_unpack_bsym = BoundSymbol.from_bsym(unpack_sym, output=tuple(new_out))\n",
    "        computation_trace.bound_symbols[unpack_idx] = new_unpack_bsym\n",
    "\n",
    "        # Now we again find the first usages of offloaded tensor\n",
    "        # This will actually point us to the first consumer of the offloaded tensor.\n",
    "        offset = unpack_idx + 1\n",
    "        _, variable_to_first_symbol = get_symbols_to_first_used_variables(computation_trace.bound_symbols[offset:])\n",
    "\n",
    "        # Step 3\n",
    "        # Load the offloaded tensors to GPU before usage.\n",
    "        # Should iterate in correct order (else insertion positions will be incorrect).\n",
    "        for idx, (vt, offloaded_t) in enumerate(\n",
    "            sorted(offloaded_tensors.items(), key=lambda kv: symbol_to_idx[variable_to_first_symbol[kv[0]]])\n",
    "        ):\n",
    "            first_used_symbol = variable_to_first_symbol[vt]\n",
    "            first_used_symbol_idx = symbol_to_idx[first_used_symbol]\n",
    "            t = vt.proxy\n",
    "            device = offloaded_tensors_dev_map[vt]\n",
    "\n",
    "            with tracectx(computation_trace):\n",
    "                new_sym = load_to_gpu.bind(offloaded_t, device, output=t)\n",
    "\n",
    "            new_sym.header = \"Created by CPU Offloading Transform\"\n",
    "            computation_trace.bound_symbols.insert(first_used_symbol_idx + idx, new_sym)\n",
    "\n",
    "        return computation_trace\n",
    "\n",
    "    def transform_trace_post_optimization(self, computation_trace: thunder.TraceCtx, **kwargs):\n",
    "        if self.forward_pass is None:\n",
    "            self.forward_pass = computation_trace\n",
    "            # Processing for the forward pass (only if we are going to compute backward).\n",
    "            if \"augmented_forward\" in computation_trace.fn.__name__:\n",
    "                computation_trace = self._offload_tensors_from_forward(computation_trace)\n",
    "        else:\n",
    "            # Skip if no tensor was offloaded.\n",
    "            if len(self._offloaded_tensors) == 0:\n",
    "                return computation_trace\n",
    "\n",
    "            # We need this because in unmodified backward trace, the first consumer of saved_for_backward maybe\n",
    "            # a reshape or permute op and the actual computation occurs 50-100 (or more) lines later.\n",
    "            # Because of this we load more tensors than required eagerly (thus decreasing the memory gains from CPU Offloading).\n",
    "            # This function is currently tailored to pattern observed in Llama-2\n",
    "            # Eg. on line 92\n",
    "            #   # Created by CPU Offloading Transform\n",
    "            #   t1319 = load_to_gpu(offloaded_t1319, 'cuda:0')  # t1319: \"cuda:0 f32[8, 1024, 11008]\"\n",
    "            #   t4021 = torch.reshape(t1319, (-1, 11008))  # t4021: \"cuda:0 f32[8192, 11008]\"\n",
    "            #     # t4021 = ltorch.reshape(t1319, (-1, 11008))  # t4021: \"cuda:0 f32[8192, 11008]\"\n",
    "            #       # t4021 = prims.reshape(t1319, (8192, 11008))  # t4021: \"cuda:0 f32[8192, 11008]\"\n",
    "            #   del t1319\n",
    "            # And it's usage in computation is at 612\n",
    "            # t4022 = torch.matmul(t4020, t4021)  # t4022: \"cuda:0 f32[4096, 11008]\"\n",
    "            #   t4022 = ltorch.matmul(t4020, t4021)  # t4022: \"cuda:0 f32[4096, 11008]\"\n",
    "            #     t4022 = prims.matmul(t4020, t4021)  # t4022: \"cuda:0 f32[4096, 11008]\"\n",
    "            computation_trace = move_closer_to_consumer(computation_trace)\n",
    "\n",
    "            # Transform the backward trace to load offloaded tensors back to the device.\n",
    "            computation_trace = self._load_tensors_for_backward(computation_trace)\n",
    "\n",
    "        return computation_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_accumulated_memory_stats()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    print(f\"Allocated Memory after cleaning {torch.cuda.memory_allocated() / 1e9} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(jmodel, model, args, kwargs):\n",
    "    # NOTE - This function takes care of warm-up\n",
    "    stmt = \"\"\"\n",
    "# Use the optimized model for prediction and backward\n",
    "o = jmodel(*args, **kwargs)\n",
    "o.sum().backward()\n",
    "for param in model.parameters():  # use original model for clear grads\n",
    "    param.grad = None\n",
    "\"\"\"\n",
    "    timer = torch.utils.benchmark.Timer(\n",
    "        stmt=stmt, globals={\"jmodel\": jmodel, \"model\": model, \"args\": args, \"kwargs\": kwargs}\n",
    "    ).timeit(number=10)\n",
    "    return timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity Check the output values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated Memory after cleaning 0.0 GB\n"
     ]
    }
   ],
   "source": [
    "class MySimpleModel(torch.nn.Module):\n",
    "    def __init__(self, n_layers=10):\n",
    "        super().__init__()\n",
    "        self.fcs = torch.nn.ModuleList([torch.nn.Linear(16, 16) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for fc in self.fcs:\n",
    "            x = torch.nn.functional.relu(fc(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "def get_model_and_args():\n",
    "    device = 'cuda'\n",
    "    model = MySimpleModel(n_layers=100).to(device)\n",
    "    args = (torch.randn(128, 16, device=device),)\n",
    "    kwargs = {}\n",
    "    return model, args, kwargs\n",
    "\n",
    "model, args, kwargs = get_model_and_args()\n",
    "\n",
    "jmodel = thunder.jit(model, transforms=[CPUOffloading()])\n",
    "\n",
    "actual = jmodel(*args, **kwargs)\n",
    "grad_output = torch.randn_like(actual)\n",
    "actual_grads = torch.autograd.grad(actual, jmodel.parameters(), grad_output)\n",
    "\n",
    "with torch.no_grad():\n",
    "    actual_cpu = actual.to(\"cpu\")\n",
    "    actual_grads_cpu = tree_map(lambda t: t.to(\"cpu\"), actual_grads)\n",
    "\n",
    "# Check against the eager model\n",
    "expected = model(*args, **kwargs)\n",
    "expected_grads = torch.autograd.grad(expected, model.parameters(), grad_output)\n",
    "\n",
    "with torch.no_grad():\n",
    "    expected_cpu = expected.to(\"cpu\")\n",
    "    expected_grads_cpu = tree_map(lambda t: t.to(\"cpu\"), expected_grads)\n",
    "\n",
    "torch.testing.assert_close(actual_cpu, expected_cpu)\n",
    "torch.testing.assert_close(actual_grads_cpu, expected_grads_cpu)\n",
    "\n",
    "del jmodel, model, args, kwargs, actual, actual_grads, expected, expected_grads, grad_output  # Free memory.\n",
    "\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated Memory after cleaning 0.0 GB\n"
     ]
    }
   ],
   "source": [
    "model, args, kwargs = get_model_and_args()\n",
    "\n",
    "measurement_thunder = benchmark(thunder.jit(model), model, args, kwargs)\n",
    "measurement_thunder_offload = benchmark(thunder.jit(model, transforms=[CPUOffloading()]), model, args, kwargs)\n",
    "\n",
    "del model, args, kwargs\n",
    "\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.benchmark.utils.common.Measurement object at 0x71a875cbbfd0>\n",
       "stmt:\n",
       "  # Use the optimized model for prediction and backward\n",
       "  o = jmodel(*args, **kwargs)\n",
       "  o.sum().backward()\n",
       "  for param in model.parameters():  # use original model for clear grads\n",
       "      param.grad = None\n",
       "\n",
       "  8.14 ms\n",
       "  1 measurement, 10 runs , 1 thread"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measurement_thunder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.benchmark.utils.common.Measurement object at 0x71a870bdc580>\n",
       "stmt:\n",
       "  # Use the optimized model for prediction and backward\n",
       "  o = jmodel(*args, **kwargs)\n",
       "  o.sum().backward()\n",
       "  for param in model.parameters():  # use original model for clear grads\n",
       "      param.grad = None\n",
       "\n",
       "  12.77 ms\n",
       "  1 measurement, 10 runs , 1 thread"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measurement_thunder_offload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try it on a real-life model Llama-3. We will run it on a smaller Llama-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kkalambarkar/git/torch-vision/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from litgpt import Config, GPT\n",
    "from functools import partial\n",
    "from torch.testing import make_tensor\n",
    "\n",
    "N_LAYER = 7\n",
    "BLOCK_SIZE = 1024\n",
    "\n",
    "def get_model_and_args(batchdims=8):\n",
    "    with torch.device(\"cuda\"):\n",
    "        cfg: Config = Config.from_name(\"Llama-3-8B\")\n",
    "        # Smaller configuration\n",
    "        cfg.n_layer = N_LAYER\n",
    "        cfg.block_size = BLOCK_SIZE\n",
    "\n",
    "        model = GPT(cfg)\n",
    "        make = partial(make_tensor, low=0, high=255, device='cuda', dtype=torch.int64, requires_grad=False)\n",
    "        shape = (batchdims,) + (cfg.block_size,)\n",
    "\n",
    "        x = make(shape)\n",
    "        args, kwargs = (x,), {}\n",
    "\n",
    "    return model, args, kwargs, cfg\n",
    "\n",
    "def print_memory_usage_and_benchmark(name):\n",
    "    print(f\"{name} took -\")\n",
    "    model, args, kwargs, cfg = get_model_and_args()\n",
    "\n",
    "    if name == 'thunder':\n",
    "        jmodel = thunder.jit(model)\n",
    "    elif name == 'thunder_offload':\n",
    "        jmodel = thunder.jit(model, transforms=[CPUOffloading()])\n",
    "    else:\n",
    "        raise RuntimeError(\"Received invalid value for `name` - try `thunder` or `thunder_offload`.\")\n",
    "\n",
    "    memory_after_model_load = torch.cuda.max_memory_allocated() / 1e9\n",
    "    print(f\"Peak memory after loading the model : {memory_after_model_load} GB\")\n",
    "\n",
    "    a = jmodel(*args, **kwargs)\n",
    "\n",
    "    memory_after_forward = torch.cuda.max_memory_allocated() / 1e9\n",
    "    print(f\"Peak memory after forward the model : {memory_after_forward} GB\")\n",
    "\n",
    "    g = torch.rand_like(a)\n",
    "    actual_grads = torch.autograd.grad(a, model.parameters(), g)\n",
    "\n",
    "    memory_after_backward = torch.cuda.max_memory_allocated() / 1e9\n",
    "    print(f\"Peak memory after backward the model : {memory_after_backward} GB\")\n",
    "\n",
    "    del a, g, actual_grads  # Clear data which is not required for benchmark to free some memory.\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    measurement = benchmark(jmodel, model, args, kwargs)\n",
    "    print(f\"Benchmark Timings - mean : {measurement.mean} - median {measurement.median}\")\n",
    "\n",
    "    del jmodel, model, cfg, args, kwargs\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.reset_accumulated_memory_stats()\n",
    "    print(\"CUDA Memory has been cleared and currently allocated memory is \", torch.cuda.memory_allocated() / 1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thunder took -\n",
      "Peak memory after loading the model : 10.311422464 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W813 16:58:55.278613820 sdp_utils.cpp:455] Warning: 1Torch was not compiled with flash attention. (function operator())\n",
      "[W813 16:58:55.278628240 sdp_utils.cpp:504] Warning: 1Torch was not compiled with memory efficient attention. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak memory after forward the model : 39.679967232 GB\n",
      "Peak memory after backward the model : 47.024177152 GB\n",
      "Benchmark Timings - mean : 4.0779321255002285 - median 4.0779321255002285\n",
      "CUDA Memory has been cleared and currently allocated memory is  0.0\n"
     ]
    }
   ],
   "source": [
    "# print_memory_usage_and_benchmark(\"thunder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thunder_offload took -\n",
      "Peak memory after loading the model : 10.311422464 GB\n",
      "Peak memory after forward the model : 14.64786944 GB\n",
      "Peak memory after backward the model : 32.004145152 GB\n",
      "Benchmark Timings - mean : 5.3143297000002345 - median 5.3143297000002345\n",
      "CUDA Memory has been cleared and currently allocated memory is  0.0\n"
     ]
    }
   ],
   "source": [
    "# print_memory_usage_and_benchmark(\"thunder_offload\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
