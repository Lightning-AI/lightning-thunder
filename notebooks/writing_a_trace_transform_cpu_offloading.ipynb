{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    "In this tutorial, we will write a Trace transformation to perform CPU Offloading of intermediate tensors.\n",
    "\n",
    "CPU Offloading is a technique to decrease the peak memory usage during training. This can allow us to train a larger model which would otherwise won't be possible. However, we have to trade of some performance (increased memory transfers) to achieve the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.utils.benchmark\n",
    "\n",
    "import thunder\n",
    "from thunder.core.transform_common import Transform\n",
    "from thunder.core.proxies import TensorProxy, variableify\n",
    "from thunder.core.pytree import tree_map\n",
    "from thunder.core.trace import tracectx, from_trace\n",
    "from thunder.extend import OperatorExecutor\n",
    "from thunder.core.symbol import BoundSymbol\n",
    "from thunder.core import prims\n",
    "from thunder.core.transforms import bsym_list_to_dag, Node, toposort_bsym_dag, TOPOSORT_ORDER\n",
    "from thunder.core.vjp_utils import get_saved_for_backward_tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforms\n",
    "\n",
    "To understand transforms, we need to know what a `Trace` is. In `thunder`, `Trace` is the representation of the jitted program in terms of thunder operations/symbol. Each operation in `Trace` is a collection of `BoundSymbol` i.e. a `Symbol` with it's input and output. We can also print the trace as a Python program for easier inspection. We will do this later in the notebook. To understand these concepts, you can read the helpful `zero_to_thunder.ipynb`. \n",
    "\n",
    "`thunder` allows us to write our custom transforms to transform trace/s. These transforms can be used for replace pointwise operations with fused implementation, compute gradient of the given computation, etc. Besides this, `thunder` enables us to apply these transforms at different stages during compilation. In this tutorial, we will use the post optimization stage, the point at which we already have the forward and the backward execution trace ready. To write our transform, we have to inherit from `Transform` class. This class implements the interface that each transform should have. By default, it provides no-op transformations. To use our transform, we provide an instance of our transform object to the `thunder.jit` via `transforms` argument.\n",
    "\n",
    "However, before writing our transform, we will make an `OperatorExecutor` with which we will create 2 operators/symbol - 1. to offload tensors to CPU 2. Load the offloaded tensors back to CUDA device. You read more about adding custom operators in `adding_custom_operator.ipynb` and also `zero_to_thunder.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new executor.\n",
    "offload_exec = OperatorExecutor(\"offload_exec\")\n",
    "\n",
    "\n",
    "# Offload the GPU tensor to a pinned CPU tensor.\n",
    "def offload_to_cpu_impl(t):\n",
    "    # Due to https://github.com/Lightning-AI/lightning-thunder/issues/950\n",
    "    # it may receive tensor on CPU.\n",
    "    if t.device == torch.device(\"cpu\"):\n",
    "        return t\n",
    "\n",
    "    packed = torch.empty(\n",
    "        t.size(),\n",
    "        dtype=t.dtype,\n",
    "        layout=t.layout,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    packed.copy_(t)\n",
    "    return packed\n",
    "\n",
    "offload_to_cpu = offload_exec.register_operator(\n",
    "    \"offload_to_cpu\",\n",
    "    meta=lambda t: TensorProxy(\"offloaded_\" + t.name, like=t, device=thunder.core.devices.Device(\"cpu\")),\n",
    "    fn=offload_to_cpu_impl,\n",
    ")\n",
    "\n",
    "\n",
    "# Load the tensor to given GPU\n",
    "def load_to_gpu_impl(t, device):\n",
    "    return t.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "load_to_gpu = offload_exec.register_operator(\n",
    "    \"load_to_gpu\",\n",
    "    meta=lambda t, device: TensorProxy(like=t, device=thunder.core.devices.Device(device)),\n",
    "    fn=load_to_gpu_impl,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will have some helper functions to implement our transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_symbols_to_last_or_first_used_variables(symbols, first_used=False):\n",
    "    variable_to_symbol = {}\n",
    "\n",
    "    def _mark_first_or_last_use(symbol, variable):\n",
    "        if not variable in variable_to_symbol:\n",
    "            variable_to_symbol[variable] = symbol\n",
    "\n",
    "    iter_symbols = symbols if first_used else reversed(symbols)\n",
    "    for symbol in iter_symbols:\n",
    "        # If this function is used in the combined nvfuser+torch executor, there are no symbols but regions.\n",
    "        # Regions do not have args, kwargs\n",
    "        if hasattr(symbol, \"inputs\"):\n",
    "            variables = tuple(symbol.inputs) + tuple(symbol.outputs)\n",
    "        else:\n",
    "            variables = (symbol.flat_variableified_proxy_args) + tuple(symbol.flat_variableified_proxy_outs)\n",
    "        tree_map(lambda x: _mark_first_or_last_use(symbol, x), variables)\n",
    "\n",
    "    return variable_to_symbol\n",
    "\n",
    "\n",
    "def get_symbols_to_last_used_variables(symbols):\n",
    "    \"\"\"\n",
    "    This function processes a sequence of symbols and determines which variables \n",
    "    are last used by each symbol. It returns a mapping from variables to the\n",
    "    symbols where they were last used.\n",
    "\n",
    "    Parameters:\n",
    "    symbols (iterable): An iterable of symbols\n",
    "\n",
    "    Returns:\n",
    "    variable_to_symbol (dict): A dictionary mapping each variable to the \n",
    "        symbol where it is last used.\n",
    "    \"\"\"\n",
    "    return _get_symbols_to_last_or_first_used_variables(symbols)\n",
    "\n",
    "\n",
    "def get_symbols_to_first_used_variables(symbols):\n",
    "    '''\n",
    "    This function processes a sequence of symbols and determines which variables \n",
    "    are first used by each symbol. It returns a mapping from variables to the\n",
    "    symbols where they were first used.\n",
    "\n",
    "    Parameters:\n",
    "    symbols (iterable): An iterable of symbols\n",
    "\n",
    "    Returns:\n",
    "    variable_to_symbol (dict): A dictionary mapping each variable to the \n",
    "        symbol where it is first used.\n",
    "    '''\n",
    "    return _get_symbols_to_last_or_first_used_variables(symbols, first_used=True)\n",
    "\n",
    "\n",
    "def get_symbol_to_idx(symbols):\n",
    "    '''\n",
    "    This function returns a map from symbol to it's position in the sequence.\n",
    "    '''\n",
    "    return {sym: idx for idx, sym in enumerate(symbols)}\n",
    "\n",
    "\n",
    "def move_closer_to_consumer(execution_trace):\n",
    "    order_in_trace = {bsym: i for i, bsym in enumerate(execution_trace.bound_symbols)}\n",
    "\n",
    "    def prefer_ops_closer_to_consumer(eligible_nodes: list[Node]) -> int:\n",
    "        def key(node: Node) -> int:\n",
    "            return order_in_trace[node.bsym]\n",
    "\n",
    "        return min(range(len(eligible_nodes)), key=lambda i: key(eligible_nodes[i]))\n",
    "\n",
    "    # This moves all del or clear collection at the bottom (as they don't return anything)\n",
    "    bound_symbols = toposort_bsym_dag(\n",
    "        bsym_list_to_dag(execution_trace.bound_symbols)[1],\n",
    "        TOPOSORT_ORDER.BOTTOM_UP,\n",
    "        selector=prefer_ops_closer_to_consumer,\n",
    "    )\n",
    "\n",
    "    for idx, bsym in enumerate(bound_symbols):\n",
    "        if bsym.sym.id == prims.PrimIDs.DEL:\n",
    "            break\n",
    "\n",
    "    new_execution_trace = from_trace(execution_trace)\n",
    "    new_execution_trace.bound_symbols = bound_symbols[:idx]\n",
    "\n",
    "    new_execution_trace = thunder.executors.passes.del_last_used(new_execution_trace, clear_mutable_collections=True)\n",
    "    return new_execution_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to the main topic, of writing the transform for CPUOffloading.\n",
    "\n",
    "The rough implementation of transform will look like this\n",
    "1. From the forward computation trace, determine which tensors we want to offload to CPU. The `return` symbol of the forward trace has a sequence of tensors which are saved for the backward trace. We go through this list of tensor and find all the intermediate tensors (i.e. which are not an input to the trace). Here, we will also call a user provided callback which can further filter this list of tensors to offload.\n",
    "2. In the forward trace, we then find the last of use of the tensors to offload from above step and insert a call to `offload_to_cpu` symbol that we created above. Note that we will also save a map of which tensors we offloaded. We also note the original device where the tensor lived so that we can load it back to correct device.\n",
    "3. In the forward trace, we then update the `return` symbol to return the offloaded tensors (which are saved for the backward pass).\n",
    "4. In the backward trace, we read from the map of the tensors which were offloaded and update the `unpack` symbol of saved tensors to replace the original tensors with our offloaded tensors.\n",
    "5. In the backward trace, we then find the first use of the offloaded tensor in a computation and insert a `load_to_gpu` call before it. Note that here, we will use the previously stored map of tensor to original device so that we load it onto the correct device.\n",
    "\n",
    "To see this steps, in our implementation -\n",
    "1. See method `transform_trace_post_optimization`, which is invoked by `thunder` with first the forward trace and then separately with the backward trace.\n",
    "2. See method `_offload_tensors_from_forward`, which implements Step 1, 2 and 3 from above.\n",
    "3. See method `_load_tensors_for_backward`, which implements Step 4, 5 from above.\n",
    "\n",
    "Note that each of the above method has more details regarding the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPUOffloading(Transform):\n",
    "    '''\n",
    "    Transform to implement CPU Offloading.\n",
    "\n",
    "    save_tensor_policy - Users can pass a callback with signature fn(offloaded_tensors, forward_trace) to filter\n",
    "                         the offloaded_tensors based on their preference eg. biggest 20% intermediate tensors or\n",
    "                         intermediates of certain operations\n",
    "    '''\n",
    "    def __init__(self, save_tensor_policy: callable =None):\n",
    "        self.forward_pass = None\n",
    "        self.backward_pass = None\n",
    "        self._offloaded_tensors = ()\n",
    "        self.save_tensor_policy = None\n",
    "        if save_tensor_policy is not None:\n",
    "            assert callable(save_tensor_policy)\n",
    "            self.save_tensor_policy = save_tensor_policy\n",
    "\n",
    "    def _get_tensors_to_offload(self, forward_trace):\n",
    "        '''\n",
    "        Based on the `forward_trace`, we find the symbols that we want to offload to CPU.\n",
    "        This function finds the intermediate tensors that are saved for backward i.e. ones that are not input or output of this trace.\n",
    "        '''\n",
    "        return_bsym = forward_trace.bound_symbols[-1]\n",
    "        trace_args = return_bsym.args[0][\"flat_args\"]\n",
    "        saved_tensors = get_saved_for_backward_tensors(forward_trace)\n",
    "\n",
    "        tensor_args_name = tuple(arg.name for arg in trace_args if isinstance(arg, TensorProxy))\n",
    "\n",
    "        def is_in_tensor_args(t):\n",
    "            return t.name in tensor_args_name\n",
    "\n",
    "        def is_cuda_tensor(t):\n",
    "            return t.device.type == \"cuda\"\n",
    "\n",
    "        # Tensors which are intermediate and not argument to the computation trace are\n",
    "        # the ones we are interested in offloading.\n",
    "        tensors_to_offload = tuple(t for t in saved_tensors if ((not is_in_tensor_args(t)) and is_cuda_tensor(t)))\n",
    "\n",
    "        # Apply users policy if present.\n",
    "        if self.save_tensor_policy is not None:\n",
    "            tensors_to_offload = self.save_tensor_policy(tensors_to_offload, forward_trace)\n",
    "        self.tensors_to_offload = tensors_to_offload\n",
    "        return self.tensors_to_offload\n",
    "\n",
    "    def _replace_saved_tensors(self, forward_trace, new_output_map):\n",
    "        return_bsym = forward_trace.bound_symbols[-1]\n",
    "        return_bsym_args = return_bsym.args\n",
    "        saved_tensors = return_bsym.args[1][0]\n",
    "\n",
    "        new_saved_tensors = []\n",
    "        for t in saved_tensors:\n",
    "            new_output = new_output_map.get(variableify(t), t)\n",
    "            new_saved_tensors.append(new_output)\n",
    "\n",
    "        new_return_bsym = BoundSymbol.from_bsym(\n",
    "            return_bsym, **{\"args\": (return_bsym_args[0], (tuple(new_saved_tensors), return_bsym_args[1][1]))}\n",
    "        )\n",
    "\n",
    "        # Replace the old return with our new return.\n",
    "        forward_trace.bound_symbols.pop(-1)\n",
    "        forward_trace.bound_symbols.append(new_return_bsym)\n",
    "\n",
    "    def _offload_tensors_from_forward(self, computation_trace):\n",
    "        '''\n",
    "        This function takes the forward computation trace and performs following step\n",
    "        1. Find the tensors to be offloaded using `_get_tensors_to_offload` (this also calls users `save_tensor_policy` if present).\n",
    "        2. Insert calls to the `offload_to_cpu` symbol with the tensor to offload. These calls are placed after the last computational\n",
    "           use of the tensors to be offloaded so that we free the memory as soon as possible.\n",
    "        3. Finally, we update the last symbol i.e. `return` symbol to return the offloaded tensors instead of the original tensors.\n",
    "        '''\n",
    "        # Step 1\n",
    "        # Find the tensors to offload.\n",
    "        # We offload saved tensors which are not arguments to the computation trace and are saved for backwards.\n",
    "        tensors_to_offload = self._get_tensors_to_offload(computation_trace)\n",
    "\n",
    "        # Step 2\n",
    "        # Insert the offloading calls after the last use of the saved tensor (which we want to offload).\n",
    "        variable_to_last_symbol = get_symbols_to_last_used_variables(\n",
    "            computation_trace.bound_symbols[:-1]\n",
    "        )  # Ignore the return statement.\n",
    "        symbol_to_idx = get_symbol_to_idx(computation_trace.bound_symbols)\n",
    "\n",
    "        # Book keeping for backward pass update.\n",
    "        new_output_map = {}\n",
    "        new_output_dev_map = {}\n",
    "\n",
    "        # Since we are inserting in the list (we need to obey increasing order) - else the insertions will be incorrect.\n",
    "        sorted_tensors_to_offload = sorted(\n",
    "            tensors_to_offload, key=lambda t: symbol_to_idx[variable_to_last_symbol[variableify(t)]]\n",
    "        )\n",
    "        for idx, t in enumerate(sorted_tensors_to_offload):\n",
    "            last_used_symbol = variable_to_last_symbol[variableify(t)]\n",
    "            last_used_symbol_idx = symbol_to_idx[last_used_symbol]\n",
    "            computation_trace.push_scope([])\n",
    "            with tracectx(computation_trace):\n",
    "                o = offload_to_cpu(t)\n",
    "                prims.python_del(t)\n",
    "            scoped_comp = computation_trace.pop_scope()\n",
    "            scoped_comp[0].header = \"Created by CPU Offloading Transform\"\n",
    "            offload_to_cpu_symbol = scoped_comp[0]\n",
    "            del_symbol = scoped_comp[1]\n",
    "\n",
    "            # This will insert `del` first and then push it down when we insert `offload_to_cpu`.\n",
    "            computation_trace.bound_symbols.insert(last_used_symbol_idx + 1 + (idx * 2), del_symbol)\n",
    "            computation_trace.bound_symbols.insert(last_used_symbol_idx + 1 + (idx * 2), offload_to_cpu_symbol)\n",
    "\n",
    "            # Update book keeping.\n",
    "            new_output_map[variableify(t)] = o\n",
    "            new_output_dev_map[variableify(t)] = t.device.device_str()\n",
    "\n",
    "        # Step 3\n",
    "        # Update the return symbol to return our offloaded tensors in saved for backward.\n",
    "        self._replace_saved_tensors(computation_trace, new_output_map)\n",
    "\n",
    "        # Book keeping for backward pass update.\n",
    "        self._offloaded_tensors = new_output_map\n",
    "        self._offloaded_tensors_dev = new_output_dev_map\n",
    "        return computation_trace\n",
    "\n",
    "    def _load_tensors_for_backward(self, computation_trace):\n",
    "        '''\n",
    "        This function takes the backward computation trace and performs following step\n",
    "        1. Finds the unpack collection symbol which unpacks the saved tensors passed to the backward trace.\n",
    "        2. Updates the unpack collection to unpack the offloaded tensors instead of the original ones.\n",
    "        3. Before the first use of the offloaded tensor in computation, we insert the `load_to_gpu` to load the tensor back on GPU.\n",
    "        '''\n",
    "        self.backward_pass = computation_trace\n",
    "        offloaded_tensors = self._offloaded_tensors\n",
    "        offloaded_tensors_dev_map = self._offloaded_tensors_dev\n",
    "\n",
    "        compute_producers, compute_consumers = thunder.core.utils.producers_and_consumers(computation_trace)\n",
    "\n",
    "        # We want to insert `loads` before the first use of offloaded_tensors.\n",
    "        variable_to_first_symbol = get_symbols_to_first_used_variables(computation_trace.bound_symbols)\n",
    "\n",
    "        symbol_to_idx = get_symbol_to_idx(computation_trace.bound_symbols)\n",
    "\n",
    "        # Step 1 and 2\n",
    "        # Update unpack collection so that it\n",
    "        # outputs the offloaded tensor proxies (not the original ones).\n",
    "        unpack_sym = compute_producers[list(offloaded_tensors.keys())[0].proxy]\n",
    "        unpack_idx = symbol_to_idx[unpack_sym]\n",
    "        unpack_sym_out = unpack_sym.output\n",
    "        new_out = []\n",
    "        for out in unpack_sym_out:\n",
    "            vout = variableify(out)\n",
    "            if vout in offloaded_tensors:\n",
    "                new_out.append(offloaded_tensors[vout])\n",
    "            else:\n",
    "                new_out.append(out)\n",
    "        new_unpack_bsym = BoundSymbol.from_bsym(unpack_sym, output=tuple(new_out))\n",
    "        computation_trace.bound_symbols[unpack_idx] = new_unpack_bsym\n",
    "\n",
    "        # Now we again find the first usages of offloaded tensor\n",
    "        # This will actually point us to the first consumer of the offloaded tensor.\n",
    "        offset = unpack_idx + 1\n",
    "        variable_to_first_symbol = get_symbols_to_first_used_variables(computation_trace.bound_symbols[offset:])\n",
    "\n",
    "        # Step 3\n",
    "        # Load the offloaded tensors to GPU before usage.\n",
    "        # Should iterate in correct order (else insertion positions will be incorrect).\n",
    "        for idx, (vt, offloaded_t) in enumerate(\n",
    "            sorted(offloaded_tensors.items(), key=lambda kv: symbol_to_idx[variable_to_first_symbol[kv[0]]])\n",
    "        ):\n",
    "            first_used_symbol = variable_to_first_symbol[vt]\n",
    "            first_used_symbol_idx = symbol_to_idx[first_used_symbol]\n",
    "            t = vt.proxy\n",
    "            device = offloaded_tensors_dev_map[vt]\n",
    "\n",
    "            with tracectx(computation_trace):\n",
    "                new_sym = load_to_gpu.bind(offloaded_t, device, output=t)\n",
    "\n",
    "            new_sym.header = \"Created by CPU Offloading Transform\"\n",
    "            computation_trace.bound_symbols.insert(first_used_symbol_idx + idx, new_sym)\n",
    "\n",
    "        return computation_trace\n",
    "\n",
    "    def transform_trace_post_optimization(self, computation_trace: thunder.TraceCtx, **kwargs):\n",
    "        if self.forward_pass is None:\n",
    "            self.forward_pass = computation_trace\n",
    "            # Processing for the forward pass (only if we are going to compute backward).\n",
    "            if \"augmented_forward\" in computation_trace.fn.__name__:\n",
    "                computation_trace = self._offload_tensors_from_forward(computation_trace)\n",
    "        else:\n",
    "            # Skip if no tensor was offloaded.\n",
    "            if len(self._offloaded_tensors) == 0:\n",
    "                return computation_trace\n",
    "\n",
    "            # We need this because in unmodified backward trace, the first consumer of saved_for_backward maybe\n",
    "            # a reshape or permute op and the actual computation occurs 50-100 (or more) lines later.\n",
    "            # Because of this we load more tensors than required eagerly (thus decreasing the memory gains from CPU Offloading).\n",
    "            # This function is currently tailored to pattern observed in Llama-2\n",
    "            # Eg. on line 92\n",
    "            #   # Created by CPU Offloading Transform\n",
    "            #   t1319 = load_to_gpu(offloaded_t1319, 'cuda:0')  # t1319: \"cuda:0 f32[8, 1024, 11008]\"\n",
    "            #   t4021 = torch.reshape(t1319, (-1, 11008))  # t4021: \"cuda:0 f32[8192, 11008]\"\n",
    "            #     # t4021 = ltorch.reshape(t1319, (-1, 11008))  # t4021: \"cuda:0 f32[8192, 11008]\"\n",
    "            #       # t4021 = prims.reshape(t1319, (8192, 11008))  # t4021: \"cuda:0 f32[8192, 11008]\"\n",
    "            #   del t1319\n",
    "            # And it's usage in computation is at 612\n",
    "            # t4022 = torch.matmul(t4020, t4021)  # t4022: \"cuda:0 f32[4096, 11008]\"\n",
    "            #   t4022 = ltorch.matmul(t4020, t4021)  # t4022: \"cuda:0 f32[4096, 11008]\"\n",
    "            #     t4022 = prims.matmul(t4020, t4021)  # t4022: \"cuda:0 f32[4096, 11008]\"\n",
    "            computation_trace = move_closer_to_consumer(computation_trace)\n",
    "\n",
    "            # Transform the backward trace to load offloaded tensors back to the device.\n",
    "            computation_trace = self._load_tensors_for_backward(computation_trace)\n",
    "\n",
    "        return computation_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_accumulated_memory_stats()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    print(f\"Allocated Memory after cleaning {torch.cuda.memory_allocated() / 1e9} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(jmodel, model, args, kwargs):\n",
    "    # NOTE - This function takes care of warm-up\n",
    "    stmt = \"\"\"\n",
    "# Use the optimized model for prediction and backward\n",
    "o = jmodel(*args, **kwargs)\n",
    "o.sum().backward()\n",
    "for param in model.parameters():  # use original model for clear grads\n",
    "    param.grad = None\n",
    "\"\"\"\n",
    "    timer = torch.utils.benchmark.Timer(\n",
    "        stmt=stmt, globals={\"jmodel\": jmodel, \"model\": model, \"args\": args, \"kwargs\": kwargs}\n",
    "    ).timeit(number=10)\n",
    "    return timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing our Transform on a Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak Memory with thunder : 2232320 bytes\n",
      "Peak Memory with CPU Offloading : 1396736 bytes\n",
      "Allocated Memory after cleaning 8.192e-06 GB\n"
     ]
    }
   ],
   "source": [
    "class MySimpleModel(torch.nn.Module):\n",
    "    def __init__(self, n_layers=10):\n",
    "        super().__init__()\n",
    "        self.fcs = torch.nn.ModuleList([torch.nn.Linear(16, 16) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for fc in self.fcs:\n",
    "            x = torch.nn.functional.relu(fc(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "def get_model_and_args():\n",
    "    device = 'cuda'\n",
    "    model = MySimpleModel(n_layers=100).to(device)\n",
    "    args = (torch.randn(128, 16, device=device),)\n",
    "    kwargs = {}\n",
    "    return model, args, kwargs\n",
    "\n",
    "model, args, kwargs = get_model_and_args()\n",
    "\n",
    "# Check against the vanilla `thunder.jit` model\n",
    "expected = thunder.jit(model)(*args, **kwargs)\n",
    "\n",
    "grad_output = torch.randn_like(expected)\n",
    "expected_grads = torch.autograd.grad(expected, model.parameters(), grad_output)\n",
    "\n",
    "print(f\"Peak Memory with thunder : {torch.cuda.max_memory_allocated()} bytes\")\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "with torch.no_grad():\n",
    "    expected_cpu = expected.to(\"cpu\")\n",
    "    expected_grads_cpu = tree_map(lambda t: t.to(\"cpu\"), expected_grads)\n",
    "\n",
    "jmodel = thunder.jit(model, transforms=[CPUOffloading()])\n",
    "\n",
    "actual = jmodel(*args, **kwargs)\n",
    "\n",
    "# Verify that saved tensors are on CPU.\n",
    "saved_tensor_devices = set()\n",
    "for t in actual.grad_fn.saved_tensors:\n",
    "    saved_tensor_devices.add(str(t.device))\n",
    "\n",
    "assert \"cpu\" in saved_tensor_devices  # Verify that we actually have saved tensors on CPU\n",
    "actual_grads = torch.autograd.grad(actual, jmodel.parameters(), grad_output)\n",
    "\n",
    "print(f\"Peak Memory with CPU Offloading : {torch.cuda.max_memory_allocated()} bytes\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    actual_cpu = actual.to(\"cpu\")\n",
    "    actual_grads_cpu = tree_map(lambda t: t.to(\"cpu\"), actual_grads)\n",
    "\n",
    "# Sanity Check that values match\n",
    "torch.testing.assert_close(actual_cpu, expected_cpu)\n",
    "torch.testing.assert_close(actual_grads_cpu, expected_grads_cpu)\n",
    "\n",
    "# Fetch the forward and backward traces for inspection\n",
    "fw_traces = thunder.last_traces(jmodel)\n",
    "bw_traces = thunder.last_backward_traces(jmodel)\n",
    "\n",
    "del jmodel, model, args, kwargs, actual, actual_grads, expected, expected_grads, grad_output  # Free memory.\n",
    "\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the forward and the backward traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# Constructed by Delete Last Used (took 2 milliseconds)\n",
       "import torch\n",
       "import torch.nn.functional\n",
       "from thunder.executors.torchex import no_autocast\n",
       "\n",
       "@torch.no_grad()\n",
       "@no_autocast\n",
       "def augmented_forward_fn(x, t_fcs_0_bias, t_fcs_0_weight, t_fcs_1_bias, t_fcs_1_weight, t_fcs_2_bias, t_fcs_2_weight, t_fcs_3_bias, t_fcs_3_weight, t_fcs_4_bias, t_fcs_4_weight, t_fcs_5_bias, t_fcs_5_weight, t_fcs_6_bias, t_fcs_6_weight, t_fcs_7_bias, t_fcs_7_weight, t_fcs_8_bias, t_fcs_8_weight, t_fcs_9_bias, t_fcs_9_weight, t_fcs_10_bias, t_fcs_10_weight, t_fcs_11_bias, t_fcs_11_weight, t_fcs_12_bias, t_fcs_12_weight, t_fcs_13_bias, t_fcs_13_weight, t_fcs_14_bias, t_fcs_14_weight, t_fcs_15_bias, t_fcs_15_weight, t_fcs_16_bias, t_fcs_16_weight, t_fcs_17_bias, t_fcs_17_weight, t_fcs_18_bias, t_fcs_18_weight, t_fcs_19_bias, t_fcs_19_weight, t_fcs_20_bias, t_fcs_20_weight, t_fcs_21_bias, t_fcs_21_weight, t_fcs_22_bias, t_fcs_22_weight, t_fcs_23_bias, t_fcs_23_weight, t_fcs_24_bias, t_fcs_24_weight, t_fcs_25_bias, t_fcs_25_weight, t_fcs_26_bias, t_fcs_26_weight, t_fcs_27_bias, t_fcs_27_weight, t_fcs_28_bias, t_fcs_28_weight, t_fcs_29_bias, t_fcs_29_weight, t_fcs_30_bias, t_fcs_30_weight, t_fcs_31_bias, t_fcs_31_weight, t_fcs_32_bias, t_fcs_32_weight, t_fcs_33_bias, t_fcs_33_weight, t_fcs_34_bias, t_fcs_34_weight, t_fcs_35_bias, t_fcs_35_weight, t_fcs_36_bias, t_fcs_36_weight, t_fcs_37_bias, t_fcs_37_weight, t_fcs_38_bias, t_fcs_38_weight, t_fcs_39_bias, t_fcs_39_weight, t_fcs_40_bias, t_fcs_40_weight, t_fcs_41_bias, t_fcs_41_weight, t_fcs_42_bias, t_fcs_42_weight, t_fcs_43_bias, t_fcs_43_weight, t_fcs_44_bias, t_fcs_44_weight, t_fcs_45_bias, t_fcs_45_weight, t_fcs_46_bias, t_fcs_46_weight, t_fcs_47_bias, t_fcs_47_weight, t_fcs_48_bias, t_fcs_48_weight, t_fcs_49_bias, t_fcs_49_weight, t_fcs_50_bias, t_fcs_50_weight, t_fcs_51_bias, t_fcs_51_weight, t_fcs_52_bias, t_fcs_52_weight, t_fcs_53_bias, t_fcs_53_weight, t_fcs_54_bias, t_fcs_54_weight, t_fcs_55_bias, t_fcs_55_weight, t_fcs_56_bias, t_fcs_56_weight, t_fcs_57_bias, t_fcs_57_weight, t_fcs_58_bias, t_fcs_58_weight, t_fcs_59_bias, t_fcs_59_weight, t_fcs_60_bias, t_fcs_60_weight, t_fcs_61_bias, t_fcs_61_weight, t_fcs_62_bias, t_fcs_62_weight, t_fcs_63_bias, t_fcs_63_weight, t_fcs_64_bias, t_fcs_64_weight, t_fcs_65_bias, t_fcs_65_weight, t_fcs_66_bias, t_fcs_66_weight, t_fcs_67_bias, t_fcs_67_weight, t_fcs_68_bias, t_fcs_68_weight, t_fcs_69_bias, t_fcs_69_weight, t_fcs_70_bias, t_fcs_70_weight, t_fcs_71_bias, t_fcs_71_weight, t_fcs_72_bias, t_fcs_72_weight, t_fcs_73_bias, t_fcs_73_weight, t_fcs_74_bias, t_fcs_74_weight, t_fcs_75_bias, t_fcs_75_weight, t_fcs_76_bias, t_fcs_76_weight, t_fcs_77_bias, t_fcs_77_weight, t_fcs_78_bias, t_fcs_78_weight, t_fcs_79_bias, t_fcs_79_weight, t_fcs_80_bias, t_fcs_80_weight, t_fcs_81_bias, t_fcs_81_weight, t_fcs_82_bias, t_fcs_82_weight, t_fcs_83_bias, t_fcs_83_weight, t_fcs_84_bias, t_fcs_84_weight, t_fcs_85_bias, t_fcs_85_weight, t_fcs_86_bias, t_fcs_86_weight, t_fcs_87_bias, t_fcs_87_weight, t_fcs_88_bias, t_fcs_88_weight, t_fcs_89_bias, t_fcs_89_weight, t_fcs_90_bias, t_fcs_90_weight, t_fcs_91_bias, t_fcs_91_weight, t_fcs_92_bias, t_fcs_92_weight, t_fcs_93_bias, t_fcs_93_weight, t_fcs_94_bias, t_fcs_94_weight, t_fcs_95_bias, t_fcs_95_weight, t_fcs_96_bias, t_fcs_96_weight, t_fcs_97_bias, t_fcs_97_weight, t_fcs_98_bias, t_fcs_98_weight, t_fcs_99_bias, t_fcs_99_weight):\n",
       "  # x: \"cuda:0 f32[128, 16]\"\n",
       "  # t_fcs_0_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_0_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_1_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_1_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_2_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_2_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_3_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_3_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_4_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_4_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_5_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_5_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_6_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_6_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_7_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_7_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_8_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_8_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_9_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_9_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_10_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_10_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_11_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_11_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_12_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_12_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_13_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_13_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_14_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_14_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_15_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_15_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_16_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_16_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_17_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_17_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_18_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_18_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_19_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_19_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_20_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_20_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_21_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_21_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_22_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_22_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_23_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_23_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_24_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_24_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_25_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_25_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_26_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_26_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_27_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_27_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_28_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_28_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_29_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_29_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_30_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_30_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_31_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_31_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_32_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_32_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_33_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_33_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_34_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_34_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_35_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_35_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_36_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_36_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_37_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_37_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_38_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_38_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_39_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_39_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_40_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_40_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_41_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_41_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_42_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_42_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_43_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_43_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_44_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_44_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_45_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_45_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_46_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_46_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_47_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_47_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_48_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_48_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_49_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_49_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_50_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_50_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_51_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_51_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_52_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_52_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_53_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_53_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_54_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_54_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_55_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_55_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_56_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_56_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_57_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_57_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_58_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_58_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_59_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_59_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_60_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_60_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_61_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_61_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_62_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_62_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_63_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_63_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_64_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_64_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_65_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_65_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_66_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_66_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_67_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_67_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_68_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_68_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_69_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_69_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_70_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_70_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_71_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_71_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_72_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_72_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_73_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_73_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_74_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_74_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_75_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_75_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_76_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_76_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_77_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_77_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_78_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_78_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_79_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_79_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_80_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_80_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_81_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_81_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_82_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_82_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_83_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_83_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_84_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_84_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_85_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_85_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_86_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_86_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_87_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_87_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_88_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_88_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_89_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_89_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_90_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_90_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_91_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_91_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_92_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_92_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_93_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_93_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_94_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_94_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_95_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_95_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_96_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_96_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_97_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_97_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_98_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_98_weight: \"cuda:0 f32[16, 16]\"\n",
       "  # t_fcs_99_bias: \"cuda:0 f32[16]\"\n",
       "  # t_fcs_99_weight: \"cuda:0 f32[16, 16]\"\n",
       "  t0 = torch.nn.functional.linear(x, t_fcs_0_weight, t_fcs_0_bias)  # t0: \"cuda:0 f32[128, 16]\"\n",
       "    # t0 = ltorch.linear(x, t_fcs_0_weight, t_fcs_0_bias)  # t0: \"cuda:0 f32[128, 16]\"\n",
       "      # t0 = prims.linear(x, t_fcs_0_weight, t_fcs_0_bias)  # t0: \"cuda:0 f32[128, 16]\"\n",
       "  [t2, t4] = nvFusion0(t0)\n",
       "    # t2 = prims.gt(t0, 0.0)  # t2: \"cuda:0 b8[128, 16]\"\n",
       "    # t4 = prims.where(t2, t0, 0.0)  # t4: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t2 = offload_to_cpu(t2)  # offloaded_t2: \"cpu b8[128, 16]\"\n",
       "  del t2\n",
       "  del t0\n",
       "  t5 = torch.nn.functional.linear(t4, t_fcs_1_weight, t_fcs_1_bias)  # t5: \"cuda:0 f32[128, 16]\"\n",
       "    # t5 = ltorch.linear(t4, t_fcs_1_weight, t_fcs_1_bias)  # t5: \"cuda:0 f32[128, 16]\"\n",
       "      # t5 = prims.linear(t4, t_fcs_1_weight, t_fcs_1_bias)  # t5: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t4 = offload_to_cpu(t4)  # offloaded_t4: \"cpu f32[128, 16]\"\n",
       "  del t4\n",
       "  [t7, t9] = nvFusion1(t5)\n",
       "    # t7 = prims.gt(t5, 0.0)  # t7: \"cuda:0 b8[128, 16]\"\n",
       "    # t9 = prims.where(t7, t5, 0.0)  # t9: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t7 = offload_to_cpu(t7)  # offloaded_t7: \"cpu b8[128, 16]\"\n",
       "  del t7\n",
       "  del t5\n",
       "  t10 = torch.nn.functional.linear(t9, t_fcs_2_weight, t_fcs_2_bias)  # t10: \"cuda:0 f32[128, 16]\"\n",
       "    # t10 = ltorch.linear(t9, t_fcs_2_weight, t_fcs_2_bias)  # t10: \"cuda:0 f32[128, 16]\"\n",
       "      # t10 = prims.linear(t9, t_fcs_2_weight, t_fcs_2_bias)  # t10: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t9 = offload_to_cpu(t9)  # offloaded_t9: \"cpu f32[128, 16]\"\n",
       "  del t9\n",
       "  [t12, t14] = nvFusion2(t10)\n",
       "    # t12 = prims.gt(t10, 0.0)  # t12: \"cuda:0 b8[128, 16]\"\n",
       "    # t14 = prims.where(t12, t10, 0.0)  # t14: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t12 = offload_to_cpu(t12)  # offloaded_t12: \"cpu b8[128, 16]\"\n",
       "  del t12\n",
       "  del t10\n",
       "  t15 = torch.nn.functional.linear(t14, t_fcs_3_weight, t_fcs_3_bias)  # t15: \"cuda:0 f32[128, 16]\"\n",
       "    # t15 = ltorch.linear(t14, t_fcs_3_weight, t_fcs_3_bias)  # t15: \"cuda:0 f32[128, 16]\"\n",
       "      # t15 = prims.linear(t14, t_fcs_3_weight, t_fcs_3_bias)  # t15: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t14 = offload_to_cpu(t14)  # offloaded_t14: \"cpu f32[128, 16]\"\n",
       "  del t14\n",
       "  [t17, t19] = nvFusion3(t15)\n",
       "    # t17 = prims.gt(t15, 0.0)  # t17: \"cuda:0 b8[128, 16]\"\n",
       "    # t19 = prims.where(t17, t15, 0.0)  # t19: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t17 = offload_to_cpu(t17)  # offloaded_t17: \"cpu b8[128, 16]\"\n",
       "  del t17\n",
       "  del t15\n",
       "  t20 = torch.nn.functional.linear(t19, t_fcs_4_weight, t_fcs_4_bias)  # t20: \"cuda:0 f32[128, 16]\"\n",
       "    # t20 = ltorch.linear(t19, t_fcs_4_weight, t_fcs_4_bias)  # t20: \"cuda:0 f32[128, 16]\"\n",
       "      # t20 = prims.linear(t19, t_fcs_4_weight, t_fcs_4_bias)  # t20: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t19 = offload_to_cpu(t19)  # offloaded_t19: \"cpu f32[128, 16]\"\n",
       "  del t19\n",
       "  [t22, t24] = nvFusion4(t20)\n",
       "    # t22 = prims.gt(t20, 0.0)  # t22: \"cuda:0 b8[128, 16]\"\n",
       "    # t24 = prims.where(t22, t20, 0.0)  # t24: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t22 = offload_to_cpu(t22)  # offloaded_t22: \"cpu b8[128, 16]\"\n",
       "  del t22\n",
       "  del t20\n",
       "  t25 = torch.nn.functional.linear(t24, t_fcs_5_weight, t_fcs_5_bias)  # t25: \"cuda:0 f32[128, 16]\"\n",
       "    # t25 = ltorch.linear(t24, t_fcs_5_weight, t_fcs_5_bias)  # t25: \"cuda:0 f32[128, 16]\"\n",
       "      # t25 = prims.linear(t24, t_fcs_5_weight, t_fcs_5_bias)  # t25: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t24 = offload_to_cpu(t24)  # offloaded_t24: \"cpu f32[128, 16]\"\n",
       "  del t24\n",
       "  [t27, t29] = nvFusion5(t25)\n",
       "    # t27 = prims.gt(t25, 0.0)  # t27: \"cuda:0 b8[128, 16]\"\n",
       "    # t29 = prims.where(t27, t25, 0.0)  # t29: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t27 = offload_to_cpu(t27)  # offloaded_t27: \"cpu b8[128, 16]\"\n",
       "  del t27\n",
       "  del t25\n",
       "  t30 = torch.nn.functional.linear(t29, t_fcs_6_weight, t_fcs_6_bias)  # t30: \"cuda:0 f32[128, 16]\"\n",
       "    # t30 = ltorch.linear(t29, t_fcs_6_weight, t_fcs_6_bias)  # t30: \"cuda:0 f32[128, 16]\"\n",
       "      # t30 = prims.linear(t29, t_fcs_6_weight, t_fcs_6_bias)  # t30: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t29 = offload_to_cpu(t29)  # offloaded_t29: \"cpu f32[128, 16]\"\n",
       "  del t29\n",
       "  [t32, t34] = nvFusion6(t30)\n",
       "    # t32 = prims.gt(t30, 0.0)  # t32: \"cuda:0 b8[128, 16]\"\n",
       "    # t34 = prims.where(t32, t30, 0.0)  # t34: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t32 = offload_to_cpu(t32)  # offloaded_t32: \"cpu b8[128, 16]\"\n",
       "  del t32\n",
       "  del t30\n",
       "  t35 = torch.nn.functional.linear(t34, t_fcs_7_weight, t_fcs_7_bias)  # t35: \"cuda:0 f32[128, 16]\"\n",
       "    # t35 = ltorch.linear(t34, t_fcs_7_weight, t_fcs_7_bias)  # t35: \"cuda:0 f32[128, 16]\"\n",
       "      # t35 = prims.linear(t34, t_fcs_7_weight, t_fcs_7_bias)  # t35: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t34 = offload_to_cpu(t34)  # offloaded_t34: \"cpu f32[128, 16]\"\n",
       "  del t34\n",
       "  [t37, t39] = nvFusion7(t35)\n",
       "    # t37 = prims.gt(t35, 0.0)  # t37: \"cuda:0 b8[128, 16]\"\n",
       "    # t39 = prims.where(t37, t35, 0.0)  # t39: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t37 = offload_to_cpu(t37)  # offloaded_t37: \"cpu b8[128, 16]\"\n",
       "  del t37\n",
       "  del t35\n",
       "  t40 = torch.nn.functional.linear(t39, t_fcs_8_weight, t_fcs_8_bias)  # t40: \"cuda:0 f32[128, 16]\"\n",
       "    # t40 = ltorch.linear(t39, t_fcs_8_weight, t_fcs_8_bias)  # t40: \"cuda:0 f32[128, 16]\"\n",
       "      # t40 = prims.linear(t39, t_fcs_8_weight, t_fcs_8_bias)  # t40: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t39 = offload_to_cpu(t39)  # offloaded_t39: \"cpu f32[128, 16]\"\n",
       "  del t39\n",
       "  [t42, t44] = nvFusion8(t40)\n",
       "    # t42 = prims.gt(t40, 0.0)  # t42: \"cuda:0 b8[128, 16]\"\n",
       "    # t44 = prims.where(t42, t40, 0.0)  # t44: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t42 = offload_to_cpu(t42)  # offloaded_t42: \"cpu b8[128, 16]\"\n",
       "  del t42\n",
       "  del t40\n",
       "  t45 = torch.nn.functional.linear(t44, t_fcs_9_weight, t_fcs_9_bias)  # t45: \"cuda:0 f32[128, 16]\"\n",
       "    # t45 = ltorch.linear(t44, t_fcs_9_weight, t_fcs_9_bias)  # t45: \"cuda:0 f32[128, 16]\"\n",
       "      # t45 = prims.linear(t44, t_fcs_9_weight, t_fcs_9_bias)  # t45: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t44 = offload_to_cpu(t44)  # offloaded_t44: \"cpu f32[128, 16]\"\n",
       "  del t44\n",
       "  [t47, t49] = nvFusion9(t45)\n",
       "    # t47 = prims.gt(t45, 0.0)  # t47: \"cuda:0 b8[128, 16]\"\n",
       "    # t49 = prims.where(t47, t45, 0.0)  # t49: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t47 = offload_to_cpu(t47)  # offloaded_t47: \"cpu b8[128, 16]\"\n",
       "  del t47\n",
       "  del t45\n",
       "  t50 = torch.nn.functional.linear(t49, t_fcs_10_weight, t_fcs_10_bias)  # t50: \"cuda:0 f32[128, 16]\"\n",
       "    # t50 = ltorch.linear(t49, t_fcs_10_weight, t_fcs_10_bias)  # t50: \"cuda:0 f32[128, 16]\"\n",
       "      # t50 = prims.linear(t49, t_fcs_10_weight, t_fcs_10_bias)  # t50: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t49 = offload_to_cpu(t49)  # offloaded_t49: \"cpu f32[128, 16]\"\n",
       "  del t49\n",
       "  [t52, t54] = nvFusion10(t50)\n",
       "    # t52 = prims.gt(t50, 0.0)  # t52: \"cuda:0 b8[128, 16]\"\n",
       "    # t54 = prims.where(t52, t50, 0.0)  # t54: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t52 = offload_to_cpu(t52)  # offloaded_t52: \"cpu b8[128, 16]\"\n",
       "  del t52\n",
       "  del t50\n",
       "  t55 = torch.nn.functional.linear(t54, t_fcs_11_weight, t_fcs_11_bias)  # t55: \"cuda:0 f32[128, 16]\"\n",
       "    # t55 = ltorch.linear(t54, t_fcs_11_weight, t_fcs_11_bias)  # t55: \"cuda:0 f32[128, 16]\"\n",
       "      # t55 = prims.linear(t54, t_fcs_11_weight, t_fcs_11_bias)  # t55: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t54 = offload_to_cpu(t54)  # offloaded_t54: \"cpu f32[128, 16]\"\n",
       "  del t54\n",
       "  [t57, t59] = nvFusion11(t55)\n",
       "    # t57 = prims.gt(t55, 0.0)  # t57: \"cuda:0 b8[128, 16]\"\n",
       "    # t59 = prims.where(t57, t55, 0.0)  # t59: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t57 = offload_to_cpu(t57)  # offloaded_t57: \"cpu b8[128, 16]\"\n",
       "  del t57\n",
       "  del t55\n",
       "  t60 = torch.nn.functional.linear(t59, t_fcs_12_weight, t_fcs_12_bias)  # t60: \"cuda:0 f32[128, 16]\"\n",
       "    # t60 = ltorch.linear(t59, t_fcs_12_weight, t_fcs_12_bias)  # t60: \"cuda:0 f32[128, 16]\"\n",
       "      # t60 = prims.linear(t59, t_fcs_12_weight, t_fcs_12_bias)  # t60: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t59 = offload_to_cpu(t59)  # offloaded_t59: \"cpu f32[128, 16]\"\n",
       "  del t59\n",
       "  [t62, t64] = nvFusion12(t60)\n",
       "    # t62 = prims.gt(t60, 0.0)  # t62: \"cuda:0 b8[128, 16]\"\n",
       "    # t64 = prims.where(t62, t60, 0.0)  # t64: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t62 = offload_to_cpu(t62)  # offloaded_t62: \"cpu b8[128, 16]\"\n",
       "  del t62\n",
       "  del t60\n",
       "  t65 = torch.nn.functional.linear(t64, t_fcs_13_weight, t_fcs_13_bias)  # t65: \"cuda:0 f32[128, 16]\"\n",
       "    # t65 = ltorch.linear(t64, t_fcs_13_weight, t_fcs_13_bias)  # t65: \"cuda:0 f32[128, 16]\"\n",
       "      # t65 = prims.linear(t64, t_fcs_13_weight, t_fcs_13_bias)  # t65: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t64 = offload_to_cpu(t64)  # offloaded_t64: \"cpu f32[128, 16]\"\n",
       "  del t64\n",
       "  [t67, t69] = nvFusion13(t65)\n",
       "    # t67 = prims.gt(t65, 0.0)  # t67: \"cuda:0 b8[128, 16]\"\n",
       "    # t69 = prims.where(t67, t65, 0.0)  # t69: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t67 = offload_to_cpu(t67)  # offloaded_t67: \"cpu b8[128, 16]\"\n",
       "  del t67\n",
       "  del t65\n",
       "  t70 = torch.nn.functional.linear(t69, t_fcs_14_weight, t_fcs_14_bias)  # t70: \"cuda:0 f32[128, 16]\"\n",
       "    # t70 = ltorch.linear(t69, t_fcs_14_weight, t_fcs_14_bias)  # t70: \"cuda:0 f32[128, 16]\"\n",
       "      # t70 = prims.linear(t69, t_fcs_14_weight, t_fcs_14_bias)  # t70: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t69 = offload_to_cpu(t69)  # offloaded_t69: \"cpu f32[128, 16]\"\n",
       "  del t69\n",
       "  [t72, t74] = nvFusion14(t70)\n",
       "    # t72 = prims.gt(t70, 0.0)  # t72: \"cuda:0 b8[128, 16]\"\n",
       "    # t74 = prims.where(t72, t70, 0.0)  # t74: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t72 = offload_to_cpu(t72)  # offloaded_t72: \"cpu b8[128, 16]\"\n",
       "  del t72\n",
       "  del t70\n",
       "  t75 = torch.nn.functional.linear(t74, t_fcs_15_weight, t_fcs_15_bias)  # t75: \"cuda:0 f32[128, 16]\"\n",
       "    # t75 = ltorch.linear(t74, t_fcs_15_weight, t_fcs_15_bias)  # t75: \"cuda:0 f32[128, 16]\"\n",
       "      # t75 = prims.linear(t74, t_fcs_15_weight, t_fcs_15_bias)  # t75: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t74 = offload_to_cpu(t74)  # offloaded_t74: \"cpu f32[128, 16]\"\n",
       "  del t74\n",
       "  [t77, t79] = nvFusion15(t75)\n",
       "    # t77 = prims.gt(t75, 0.0)  # t77: \"cuda:0 b8[128, 16]\"\n",
       "    # t79 = prims.where(t77, t75, 0.0)  # t79: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t77 = offload_to_cpu(t77)  # offloaded_t77: \"cpu b8[128, 16]\"\n",
       "  del t77\n",
       "  del t75\n",
       "  t80 = torch.nn.functional.linear(t79, t_fcs_16_weight, t_fcs_16_bias)  # t80: \"cuda:0 f32[128, 16]\"\n",
       "    # t80 = ltorch.linear(t79, t_fcs_16_weight, t_fcs_16_bias)  # t80: \"cuda:0 f32[128, 16]\"\n",
       "      # t80 = prims.linear(t79, t_fcs_16_weight, t_fcs_16_bias)  # t80: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t79 = offload_to_cpu(t79)  # offloaded_t79: \"cpu f32[128, 16]\"\n",
       "  del t79\n",
       "  [t82, t84] = nvFusion16(t80)\n",
       "    # t82 = prims.gt(t80, 0.0)  # t82: \"cuda:0 b8[128, 16]\"\n",
       "    # t84 = prims.where(t82, t80, 0.0)  # t84: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t82 = offload_to_cpu(t82)  # offloaded_t82: \"cpu b8[128, 16]\"\n",
       "  del t82\n",
       "  del t80\n",
       "  t85 = torch.nn.functional.linear(t84, t_fcs_17_weight, t_fcs_17_bias)  # t85: \"cuda:0 f32[128, 16]\"\n",
       "    # t85 = ltorch.linear(t84, t_fcs_17_weight, t_fcs_17_bias)  # t85: \"cuda:0 f32[128, 16]\"\n",
       "      # t85 = prims.linear(t84, t_fcs_17_weight, t_fcs_17_bias)  # t85: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t84 = offload_to_cpu(t84)  # offloaded_t84: \"cpu f32[128, 16]\"\n",
       "  del t84\n",
       "  [t87, t89] = nvFusion17(t85)\n",
       "    # t87 = prims.gt(t85, 0.0)  # t87: \"cuda:0 b8[128, 16]\"\n",
       "    # t89 = prims.where(t87, t85, 0.0)  # t89: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t87 = offload_to_cpu(t87)  # offloaded_t87: \"cpu b8[128, 16]\"\n",
       "  del t87\n",
       "  del t85\n",
       "  t90 = torch.nn.functional.linear(t89, t_fcs_18_weight, t_fcs_18_bias)  # t90: \"cuda:0 f32[128, 16]\"\n",
       "    # t90 = ltorch.linear(t89, t_fcs_18_weight, t_fcs_18_bias)  # t90: \"cuda:0 f32[128, 16]\"\n",
       "      # t90 = prims.linear(t89, t_fcs_18_weight, t_fcs_18_bias)  # t90: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t89 = offload_to_cpu(t89)  # offloaded_t89: \"cpu f32[128, 16]\"\n",
       "  del t89\n",
       "  [t92, t94] = nvFusion18(t90)\n",
       "    # t92 = prims.gt(t90, 0.0)  # t92: \"cuda:0 b8[128, 16]\"\n",
       "    # t94 = prims.where(t92, t90, 0.0)  # t94: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t92 = offload_to_cpu(t92)  # offloaded_t92: \"cpu b8[128, 16]\"\n",
       "  del t92\n",
       "  del t90\n",
       "  t95 = torch.nn.functional.linear(t94, t_fcs_19_weight, t_fcs_19_bias)  # t95: \"cuda:0 f32[128, 16]\"\n",
       "    # t95 = ltorch.linear(t94, t_fcs_19_weight, t_fcs_19_bias)  # t95: \"cuda:0 f32[128, 16]\"\n",
       "      # t95 = prims.linear(t94, t_fcs_19_weight, t_fcs_19_bias)  # t95: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t94 = offload_to_cpu(t94)  # offloaded_t94: \"cpu f32[128, 16]\"\n",
       "  del t94\n",
       "  [t97, t99] = nvFusion19(t95)\n",
       "    # t97 = prims.gt(t95, 0.0)  # t97: \"cuda:0 b8[128, 16]\"\n",
       "    # t99 = prims.where(t97, t95, 0.0)  # t99: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t97 = offload_to_cpu(t97)  # offloaded_t97: \"cpu b8[128, 16]\"\n",
       "  del t97\n",
       "  del t95\n",
       "  t100 = torch.nn.functional.linear(t99, t_fcs_20_weight, t_fcs_20_bias)  # t100: \"cuda:0 f32[128, 16]\"\n",
       "    # t100 = ltorch.linear(t99, t_fcs_20_weight, t_fcs_20_bias)  # t100: \"cuda:0 f32[128, 16]\"\n",
       "      # t100 = prims.linear(t99, t_fcs_20_weight, t_fcs_20_bias)  # t100: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t99 = offload_to_cpu(t99)  # offloaded_t99: \"cpu f32[128, 16]\"\n",
       "  del t99\n",
       "  [t102, t104] = nvFusion20(t100)\n",
       "    # t102 = prims.gt(t100, 0.0)  # t102: \"cuda:0 b8[128, 16]\"\n",
       "    # t104 = prims.where(t102, t100, 0.0)  # t104: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t102 = offload_to_cpu(t102)  # offloaded_t102: \"cpu b8[128, 16]\"\n",
       "  del t102\n",
       "  del t100\n",
       "  t105 = torch.nn.functional.linear(t104, t_fcs_21_weight, t_fcs_21_bias)  # t105: \"cuda:0 f32[128, 16]\"\n",
       "    # t105 = ltorch.linear(t104, t_fcs_21_weight, t_fcs_21_bias)  # t105: \"cuda:0 f32[128, 16]\"\n",
       "      # t105 = prims.linear(t104, t_fcs_21_weight, t_fcs_21_bias)  # t105: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t104 = offload_to_cpu(t104)  # offloaded_t104: \"cpu f32[128, 16]\"\n",
       "  del t104\n",
       "  [t107, t109] = nvFusion21(t105)\n",
       "    # t107 = prims.gt(t105, 0.0)  # t107: \"cuda:0 b8[128, 16]\"\n",
       "    # t109 = prims.where(t107, t105, 0.0)  # t109: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t107 = offload_to_cpu(t107)  # offloaded_t107: \"cpu b8[128, 16]\"\n",
       "  del t107\n",
       "  del t105\n",
       "  t110 = torch.nn.functional.linear(t109, t_fcs_22_weight, t_fcs_22_bias)  # t110: \"cuda:0 f32[128, 16]\"\n",
       "    # t110 = ltorch.linear(t109, t_fcs_22_weight, t_fcs_22_bias)  # t110: \"cuda:0 f32[128, 16]\"\n",
       "      # t110 = prims.linear(t109, t_fcs_22_weight, t_fcs_22_bias)  # t110: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t109 = offload_to_cpu(t109)  # offloaded_t109: \"cpu f32[128, 16]\"\n",
       "  del t109\n",
       "  [t112, t114] = nvFusion22(t110)\n",
       "    # t112 = prims.gt(t110, 0.0)  # t112: \"cuda:0 b8[128, 16]\"\n",
       "    # t114 = prims.where(t112, t110, 0.0)  # t114: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t112 = offload_to_cpu(t112)  # offloaded_t112: \"cpu b8[128, 16]\"\n",
       "  del t112\n",
       "  del t110\n",
       "  t115 = torch.nn.functional.linear(t114, t_fcs_23_weight, t_fcs_23_bias)  # t115: \"cuda:0 f32[128, 16]\"\n",
       "    # t115 = ltorch.linear(t114, t_fcs_23_weight, t_fcs_23_bias)  # t115: \"cuda:0 f32[128, 16]\"\n",
       "      # t115 = prims.linear(t114, t_fcs_23_weight, t_fcs_23_bias)  # t115: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t114 = offload_to_cpu(t114)  # offloaded_t114: \"cpu f32[128, 16]\"\n",
       "  del t114\n",
       "  [t117, t119] = nvFusion23(t115)\n",
       "    # t117 = prims.gt(t115, 0.0)  # t117: \"cuda:0 b8[128, 16]\"\n",
       "    # t119 = prims.where(t117, t115, 0.0)  # t119: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t117 = offload_to_cpu(t117)  # offloaded_t117: \"cpu b8[128, 16]\"\n",
       "  del t117\n",
       "  del t115\n",
       "  t120 = torch.nn.functional.linear(t119, t_fcs_24_weight, t_fcs_24_bias)  # t120: \"cuda:0 f32[128, 16]\"\n",
       "    # t120 = ltorch.linear(t119, t_fcs_24_weight, t_fcs_24_bias)  # t120: \"cuda:0 f32[128, 16]\"\n",
       "      # t120 = prims.linear(t119, t_fcs_24_weight, t_fcs_24_bias)  # t120: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t119 = offload_to_cpu(t119)  # offloaded_t119: \"cpu f32[128, 16]\"\n",
       "  del t119\n",
       "  [t122, t124] = nvFusion24(t120)\n",
       "    # t122 = prims.gt(t120, 0.0)  # t122: \"cuda:0 b8[128, 16]\"\n",
       "    # t124 = prims.where(t122, t120, 0.0)  # t124: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t122 = offload_to_cpu(t122)  # offloaded_t122: \"cpu b8[128, 16]\"\n",
       "  del t122\n",
       "  del t120\n",
       "  t125 = torch.nn.functional.linear(t124, t_fcs_25_weight, t_fcs_25_bias)  # t125: \"cuda:0 f32[128, 16]\"\n",
       "    # t125 = ltorch.linear(t124, t_fcs_25_weight, t_fcs_25_bias)  # t125: \"cuda:0 f32[128, 16]\"\n",
       "      # t125 = prims.linear(t124, t_fcs_25_weight, t_fcs_25_bias)  # t125: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t124 = offload_to_cpu(t124)  # offloaded_t124: \"cpu f32[128, 16]\"\n",
       "  del t124\n",
       "  [t127, t129] = nvFusion25(t125)\n",
       "    # t127 = prims.gt(t125, 0.0)  # t127: \"cuda:0 b8[128, 16]\"\n",
       "    # t129 = prims.where(t127, t125, 0.0)  # t129: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t127 = offload_to_cpu(t127)  # offloaded_t127: \"cpu b8[128, 16]\"\n",
       "  del t127\n",
       "  del t125\n",
       "  t130 = torch.nn.functional.linear(t129, t_fcs_26_weight, t_fcs_26_bias)  # t130: \"cuda:0 f32[128, 16]\"\n",
       "    # t130 = ltorch.linear(t129, t_fcs_26_weight, t_fcs_26_bias)  # t130: \"cuda:0 f32[128, 16]\"\n",
       "      # t130 = prims.linear(t129, t_fcs_26_weight, t_fcs_26_bias)  # t130: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t129 = offload_to_cpu(t129)  # offloaded_t129: \"cpu f32[128, 16]\"\n",
       "  del t129\n",
       "  [t132, t134] = nvFusion26(t130)\n",
       "    # t132 = prims.gt(t130, 0.0)  # t132: \"cuda:0 b8[128, 16]\"\n",
       "    # t134 = prims.where(t132, t130, 0.0)  # t134: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t132 = offload_to_cpu(t132)  # offloaded_t132: \"cpu b8[128, 16]\"\n",
       "  del t132\n",
       "  del t130\n",
       "  t135 = torch.nn.functional.linear(t134, t_fcs_27_weight, t_fcs_27_bias)  # t135: \"cuda:0 f32[128, 16]\"\n",
       "    # t135 = ltorch.linear(t134, t_fcs_27_weight, t_fcs_27_bias)  # t135: \"cuda:0 f32[128, 16]\"\n",
       "      # t135 = prims.linear(t134, t_fcs_27_weight, t_fcs_27_bias)  # t135: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t134 = offload_to_cpu(t134)  # offloaded_t134: \"cpu f32[128, 16]\"\n",
       "  del t134\n",
       "  [t137, t139] = nvFusion27(t135)\n",
       "    # t137 = prims.gt(t135, 0.0)  # t137: \"cuda:0 b8[128, 16]\"\n",
       "    # t139 = prims.where(t137, t135, 0.0)  # t139: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t137 = offload_to_cpu(t137)  # offloaded_t137: \"cpu b8[128, 16]\"\n",
       "  del t137\n",
       "  del t135\n",
       "  t140 = torch.nn.functional.linear(t139, t_fcs_28_weight, t_fcs_28_bias)  # t140: \"cuda:0 f32[128, 16]\"\n",
       "    # t140 = ltorch.linear(t139, t_fcs_28_weight, t_fcs_28_bias)  # t140: \"cuda:0 f32[128, 16]\"\n",
       "      # t140 = prims.linear(t139, t_fcs_28_weight, t_fcs_28_bias)  # t140: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t139 = offload_to_cpu(t139)  # offloaded_t139: \"cpu f32[128, 16]\"\n",
       "  del t139\n",
       "  [t142, t144] = nvFusion28(t140)\n",
       "    # t142 = prims.gt(t140, 0.0)  # t142: \"cuda:0 b8[128, 16]\"\n",
       "    # t144 = prims.where(t142, t140, 0.0)  # t144: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t142 = offload_to_cpu(t142)  # offloaded_t142: \"cpu b8[128, 16]\"\n",
       "  del t142\n",
       "  del t140\n",
       "  t145 = torch.nn.functional.linear(t144, t_fcs_29_weight, t_fcs_29_bias)  # t145: \"cuda:0 f32[128, 16]\"\n",
       "    # t145 = ltorch.linear(t144, t_fcs_29_weight, t_fcs_29_bias)  # t145: \"cuda:0 f32[128, 16]\"\n",
       "      # t145 = prims.linear(t144, t_fcs_29_weight, t_fcs_29_bias)  # t145: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t144 = offload_to_cpu(t144)  # offloaded_t144: \"cpu f32[128, 16]\"\n",
       "  del t144\n",
       "  [t147, t149] = nvFusion29(t145)\n",
       "    # t147 = prims.gt(t145, 0.0)  # t147: \"cuda:0 b8[128, 16]\"\n",
       "    # t149 = prims.where(t147, t145, 0.0)  # t149: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t147 = offload_to_cpu(t147)  # offloaded_t147: \"cpu b8[128, 16]\"\n",
       "  del t147\n",
       "  del t145\n",
       "  t150 = torch.nn.functional.linear(t149, t_fcs_30_weight, t_fcs_30_bias)  # t150: \"cuda:0 f32[128, 16]\"\n",
       "    # t150 = ltorch.linear(t149, t_fcs_30_weight, t_fcs_30_bias)  # t150: \"cuda:0 f32[128, 16]\"\n",
       "      # t150 = prims.linear(t149, t_fcs_30_weight, t_fcs_30_bias)  # t150: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t149 = offload_to_cpu(t149)  # offloaded_t149: \"cpu f32[128, 16]\"\n",
       "  del t149\n",
       "  [t152, t154] = nvFusion30(t150)\n",
       "    # t152 = prims.gt(t150, 0.0)  # t152: \"cuda:0 b8[128, 16]\"\n",
       "    # t154 = prims.where(t152, t150, 0.0)  # t154: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t152 = offload_to_cpu(t152)  # offloaded_t152: \"cpu b8[128, 16]\"\n",
       "  del t152\n",
       "  del t150\n",
       "  t155 = torch.nn.functional.linear(t154, t_fcs_31_weight, t_fcs_31_bias)  # t155: \"cuda:0 f32[128, 16]\"\n",
       "    # t155 = ltorch.linear(t154, t_fcs_31_weight, t_fcs_31_bias)  # t155: \"cuda:0 f32[128, 16]\"\n",
       "      # t155 = prims.linear(t154, t_fcs_31_weight, t_fcs_31_bias)  # t155: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t154 = offload_to_cpu(t154)  # offloaded_t154: \"cpu f32[128, 16]\"\n",
       "  del t154\n",
       "  [t157, t159] = nvFusion31(t155)\n",
       "    # t157 = prims.gt(t155, 0.0)  # t157: \"cuda:0 b8[128, 16]\"\n",
       "    # t159 = prims.where(t157, t155, 0.0)  # t159: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t157 = offload_to_cpu(t157)  # offloaded_t157: \"cpu b8[128, 16]\"\n",
       "  del t157\n",
       "  del t155\n",
       "  t160 = torch.nn.functional.linear(t159, t_fcs_32_weight, t_fcs_32_bias)  # t160: \"cuda:0 f32[128, 16]\"\n",
       "    # t160 = ltorch.linear(t159, t_fcs_32_weight, t_fcs_32_bias)  # t160: \"cuda:0 f32[128, 16]\"\n",
       "      # t160 = prims.linear(t159, t_fcs_32_weight, t_fcs_32_bias)  # t160: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t159 = offload_to_cpu(t159)  # offloaded_t159: \"cpu f32[128, 16]\"\n",
       "  del t159\n",
       "  [t162, t164] = nvFusion32(t160)\n",
       "    # t162 = prims.gt(t160, 0.0)  # t162: \"cuda:0 b8[128, 16]\"\n",
       "    # t164 = prims.where(t162, t160, 0.0)  # t164: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t162 = offload_to_cpu(t162)  # offloaded_t162: \"cpu b8[128, 16]\"\n",
       "  del t162\n",
       "  del t160\n",
       "  t165 = torch.nn.functional.linear(t164, t_fcs_33_weight, t_fcs_33_bias)  # t165: \"cuda:0 f32[128, 16]\"\n",
       "    # t165 = ltorch.linear(t164, t_fcs_33_weight, t_fcs_33_bias)  # t165: \"cuda:0 f32[128, 16]\"\n",
       "      # t165 = prims.linear(t164, t_fcs_33_weight, t_fcs_33_bias)  # t165: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t164 = offload_to_cpu(t164)  # offloaded_t164: \"cpu f32[128, 16]\"\n",
       "  del t164\n",
       "  [t167, t169] = nvFusion33(t165)\n",
       "    # t167 = prims.gt(t165, 0.0)  # t167: \"cuda:0 b8[128, 16]\"\n",
       "    # t169 = prims.where(t167, t165, 0.0)  # t169: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t167 = offload_to_cpu(t167)  # offloaded_t167: \"cpu b8[128, 16]\"\n",
       "  del t167\n",
       "  del t165\n",
       "  t170 = torch.nn.functional.linear(t169, t_fcs_34_weight, t_fcs_34_bias)  # t170: \"cuda:0 f32[128, 16]\"\n",
       "    # t170 = ltorch.linear(t169, t_fcs_34_weight, t_fcs_34_bias)  # t170: \"cuda:0 f32[128, 16]\"\n",
       "      # t170 = prims.linear(t169, t_fcs_34_weight, t_fcs_34_bias)  # t170: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t169 = offload_to_cpu(t169)  # offloaded_t169: \"cpu f32[128, 16]\"\n",
       "  del t169\n",
       "  [t172, t174] = nvFusion34(t170)\n",
       "    # t172 = prims.gt(t170, 0.0)  # t172: \"cuda:0 b8[128, 16]\"\n",
       "    # t174 = prims.where(t172, t170, 0.0)  # t174: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t172 = offload_to_cpu(t172)  # offloaded_t172: \"cpu b8[128, 16]\"\n",
       "  del t172\n",
       "  del t170\n",
       "  t175 = torch.nn.functional.linear(t174, t_fcs_35_weight, t_fcs_35_bias)  # t175: \"cuda:0 f32[128, 16]\"\n",
       "    # t175 = ltorch.linear(t174, t_fcs_35_weight, t_fcs_35_bias)  # t175: \"cuda:0 f32[128, 16]\"\n",
       "      # t175 = prims.linear(t174, t_fcs_35_weight, t_fcs_35_bias)  # t175: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t174 = offload_to_cpu(t174)  # offloaded_t174: \"cpu f32[128, 16]\"\n",
       "  del t174\n",
       "  [t177, t179] = nvFusion35(t175)\n",
       "    # t177 = prims.gt(t175, 0.0)  # t177: \"cuda:0 b8[128, 16]\"\n",
       "    # t179 = prims.where(t177, t175, 0.0)  # t179: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t177 = offload_to_cpu(t177)  # offloaded_t177: \"cpu b8[128, 16]\"\n",
       "  del t177\n",
       "  del t175\n",
       "  t180 = torch.nn.functional.linear(t179, t_fcs_36_weight, t_fcs_36_bias)  # t180: \"cuda:0 f32[128, 16]\"\n",
       "    # t180 = ltorch.linear(t179, t_fcs_36_weight, t_fcs_36_bias)  # t180: \"cuda:0 f32[128, 16]\"\n",
       "      # t180 = prims.linear(t179, t_fcs_36_weight, t_fcs_36_bias)  # t180: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t179 = offload_to_cpu(t179)  # offloaded_t179: \"cpu f32[128, 16]\"\n",
       "  del t179\n",
       "  [t182, t184] = nvFusion36(t180)\n",
       "    # t182 = prims.gt(t180, 0.0)  # t182: \"cuda:0 b8[128, 16]\"\n",
       "    # t184 = prims.where(t182, t180, 0.0)  # t184: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t182 = offload_to_cpu(t182)  # offloaded_t182: \"cpu b8[128, 16]\"\n",
       "  del t182\n",
       "  del t180\n",
       "  t185 = torch.nn.functional.linear(t184, t_fcs_37_weight, t_fcs_37_bias)  # t185: \"cuda:0 f32[128, 16]\"\n",
       "    # t185 = ltorch.linear(t184, t_fcs_37_weight, t_fcs_37_bias)  # t185: \"cuda:0 f32[128, 16]\"\n",
       "      # t185 = prims.linear(t184, t_fcs_37_weight, t_fcs_37_bias)  # t185: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t184 = offload_to_cpu(t184)  # offloaded_t184: \"cpu f32[128, 16]\"\n",
       "  del t184\n",
       "  [t187, t189] = nvFusion37(t185)\n",
       "    # t187 = prims.gt(t185, 0.0)  # t187: \"cuda:0 b8[128, 16]\"\n",
       "    # t189 = prims.where(t187, t185, 0.0)  # t189: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t187 = offload_to_cpu(t187)  # offloaded_t187: \"cpu b8[128, 16]\"\n",
       "  del t187\n",
       "  del t185\n",
       "  t190 = torch.nn.functional.linear(t189, t_fcs_38_weight, t_fcs_38_bias)  # t190: \"cuda:0 f32[128, 16]\"\n",
       "    # t190 = ltorch.linear(t189, t_fcs_38_weight, t_fcs_38_bias)  # t190: \"cuda:0 f32[128, 16]\"\n",
       "      # t190 = prims.linear(t189, t_fcs_38_weight, t_fcs_38_bias)  # t190: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t189 = offload_to_cpu(t189)  # offloaded_t189: \"cpu f32[128, 16]\"\n",
       "  del t189\n",
       "  [t192, t194] = nvFusion38(t190)\n",
       "    # t192 = prims.gt(t190, 0.0)  # t192: \"cuda:0 b8[128, 16]\"\n",
       "    # t194 = prims.where(t192, t190, 0.0)  # t194: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t192 = offload_to_cpu(t192)  # offloaded_t192: \"cpu b8[128, 16]\"\n",
       "  del t192\n",
       "  del t190\n",
       "  t195 = torch.nn.functional.linear(t194, t_fcs_39_weight, t_fcs_39_bias)  # t195: \"cuda:0 f32[128, 16]\"\n",
       "    # t195 = ltorch.linear(t194, t_fcs_39_weight, t_fcs_39_bias)  # t195: \"cuda:0 f32[128, 16]\"\n",
       "      # t195 = prims.linear(t194, t_fcs_39_weight, t_fcs_39_bias)  # t195: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t194 = offload_to_cpu(t194)  # offloaded_t194: \"cpu f32[128, 16]\"\n",
       "  del t194\n",
       "  [t197, t199] = nvFusion39(t195)\n",
       "    # t197 = prims.gt(t195, 0.0)  # t197: \"cuda:0 b8[128, 16]\"\n",
       "    # t199 = prims.where(t197, t195, 0.0)  # t199: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t197 = offload_to_cpu(t197)  # offloaded_t197: \"cpu b8[128, 16]\"\n",
       "  del t197\n",
       "  del t195\n",
       "  t200 = torch.nn.functional.linear(t199, t_fcs_40_weight, t_fcs_40_bias)  # t200: \"cuda:0 f32[128, 16]\"\n",
       "    # t200 = ltorch.linear(t199, t_fcs_40_weight, t_fcs_40_bias)  # t200: \"cuda:0 f32[128, 16]\"\n",
       "      # t200 = prims.linear(t199, t_fcs_40_weight, t_fcs_40_bias)  # t200: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t199 = offload_to_cpu(t199)  # offloaded_t199: \"cpu f32[128, 16]\"\n",
       "  del t199\n",
       "  [t202, t204] = nvFusion40(t200)\n",
       "    # t202 = prims.gt(t200, 0.0)  # t202: \"cuda:0 b8[128, 16]\"\n",
       "    # t204 = prims.where(t202, t200, 0.0)  # t204: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t202 = offload_to_cpu(t202)  # offloaded_t202: \"cpu b8[128, 16]\"\n",
       "  del t202\n",
       "  del t200\n",
       "  t205 = torch.nn.functional.linear(t204, t_fcs_41_weight, t_fcs_41_bias)  # t205: \"cuda:0 f32[128, 16]\"\n",
       "    # t205 = ltorch.linear(t204, t_fcs_41_weight, t_fcs_41_bias)  # t205: \"cuda:0 f32[128, 16]\"\n",
       "      # t205 = prims.linear(t204, t_fcs_41_weight, t_fcs_41_bias)  # t205: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t204 = offload_to_cpu(t204)  # offloaded_t204: \"cpu f32[128, 16]\"\n",
       "  del t204\n",
       "  [t207, t209] = nvFusion41(t205)\n",
       "    # t207 = prims.gt(t205, 0.0)  # t207: \"cuda:0 b8[128, 16]\"\n",
       "    # t209 = prims.where(t207, t205, 0.0)  # t209: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t207 = offload_to_cpu(t207)  # offloaded_t207: \"cpu b8[128, 16]\"\n",
       "  del t207\n",
       "  del t205\n",
       "  t210 = torch.nn.functional.linear(t209, t_fcs_42_weight, t_fcs_42_bias)  # t210: \"cuda:0 f32[128, 16]\"\n",
       "    # t210 = ltorch.linear(t209, t_fcs_42_weight, t_fcs_42_bias)  # t210: \"cuda:0 f32[128, 16]\"\n",
       "      # t210 = prims.linear(t209, t_fcs_42_weight, t_fcs_42_bias)  # t210: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t209 = offload_to_cpu(t209)  # offloaded_t209: \"cpu f32[128, 16]\"\n",
       "  del t209\n",
       "  [t212, t214] = nvFusion42(t210)\n",
       "    # t212 = prims.gt(t210, 0.0)  # t212: \"cuda:0 b8[128, 16]\"\n",
       "    # t214 = prims.where(t212, t210, 0.0)  # t214: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t212 = offload_to_cpu(t212)  # offloaded_t212: \"cpu b8[128, 16]\"\n",
       "  del t212\n",
       "  del t210\n",
       "  t215 = torch.nn.functional.linear(t214, t_fcs_43_weight, t_fcs_43_bias)  # t215: \"cuda:0 f32[128, 16]\"\n",
       "    # t215 = ltorch.linear(t214, t_fcs_43_weight, t_fcs_43_bias)  # t215: \"cuda:0 f32[128, 16]\"\n",
       "      # t215 = prims.linear(t214, t_fcs_43_weight, t_fcs_43_bias)  # t215: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t214 = offload_to_cpu(t214)  # offloaded_t214: \"cpu f32[128, 16]\"\n",
       "  del t214\n",
       "  [t217, t219] = nvFusion43(t215)\n",
       "    # t217 = prims.gt(t215, 0.0)  # t217: \"cuda:0 b8[128, 16]\"\n",
       "    # t219 = prims.where(t217, t215, 0.0)  # t219: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t217 = offload_to_cpu(t217)  # offloaded_t217: \"cpu b8[128, 16]\"\n",
       "  del t217\n",
       "  del t215\n",
       "  t220 = torch.nn.functional.linear(t219, t_fcs_44_weight, t_fcs_44_bias)  # t220: \"cuda:0 f32[128, 16]\"\n",
       "    # t220 = ltorch.linear(t219, t_fcs_44_weight, t_fcs_44_bias)  # t220: \"cuda:0 f32[128, 16]\"\n",
       "      # t220 = prims.linear(t219, t_fcs_44_weight, t_fcs_44_bias)  # t220: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t219 = offload_to_cpu(t219)  # offloaded_t219: \"cpu f32[128, 16]\"\n",
       "  del t219\n",
       "  [t222, t224] = nvFusion44(t220)\n",
       "    # t222 = prims.gt(t220, 0.0)  # t222: \"cuda:0 b8[128, 16]\"\n",
       "    # t224 = prims.where(t222, t220, 0.0)  # t224: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t222 = offload_to_cpu(t222)  # offloaded_t222: \"cpu b8[128, 16]\"\n",
       "  del t222\n",
       "  del t220\n",
       "  t225 = torch.nn.functional.linear(t224, t_fcs_45_weight, t_fcs_45_bias)  # t225: \"cuda:0 f32[128, 16]\"\n",
       "    # t225 = ltorch.linear(t224, t_fcs_45_weight, t_fcs_45_bias)  # t225: \"cuda:0 f32[128, 16]\"\n",
       "      # t225 = prims.linear(t224, t_fcs_45_weight, t_fcs_45_bias)  # t225: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t224 = offload_to_cpu(t224)  # offloaded_t224: \"cpu f32[128, 16]\"\n",
       "  del t224\n",
       "  [t227, t229] = nvFusion45(t225)\n",
       "    # t227 = prims.gt(t225, 0.0)  # t227: \"cuda:0 b8[128, 16]\"\n",
       "    # t229 = prims.where(t227, t225, 0.0)  # t229: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t227 = offload_to_cpu(t227)  # offloaded_t227: \"cpu b8[128, 16]\"\n",
       "  del t227\n",
       "  del t225\n",
       "  t230 = torch.nn.functional.linear(t229, t_fcs_46_weight, t_fcs_46_bias)  # t230: \"cuda:0 f32[128, 16]\"\n",
       "    # t230 = ltorch.linear(t229, t_fcs_46_weight, t_fcs_46_bias)  # t230: \"cuda:0 f32[128, 16]\"\n",
       "      # t230 = prims.linear(t229, t_fcs_46_weight, t_fcs_46_bias)  # t230: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t229 = offload_to_cpu(t229)  # offloaded_t229: \"cpu f32[128, 16]\"\n",
       "  del t229\n",
       "  [t232, t234] = nvFusion46(t230)\n",
       "    # t232 = prims.gt(t230, 0.0)  # t232: \"cuda:0 b8[128, 16]\"\n",
       "    # t234 = prims.where(t232, t230, 0.0)  # t234: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t232 = offload_to_cpu(t232)  # offloaded_t232: \"cpu b8[128, 16]\"\n",
       "  del t232\n",
       "  del t230\n",
       "  t235 = torch.nn.functional.linear(t234, t_fcs_47_weight, t_fcs_47_bias)  # t235: \"cuda:0 f32[128, 16]\"\n",
       "    # t235 = ltorch.linear(t234, t_fcs_47_weight, t_fcs_47_bias)  # t235: \"cuda:0 f32[128, 16]\"\n",
       "      # t235 = prims.linear(t234, t_fcs_47_weight, t_fcs_47_bias)  # t235: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t234 = offload_to_cpu(t234)  # offloaded_t234: \"cpu f32[128, 16]\"\n",
       "  del t234\n",
       "  [t237, t239] = nvFusion47(t235)\n",
       "    # t237 = prims.gt(t235, 0.0)  # t237: \"cuda:0 b8[128, 16]\"\n",
       "    # t239 = prims.where(t237, t235, 0.0)  # t239: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t237 = offload_to_cpu(t237)  # offloaded_t237: \"cpu b8[128, 16]\"\n",
       "  del t237\n",
       "  del t235\n",
       "  t240 = torch.nn.functional.linear(t239, t_fcs_48_weight, t_fcs_48_bias)  # t240: \"cuda:0 f32[128, 16]\"\n",
       "    # t240 = ltorch.linear(t239, t_fcs_48_weight, t_fcs_48_bias)  # t240: \"cuda:0 f32[128, 16]\"\n",
       "      # t240 = prims.linear(t239, t_fcs_48_weight, t_fcs_48_bias)  # t240: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t239 = offload_to_cpu(t239)  # offloaded_t239: \"cpu f32[128, 16]\"\n",
       "  del t239\n",
       "  [t242, t244] = nvFusion48(t240)\n",
       "    # t242 = prims.gt(t240, 0.0)  # t242: \"cuda:0 b8[128, 16]\"\n",
       "    # t244 = prims.where(t242, t240, 0.0)  # t244: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t242 = offload_to_cpu(t242)  # offloaded_t242: \"cpu b8[128, 16]\"\n",
       "  del t242\n",
       "  del t240\n",
       "  t245 = torch.nn.functional.linear(t244, t_fcs_49_weight, t_fcs_49_bias)  # t245: \"cuda:0 f32[128, 16]\"\n",
       "    # t245 = ltorch.linear(t244, t_fcs_49_weight, t_fcs_49_bias)  # t245: \"cuda:0 f32[128, 16]\"\n",
       "      # t245 = prims.linear(t244, t_fcs_49_weight, t_fcs_49_bias)  # t245: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t244 = offload_to_cpu(t244)  # offloaded_t244: \"cpu f32[128, 16]\"\n",
       "  del t244\n",
       "  [t247, t249] = nvFusion49(t245)\n",
       "    # t247 = prims.gt(t245, 0.0)  # t247: \"cuda:0 b8[128, 16]\"\n",
       "    # t249 = prims.where(t247, t245, 0.0)  # t249: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t247 = offload_to_cpu(t247)  # offloaded_t247: \"cpu b8[128, 16]\"\n",
       "  del t247\n",
       "  del t245\n",
       "  t250 = torch.nn.functional.linear(t249, t_fcs_50_weight, t_fcs_50_bias)  # t250: \"cuda:0 f32[128, 16]\"\n",
       "    # t250 = ltorch.linear(t249, t_fcs_50_weight, t_fcs_50_bias)  # t250: \"cuda:0 f32[128, 16]\"\n",
       "      # t250 = prims.linear(t249, t_fcs_50_weight, t_fcs_50_bias)  # t250: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t249 = offload_to_cpu(t249)  # offloaded_t249: \"cpu f32[128, 16]\"\n",
       "  del t249\n",
       "  [t252, t254] = nvFusion50(t250)\n",
       "    # t252 = prims.gt(t250, 0.0)  # t252: \"cuda:0 b8[128, 16]\"\n",
       "    # t254 = prims.where(t252, t250, 0.0)  # t254: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t252 = offload_to_cpu(t252)  # offloaded_t252: \"cpu b8[128, 16]\"\n",
       "  del t252\n",
       "  del t250\n",
       "  t255 = torch.nn.functional.linear(t254, t_fcs_51_weight, t_fcs_51_bias)  # t255: \"cuda:0 f32[128, 16]\"\n",
       "    # t255 = ltorch.linear(t254, t_fcs_51_weight, t_fcs_51_bias)  # t255: \"cuda:0 f32[128, 16]\"\n",
       "      # t255 = prims.linear(t254, t_fcs_51_weight, t_fcs_51_bias)  # t255: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t254 = offload_to_cpu(t254)  # offloaded_t254: \"cpu f32[128, 16]\"\n",
       "  del t254\n",
       "  [t257, t259] = nvFusion51(t255)\n",
       "    # t257 = prims.gt(t255, 0.0)  # t257: \"cuda:0 b8[128, 16]\"\n",
       "    # t259 = prims.where(t257, t255, 0.0)  # t259: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t257 = offload_to_cpu(t257)  # offloaded_t257: \"cpu b8[128, 16]\"\n",
       "  del t257\n",
       "  del t255\n",
       "  t260 = torch.nn.functional.linear(t259, t_fcs_52_weight, t_fcs_52_bias)  # t260: \"cuda:0 f32[128, 16]\"\n",
       "    # t260 = ltorch.linear(t259, t_fcs_52_weight, t_fcs_52_bias)  # t260: \"cuda:0 f32[128, 16]\"\n",
       "      # t260 = prims.linear(t259, t_fcs_52_weight, t_fcs_52_bias)  # t260: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t259 = offload_to_cpu(t259)  # offloaded_t259: \"cpu f32[128, 16]\"\n",
       "  del t259\n",
       "  [t262, t264] = nvFusion52(t260)\n",
       "    # t262 = prims.gt(t260, 0.0)  # t262: \"cuda:0 b8[128, 16]\"\n",
       "    # t264 = prims.where(t262, t260, 0.0)  # t264: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t262 = offload_to_cpu(t262)  # offloaded_t262: \"cpu b8[128, 16]\"\n",
       "  del t262\n",
       "  del t260\n",
       "  t265 = torch.nn.functional.linear(t264, t_fcs_53_weight, t_fcs_53_bias)  # t265: \"cuda:0 f32[128, 16]\"\n",
       "    # t265 = ltorch.linear(t264, t_fcs_53_weight, t_fcs_53_bias)  # t265: \"cuda:0 f32[128, 16]\"\n",
       "      # t265 = prims.linear(t264, t_fcs_53_weight, t_fcs_53_bias)  # t265: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t264 = offload_to_cpu(t264)  # offloaded_t264: \"cpu f32[128, 16]\"\n",
       "  del t264\n",
       "  [t267, t269] = nvFusion53(t265)\n",
       "    # t267 = prims.gt(t265, 0.0)  # t267: \"cuda:0 b8[128, 16]\"\n",
       "    # t269 = prims.where(t267, t265, 0.0)  # t269: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t267 = offload_to_cpu(t267)  # offloaded_t267: \"cpu b8[128, 16]\"\n",
       "  del t267\n",
       "  del t265\n",
       "  t270 = torch.nn.functional.linear(t269, t_fcs_54_weight, t_fcs_54_bias)  # t270: \"cuda:0 f32[128, 16]\"\n",
       "    # t270 = ltorch.linear(t269, t_fcs_54_weight, t_fcs_54_bias)  # t270: \"cuda:0 f32[128, 16]\"\n",
       "      # t270 = prims.linear(t269, t_fcs_54_weight, t_fcs_54_bias)  # t270: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t269 = offload_to_cpu(t269)  # offloaded_t269: \"cpu f32[128, 16]\"\n",
       "  del t269\n",
       "  [t272, t274] = nvFusion54(t270)\n",
       "    # t272 = prims.gt(t270, 0.0)  # t272: \"cuda:0 b8[128, 16]\"\n",
       "    # t274 = prims.where(t272, t270, 0.0)  # t274: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t272 = offload_to_cpu(t272)  # offloaded_t272: \"cpu b8[128, 16]\"\n",
       "  del t272\n",
       "  del t270\n",
       "  t275 = torch.nn.functional.linear(t274, t_fcs_55_weight, t_fcs_55_bias)  # t275: \"cuda:0 f32[128, 16]\"\n",
       "    # t275 = ltorch.linear(t274, t_fcs_55_weight, t_fcs_55_bias)  # t275: \"cuda:0 f32[128, 16]\"\n",
       "      # t275 = prims.linear(t274, t_fcs_55_weight, t_fcs_55_bias)  # t275: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t274 = offload_to_cpu(t274)  # offloaded_t274: \"cpu f32[128, 16]\"\n",
       "  del t274\n",
       "  [t277, t279] = nvFusion55(t275)\n",
       "    # t277 = prims.gt(t275, 0.0)  # t277: \"cuda:0 b8[128, 16]\"\n",
       "    # t279 = prims.where(t277, t275, 0.0)  # t279: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t277 = offload_to_cpu(t277)  # offloaded_t277: \"cpu b8[128, 16]\"\n",
       "  del t277\n",
       "  del t275\n",
       "  t280 = torch.nn.functional.linear(t279, t_fcs_56_weight, t_fcs_56_bias)  # t280: \"cuda:0 f32[128, 16]\"\n",
       "    # t280 = ltorch.linear(t279, t_fcs_56_weight, t_fcs_56_bias)  # t280: \"cuda:0 f32[128, 16]\"\n",
       "      # t280 = prims.linear(t279, t_fcs_56_weight, t_fcs_56_bias)  # t280: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t279 = offload_to_cpu(t279)  # offloaded_t279: \"cpu f32[128, 16]\"\n",
       "  del t279\n",
       "  [t282, t284] = nvFusion56(t280)\n",
       "    # t282 = prims.gt(t280, 0.0)  # t282: \"cuda:0 b8[128, 16]\"\n",
       "    # t284 = prims.where(t282, t280, 0.0)  # t284: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t282 = offload_to_cpu(t282)  # offloaded_t282: \"cpu b8[128, 16]\"\n",
       "  del t282\n",
       "  del t280\n",
       "  t285 = torch.nn.functional.linear(t284, t_fcs_57_weight, t_fcs_57_bias)  # t285: \"cuda:0 f32[128, 16]\"\n",
       "    # t285 = ltorch.linear(t284, t_fcs_57_weight, t_fcs_57_bias)  # t285: \"cuda:0 f32[128, 16]\"\n",
       "      # t285 = prims.linear(t284, t_fcs_57_weight, t_fcs_57_bias)  # t285: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t284 = offload_to_cpu(t284)  # offloaded_t284: \"cpu f32[128, 16]\"\n",
       "  del t284\n",
       "  [t287, t289] = nvFusion57(t285)\n",
       "    # t287 = prims.gt(t285, 0.0)  # t287: \"cuda:0 b8[128, 16]\"\n",
       "    # t289 = prims.where(t287, t285, 0.0)  # t289: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t287 = offload_to_cpu(t287)  # offloaded_t287: \"cpu b8[128, 16]\"\n",
       "  del t287\n",
       "  del t285\n",
       "  t290 = torch.nn.functional.linear(t289, t_fcs_58_weight, t_fcs_58_bias)  # t290: \"cuda:0 f32[128, 16]\"\n",
       "    # t290 = ltorch.linear(t289, t_fcs_58_weight, t_fcs_58_bias)  # t290: \"cuda:0 f32[128, 16]\"\n",
       "      # t290 = prims.linear(t289, t_fcs_58_weight, t_fcs_58_bias)  # t290: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t289 = offload_to_cpu(t289)  # offloaded_t289: \"cpu f32[128, 16]\"\n",
       "  del t289\n",
       "  [t292, t294] = nvFusion58(t290)\n",
       "    # t292 = prims.gt(t290, 0.0)  # t292: \"cuda:0 b8[128, 16]\"\n",
       "    # t294 = prims.where(t292, t290, 0.0)  # t294: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t292 = offload_to_cpu(t292)  # offloaded_t292: \"cpu b8[128, 16]\"\n",
       "  del t292\n",
       "  del t290\n",
       "  t295 = torch.nn.functional.linear(t294, t_fcs_59_weight, t_fcs_59_bias)  # t295: \"cuda:0 f32[128, 16]\"\n",
       "    # t295 = ltorch.linear(t294, t_fcs_59_weight, t_fcs_59_bias)  # t295: \"cuda:0 f32[128, 16]\"\n",
       "      # t295 = prims.linear(t294, t_fcs_59_weight, t_fcs_59_bias)  # t295: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t294 = offload_to_cpu(t294)  # offloaded_t294: \"cpu f32[128, 16]\"\n",
       "  del t294\n",
       "  [t297, t299] = nvFusion59(t295)\n",
       "    # t297 = prims.gt(t295, 0.0)  # t297: \"cuda:0 b8[128, 16]\"\n",
       "    # t299 = prims.where(t297, t295, 0.0)  # t299: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t297 = offload_to_cpu(t297)  # offloaded_t297: \"cpu b8[128, 16]\"\n",
       "  del t297\n",
       "  del t295\n",
       "  t300 = torch.nn.functional.linear(t299, t_fcs_60_weight, t_fcs_60_bias)  # t300: \"cuda:0 f32[128, 16]\"\n",
       "    # t300 = ltorch.linear(t299, t_fcs_60_weight, t_fcs_60_bias)  # t300: \"cuda:0 f32[128, 16]\"\n",
       "      # t300 = prims.linear(t299, t_fcs_60_weight, t_fcs_60_bias)  # t300: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t299 = offload_to_cpu(t299)  # offloaded_t299: \"cpu f32[128, 16]\"\n",
       "  del t299\n",
       "  [t302, t304] = nvFusion60(t300)\n",
       "    # t302 = prims.gt(t300, 0.0)  # t302: \"cuda:0 b8[128, 16]\"\n",
       "    # t304 = prims.where(t302, t300, 0.0)  # t304: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t302 = offload_to_cpu(t302)  # offloaded_t302: \"cpu b8[128, 16]\"\n",
       "  del t302\n",
       "  del t300\n",
       "  t305 = torch.nn.functional.linear(t304, t_fcs_61_weight, t_fcs_61_bias)  # t305: \"cuda:0 f32[128, 16]\"\n",
       "    # t305 = ltorch.linear(t304, t_fcs_61_weight, t_fcs_61_bias)  # t305: \"cuda:0 f32[128, 16]\"\n",
       "      # t305 = prims.linear(t304, t_fcs_61_weight, t_fcs_61_bias)  # t305: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t304 = offload_to_cpu(t304)  # offloaded_t304: \"cpu f32[128, 16]\"\n",
       "  del t304\n",
       "  [t307, t309] = nvFusion61(t305)\n",
       "    # t307 = prims.gt(t305, 0.0)  # t307: \"cuda:0 b8[128, 16]\"\n",
       "    # t309 = prims.where(t307, t305, 0.0)  # t309: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t307 = offload_to_cpu(t307)  # offloaded_t307: \"cpu b8[128, 16]\"\n",
       "  del t307\n",
       "  del t305\n",
       "  t310 = torch.nn.functional.linear(t309, t_fcs_62_weight, t_fcs_62_bias)  # t310: \"cuda:0 f32[128, 16]\"\n",
       "    # t310 = ltorch.linear(t309, t_fcs_62_weight, t_fcs_62_bias)  # t310: \"cuda:0 f32[128, 16]\"\n",
       "      # t310 = prims.linear(t309, t_fcs_62_weight, t_fcs_62_bias)  # t310: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t309 = offload_to_cpu(t309)  # offloaded_t309: \"cpu f32[128, 16]\"\n",
       "  del t309\n",
       "  [t312, t314] = nvFusion62(t310)\n",
       "    # t312 = prims.gt(t310, 0.0)  # t312: \"cuda:0 b8[128, 16]\"\n",
       "    # t314 = prims.where(t312, t310, 0.0)  # t314: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t312 = offload_to_cpu(t312)  # offloaded_t312: \"cpu b8[128, 16]\"\n",
       "  del t312\n",
       "  del t310\n",
       "  t315 = torch.nn.functional.linear(t314, t_fcs_63_weight, t_fcs_63_bias)  # t315: \"cuda:0 f32[128, 16]\"\n",
       "    # t315 = ltorch.linear(t314, t_fcs_63_weight, t_fcs_63_bias)  # t315: \"cuda:0 f32[128, 16]\"\n",
       "      # t315 = prims.linear(t314, t_fcs_63_weight, t_fcs_63_bias)  # t315: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t314 = offload_to_cpu(t314)  # offloaded_t314: \"cpu f32[128, 16]\"\n",
       "  del t314\n",
       "  [t317, t319] = nvFusion63(t315)\n",
       "    # t317 = prims.gt(t315, 0.0)  # t317: \"cuda:0 b8[128, 16]\"\n",
       "    # t319 = prims.where(t317, t315, 0.0)  # t319: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t317 = offload_to_cpu(t317)  # offloaded_t317: \"cpu b8[128, 16]\"\n",
       "  del t317\n",
       "  del t315\n",
       "  t320 = torch.nn.functional.linear(t319, t_fcs_64_weight, t_fcs_64_bias)  # t320: \"cuda:0 f32[128, 16]\"\n",
       "    # t320 = ltorch.linear(t319, t_fcs_64_weight, t_fcs_64_bias)  # t320: \"cuda:0 f32[128, 16]\"\n",
       "      # t320 = prims.linear(t319, t_fcs_64_weight, t_fcs_64_bias)  # t320: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t319 = offload_to_cpu(t319)  # offloaded_t319: \"cpu f32[128, 16]\"\n",
       "  del t319\n",
       "  [t322, t324] = nvFusion64(t320)\n",
       "    # t322 = prims.gt(t320, 0.0)  # t322: \"cuda:0 b8[128, 16]\"\n",
       "    # t324 = prims.where(t322, t320, 0.0)  # t324: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t322 = offload_to_cpu(t322)  # offloaded_t322: \"cpu b8[128, 16]\"\n",
       "  del t322\n",
       "  del t320\n",
       "  t325 = torch.nn.functional.linear(t324, t_fcs_65_weight, t_fcs_65_bias)  # t325: \"cuda:0 f32[128, 16]\"\n",
       "    # t325 = ltorch.linear(t324, t_fcs_65_weight, t_fcs_65_bias)  # t325: \"cuda:0 f32[128, 16]\"\n",
       "      # t325 = prims.linear(t324, t_fcs_65_weight, t_fcs_65_bias)  # t325: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t324 = offload_to_cpu(t324)  # offloaded_t324: \"cpu f32[128, 16]\"\n",
       "  del t324\n",
       "  [t327, t329] = nvFusion65(t325)\n",
       "    # t327 = prims.gt(t325, 0.0)  # t327: \"cuda:0 b8[128, 16]\"\n",
       "    # t329 = prims.where(t327, t325, 0.0)  # t329: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t327 = offload_to_cpu(t327)  # offloaded_t327: \"cpu b8[128, 16]\"\n",
       "  del t327\n",
       "  del t325\n",
       "  t330 = torch.nn.functional.linear(t329, t_fcs_66_weight, t_fcs_66_bias)  # t330: \"cuda:0 f32[128, 16]\"\n",
       "    # t330 = ltorch.linear(t329, t_fcs_66_weight, t_fcs_66_bias)  # t330: \"cuda:0 f32[128, 16]\"\n",
       "      # t330 = prims.linear(t329, t_fcs_66_weight, t_fcs_66_bias)  # t330: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t329 = offload_to_cpu(t329)  # offloaded_t329: \"cpu f32[128, 16]\"\n",
       "  del t329\n",
       "  [t332, t334] = nvFusion66(t330)\n",
       "    # t332 = prims.gt(t330, 0.0)  # t332: \"cuda:0 b8[128, 16]\"\n",
       "    # t334 = prims.where(t332, t330, 0.0)  # t334: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t332 = offload_to_cpu(t332)  # offloaded_t332: \"cpu b8[128, 16]\"\n",
       "  del t332\n",
       "  del t330\n",
       "  t335 = torch.nn.functional.linear(t334, t_fcs_67_weight, t_fcs_67_bias)  # t335: \"cuda:0 f32[128, 16]\"\n",
       "    # t335 = ltorch.linear(t334, t_fcs_67_weight, t_fcs_67_bias)  # t335: \"cuda:0 f32[128, 16]\"\n",
       "      # t335 = prims.linear(t334, t_fcs_67_weight, t_fcs_67_bias)  # t335: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t334 = offload_to_cpu(t334)  # offloaded_t334: \"cpu f32[128, 16]\"\n",
       "  del t334\n",
       "  [t337, t339] = nvFusion67(t335)\n",
       "    # t337 = prims.gt(t335, 0.0)  # t337: \"cuda:0 b8[128, 16]\"\n",
       "    # t339 = prims.where(t337, t335, 0.0)  # t339: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t337 = offload_to_cpu(t337)  # offloaded_t337: \"cpu b8[128, 16]\"\n",
       "  del t337\n",
       "  del t335\n",
       "  t340 = torch.nn.functional.linear(t339, t_fcs_68_weight, t_fcs_68_bias)  # t340: \"cuda:0 f32[128, 16]\"\n",
       "    # t340 = ltorch.linear(t339, t_fcs_68_weight, t_fcs_68_bias)  # t340: \"cuda:0 f32[128, 16]\"\n",
       "      # t340 = prims.linear(t339, t_fcs_68_weight, t_fcs_68_bias)  # t340: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t339 = offload_to_cpu(t339)  # offloaded_t339: \"cpu f32[128, 16]\"\n",
       "  del t339\n",
       "  [t342, t344] = nvFusion68(t340)\n",
       "    # t342 = prims.gt(t340, 0.0)  # t342: \"cuda:0 b8[128, 16]\"\n",
       "    # t344 = prims.where(t342, t340, 0.0)  # t344: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t342 = offload_to_cpu(t342)  # offloaded_t342: \"cpu b8[128, 16]\"\n",
       "  del t342\n",
       "  del t340\n",
       "  t345 = torch.nn.functional.linear(t344, t_fcs_69_weight, t_fcs_69_bias)  # t345: \"cuda:0 f32[128, 16]\"\n",
       "    # t345 = ltorch.linear(t344, t_fcs_69_weight, t_fcs_69_bias)  # t345: \"cuda:0 f32[128, 16]\"\n",
       "      # t345 = prims.linear(t344, t_fcs_69_weight, t_fcs_69_bias)  # t345: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t344 = offload_to_cpu(t344)  # offloaded_t344: \"cpu f32[128, 16]\"\n",
       "  del t344\n",
       "  [t347, t349] = nvFusion69(t345)\n",
       "    # t347 = prims.gt(t345, 0.0)  # t347: \"cuda:0 b8[128, 16]\"\n",
       "    # t349 = prims.where(t347, t345, 0.0)  # t349: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t347 = offload_to_cpu(t347)  # offloaded_t347: \"cpu b8[128, 16]\"\n",
       "  del t347\n",
       "  del t345\n",
       "  t350 = torch.nn.functional.linear(t349, t_fcs_70_weight, t_fcs_70_bias)  # t350: \"cuda:0 f32[128, 16]\"\n",
       "    # t350 = ltorch.linear(t349, t_fcs_70_weight, t_fcs_70_bias)  # t350: \"cuda:0 f32[128, 16]\"\n",
       "      # t350 = prims.linear(t349, t_fcs_70_weight, t_fcs_70_bias)  # t350: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t349 = offload_to_cpu(t349)  # offloaded_t349: \"cpu f32[128, 16]\"\n",
       "  del t349\n",
       "  [t352, t354] = nvFusion70(t350)\n",
       "    # t352 = prims.gt(t350, 0.0)  # t352: \"cuda:0 b8[128, 16]\"\n",
       "    # t354 = prims.where(t352, t350, 0.0)  # t354: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t352 = offload_to_cpu(t352)  # offloaded_t352: \"cpu b8[128, 16]\"\n",
       "  del t352\n",
       "  del t350\n",
       "  t355 = torch.nn.functional.linear(t354, t_fcs_71_weight, t_fcs_71_bias)  # t355: \"cuda:0 f32[128, 16]\"\n",
       "    # t355 = ltorch.linear(t354, t_fcs_71_weight, t_fcs_71_bias)  # t355: \"cuda:0 f32[128, 16]\"\n",
       "      # t355 = prims.linear(t354, t_fcs_71_weight, t_fcs_71_bias)  # t355: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t354 = offload_to_cpu(t354)  # offloaded_t354: \"cpu f32[128, 16]\"\n",
       "  del t354\n",
       "  [t357, t359] = nvFusion71(t355)\n",
       "    # t357 = prims.gt(t355, 0.0)  # t357: \"cuda:0 b8[128, 16]\"\n",
       "    # t359 = prims.where(t357, t355, 0.0)  # t359: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t357 = offload_to_cpu(t357)  # offloaded_t357: \"cpu b8[128, 16]\"\n",
       "  del t357\n",
       "  del t355\n",
       "  t360 = torch.nn.functional.linear(t359, t_fcs_72_weight, t_fcs_72_bias)  # t360: \"cuda:0 f32[128, 16]\"\n",
       "    # t360 = ltorch.linear(t359, t_fcs_72_weight, t_fcs_72_bias)  # t360: \"cuda:0 f32[128, 16]\"\n",
       "      # t360 = prims.linear(t359, t_fcs_72_weight, t_fcs_72_bias)  # t360: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t359 = offload_to_cpu(t359)  # offloaded_t359: \"cpu f32[128, 16]\"\n",
       "  del t359\n",
       "  [t362, t364] = nvFusion72(t360)\n",
       "    # t362 = prims.gt(t360, 0.0)  # t362: \"cuda:0 b8[128, 16]\"\n",
       "    # t364 = prims.where(t362, t360, 0.0)  # t364: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t362 = offload_to_cpu(t362)  # offloaded_t362: \"cpu b8[128, 16]\"\n",
       "  del t362\n",
       "  del t360\n",
       "  t365 = torch.nn.functional.linear(t364, t_fcs_73_weight, t_fcs_73_bias)  # t365: \"cuda:0 f32[128, 16]\"\n",
       "    # t365 = ltorch.linear(t364, t_fcs_73_weight, t_fcs_73_bias)  # t365: \"cuda:0 f32[128, 16]\"\n",
       "      # t365 = prims.linear(t364, t_fcs_73_weight, t_fcs_73_bias)  # t365: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t364 = offload_to_cpu(t364)  # offloaded_t364: \"cpu f32[128, 16]\"\n",
       "  del t364\n",
       "  [t367, t369] = nvFusion73(t365)\n",
       "    # t367 = prims.gt(t365, 0.0)  # t367: \"cuda:0 b8[128, 16]\"\n",
       "    # t369 = prims.where(t367, t365, 0.0)  # t369: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t367 = offload_to_cpu(t367)  # offloaded_t367: \"cpu b8[128, 16]\"\n",
       "  del t367\n",
       "  del t365\n",
       "  t370 = torch.nn.functional.linear(t369, t_fcs_74_weight, t_fcs_74_bias)  # t370: \"cuda:0 f32[128, 16]\"\n",
       "    # t370 = ltorch.linear(t369, t_fcs_74_weight, t_fcs_74_bias)  # t370: \"cuda:0 f32[128, 16]\"\n",
       "      # t370 = prims.linear(t369, t_fcs_74_weight, t_fcs_74_bias)  # t370: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t369 = offload_to_cpu(t369)  # offloaded_t369: \"cpu f32[128, 16]\"\n",
       "  del t369\n",
       "  [t372, t374] = nvFusion74(t370)\n",
       "    # t372 = prims.gt(t370, 0.0)  # t372: \"cuda:0 b8[128, 16]\"\n",
       "    # t374 = prims.where(t372, t370, 0.0)  # t374: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t372 = offload_to_cpu(t372)  # offloaded_t372: \"cpu b8[128, 16]\"\n",
       "  del t372\n",
       "  del t370\n",
       "  t375 = torch.nn.functional.linear(t374, t_fcs_75_weight, t_fcs_75_bias)  # t375: \"cuda:0 f32[128, 16]\"\n",
       "    # t375 = ltorch.linear(t374, t_fcs_75_weight, t_fcs_75_bias)  # t375: \"cuda:0 f32[128, 16]\"\n",
       "      # t375 = prims.linear(t374, t_fcs_75_weight, t_fcs_75_bias)  # t375: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t374 = offload_to_cpu(t374)  # offloaded_t374: \"cpu f32[128, 16]\"\n",
       "  del t374\n",
       "  [t377, t379] = nvFusion75(t375)\n",
       "    # t377 = prims.gt(t375, 0.0)  # t377: \"cuda:0 b8[128, 16]\"\n",
       "    # t379 = prims.where(t377, t375, 0.0)  # t379: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t377 = offload_to_cpu(t377)  # offloaded_t377: \"cpu b8[128, 16]\"\n",
       "  del t377\n",
       "  del t375\n",
       "  t380 = torch.nn.functional.linear(t379, t_fcs_76_weight, t_fcs_76_bias)  # t380: \"cuda:0 f32[128, 16]\"\n",
       "    # t380 = ltorch.linear(t379, t_fcs_76_weight, t_fcs_76_bias)  # t380: \"cuda:0 f32[128, 16]\"\n",
       "      # t380 = prims.linear(t379, t_fcs_76_weight, t_fcs_76_bias)  # t380: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t379 = offload_to_cpu(t379)  # offloaded_t379: \"cpu f32[128, 16]\"\n",
       "  del t379\n",
       "  [t382, t384] = nvFusion76(t380)\n",
       "    # t382 = prims.gt(t380, 0.0)  # t382: \"cuda:0 b8[128, 16]\"\n",
       "    # t384 = prims.where(t382, t380, 0.0)  # t384: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t382 = offload_to_cpu(t382)  # offloaded_t382: \"cpu b8[128, 16]\"\n",
       "  del t382\n",
       "  del t380\n",
       "  t385 = torch.nn.functional.linear(t384, t_fcs_77_weight, t_fcs_77_bias)  # t385: \"cuda:0 f32[128, 16]\"\n",
       "    # t385 = ltorch.linear(t384, t_fcs_77_weight, t_fcs_77_bias)  # t385: \"cuda:0 f32[128, 16]\"\n",
       "      # t385 = prims.linear(t384, t_fcs_77_weight, t_fcs_77_bias)  # t385: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t384 = offload_to_cpu(t384)  # offloaded_t384: \"cpu f32[128, 16]\"\n",
       "  del t384\n",
       "  [t387, t389] = nvFusion77(t385)\n",
       "    # t387 = prims.gt(t385, 0.0)  # t387: \"cuda:0 b8[128, 16]\"\n",
       "    # t389 = prims.where(t387, t385, 0.0)  # t389: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t387 = offload_to_cpu(t387)  # offloaded_t387: \"cpu b8[128, 16]\"\n",
       "  del t387\n",
       "  del t385\n",
       "  t390 = torch.nn.functional.linear(t389, t_fcs_78_weight, t_fcs_78_bias)  # t390: \"cuda:0 f32[128, 16]\"\n",
       "    # t390 = ltorch.linear(t389, t_fcs_78_weight, t_fcs_78_bias)  # t390: \"cuda:0 f32[128, 16]\"\n",
       "      # t390 = prims.linear(t389, t_fcs_78_weight, t_fcs_78_bias)  # t390: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t389 = offload_to_cpu(t389)  # offloaded_t389: \"cpu f32[128, 16]\"\n",
       "  del t389\n",
       "  [t392, t394] = nvFusion78(t390)\n",
       "    # t392 = prims.gt(t390, 0.0)  # t392: \"cuda:0 b8[128, 16]\"\n",
       "    # t394 = prims.where(t392, t390, 0.0)  # t394: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t392 = offload_to_cpu(t392)  # offloaded_t392: \"cpu b8[128, 16]\"\n",
       "  del t392\n",
       "  del t390\n",
       "  t395 = torch.nn.functional.linear(t394, t_fcs_79_weight, t_fcs_79_bias)  # t395: \"cuda:0 f32[128, 16]\"\n",
       "    # t395 = ltorch.linear(t394, t_fcs_79_weight, t_fcs_79_bias)  # t395: \"cuda:0 f32[128, 16]\"\n",
       "      # t395 = prims.linear(t394, t_fcs_79_weight, t_fcs_79_bias)  # t395: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t394 = offload_to_cpu(t394)  # offloaded_t394: \"cpu f32[128, 16]\"\n",
       "  del t394\n",
       "  [t397, t399] = nvFusion79(t395)\n",
       "    # t397 = prims.gt(t395, 0.0)  # t397: \"cuda:0 b8[128, 16]\"\n",
       "    # t399 = prims.where(t397, t395, 0.0)  # t399: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t397 = offload_to_cpu(t397)  # offloaded_t397: \"cpu b8[128, 16]\"\n",
       "  del t397\n",
       "  del t395\n",
       "  t400 = torch.nn.functional.linear(t399, t_fcs_80_weight, t_fcs_80_bias)  # t400: \"cuda:0 f32[128, 16]\"\n",
       "    # t400 = ltorch.linear(t399, t_fcs_80_weight, t_fcs_80_bias)  # t400: \"cuda:0 f32[128, 16]\"\n",
       "      # t400 = prims.linear(t399, t_fcs_80_weight, t_fcs_80_bias)  # t400: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t399 = offload_to_cpu(t399)  # offloaded_t399: \"cpu f32[128, 16]\"\n",
       "  del t399\n",
       "  [t402, t404] = nvFusion80(t400)\n",
       "    # t402 = prims.gt(t400, 0.0)  # t402: \"cuda:0 b8[128, 16]\"\n",
       "    # t404 = prims.where(t402, t400, 0.0)  # t404: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t402 = offload_to_cpu(t402)  # offloaded_t402: \"cpu b8[128, 16]\"\n",
       "  del t402\n",
       "  del t400\n",
       "  t405 = torch.nn.functional.linear(t404, t_fcs_81_weight, t_fcs_81_bias)  # t405: \"cuda:0 f32[128, 16]\"\n",
       "    # t405 = ltorch.linear(t404, t_fcs_81_weight, t_fcs_81_bias)  # t405: \"cuda:0 f32[128, 16]\"\n",
       "      # t405 = prims.linear(t404, t_fcs_81_weight, t_fcs_81_bias)  # t405: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t404 = offload_to_cpu(t404)  # offloaded_t404: \"cpu f32[128, 16]\"\n",
       "  del t404\n",
       "  [t407, t409] = nvFusion81(t405)\n",
       "    # t407 = prims.gt(t405, 0.0)  # t407: \"cuda:0 b8[128, 16]\"\n",
       "    # t409 = prims.where(t407, t405, 0.0)  # t409: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t407 = offload_to_cpu(t407)  # offloaded_t407: \"cpu b8[128, 16]\"\n",
       "  del t407\n",
       "  del t405\n",
       "  t410 = torch.nn.functional.linear(t409, t_fcs_82_weight, t_fcs_82_bias)  # t410: \"cuda:0 f32[128, 16]\"\n",
       "    # t410 = ltorch.linear(t409, t_fcs_82_weight, t_fcs_82_bias)  # t410: \"cuda:0 f32[128, 16]\"\n",
       "      # t410 = prims.linear(t409, t_fcs_82_weight, t_fcs_82_bias)  # t410: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t409 = offload_to_cpu(t409)  # offloaded_t409: \"cpu f32[128, 16]\"\n",
       "  del t409\n",
       "  [t412, t414] = nvFusion82(t410)\n",
       "    # t412 = prims.gt(t410, 0.0)  # t412: \"cuda:0 b8[128, 16]\"\n",
       "    # t414 = prims.where(t412, t410, 0.0)  # t414: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t412 = offload_to_cpu(t412)  # offloaded_t412: \"cpu b8[128, 16]\"\n",
       "  del t412\n",
       "  del t410\n",
       "  t415 = torch.nn.functional.linear(t414, t_fcs_83_weight, t_fcs_83_bias)  # t415: \"cuda:0 f32[128, 16]\"\n",
       "    # t415 = ltorch.linear(t414, t_fcs_83_weight, t_fcs_83_bias)  # t415: \"cuda:0 f32[128, 16]\"\n",
       "      # t415 = prims.linear(t414, t_fcs_83_weight, t_fcs_83_bias)  # t415: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t414 = offload_to_cpu(t414)  # offloaded_t414: \"cpu f32[128, 16]\"\n",
       "  del t414\n",
       "  [t417, t419] = nvFusion83(t415)\n",
       "    # t417 = prims.gt(t415, 0.0)  # t417: \"cuda:0 b8[128, 16]\"\n",
       "    # t419 = prims.where(t417, t415, 0.0)  # t419: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t417 = offload_to_cpu(t417)  # offloaded_t417: \"cpu b8[128, 16]\"\n",
       "  del t417\n",
       "  del t415\n",
       "  t420 = torch.nn.functional.linear(t419, t_fcs_84_weight, t_fcs_84_bias)  # t420: \"cuda:0 f32[128, 16]\"\n",
       "    # t420 = ltorch.linear(t419, t_fcs_84_weight, t_fcs_84_bias)  # t420: \"cuda:0 f32[128, 16]\"\n",
       "      # t420 = prims.linear(t419, t_fcs_84_weight, t_fcs_84_bias)  # t420: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t419 = offload_to_cpu(t419)  # offloaded_t419: \"cpu f32[128, 16]\"\n",
       "  del t419\n",
       "  [t422, t424] = nvFusion84(t420)\n",
       "    # t422 = prims.gt(t420, 0.0)  # t422: \"cuda:0 b8[128, 16]\"\n",
       "    # t424 = prims.where(t422, t420, 0.0)  # t424: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t422 = offload_to_cpu(t422)  # offloaded_t422: \"cpu b8[128, 16]\"\n",
       "  del t422\n",
       "  del t420\n",
       "  t425 = torch.nn.functional.linear(t424, t_fcs_85_weight, t_fcs_85_bias)  # t425: \"cuda:0 f32[128, 16]\"\n",
       "    # t425 = ltorch.linear(t424, t_fcs_85_weight, t_fcs_85_bias)  # t425: \"cuda:0 f32[128, 16]\"\n",
       "      # t425 = prims.linear(t424, t_fcs_85_weight, t_fcs_85_bias)  # t425: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t424 = offload_to_cpu(t424)  # offloaded_t424: \"cpu f32[128, 16]\"\n",
       "  del t424\n",
       "  [t427, t429] = nvFusion85(t425)\n",
       "    # t427 = prims.gt(t425, 0.0)  # t427: \"cuda:0 b8[128, 16]\"\n",
       "    # t429 = prims.where(t427, t425, 0.0)  # t429: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t427 = offload_to_cpu(t427)  # offloaded_t427: \"cpu b8[128, 16]\"\n",
       "  del t427\n",
       "  del t425\n",
       "  t430 = torch.nn.functional.linear(t429, t_fcs_86_weight, t_fcs_86_bias)  # t430: \"cuda:0 f32[128, 16]\"\n",
       "    # t430 = ltorch.linear(t429, t_fcs_86_weight, t_fcs_86_bias)  # t430: \"cuda:0 f32[128, 16]\"\n",
       "      # t430 = prims.linear(t429, t_fcs_86_weight, t_fcs_86_bias)  # t430: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t429 = offload_to_cpu(t429)  # offloaded_t429: \"cpu f32[128, 16]\"\n",
       "  del t429\n",
       "  [t432, t434] = nvFusion86(t430)\n",
       "    # t432 = prims.gt(t430, 0.0)  # t432: \"cuda:0 b8[128, 16]\"\n",
       "    # t434 = prims.where(t432, t430, 0.0)  # t434: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t432 = offload_to_cpu(t432)  # offloaded_t432: \"cpu b8[128, 16]\"\n",
       "  del t432\n",
       "  del t430\n",
       "  t435 = torch.nn.functional.linear(t434, t_fcs_87_weight, t_fcs_87_bias)  # t435: \"cuda:0 f32[128, 16]\"\n",
       "    # t435 = ltorch.linear(t434, t_fcs_87_weight, t_fcs_87_bias)  # t435: \"cuda:0 f32[128, 16]\"\n",
       "      # t435 = prims.linear(t434, t_fcs_87_weight, t_fcs_87_bias)  # t435: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t434 = offload_to_cpu(t434)  # offloaded_t434: \"cpu f32[128, 16]\"\n",
       "  del t434\n",
       "  [t437, t439] = nvFusion87(t435)\n",
       "    # t437 = prims.gt(t435, 0.0)  # t437: \"cuda:0 b8[128, 16]\"\n",
       "    # t439 = prims.where(t437, t435, 0.0)  # t439: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t437 = offload_to_cpu(t437)  # offloaded_t437: \"cpu b8[128, 16]\"\n",
       "  del t437\n",
       "  del t435\n",
       "  t440 = torch.nn.functional.linear(t439, t_fcs_88_weight, t_fcs_88_bias)  # t440: \"cuda:0 f32[128, 16]\"\n",
       "    # t440 = ltorch.linear(t439, t_fcs_88_weight, t_fcs_88_bias)  # t440: \"cuda:0 f32[128, 16]\"\n",
       "      # t440 = prims.linear(t439, t_fcs_88_weight, t_fcs_88_bias)  # t440: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t439 = offload_to_cpu(t439)  # offloaded_t439: \"cpu f32[128, 16]\"\n",
       "  del t439\n",
       "  [t442, t444] = nvFusion88(t440)\n",
       "    # t442 = prims.gt(t440, 0.0)  # t442: \"cuda:0 b8[128, 16]\"\n",
       "    # t444 = prims.where(t442, t440, 0.0)  # t444: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t442 = offload_to_cpu(t442)  # offloaded_t442: \"cpu b8[128, 16]\"\n",
       "  del t442\n",
       "  del t440\n",
       "  t445 = torch.nn.functional.linear(t444, t_fcs_89_weight, t_fcs_89_bias)  # t445: \"cuda:0 f32[128, 16]\"\n",
       "    # t445 = ltorch.linear(t444, t_fcs_89_weight, t_fcs_89_bias)  # t445: \"cuda:0 f32[128, 16]\"\n",
       "      # t445 = prims.linear(t444, t_fcs_89_weight, t_fcs_89_bias)  # t445: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t444 = offload_to_cpu(t444)  # offloaded_t444: \"cpu f32[128, 16]\"\n",
       "  del t444\n",
       "  [t447, t449] = nvFusion89(t445)\n",
       "    # t447 = prims.gt(t445, 0.0)  # t447: \"cuda:0 b8[128, 16]\"\n",
       "    # t449 = prims.where(t447, t445, 0.0)  # t449: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t447 = offload_to_cpu(t447)  # offloaded_t447: \"cpu b8[128, 16]\"\n",
       "  del t447\n",
       "  del t445\n",
       "  t450 = torch.nn.functional.linear(t449, t_fcs_90_weight, t_fcs_90_bias)  # t450: \"cuda:0 f32[128, 16]\"\n",
       "    # t450 = ltorch.linear(t449, t_fcs_90_weight, t_fcs_90_bias)  # t450: \"cuda:0 f32[128, 16]\"\n",
       "      # t450 = prims.linear(t449, t_fcs_90_weight, t_fcs_90_bias)  # t450: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t449 = offload_to_cpu(t449)  # offloaded_t449: \"cpu f32[128, 16]\"\n",
       "  del t449\n",
       "  [t452, t454] = nvFusion90(t450)\n",
       "    # t452 = prims.gt(t450, 0.0)  # t452: \"cuda:0 b8[128, 16]\"\n",
       "    # t454 = prims.where(t452, t450, 0.0)  # t454: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t452 = offload_to_cpu(t452)  # offloaded_t452: \"cpu b8[128, 16]\"\n",
       "  del t452\n",
       "  del t450\n",
       "  t455 = torch.nn.functional.linear(t454, t_fcs_91_weight, t_fcs_91_bias)  # t455: \"cuda:0 f32[128, 16]\"\n",
       "    # t455 = ltorch.linear(t454, t_fcs_91_weight, t_fcs_91_bias)  # t455: \"cuda:0 f32[128, 16]\"\n",
       "      # t455 = prims.linear(t454, t_fcs_91_weight, t_fcs_91_bias)  # t455: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t454 = offload_to_cpu(t454)  # offloaded_t454: \"cpu f32[128, 16]\"\n",
       "  del t454\n",
       "  [t457, t459] = nvFusion91(t455)\n",
       "    # t457 = prims.gt(t455, 0.0)  # t457: \"cuda:0 b8[128, 16]\"\n",
       "    # t459 = prims.where(t457, t455, 0.0)  # t459: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t457 = offload_to_cpu(t457)  # offloaded_t457: \"cpu b8[128, 16]\"\n",
       "  del t457\n",
       "  del t455\n",
       "  t460 = torch.nn.functional.linear(t459, t_fcs_92_weight, t_fcs_92_bias)  # t460: \"cuda:0 f32[128, 16]\"\n",
       "    # t460 = ltorch.linear(t459, t_fcs_92_weight, t_fcs_92_bias)  # t460: \"cuda:0 f32[128, 16]\"\n",
       "      # t460 = prims.linear(t459, t_fcs_92_weight, t_fcs_92_bias)  # t460: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t459 = offload_to_cpu(t459)  # offloaded_t459: \"cpu f32[128, 16]\"\n",
       "  del t459\n",
       "  [t462, t464] = nvFusion92(t460)\n",
       "    # t462 = prims.gt(t460, 0.0)  # t462: \"cuda:0 b8[128, 16]\"\n",
       "    # t464 = prims.where(t462, t460, 0.0)  # t464: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t462 = offload_to_cpu(t462)  # offloaded_t462: \"cpu b8[128, 16]\"\n",
       "  del t462\n",
       "  del t460\n",
       "  t465 = torch.nn.functional.linear(t464, t_fcs_93_weight, t_fcs_93_bias)  # t465: \"cuda:0 f32[128, 16]\"\n",
       "    # t465 = ltorch.linear(t464, t_fcs_93_weight, t_fcs_93_bias)  # t465: \"cuda:0 f32[128, 16]\"\n",
       "      # t465 = prims.linear(t464, t_fcs_93_weight, t_fcs_93_bias)  # t465: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t464 = offload_to_cpu(t464)  # offloaded_t464: \"cpu f32[128, 16]\"\n",
       "  del t464\n",
       "  [t467, t469] = nvFusion93(t465)\n",
       "    # t467 = prims.gt(t465, 0.0)  # t467: \"cuda:0 b8[128, 16]\"\n",
       "    # t469 = prims.where(t467, t465, 0.0)  # t469: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t467 = offload_to_cpu(t467)  # offloaded_t467: \"cpu b8[128, 16]\"\n",
       "  del t467\n",
       "  del t465\n",
       "  t470 = torch.nn.functional.linear(t469, t_fcs_94_weight, t_fcs_94_bias)  # t470: \"cuda:0 f32[128, 16]\"\n",
       "    # t470 = ltorch.linear(t469, t_fcs_94_weight, t_fcs_94_bias)  # t470: \"cuda:0 f32[128, 16]\"\n",
       "      # t470 = prims.linear(t469, t_fcs_94_weight, t_fcs_94_bias)  # t470: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t469 = offload_to_cpu(t469)  # offloaded_t469: \"cpu f32[128, 16]\"\n",
       "  del t469\n",
       "  [t472, t474] = nvFusion94(t470)\n",
       "    # t472 = prims.gt(t470, 0.0)  # t472: \"cuda:0 b8[128, 16]\"\n",
       "    # t474 = prims.where(t472, t470, 0.0)  # t474: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t472 = offload_to_cpu(t472)  # offloaded_t472: \"cpu b8[128, 16]\"\n",
       "  del t472\n",
       "  del t470\n",
       "  t475 = torch.nn.functional.linear(t474, t_fcs_95_weight, t_fcs_95_bias)  # t475: \"cuda:0 f32[128, 16]\"\n",
       "    # t475 = ltorch.linear(t474, t_fcs_95_weight, t_fcs_95_bias)  # t475: \"cuda:0 f32[128, 16]\"\n",
       "      # t475 = prims.linear(t474, t_fcs_95_weight, t_fcs_95_bias)  # t475: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t474 = offload_to_cpu(t474)  # offloaded_t474: \"cpu f32[128, 16]\"\n",
       "  del t474\n",
       "  [t477, t479] = nvFusion95(t475)\n",
       "    # t477 = prims.gt(t475, 0.0)  # t477: \"cuda:0 b8[128, 16]\"\n",
       "    # t479 = prims.where(t477, t475, 0.0)  # t479: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t477 = offload_to_cpu(t477)  # offloaded_t477: \"cpu b8[128, 16]\"\n",
       "  del t477\n",
       "  del t475\n",
       "  t480 = torch.nn.functional.linear(t479, t_fcs_96_weight, t_fcs_96_bias)  # t480: \"cuda:0 f32[128, 16]\"\n",
       "    # t480 = ltorch.linear(t479, t_fcs_96_weight, t_fcs_96_bias)  # t480: \"cuda:0 f32[128, 16]\"\n",
       "      # t480 = prims.linear(t479, t_fcs_96_weight, t_fcs_96_bias)  # t480: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t479 = offload_to_cpu(t479)  # offloaded_t479: \"cpu f32[128, 16]\"\n",
       "  del t479\n",
       "  [t482, t484] = nvFusion96(t480)\n",
       "    # t482 = prims.gt(t480, 0.0)  # t482: \"cuda:0 b8[128, 16]\"\n",
       "    # t484 = prims.where(t482, t480, 0.0)  # t484: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t482 = offload_to_cpu(t482)  # offloaded_t482: \"cpu b8[128, 16]\"\n",
       "  del t482\n",
       "  del t480\n",
       "  t485 = torch.nn.functional.linear(t484, t_fcs_97_weight, t_fcs_97_bias)  # t485: \"cuda:0 f32[128, 16]\"\n",
       "    # t485 = ltorch.linear(t484, t_fcs_97_weight, t_fcs_97_bias)  # t485: \"cuda:0 f32[128, 16]\"\n",
       "      # t485 = prims.linear(t484, t_fcs_97_weight, t_fcs_97_bias)  # t485: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t484 = offload_to_cpu(t484)  # offloaded_t484: \"cpu f32[128, 16]\"\n",
       "  del t484\n",
       "  [t487, t489] = nvFusion97(t485)\n",
       "    # t487 = prims.gt(t485, 0.0)  # t487: \"cuda:0 b8[128, 16]\"\n",
       "    # t489 = prims.where(t487, t485, 0.0)  # t489: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t487 = offload_to_cpu(t487)  # offloaded_t487: \"cpu b8[128, 16]\"\n",
       "  del t487\n",
       "  del t485\n",
       "  t490 = torch.nn.functional.linear(t489, t_fcs_98_weight, t_fcs_98_bias)  # t490: \"cuda:0 f32[128, 16]\"\n",
       "    # t490 = ltorch.linear(t489, t_fcs_98_weight, t_fcs_98_bias)  # t490: \"cuda:0 f32[128, 16]\"\n",
       "      # t490 = prims.linear(t489, t_fcs_98_weight, t_fcs_98_bias)  # t490: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t489 = offload_to_cpu(t489)  # offloaded_t489: \"cpu f32[128, 16]\"\n",
       "  del t489\n",
       "  [t492, t494] = nvFusion98(t490)\n",
       "    # t492 = prims.gt(t490, 0.0)  # t492: \"cuda:0 b8[128, 16]\"\n",
       "    # t494 = prims.where(t492, t490, 0.0)  # t494: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t492 = offload_to_cpu(t492)  # offloaded_t492: \"cpu b8[128, 16]\"\n",
       "  del t492\n",
       "  del t490\n",
       "  t495 = torch.nn.functional.linear(t494, t_fcs_99_weight, t_fcs_99_bias)  # t495: \"cuda:0 f32[128, 16]\"\n",
       "    # t495 = ltorch.linear(t494, t_fcs_99_weight, t_fcs_99_bias)  # t495: \"cuda:0 f32[128, 16]\"\n",
       "      # t495 = prims.linear(t494, t_fcs_99_weight, t_fcs_99_bias)  # t495: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t494 = offload_to_cpu(t494)  # offloaded_t494: \"cpu f32[128, 16]\"\n",
       "  del t494\n",
       "  [t497, t499] = nvFusion99(t495)\n",
       "    # t497 = prims.gt(t495, 0.0)  # t497: \"cuda:0 b8[128, 16]\"\n",
       "    # t499 = prims.where(t497, t495, 0.0)  # t499: \"cuda:0 f32[128, 16]\"\n",
       "  # Created by CPU Offloading Transform\n",
       "  offloaded_t497 = offload_to_cpu(t497)  # offloaded_t497: \"cpu b8[128, 16]\"\n",
       "  del t497\n",
       "  del t495\n",
       "  return {'output': t499, 'flat_args': [x, t_fcs_0_bias, t_fcs_0_weight, t_fcs_1_bias, t_fcs_1_weight, t_fcs_2_bias, t_fcs_2_weight, t_fcs_3_bias, t_fcs_3_weight, t_fcs_4_bias, t_fcs_4_weight, t_fcs_5_bias, t_fcs_5_weight, t_fcs_6_bias, t_fcs_6_weight, t_fcs_7_bias, t_fcs_7_weight, t_fcs_8_bias, t_fcs_8_weight, t_fcs_9_bias, t_fcs_9_weight, t_fcs_10_bias, t_fcs_10_weight, t_fcs_11_bias, t_fcs_11_weight, t_fcs_12_bias, t_fcs_12_weight, t_fcs_13_bias, t_fcs_13_weight, t_fcs_14_bias, t_fcs_14_weight, t_fcs_15_bias, t_fcs_15_weight, t_fcs_16_bias, t_fcs_16_weight, t_fcs_17_bias, t_fcs_17_weight, t_fcs_18_bias, t_fcs_18_weight, t_fcs_19_bias, t_fcs_19_weight, t_fcs_20_bias, t_fcs_20_weight, t_fcs_21_bias, t_fcs_21_weight, t_fcs_22_bias, t_fcs_22_weight, t_fcs_23_bias, t_fcs_23_weight, t_fcs_24_bias, t_fcs_24_weight, t_fcs_25_bias, t_fcs_25_weight, t_fcs_26_bias, t_fcs_26_weight, t_fcs_27_bias, t_fcs_27_weight, t_fcs_28_bias, t_fcs_28_weight, t_fcs_29_bias, t_fcs_29_weight, t_fcs_30_bias, t_fcs_30_weight, t_fcs_31_bias, t_fcs_31_weight, t_fcs_32_bias, t_fcs_32_weight, t_fcs_33_bias, t_fcs_33_weight, t_fcs_34_bias, t_fcs_34_weight, t_fcs_35_bias, t_fcs_35_weight, t_fcs_36_bias, t_fcs_36_weight, t_fcs_37_bias, t_fcs_37_weight, t_fcs_38_bias, t_fcs_38_weight, t_fcs_39_bias, t_fcs_39_weight, t_fcs_40_bias, t_fcs_40_weight, t_fcs_41_bias, t_fcs_41_weight, t_fcs_42_bias, t_fcs_42_weight, t_fcs_43_bias, t_fcs_43_weight, t_fcs_44_bias, t_fcs_44_weight, t_fcs_45_bias, t_fcs_45_weight, t_fcs_46_bias, t_fcs_46_weight, t_fcs_47_bias, t_fcs_47_weight, t_fcs_48_bias, t_fcs_48_weight, t_fcs_49_bias, t_fcs_49_weight, t_fcs_50_bias, t_fcs_50_weight, t_fcs_51_bias, t_fcs_51_weight, t_fcs_52_bias, t_fcs_52_weight, t_fcs_53_bias, t_fcs_53_weight, t_fcs_54_bias, t_fcs_54_weight, t_fcs_55_bias, t_fcs_55_weight, t_fcs_56_bias, t_fcs_56_weight, t_fcs_57_bias, t_fcs_57_weight, t_fcs_58_bias, t_fcs_58_weight, t_fcs_59_bias, t_fcs_59_weight, t_fcs_60_bias, t_fcs_60_weight, t_fcs_61_bias, t_fcs_61_weight, t_fcs_62_bias, t_fcs_62_weight, t_fcs_63_bias, t_fcs_63_weight, t_fcs_64_bias, t_fcs_64_weight, t_fcs_65_bias, t_fcs_65_weight, t_fcs_66_bias, t_fcs_66_weight, t_fcs_67_bias, t_fcs_67_weight, t_fcs_68_bias, t_fcs_68_weight, t_fcs_69_bias, t_fcs_69_weight, t_fcs_70_bias, t_fcs_70_weight, t_fcs_71_bias, t_fcs_71_weight, t_fcs_72_bias, t_fcs_72_weight, t_fcs_73_bias, t_fcs_73_weight, t_fcs_74_bias, t_fcs_74_weight, t_fcs_75_bias, t_fcs_75_weight, t_fcs_76_bias, t_fcs_76_weight, t_fcs_77_bias, t_fcs_77_weight, t_fcs_78_bias, t_fcs_78_weight, t_fcs_79_bias, t_fcs_79_weight, t_fcs_80_bias, t_fcs_80_weight, t_fcs_81_bias, t_fcs_81_weight, t_fcs_82_bias, t_fcs_82_weight, t_fcs_83_bias, t_fcs_83_weight, t_fcs_84_bias, t_fcs_84_weight, t_fcs_85_bias, t_fcs_85_weight, t_fcs_86_bias, t_fcs_86_weight, t_fcs_87_bias, t_fcs_87_weight, t_fcs_88_bias, t_fcs_88_weight, t_fcs_89_bias, t_fcs_89_weight, t_fcs_90_bias, t_fcs_90_weight, t_fcs_91_bias, t_fcs_91_weight, t_fcs_92_bias, t_fcs_92_weight, t_fcs_93_bias, t_fcs_93_weight, t_fcs_94_bias, t_fcs_94_weight, t_fcs_95_bias, t_fcs_95_weight, t_fcs_96_bias, t_fcs_96_weight, t_fcs_97_bias, t_fcs_97_weight, t_fcs_98_bias, t_fcs_98_weight, t_fcs_99_bias, t_fcs_99_weight], 'flat_output': (t499,)}, ((offloaded_t102, offloaded_t104, offloaded_t107, offloaded_t109, offloaded_t112, offloaded_t114, offloaded_t117, offloaded_t119, offloaded_t12, offloaded_t122, offloaded_t124, offloaded_t127, offloaded_t129, offloaded_t132, offloaded_t134, offloaded_t137, offloaded_t139, offloaded_t14, offloaded_t142, offloaded_t144, offloaded_t147, offloaded_t149, offloaded_t152, offloaded_t154, offloaded_t157, offloaded_t159, offloaded_t162, offloaded_t164, offloaded_t167, offloaded_t169, offloaded_t17, offloaded_t172, offloaded_t174, offloaded_t177, offloaded_t179, offloaded_t182, offloaded_t184, offloaded_t187, offloaded_t189, offloaded_t19, offloaded_t192, offloaded_t194, offloaded_t197, offloaded_t199, offloaded_t2, offloaded_t202, offloaded_t204, offloaded_t207, offloaded_t209, offloaded_t212, offloaded_t214, offloaded_t217, offloaded_t219, offloaded_t22, offloaded_t222, offloaded_t224, offloaded_t227, offloaded_t229, offloaded_t232, offloaded_t234, offloaded_t237, offloaded_t239, offloaded_t24, offloaded_t242, offloaded_t244, offloaded_t247, offloaded_t249, offloaded_t252, offloaded_t254, offloaded_t257, offloaded_t259, offloaded_t262, offloaded_t264, offloaded_t267, offloaded_t269, offloaded_t27, offloaded_t272, offloaded_t274, offloaded_t277, offloaded_t279, offloaded_t282, offloaded_t284, offloaded_t287, offloaded_t289, offloaded_t29, offloaded_t292, offloaded_t294, offloaded_t297, offloaded_t299, offloaded_t302, offloaded_t304, offloaded_t307, offloaded_t309, offloaded_t312, offloaded_t314, offloaded_t317, offloaded_t319, offloaded_t32, offloaded_t322, offloaded_t324, offloaded_t327, offloaded_t329, offloaded_t332, offloaded_t334, offloaded_t337, offloaded_t339, offloaded_t34, offloaded_t342, offloaded_t344, offloaded_t347, offloaded_t349, offloaded_t352, offloaded_t354, offloaded_t357, offloaded_t359, offloaded_t362, offloaded_t364, offloaded_t367, offloaded_t369, offloaded_t37, offloaded_t372, offloaded_t374, offloaded_t377, offloaded_t379, offloaded_t382, offloaded_t384, offloaded_t387, offloaded_t389, offloaded_t39, offloaded_t392, offloaded_t394, offloaded_t397, offloaded_t399, offloaded_t4, offloaded_t402, offloaded_t404, offloaded_t407, offloaded_t409, offloaded_t412, offloaded_t414, offloaded_t417, offloaded_t419, offloaded_t42, offloaded_t422, offloaded_t424, offloaded_t427, offloaded_t429, offloaded_t432, offloaded_t434, offloaded_t437, offloaded_t439, offloaded_t44, offloaded_t442, offloaded_t444, offloaded_t447, offloaded_t449, offloaded_t452, offloaded_t454, offloaded_t457, offloaded_t459, offloaded_t462, offloaded_t464, offloaded_t467, offloaded_t469, offloaded_t47, offloaded_t472, offloaded_t474, offloaded_t477, offloaded_t479, offloaded_t482, offloaded_t484, offloaded_t487, offloaded_t489, offloaded_t49, offloaded_t492, offloaded_t494, offloaded_t497, offloaded_t52, offloaded_t54, offloaded_t57, offloaded_t59, offloaded_t62, offloaded_t64, offloaded_t67, offloaded_t69, offloaded_t7, offloaded_t72, offloaded_t74, offloaded_t77, offloaded_t79, offloaded_t82, offloaded_t84, offloaded_t87, offloaded_t89, offloaded_t9, offloaded_t92, offloaded_t94, offloaded_t97, offloaded_t99, t_fcs_10_weight, t_fcs_11_weight, t_fcs_12_weight, t_fcs_13_weight, t_fcs_14_weight, t_fcs_15_weight, t_fcs_16_weight, t_fcs_17_weight, t_fcs_18_weight, t_fcs_19_weight, t_fcs_1_weight, t_fcs_20_weight, t_fcs_21_weight, t_fcs_22_weight, t_fcs_23_weight, t_fcs_24_weight, t_fcs_25_weight, t_fcs_26_weight, t_fcs_27_weight, t_fcs_28_weight, t_fcs_29_weight, t_fcs_2_weight, t_fcs_30_weight, t_fcs_31_weight, t_fcs_32_weight, t_fcs_33_weight, t_fcs_34_weight, t_fcs_35_weight, t_fcs_36_weight, t_fcs_37_weight, t_fcs_38_weight, t_fcs_39_weight, t_fcs_3_weight, t_fcs_40_weight, t_fcs_41_weight, t_fcs_42_weight, t_fcs_43_weight, t_fcs_44_weight, t_fcs_45_weight, t_fcs_46_weight, t_fcs_47_weight, t_fcs_48_weight, t_fcs_49_weight, t_fcs_4_weight, t_fcs_50_weight, t_fcs_51_weight, t_fcs_52_weight, t_fcs_53_weight, t_fcs_54_weight, t_fcs_55_weight, t_fcs_56_weight, t_fcs_57_weight, t_fcs_58_weight, t_fcs_59_weight, t_fcs_5_weight, t_fcs_60_weight, t_fcs_61_weight, t_fcs_62_weight, t_fcs_63_weight, t_fcs_64_weight, t_fcs_65_weight, t_fcs_66_weight, t_fcs_67_weight, t_fcs_68_weight, t_fcs_69_weight, t_fcs_6_weight, t_fcs_70_weight, t_fcs_71_weight, t_fcs_72_weight, t_fcs_73_weight, t_fcs_74_weight, t_fcs_75_weight, t_fcs_76_weight, t_fcs_77_weight, t_fcs_78_weight, t_fcs_79_weight, t_fcs_7_weight, t_fcs_80_weight, t_fcs_81_weight, t_fcs_82_weight, t_fcs_83_weight, t_fcs_84_weight, t_fcs_85_weight, t_fcs_86_weight, t_fcs_87_weight, t_fcs_88_weight, t_fcs_89_weight, t_fcs_8_weight, t_fcs_90_weight, t_fcs_91_weight, t_fcs_92_weight, t_fcs_93_weight, t_fcs_94_weight, t_fcs_95_weight, t_fcs_96_weight, t_fcs_97_weight, t_fcs_98_weight, t_fcs_99_weight, t_fcs_9_weight, x), ())"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fw_traces[-1]  # Note the calls to `offload_to_cpu` and verify that they after the last usage of the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# Constructed by Delete Last Used (took 5 milliseconds)\n",
       "import torch\n",
       "from thunder.executors.torchex import no_autocast\n",
       "\n",
       "@torch.no_grad()\n",
       "@no_autocast\n",
       "def backward_fn(saved_for_backward, cotangents):\n",
       "  # saved_for_backward: \"Collection\"\n",
       "  C0, _, = saved_for_backward\n",
       "  clear_mutable_collection(saved_for_backward)\n",
       "  del saved_for_backward\n",
       "  offloaded_t102, offloaded_t104, offloaded_t107, offloaded_t109, offloaded_t112, \\\n",
       "  offloaded_t114, offloaded_t117, offloaded_t119, offloaded_t12, offloaded_t122, \\\n",
       "  offloaded_t124, offloaded_t127, offloaded_t129, offloaded_t132, offloaded_t134, \\\n",
       "  offloaded_t137, offloaded_t139, offloaded_t14, offloaded_t142, offloaded_t144, \\\n",
       "  offloaded_t147, offloaded_t149, offloaded_t152, offloaded_t154, offloaded_t157, \\\n",
       "  offloaded_t159, offloaded_t162, offloaded_t164, offloaded_t167, offloaded_t169, \\\n",
       "  offloaded_t17, offloaded_t172, offloaded_t174, offloaded_t177, offloaded_t179, \\\n",
       "  offloaded_t182, offloaded_t184, offloaded_t187, offloaded_t189, offloaded_t19, \\\n",
       "  offloaded_t192, offloaded_t194, offloaded_t197, offloaded_t199, offloaded_t2, \\\n",
       "  offloaded_t202, offloaded_t204, offloaded_t207, offloaded_t209, offloaded_t212, \\\n",
       "  offloaded_t214, offloaded_t217, offloaded_t219, offloaded_t22, offloaded_t222, \\\n",
       "  offloaded_t224, offloaded_t227, offloaded_t229, offloaded_t232, offloaded_t234, \\\n",
       "  offloaded_t237, offloaded_t239, offloaded_t24, offloaded_t242, offloaded_t244, \\\n",
       "  offloaded_t247, offloaded_t249, offloaded_t252, offloaded_t254, offloaded_t257, \\\n",
       "  offloaded_t259, offloaded_t262, offloaded_t264, offloaded_t267, offloaded_t269, \\\n",
       "  offloaded_t27, offloaded_t272, offloaded_t274, offloaded_t277, offloaded_t279, \\\n",
       "  offloaded_t282, offloaded_t284, offloaded_t287, offloaded_t289, offloaded_t29, \\\n",
       "  offloaded_t292, offloaded_t294, offloaded_t297, offloaded_t299, offloaded_t302, \\\n",
       "  offloaded_t304, offloaded_t307, offloaded_t309, offloaded_t312, offloaded_t314, \\\n",
       "  offloaded_t317, offloaded_t319, offloaded_t32, offloaded_t322, offloaded_t324, \\\n",
       "  offloaded_t327, offloaded_t329, offloaded_t332, offloaded_t334, offloaded_t337, \\\n",
       "  offloaded_t339, offloaded_t34, offloaded_t342, offloaded_t344, offloaded_t347, \\\n",
       "  offloaded_t349, offloaded_t352, offloaded_t354, offloaded_t357, offloaded_t359, \\\n",
       "  offloaded_t362, offloaded_t364, offloaded_t367, offloaded_t369, offloaded_t37, \\\n",
       "  offloaded_t372, offloaded_t374, offloaded_t377, offloaded_t379, offloaded_t382, \\\n",
       "  offloaded_t384, offloaded_t387, offloaded_t389, offloaded_t39, offloaded_t392, \\\n",
       "  offloaded_t394, offloaded_t397, offloaded_t399, offloaded_t4, offloaded_t402, \\\n",
       "  offloaded_t404, offloaded_t407, offloaded_t409, offloaded_t412, offloaded_t414, \\\n",
       "  offloaded_t417, offloaded_t419, offloaded_t42, offloaded_t422, offloaded_t424, \\\n",
       "  offloaded_t427, offloaded_t429, offloaded_t432, offloaded_t434, offloaded_t437, \\\n",
       "  offloaded_t439, offloaded_t44, offloaded_t442, offloaded_t444, offloaded_t447, \\\n",
       "  offloaded_t449, offloaded_t452, offloaded_t454, offloaded_t457, offloaded_t459, \\\n",
       "  offloaded_t462, offloaded_t464, offloaded_t467, offloaded_t469, offloaded_t47, \\\n",
       "  offloaded_t472, offloaded_t474, offloaded_t477, offloaded_t479, offloaded_t482, \\\n",
       "  offloaded_t484, offloaded_t487, offloaded_t489, offloaded_t49, offloaded_t492, \\\n",
       "  offloaded_t494, offloaded_t497, offloaded_t52, offloaded_t54, offloaded_t57, \\\n",
       "  offloaded_t59, offloaded_t62, offloaded_t64, offloaded_t67, offloaded_t69, \\\n",
       "  offloaded_t7, offloaded_t72, offloaded_t74, offloaded_t77, offloaded_t79, \\\n",
       "  offloaded_t82, offloaded_t84, offloaded_t87, offloaded_t89, offloaded_t9, \\\n",
       "  offloaded_t92, offloaded_t94, offloaded_t97, offloaded_t99, t_fcs_10_weight, \\\n",
       "  t_fcs_11_weight, t_fcs_12_weight, t_fcs_13_weight, t_fcs_14_weight, \\\n",
       "  t_fcs_15_weight, t_fcs_16_weight, t_fcs_17_weight, t_fcs_18_weight, \\\n",
       "  t_fcs_19_weight, t_fcs_1_weight, t_fcs_20_weight, t_fcs_21_weight, \\\n",
       "  t_fcs_22_weight, t_fcs_23_weight, t_fcs_24_weight, t_fcs_25_weight, \\\n",
       "  t_fcs_26_weight, t_fcs_27_weight, t_fcs_28_weight, t_fcs_29_weight, \\\n",
       "  t_fcs_2_weight, t_fcs_30_weight, t_fcs_31_weight, t_fcs_32_weight, \\\n",
       "  t_fcs_33_weight, t_fcs_34_weight, t_fcs_35_weight, t_fcs_36_weight, \\\n",
       "  t_fcs_37_weight, t_fcs_38_weight, t_fcs_39_weight, t_fcs_3_weight, \\\n",
       "  t_fcs_40_weight, t_fcs_41_weight, t_fcs_42_weight, t_fcs_43_weight, \\\n",
       "  t_fcs_44_weight, t_fcs_45_weight, t_fcs_46_weight, t_fcs_47_weight, \\\n",
       "  t_fcs_48_weight, t_fcs_49_weight, t_fcs_4_weight, t_fcs_50_weight, \\\n",
       "  t_fcs_51_weight, t_fcs_52_weight, t_fcs_53_weight, t_fcs_54_weight, \\\n",
       "  t_fcs_55_weight, t_fcs_56_weight, t_fcs_57_weight, t_fcs_58_weight, \\\n",
       "  t_fcs_59_weight, t_fcs_5_weight, t_fcs_60_weight, t_fcs_61_weight, \\\n",
       "  t_fcs_62_weight, t_fcs_63_weight, t_fcs_64_weight, t_fcs_65_weight, \\\n",
       "  t_fcs_66_weight, t_fcs_67_weight, t_fcs_68_weight, t_fcs_69_weight, \\\n",
       "  t_fcs_6_weight, t_fcs_70_weight, t_fcs_71_weight, t_fcs_72_weight, \\\n",
       "  t_fcs_73_weight, t_fcs_74_weight, t_fcs_75_weight, t_fcs_76_weight, \\\n",
       "  t_fcs_77_weight, t_fcs_78_weight, t_fcs_79_weight, t_fcs_7_weight, \\\n",
       "  t_fcs_80_weight, t_fcs_81_weight, t_fcs_82_weight, t_fcs_83_weight, \\\n",
       "  t_fcs_84_weight, t_fcs_85_weight, t_fcs_86_weight, t_fcs_87_weight, \\\n",
       "  t_fcs_88_weight, t_fcs_89_weight, t_fcs_8_weight, t_fcs_90_weight, \\\n",
       "  t_fcs_91_weight, t_fcs_92_weight, t_fcs_93_weight, t_fcs_94_weight, \\\n",
       "  t_fcs_95_weight, t_fcs_96_weight, t_fcs_97_weight, t_fcs_98_weight, \\\n",
       "  t_fcs_99_weight, t_fcs_9_weight, x, = C0\n",
       "  clear_mutable_collection(C0)\n",
       "  del C0\n",
       "  # cotangents: \"Collection\"\n",
       "  t1, = cotangents\n",
       "  clear_mutable_collection(cotangents)\n",
       "  del cotangents\n",
       "  # Created by CPU Offloading Transform\n",
       "  t497 = load_to_gpu(offloaded_t497, 'cuda:0')  # t497: \"cuda:0 b8[128, 16]\"\n",
       "  [t2001, t2008] = nvFusion0(t497, t1)\n",
       "    # t2001 = prims.where(t497, t1, 0.0)  # t2001: \"cuda:0 f32[128, 16]\"\n",
       "    # t2008 = prims.sum(t2001, (0,))  # t2008: \"cuda:0 f32[16]\"\n",
       "  del t497, t1\n",
       "  t2002 = torch.reshape(t2001, (-1, 16))  # t2002: \"cuda:0 f32[128, 16]\"\n",
       "    # t2002 = ltorch.reshape(t2001, (-1, 16))  # t2002: \"cuda:0 f32[128, 16]\"\n",
       "      # t2002 = prims.reshape(t2001, (128, 16))  # t2002: \"cuda:0 f32[128, 16]\"\n",
       "  del t2001\n",
       "  t2003 = torch.matmul(t2002, t_fcs_99_weight)  # t2003: \"cuda:0 f32[128, 16]\"\n",
       "    # t2003 = ltorch.matmul(t2002, t_fcs_99_weight)  # t2003: \"cuda:0 f32[128, 16]\"\n",
       "      # t2003 = prims.matmul(t2002, t_fcs_99_weight)  # t2003: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_99_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t492 = load_to_gpu(offloaded_t492, 'cuda:0')  # t492: \"cuda:0 b8[128, 16]\"\n",
       "  [t2010, t2017] = nvFusion1(t492, t2003)\n",
       "    # t2010 = prims.where(t492, t2003, 0.0)  # t2010: \"cuda:0 f32[128, 16]\"\n",
       "    # t2017 = prims.sum(t2010, (0,))  # t2017: \"cuda:0 f32[16]\"\n",
       "  del t492, t2003\n",
       "  t2011 = torch.reshape(t2010, (-1, 16))  # t2011: \"cuda:0 f32[128, 16]\"\n",
       "    # t2011 = ltorch.reshape(t2010, (-1, 16))  # t2011: \"cuda:0 f32[128, 16]\"\n",
       "      # t2011 = prims.reshape(t2010, (128, 16))  # t2011: \"cuda:0 f32[128, 16]\"\n",
       "  del t2010\n",
       "  t2012 = torch.matmul(t2011, t_fcs_98_weight)  # t2012: \"cuda:0 f32[128, 16]\"\n",
       "    # t2012 = ltorch.matmul(t2011, t_fcs_98_weight)  # t2012: \"cuda:0 f32[128, 16]\"\n",
       "      # t2012 = prims.matmul(t2011, t_fcs_98_weight)  # t2012: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_98_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t487 = load_to_gpu(offloaded_t487, 'cuda:0')  # t487: \"cuda:0 b8[128, 16]\"\n",
       "  [t2019, t2026] = nvFusion2(t487, t2012)\n",
       "    # t2019 = prims.where(t487, t2012, 0.0)  # t2019: \"cuda:0 f32[128, 16]\"\n",
       "    # t2026 = prims.sum(t2019, (0,))  # t2026: \"cuda:0 f32[16]\"\n",
       "  del t487, t2012\n",
       "  t2020 = torch.reshape(t2019, (-1, 16))  # t2020: \"cuda:0 f32[128, 16]\"\n",
       "    # t2020 = ltorch.reshape(t2019, (-1, 16))  # t2020: \"cuda:0 f32[128, 16]\"\n",
       "      # t2020 = prims.reshape(t2019, (128, 16))  # t2020: \"cuda:0 f32[128, 16]\"\n",
       "  del t2019\n",
       "  t2021 = torch.matmul(t2020, t_fcs_97_weight)  # t2021: \"cuda:0 f32[128, 16]\"\n",
       "    # t2021 = ltorch.matmul(t2020, t_fcs_97_weight)  # t2021: \"cuda:0 f32[128, 16]\"\n",
       "      # t2021 = prims.matmul(t2020, t_fcs_97_weight)  # t2021: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_97_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t482 = load_to_gpu(offloaded_t482, 'cuda:0')  # t482: \"cuda:0 b8[128, 16]\"\n",
       "  [t2028, t2035] = nvFusion3(t482, t2021)\n",
       "    # t2028 = prims.where(t482, t2021, 0.0)  # t2028: \"cuda:0 f32[128, 16]\"\n",
       "    # t2035 = prims.sum(t2028, (0,))  # t2035: \"cuda:0 f32[16]\"\n",
       "  del t482, t2021\n",
       "  t2029 = torch.reshape(t2028, (-1, 16))  # t2029: \"cuda:0 f32[128, 16]\"\n",
       "    # t2029 = ltorch.reshape(t2028, (-1, 16))  # t2029: \"cuda:0 f32[128, 16]\"\n",
       "      # t2029 = prims.reshape(t2028, (128, 16))  # t2029: \"cuda:0 f32[128, 16]\"\n",
       "  del t2028\n",
       "  t2030 = torch.matmul(t2029, t_fcs_96_weight)  # t2030: \"cuda:0 f32[128, 16]\"\n",
       "    # t2030 = ltorch.matmul(t2029, t_fcs_96_weight)  # t2030: \"cuda:0 f32[128, 16]\"\n",
       "      # t2030 = prims.matmul(t2029, t_fcs_96_weight)  # t2030: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_96_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t477 = load_to_gpu(offloaded_t477, 'cuda:0')  # t477: \"cuda:0 b8[128, 16]\"\n",
       "  [t2037, t2044] = nvFusion4(t477, t2030)\n",
       "    # t2037 = prims.where(t477, t2030, 0.0)  # t2037: \"cuda:0 f32[128, 16]\"\n",
       "    # t2044 = prims.sum(t2037, (0,))  # t2044: \"cuda:0 f32[16]\"\n",
       "  del t477, t2030\n",
       "  t2038 = torch.reshape(t2037, (-1, 16))  # t2038: \"cuda:0 f32[128, 16]\"\n",
       "    # t2038 = ltorch.reshape(t2037, (-1, 16))  # t2038: \"cuda:0 f32[128, 16]\"\n",
       "      # t2038 = prims.reshape(t2037, (128, 16))  # t2038: \"cuda:0 f32[128, 16]\"\n",
       "  del t2037\n",
       "  t2039 = torch.matmul(t2038, t_fcs_95_weight)  # t2039: \"cuda:0 f32[128, 16]\"\n",
       "    # t2039 = ltorch.matmul(t2038, t_fcs_95_weight)  # t2039: \"cuda:0 f32[128, 16]\"\n",
       "      # t2039 = prims.matmul(t2038, t_fcs_95_weight)  # t2039: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_95_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t472 = load_to_gpu(offloaded_t472, 'cuda:0')  # t472: \"cuda:0 b8[128, 16]\"\n",
       "  [t2046, t2053] = nvFusion5(t472, t2039)\n",
       "    # t2046 = prims.where(t472, t2039, 0.0)  # t2046: \"cuda:0 f32[128, 16]\"\n",
       "    # t2053 = prims.sum(t2046, (0,))  # t2053: \"cuda:0 f32[16]\"\n",
       "  del t472, t2039\n",
       "  t2047 = torch.reshape(t2046, (-1, 16))  # t2047: \"cuda:0 f32[128, 16]\"\n",
       "    # t2047 = ltorch.reshape(t2046, (-1, 16))  # t2047: \"cuda:0 f32[128, 16]\"\n",
       "      # t2047 = prims.reshape(t2046, (128, 16))  # t2047: \"cuda:0 f32[128, 16]\"\n",
       "  del t2046\n",
       "  t2048 = torch.matmul(t2047, t_fcs_94_weight)  # t2048: \"cuda:0 f32[128, 16]\"\n",
       "    # t2048 = ltorch.matmul(t2047, t_fcs_94_weight)  # t2048: \"cuda:0 f32[128, 16]\"\n",
       "      # t2048 = prims.matmul(t2047, t_fcs_94_weight)  # t2048: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_94_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t467 = load_to_gpu(offloaded_t467, 'cuda:0')  # t467: \"cuda:0 b8[128, 16]\"\n",
       "  [t2055, t2062] = nvFusion6(t467, t2048)\n",
       "    # t2055 = prims.where(t467, t2048, 0.0)  # t2055: \"cuda:0 f32[128, 16]\"\n",
       "    # t2062 = prims.sum(t2055, (0,))  # t2062: \"cuda:0 f32[16]\"\n",
       "  del t467, t2048\n",
       "  t2056 = torch.reshape(t2055, (-1, 16))  # t2056: \"cuda:0 f32[128, 16]\"\n",
       "    # t2056 = ltorch.reshape(t2055, (-1, 16))  # t2056: \"cuda:0 f32[128, 16]\"\n",
       "      # t2056 = prims.reshape(t2055, (128, 16))  # t2056: \"cuda:0 f32[128, 16]\"\n",
       "  del t2055\n",
       "  t2057 = torch.matmul(t2056, t_fcs_93_weight)  # t2057: \"cuda:0 f32[128, 16]\"\n",
       "    # t2057 = ltorch.matmul(t2056, t_fcs_93_weight)  # t2057: \"cuda:0 f32[128, 16]\"\n",
       "      # t2057 = prims.matmul(t2056, t_fcs_93_weight)  # t2057: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_93_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t462 = load_to_gpu(offloaded_t462, 'cuda:0')  # t462: \"cuda:0 b8[128, 16]\"\n",
       "  [t2064, t2071] = nvFusion7(t462, t2057)\n",
       "    # t2064 = prims.where(t462, t2057, 0.0)  # t2064: \"cuda:0 f32[128, 16]\"\n",
       "    # t2071 = prims.sum(t2064, (0,))  # t2071: \"cuda:0 f32[16]\"\n",
       "  del t462, t2057\n",
       "  t2065 = torch.reshape(t2064, (-1, 16))  # t2065: \"cuda:0 f32[128, 16]\"\n",
       "    # t2065 = ltorch.reshape(t2064, (-1, 16))  # t2065: \"cuda:0 f32[128, 16]\"\n",
       "      # t2065 = prims.reshape(t2064, (128, 16))  # t2065: \"cuda:0 f32[128, 16]\"\n",
       "  del t2064\n",
       "  t2066 = torch.matmul(t2065, t_fcs_92_weight)  # t2066: \"cuda:0 f32[128, 16]\"\n",
       "    # t2066 = ltorch.matmul(t2065, t_fcs_92_weight)  # t2066: \"cuda:0 f32[128, 16]\"\n",
       "      # t2066 = prims.matmul(t2065, t_fcs_92_weight)  # t2066: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_92_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t457 = load_to_gpu(offloaded_t457, 'cuda:0')  # t457: \"cuda:0 b8[128, 16]\"\n",
       "  [t2073, t2080] = nvFusion8(t457, t2066)\n",
       "    # t2073 = prims.where(t457, t2066, 0.0)  # t2073: \"cuda:0 f32[128, 16]\"\n",
       "    # t2080 = prims.sum(t2073, (0,))  # t2080: \"cuda:0 f32[16]\"\n",
       "  del t457, t2066\n",
       "  t2074 = torch.reshape(t2073, (-1, 16))  # t2074: \"cuda:0 f32[128, 16]\"\n",
       "    # t2074 = ltorch.reshape(t2073, (-1, 16))  # t2074: \"cuda:0 f32[128, 16]\"\n",
       "      # t2074 = prims.reshape(t2073, (128, 16))  # t2074: \"cuda:0 f32[128, 16]\"\n",
       "  del t2073\n",
       "  t2075 = torch.matmul(t2074, t_fcs_91_weight)  # t2075: \"cuda:0 f32[128, 16]\"\n",
       "    # t2075 = ltorch.matmul(t2074, t_fcs_91_weight)  # t2075: \"cuda:0 f32[128, 16]\"\n",
       "      # t2075 = prims.matmul(t2074, t_fcs_91_weight)  # t2075: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_91_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t452 = load_to_gpu(offloaded_t452, 'cuda:0')  # t452: \"cuda:0 b8[128, 16]\"\n",
       "  [t2082, t2089] = nvFusion9(t452, t2075)\n",
       "    # t2082 = prims.where(t452, t2075, 0.0)  # t2082: \"cuda:0 f32[128, 16]\"\n",
       "    # t2089 = prims.sum(t2082, (0,))  # t2089: \"cuda:0 f32[16]\"\n",
       "  del t452, t2075\n",
       "  t2083 = torch.reshape(t2082, (-1, 16))  # t2083: \"cuda:0 f32[128, 16]\"\n",
       "    # t2083 = ltorch.reshape(t2082, (-1, 16))  # t2083: \"cuda:0 f32[128, 16]\"\n",
       "      # t2083 = prims.reshape(t2082, (128, 16))  # t2083: \"cuda:0 f32[128, 16]\"\n",
       "  del t2082\n",
       "  t2084 = torch.matmul(t2083, t_fcs_90_weight)  # t2084: \"cuda:0 f32[128, 16]\"\n",
       "    # t2084 = ltorch.matmul(t2083, t_fcs_90_weight)  # t2084: \"cuda:0 f32[128, 16]\"\n",
       "      # t2084 = prims.matmul(t2083, t_fcs_90_weight)  # t2084: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_90_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t447 = load_to_gpu(offloaded_t447, 'cuda:0')  # t447: \"cuda:0 b8[128, 16]\"\n",
       "  [t2091, t2098] = nvFusion10(t447, t2084)\n",
       "    # t2091 = prims.where(t447, t2084, 0.0)  # t2091: \"cuda:0 f32[128, 16]\"\n",
       "    # t2098 = prims.sum(t2091, (0,))  # t2098: \"cuda:0 f32[16]\"\n",
       "  del t447, t2084\n",
       "  t2092 = torch.reshape(t2091, (-1, 16))  # t2092: \"cuda:0 f32[128, 16]\"\n",
       "    # t2092 = ltorch.reshape(t2091, (-1, 16))  # t2092: \"cuda:0 f32[128, 16]\"\n",
       "      # t2092 = prims.reshape(t2091, (128, 16))  # t2092: \"cuda:0 f32[128, 16]\"\n",
       "  del t2091\n",
       "  t2093 = torch.matmul(t2092, t_fcs_89_weight)  # t2093: \"cuda:0 f32[128, 16]\"\n",
       "    # t2093 = ltorch.matmul(t2092, t_fcs_89_weight)  # t2093: \"cuda:0 f32[128, 16]\"\n",
       "      # t2093 = prims.matmul(t2092, t_fcs_89_weight)  # t2093: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_89_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t442 = load_to_gpu(offloaded_t442, 'cuda:0')  # t442: \"cuda:0 b8[128, 16]\"\n",
       "  [t2100, t2107] = nvFusion11(t442, t2093)\n",
       "    # t2100 = prims.where(t442, t2093, 0.0)  # t2100: \"cuda:0 f32[128, 16]\"\n",
       "    # t2107 = prims.sum(t2100, (0,))  # t2107: \"cuda:0 f32[16]\"\n",
       "  del t442, t2093\n",
       "  t2101 = torch.reshape(t2100, (-1, 16))  # t2101: \"cuda:0 f32[128, 16]\"\n",
       "    # t2101 = ltorch.reshape(t2100, (-1, 16))  # t2101: \"cuda:0 f32[128, 16]\"\n",
       "      # t2101 = prims.reshape(t2100, (128, 16))  # t2101: \"cuda:0 f32[128, 16]\"\n",
       "  del t2100\n",
       "  t2102 = torch.matmul(t2101, t_fcs_88_weight)  # t2102: \"cuda:0 f32[128, 16]\"\n",
       "    # t2102 = ltorch.matmul(t2101, t_fcs_88_weight)  # t2102: \"cuda:0 f32[128, 16]\"\n",
       "      # t2102 = prims.matmul(t2101, t_fcs_88_weight)  # t2102: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_88_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t437 = load_to_gpu(offloaded_t437, 'cuda:0')  # t437: \"cuda:0 b8[128, 16]\"\n",
       "  [t2109, t2116] = nvFusion12(t437, t2102)\n",
       "    # t2109 = prims.where(t437, t2102, 0.0)  # t2109: \"cuda:0 f32[128, 16]\"\n",
       "    # t2116 = prims.sum(t2109, (0,))  # t2116: \"cuda:0 f32[16]\"\n",
       "  del t437, t2102\n",
       "  t2110 = torch.reshape(t2109, (-1, 16))  # t2110: \"cuda:0 f32[128, 16]\"\n",
       "    # t2110 = ltorch.reshape(t2109, (-1, 16))  # t2110: \"cuda:0 f32[128, 16]\"\n",
       "      # t2110 = prims.reshape(t2109, (128, 16))  # t2110: \"cuda:0 f32[128, 16]\"\n",
       "  del t2109\n",
       "  t2111 = torch.matmul(t2110, t_fcs_87_weight)  # t2111: \"cuda:0 f32[128, 16]\"\n",
       "    # t2111 = ltorch.matmul(t2110, t_fcs_87_weight)  # t2111: \"cuda:0 f32[128, 16]\"\n",
       "      # t2111 = prims.matmul(t2110, t_fcs_87_weight)  # t2111: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_87_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t432 = load_to_gpu(offloaded_t432, 'cuda:0')  # t432: \"cuda:0 b8[128, 16]\"\n",
       "  [t2118, t2125] = nvFusion13(t432, t2111)\n",
       "    # t2118 = prims.where(t432, t2111, 0.0)  # t2118: \"cuda:0 f32[128, 16]\"\n",
       "    # t2125 = prims.sum(t2118, (0,))  # t2125: \"cuda:0 f32[16]\"\n",
       "  del t432, t2111\n",
       "  t2119 = torch.reshape(t2118, (-1, 16))  # t2119: \"cuda:0 f32[128, 16]\"\n",
       "    # t2119 = ltorch.reshape(t2118, (-1, 16))  # t2119: \"cuda:0 f32[128, 16]\"\n",
       "      # t2119 = prims.reshape(t2118, (128, 16))  # t2119: \"cuda:0 f32[128, 16]\"\n",
       "  del t2118\n",
       "  t2120 = torch.matmul(t2119, t_fcs_86_weight)  # t2120: \"cuda:0 f32[128, 16]\"\n",
       "    # t2120 = ltorch.matmul(t2119, t_fcs_86_weight)  # t2120: \"cuda:0 f32[128, 16]\"\n",
       "      # t2120 = prims.matmul(t2119, t_fcs_86_weight)  # t2120: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_86_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t427 = load_to_gpu(offloaded_t427, 'cuda:0')  # t427: \"cuda:0 b8[128, 16]\"\n",
       "  [t2127, t2134] = nvFusion14(t427, t2120)\n",
       "    # t2127 = prims.where(t427, t2120, 0.0)  # t2127: \"cuda:0 f32[128, 16]\"\n",
       "    # t2134 = prims.sum(t2127, (0,))  # t2134: \"cuda:0 f32[16]\"\n",
       "  del t427, t2120\n",
       "  t2128 = torch.reshape(t2127, (-1, 16))  # t2128: \"cuda:0 f32[128, 16]\"\n",
       "    # t2128 = ltorch.reshape(t2127, (-1, 16))  # t2128: \"cuda:0 f32[128, 16]\"\n",
       "      # t2128 = prims.reshape(t2127, (128, 16))  # t2128: \"cuda:0 f32[128, 16]\"\n",
       "  del t2127\n",
       "  t2129 = torch.matmul(t2128, t_fcs_85_weight)  # t2129: \"cuda:0 f32[128, 16]\"\n",
       "    # t2129 = ltorch.matmul(t2128, t_fcs_85_weight)  # t2129: \"cuda:0 f32[128, 16]\"\n",
       "      # t2129 = prims.matmul(t2128, t_fcs_85_weight)  # t2129: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_85_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t422 = load_to_gpu(offloaded_t422, 'cuda:0')  # t422: \"cuda:0 b8[128, 16]\"\n",
       "  [t2136, t2143] = nvFusion15(t422, t2129)\n",
       "    # t2136 = prims.where(t422, t2129, 0.0)  # t2136: \"cuda:0 f32[128, 16]\"\n",
       "    # t2143 = prims.sum(t2136, (0,))  # t2143: \"cuda:0 f32[16]\"\n",
       "  del t422, t2129\n",
       "  t2137 = torch.reshape(t2136, (-1, 16))  # t2137: \"cuda:0 f32[128, 16]\"\n",
       "    # t2137 = ltorch.reshape(t2136, (-1, 16))  # t2137: \"cuda:0 f32[128, 16]\"\n",
       "      # t2137 = prims.reshape(t2136, (128, 16))  # t2137: \"cuda:0 f32[128, 16]\"\n",
       "  del t2136\n",
       "  t2138 = torch.matmul(t2137, t_fcs_84_weight)  # t2138: \"cuda:0 f32[128, 16]\"\n",
       "    # t2138 = ltorch.matmul(t2137, t_fcs_84_weight)  # t2138: \"cuda:0 f32[128, 16]\"\n",
       "      # t2138 = prims.matmul(t2137, t_fcs_84_weight)  # t2138: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_84_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t417 = load_to_gpu(offloaded_t417, 'cuda:0')  # t417: \"cuda:0 b8[128, 16]\"\n",
       "  [t2145, t2152] = nvFusion16(t417, t2138)\n",
       "    # t2145 = prims.where(t417, t2138, 0.0)  # t2145: \"cuda:0 f32[128, 16]\"\n",
       "    # t2152 = prims.sum(t2145, (0,))  # t2152: \"cuda:0 f32[16]\"\n",
       "  del t417, t2138\n",
       "  t2146 = torch.reshape(t2145, (-1, 16))  # t2146: \"cuda:0 f32[128, 16]\"\n",
       "    # t2146 = ltorch.reshape(t2145, (-1, 16))  # t2146: \"cuda:0 f32[128, 16]\"\n",
       "      # t2146 = prims.reshape(t2145, (128, 16))  # t2146: \"cuda:0 f32[128, 16]\"\n",
       "  del t2145\n",
       "  t2147 = torch.matmul(t2146, t_fcs_83_weight)  # t2147: \"cuda:0 f32[128, 16]\"\n",
       "    # t2147 = ltorch.matmul(t2146, t_fcs_83_weight)  # t2147: \"cuda:0 f32[128, 16]\"\n",
       "      # t2147 = prims.matmul(t2146, t_fcs_83_weight)  # t2147: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_83_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t412 = load_to_gpu(offloaded_t412, 'cuda:0')  # t412: \"cuda:0 b8[128, 16]\"\n",
       "  [t2154, t2161] = nvFusion17(t412, t2147)\n",
       "    # t2154 = prims.where(t412, t2147, 0.0)  # t2154: \"cuda:0 f32[128, 16]\"\n",
       "    # t2161 = prims.sum(t2154, (0,))  # t2161: \"cuda:0 f32[16]\"\n",
       "  del t412, t2147\n",
       "  t2155 = torch.reshape(t2154, (-1, 16))  # t2155: \"cuda:0 f32[128, 16]\"\n",
       "    # t2155 = ltorch.reshape(t2154, (-1, 16))  # t2155: \"cuda:0 f32[128, 16]\"\n",
       "      # t2155 = prims.reshape(t2154, (128, 16))  # t2155: \"cuda:0 f32[128, 16]\"\n",
       "  del t2154\n",
       "  t2156 = torch.matmul(t2155, t_fcs_82_weight)  # t2156: \"cuda:0 f32[128, 16]\"\n",
       "    # t2156 = ltorch.matmul(t2155, t_fcs_82_weight)  # t2156: \"cuda:0 f32[128, 16]\"\n",
       "      # t2156 = prims.matmul(t2155, t_fcs_82_weight)  # t2156: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_82_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t407 = load_to_gpu(offloaded_t407, 'cuda:0')  # t407: \"cuda:0 b8[128, 16]\"\n",
       "  [t2163, t2170] = nvFusion18(t407, t2156)\n",
       "    # t2163 = prims.where(t407, t2156, 0.0)  # t2163: \"cuda:0 f32[128, 16]\"\n",
       "    # t2170 = prims.sum(t2163, (0,))  # t2170: \"cuda:0 f32[16]\"\n",
       "  del t407, t2156\n",
       "  t2164 = torch.reshape(t2163, (-1, 16))  # t2164: \"cuda:0 f32[128, 16]\"\n",
       "    # t2164 = ltorch.reshape(t2163, (-1, 16))  # t2164: \"cuda:0 f32[128, 16]\"\n",
       "      # t2164 = prims.reshape(t2163, (128, 16))  # t2164: \"cuda:0 f32[128, 16]\"\n",
       "  del t2163\n",
       "  t2165 = torch.matmul(t2164, t_fcs_81_weight)  # t2165: \"cuda:0 f32[128, 16]\"\n",
       "    # t2165 = ltorch.matmul(t2164, t_fcs_81_weight)  # t2165: \"cuda:0 f32[128, 16]\"\n",
       "      # t2165 = prims.matmul(t2164, t_fcs_81_weight)  # t2165: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_81_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t402 = load_to_gpu(offloaded_t402, 'cuda:0')  # t402: \"cuda:0 b8[128, 16]\"\n",
       "  [t2172, t2179] = nvFusion19(t402, t2165)\n",
       "    # t2172 = prims.where(t402, t2165, 0.0)  # t2172: \"cuda:0 f32[128, 16]\"\n",
       "    # t2179 = prims.sum(t2172, (0,))  # t2179: \"cuda:0 f32[16]\"\n",
       "  del t402, t2165\n",
       "  t2173 = torch.reshape(t2172, (-1, 16))  # t2173: \"cuda:0 f32[128, 16]\"\n",
       "    # t2173 = ltorch.reshape(t2172, (-1, 16))  # t2173: \"cuda:0 f32[128, 16]\"\n",
       "      # t2173 = prims.reshape(t2172, (128, 16))  # t2173: \"cuda:0 f32[128, 16]\"\n",
       "  del t2172\n",
       "  t2174 = torch.matmul(t2173, t_fcs_80_weight)  # t2174: \"cuda:0 f32[128, 16]\"\n",
       "    # t2174 = ltorch.matmul(t2173, t_fcs_80_weight)  # t2174: \"cuda:0 f32[128, 16]\"\n",
       "      # t2174 = prims.matmul(t2173, t_fcs_80_weight)  # t2174: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_80_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t397 = load_to_gpu(offloaded_t397, 'cuda:0')  # t397: \"cuda:0 b8[128, 16]\"\n",
       "  [t2181, t2188] = nvFusion20(t397, t2174)\n",
       "    # t2181 = prims.where(t397, t2174, 0.0)  # t2181: \"cuda:0 f32[128, 16]\"\n",
       "    # t2188 = prims.sum(t2181, (0,))  # t2188: \"cuda:0 f32[16]\"\n",
       "  del t397, t2174\n",
       "  t2182 = torch.reshape(t2181, (-1, 16))  # t2182: \"cuda:0 f32[128, 16]\"\n",
       "    # t2182 = ltorch.reshape(t2181, (-1, 16))  # t2182: \"cuda:0 f32[128, 16]\"\n",
       "      # t2182 = prims.reshape(t2181, (128, 16))  # t2182: \"cuda:0 f32[128, 16]\"\n",
       "  del t2181\n",
       "  t2183 = torch.matmul(t2182, t_fcs_79_weight)  # t2183: \"cuda:0 f32[128, 16]\"\n",
       "    # t2183 = ltorch.matmul(t2182, t_fcs_79_weight)  # t2183: \"cuda:0 f32[128, 16]\"\n",
       "      # t2183 = prims.matmul(t2182, t_fcs_79_weight)  # t2183: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_79_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t392 = load_to_gpu(offloaded_t392, 'cuda:0')  # t392: \"cuda:0 b8[128, 16]\"\n",
       "  [t2190, t2197] = nvFusion21(t392, t2183)\n",
       "    # t2190 = prims.where(t392, t2183, 0.0)  # t2190: \"cuda:0 f32[128, 16]\"\n",
       "    # t2197 = prims.sum(t2190, (0,))  # t2197: \"cuda:0 f32[16]\"\n",
       "  del t392, t2183\n",
       "  t2191 = torch.reshape(t2190, (-1, 16))  # t2191: \"cuda:0 f32[128, 16]\"\n",
       "    # t2191 = ltorch.reshape(t2190, (-1, 16))  # t2191: \"cuda:0 f32[128, 16]\"\n",
       "      # t2191 = prims.reshape(t2190, (128, 16))  # t2191: \"cuda:0 f32[128, 16]\"\n",
       "  del t2190\n",
       "  t2192 = torch.matmul(t2191, t_fcs_78_weight)  # t2192: \"cuda:0 f32[128, 16]\"\n",
       "    # t2192 = ltorch.matmul(t2191, t_fcs_78_weight)  # t2192: \"cuda:0 f32[128, 16]\"\n",
       "      # t2192 = prims.matmul(t2191, t_fcs_78_weight)  # t2192: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_78_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t387 = load_to_gpu(offloaded_t387, 'cuda:0')  # t387: \"cuda:0 b8[128, 16]\"\n",
       "  [t2199, t2206] = nvFusion22(t387, t2192)\n",
       "    # t2199 = prims.where(t387, t2192, 0.0)  # t2199: \"cuda:0 f32[128, 16]\"\n",
       "    # t2206 = prims.sum(t2199, (0,))  # t2206: \"cuda:0 f32[16]\"\n",
       "  del t387, t2192\n",
       "  t2200 = torch.reshape(t2199, (-1, 16))  # t2200: \"cuda:0 f32[128, 16]\"\n",
       "    # t2200 = ltorch.reshape(t2199, (-1, 16))  # t2200: \"cuda:0 f32[128, 16]\"\n",
       "      # t2200 = prims.reshape(t2199, (128, 16))  # t2200: \"cuda:0 f32[128, 16]\"\n",
       "  del t2199\n",
       "  t2201 = torch.matmul(t2200, t_fcs_77_weight)  # t2201: \"cuda:0 f32[128, 16]\"\n",
       "    # t2201 = ltorch.matmul(t2200, t_fcs_77_weight)  # t2201: \"cuda:0 f32[128, 16]\"\n",
       "      # t2201 = prims.matmul(t2200, t_fcs_77_weight)  # t2201: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_77_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t382 = load_to_gpu(offloaded_t382, 'cuda:0')  # t382: \"cuda:0 b8[128, 16]\"\n",
       "  [t2208, t2215] = nvFusion23(t382, t2201)\n",
       "    # t2208 = prims.where(t382, t2201, 0.0)  # t2208: \"cuda:0 f32[128, 16]\"\n",
       "    # t2215 = prims.sum(t2208, (0,))  # t2215: \"cuda:0 f32[16]\"\n",
       "  del t382, t2201\n",
       "  t2209 = torch.reshape(t2208, (-1, 16))  # t2209: \"cuda:0 f32[128, 16]\"\n",
       "    # t2209 = ltorch.reshape(t2208, (-1, 16))  # t2209: \"cuda:0 f32[128, 16]\"\n",
       "      # t2209 = prims.reshape(t2208, (128, 16))  # t2209: \"cuda:0 f32[128, 16]\"\n",
       "  del t2208\n",
       "  t2210 = torch.matmul(t2209, t_fcs_76_weight)  # t2210: \"cuda:0 f32[128, 16]\"\n",
       "    # t2210 = ltorch.matmul(t2209, t_fcs_76_weight)  # t2210: \"cuda:0 f32[128, 16]\"\n",
       "      # t2210 = prims.matmul(t2209, t_fcs_76_weight)  # t2210: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_76_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t377 = load_to_gpu(offloaded_t377, 'cuda:0')  # t377: \"cuda:0 b8[128, 16]\"\n",
       "  [t2217, t2224] = nvFusion24(t377, t2210)\n",
       "    # t2217 = prims.where(t377, t2210, 0.0)  # t2217: \"cuda:0 f32[128, 16]\"\n",
       "    # t2224 = prims.sum(t2217, (0,))  # t2224: \"cuda:0 f32[16]\"\n",
       "  del t377, t2210\n",
       "  t2218 = torch.reshape(t2217, (-1, 16))  # t2218: \"cuda:0 f32[128, 16]\"\n",
       "    # t2218 = ltorch.reshape(t2217, (-1, 16))  # t2218: \"cuda:0 f32[128, 16]\"\n",
       "      # t2218 = prims.reshape(t2217, (128, 16))  # t2218: \"cuda:0 f32[128, 16]\"\n",
       "  del t2217\n",
       "  t2219 = torch.matmul(t2218, t_fcs_75_weight)  # t2219: \"cuda:0 f32[128, 16]\"\n",
       "    # t2219 = ltorch.matmul(t2218, t_fcs_75_weight)  # t2219: \"cuda:0 f32[128, 16]\"\n",
       "      # t2219 = prims.matmul(t2218, t_fcs_75_weight)  # t2219: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_75_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t372 = load_to_gpu(offloaded_t372, 'cuda:0')  # t372: \"cuda:0 b8[128, 16]\"\n",
       "  [t2226, t2233] = nvFusion25(t372, t2219)\n",
       "    # t2226 = prims.where(t372, t2219, 0.0)  # t2226: \"cuda:0 f32[128, 16]\"\n",
       "    # t2233 = prims.sum(t2226, (0,))  # t2233: \"cuda:0 f32[16]\"\n",
       "  del t372, t2219\n",
       "  t2227 = torch.reshape(t2226, (-1, 16))  # t2227: \"cuda:0 f32[128, 16]\"\n",
       "    # t2227 = ltorch.reshape(t2226, (-1, 16))  # t2227: \"cuda:0 f32[128, 16]\"\n",
       "      # t2227 = prims.reshape(t2226, (128, 16))  # t2227: \"cuda:0 f32[128, 16]\"\n",
       "  del t2226\n",
       "  t2228 = torch.matmul(t2227, t_fcs_74_weight)  # t2228: \"cuda:0 f32[128, 16]\"\n",
       "    # t2228 = ltorch.matmul(t2227, t_fcs_74_weight)  # t2228: \"cuda:0 f32[128, 16]\"\n",
       "      # t2228 = prims.matmul(t2227, t_fcs_74_weight)  # t2228: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_74_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t367 = load_to_gpu(offloaded_t367, 'cuda:0')  # t367: \"cuda:0 b8[128, 16]\"\n",
       "  [t2235, t2242] = nvFusion26(t367, t2228)\n",
       "    # t2235 = prims.where(t367, t2228, 0.0)  # t2235: \"cuda:0 f32[128, 16]\"\n",
       "    # t2242 = prims.sum(t2235, (0,))  # t2242: \"cuda:0 f32[16]\"\n",
       "  del t367, t2228\n",
       "  t2236 = torch.reshape(t2235, (-1, 16))  # t2236: \"cuda:0 f32[128, 16]\"\n",
       "    # t2236 = ltorch.reshape(t2235, (-1, 16))  # t2236: \"cuda:0 f32[128, 16]\"\n",
       "      # t2236 = prims.reshape(t2235, (128, 16))  # t2236: \"cuda:0 f32[128, 16]\"\n",
       "  del t2235\n",
       "  t2237 = torch.matmul(t2236, t_fcs_73_weight)  # t2237: \"cuda:0 f32[128, 16]\"\n",
       "    # t2237 = ltorch.matmul(t2236, t_fcs_73_weight)  # t2237: \"cuda:0 f32[128, 16]\"\n",
       "      # t2237 = prims.matmul(t2236, t_fcs_73_weight)  # t2237: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_73_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t362 = load_to_gpu(offloaded_t362, 'cuda:0')  # t362: \"cuda:0 b8[128, 16]\"\n",
       "  [t2244, t2251] = nvFusion27(t362, t2237)\n",
       "    # t2244 = prims.where(t362, t2237, 0.0)  # t2244: \"cuda:0 f32[128, 16]\"\n",
       "    # t2251 = prims.sum(t2244, (0,))  # t2251: \"cuda:0 f32[16]\"\n",
       "  del t362, t2237\n",
       "  t2245 = torch.reshape(t2244, (-1, 16))  # t2245: \"cuda:0 f32[128, 16]\"\n",
       "    # t2245 = ltorch.reshape(t2244, (-1, 16))  # t2245: \"cuda:0 f32[128, 16]\"\n",
       "      # t2245 = prims.reshape(t2244, (128, 16))  # t2245: \"cuda:0 f32[128, 16]\"\n",
       "  del t2244\n",
       "  t2246 = torch.matmul(t2245, t_fcs_72_weight)  # t2246: \"cuda:0 f32[128, 16]\"\n",
       "    # t2246 = ltorch.matmul(t2245, t_fcs_72_weight)  # t2246: \"cuda:0 f32[128, 16]\"\n",
       "      # t2246 = prims.matmul(t2245, t_fcs_72_weight)  # t2246: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_72_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t357 = load_to_gpu(offloaded_t357, 'cuda:0')  # t357: \"cuda:0 b8[128, 16]\"\n",
       "  [t2253, t2260] = nvFusion28(t357, t2246)\n",
       "    # t2253 = prims.where(t357, t2246, 0.0)  # t2253: \"cuda:0 f32[128, 16]\"\n",
       "    # t2260 = prims.sum(t2253, (0,))  # t2260: \"cuda:0 f32[16]\"\n",
       "  del t357, t2246\n",
       "  t2254 = torch.reshape(t2253, (-1, 16))  # t2254: \"cuda:0 f32[128, 16]\"\n",
       "    # t2254 = ltorch.reshape(t2253, (-1, 16))  # t2254: \"cuda:0 f32[128, 16]\"\n",
       "      # t2254 = prims.reshape(t2253, (128, 16))  # t2254: \"cuda:0 f32[128, 16]\"\n",
       "  del t2253\n",
       "  t2255 = torch.matmul(t2254, t_fcs_71_weight)  # t2255: \"cuda:0 f32[128, 16]\"\n",
       "    # t2255 = ltorch.matmul(t2254, t_fcs_71_weight)  # t2255: \"cuda:0 f32[128, 16]\"\n",
       "      # t2255 = prims.matmul(t2254, t_fcs_71_weight)  # t2255: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_71_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t352 = load_to_gpu(offloaded_t352, 'cuda:0')  # t352: \"cuda:0 b8[128, 16]\"\n",
       "  [t2262, t2269] = nvFusion29(t352, t2255)\n",
       "    # t2262 = prims.where(t352, t2255, 0.0)  # t2262: \"cuda:0 f32[128, 16]\"\n",
       "    # t2269 = prims.sum(t2262, (0,))  # t2269: \"cuda:0 f32[16]\"\n",
       "  del t352, t2255\n",
       "  t2263 = torch.reshape(t2262, (-1, 16))  # t2263: \"cuda:0 f32[128, 16]\"\n",
       "    # t2263 = ltorch.reshape(t2262, (-1, 16))  # t2263: \"cuda:0 f32[128, 16]\"\n",
       "      # t2263 = prims.reshape(t2262, (128, 16))  # t2263: \"cuda:0 f32[128, 16]\"\n",
       "  del t2262\n",
       "  t2264 = torch.matmul(t2263, t_fcs_70_weight)  # t2264: \"cuda:0 f32[128, 16]\"\n",
       "    # t2264 = ltorch.matmul(t2263, t_fcs_70_weight)  # t2264: \"cuda:0 f32[128, 16]\"\n",
       "      # t2264 = prims.matmul(t2263, t_fcs_70_weight)  # t2264: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_70_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t347 = load_to_gpu(offloaded_t347, 'cuda:0')  # t347: \"cuda:0 b8[128, 16]\"\n",
       "  [t2271, t2278] = nvFusion30(t347, t2264)\n",
       "    # t2271 = prims.where(t347, t2264, 0.0)  # t2271: \"cuda:0 f32[128, 16]\"\n",
       "    # t2278 = prims.sum(t2271, (0,))  # t2278: \"cuda:0 f32[16]\"\n",
       "  del t347, t2264\n",
       "  t2272 = torch.reshape(t2271, (-1, 16))  # t2272: \"cuda:0 f32[128, 16]\"\n",
       "    # t2272 = ltorch.reshape(t2271, (-1, 16))  # t2272: \"cuda:0 f32[128, 16]\"\n",
       "      # t2272 = prims.reshape(t2271, (128, 16))  # t2272: \"cuda:0 f32[128, 16]\"\n",
       "  del t2271\n",
       "  t2273 = torch.matmul(t2272, t_fcs_69_weight)  # t2273: \"cuda:0 f32[128, 16]\"\n",
       "    # t2273 = ltorch.matmul(t2272, t_fcs_69_weight)  # t2273: \"cuda:0 f32[128, 16]\"\n",
       "      # t2273 = prims.matmul(t2272, t_fcs_69_weight)  # t2273: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_69_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t342 = load_to_gpu(offloaded_t342, 'cuda:0')  # t342: \"cuda:0 b8[128, 16]\"\n",
       "  [t2280, t2287] = nvFusion31(t342, t2273)\n",
       "    # t2280 = prims.where(t342, t2273, 0.0)  # t2280: \"cuda:0 f32[128, 16]\"\n",
       "    # t2287 = prims.sum(t2280, (0,))  # t2287: \"cuda:0 f32[16]\"\n",
       "  del t342, t2273\n",
       "  t2281 = torch.reshape(t2280, (-1, 16))  # t2281: \"cuda:0 f32[128, 16]\"\n",
       "    # t2281 = ltorch.reshape(t2280, (-1, 16))  # t2281: \"cuda:0 f32[128, 16]\"\n",
       "      # t2281 = prims.reshape(t2280, (128, 16))  # t2281: \"cuda:0 f32[128, 16]\"\n",
       "  del t2280\n",
       "  t2282 = torch.matmul(t2281, t_fcs_68_weight)  # t2282: \"cuda:0 f32[128, 16]\"\n",
       "    # t2282 = ltorch.matmul(t2281, t_fcs_68_weight)  # t2282: \"cuda:0 f32[128, 16]\"\n",
       "      # t2282 = prims.matmul(t2281, t_fcs_68_weight)  # t2282: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_68_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t337 = load_to_gpu(offloaded_t337, 'cuda:0')  # t337: \"cuda:0 b8[128, 16]\"\n",
       "  [t2289, t2296] = nvFusion32(t337, t2282)\n",
       "    # t2289 = prims.where(t337, t2282, 0.0)  # t2289: \"cuda:0 f32[128, 16]\"\n",
       "    # t2296 = prims.sum(t2289, (0,))  # t2296: \"cuda:0 f32[16]\"\n",
       "  del t337, t2282\n",
       "  t2290 = torch.reshape(t2289, (-1, 16))  # t2290: \"cuda:0 f32[128, 16]\"\n",
       "    # t2290 = ltorch.reshape(t2289, (-1, 16))  # t2290: \"cuda:0 f32[128, 16]\"\n",
       "      # t2290 = prims.reshape(t2289, (128, 16))  # t2290: \"cuda:0 f32[128, 16]\"\n",
       "  del t2289\n",
       "  t2291 = torch.matmul(t2290, t_fcs_67_weight)  # t2291: \"cuda:0 f32[128, 16]\"\n",
       "    # t2291 = ltorch.matmul(t2290, t_fcs_67_weight)  # t2291: \"cuda:0 f32[128, 16]\"\n",
       "      # t2291 = prims.matmul(t2290, t_fcs_67_weight)  # t2291: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_67_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t332 = load_to_gpu(offloaded_t332, 'cuda:0')  # t332: \"cuda:0 b8[128, 16]\"\n",
       "  [t2298, t2305] = nvFusion33(t332, t2291)\n",
       "    # t2298 = prims.where(t332, t2291, 0.0)  # t2298: \"cuda:0 f32[128, 16]\"\n",
       "    # t2305 = prims.sum(t2298, (0,))  # t2305: \"cuda:0 f32[16]\"\n",
       "  del t332, t2291\n",
       "  t2299 = torch.reshape(t2298, (-1, 16))  # t2299: \"cuda:0 f32[128, 16]\"\n",
       "    # t2299 = ltorch.reshape(t2298, (-1, 16))  # t2299: \"cuda:0 f32[128, 16]\"\n",
       "      # t2299 = prims.reshape(t2298, (128, 16))  # t2299: \"cuda:0 f32[128, 16]\"\n",
       "  del t2298\n",
       "  t2300 = torch.matmul(t2299, t_fcs_66_weight)  # t2300: \"cuda:0 f32[128, 16]\"\n",
       "    # t2300 = ltorch.matmul(t2299, t_fcs_66_weight)  # t2300: \"cuda:0 f32[128, 16]\"\n",
       "      # t2300 = prims.matmul(t2299, t_fcs_66_weight)  # t2300: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_66_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t327 = load_to_gpu(offloaded_t327, 'cuda:0')  # t327: \"cuda:0 b8[128, 16]\"\n",
       "  [t2307, t2314] = nvFusion34(t327, t2300)\n",
       "    # t2307 = prims.where(t327, t2300, 0.0)  # t2307: \"cuda:0 f32[128, 16]\"\n",
       "    # t2314 = prims.sum(t2307, (0,))  # t2314: \"cuda:0 f32[16]\"\n",
       "  del t327, t2300\n",
       "  t2308 = torch.reshape(t2307, (-1, 16))  # t2308: \"cuda:0 f32[128, 16]\"\n",
       "    # t2308 = ltorch.reshape(t2307, (-1, 16))  # t2308: \"cuda:0 f32[128, 16]\"\n",
       "      # t2308 = prims.reshape(t2307, (128, 16))  # t2308: \"cuda:0 f32[128, 16]\"\n",
       "  del t2307\n",
       "  t2309 = torch.matmul(t2308, t_fcs_65_weight)  # t2309: \"cuda:0 f32[128, 16]\"\n",
       "    # t2309 = ltorch.matmul(t2308, t_fcs_65_weight)  # t2309: \"cuda:0 f32[128, 16]\"\n",
       "      # t2309 = prims.matmul(t2308, t_fcs_65_weight)  # t2309: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_65_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t322 = load_to_gpu(offloaded_t322, 'cuda:0')  # t322: \"cuda:0 b8[128, 16]\"\n",
       "  [t2316, t2323] = nvFusion35(t322, t2309)\n",
       "    # t2316 = prims.where(t322, t2309, 0.0)  # t2316: \"cuda:0 f32[128, 16]\"\n",
       "    # t2323 = prims.sum(t2316, (0,))  # t2323: \"cuda:0 f32[16]\"\n",
       "  del t322, t2309\n",
       "  t2317 = torch.reshape(t2316, (-1, 16))  # t2317: \"cuda:0 f32[128, 16]\"\n",
       "    # t2317 = ltorch.reshape(t2316, (-1, 16))  # t2317: \"cuda:0 f32[128, 16]\"\n",
       "      # t2317 = prims.reshape(t2316, (128, 16))  # t2317: \"cuda:0 f32[128, 16]\"\n",
       "  del t2316\n",
       "  t2318 = torch.matmul(t2317, t_fcs_64_weight)  # t2318: \"cuda:0 f32[128, 16]\"\n",
       "    # t2318 = ltorch.matmul(t2317, t_fcs_64_weight)  # t2318: \"cuda:0 f32[128, 16]\"\n",
       "      # t2318 = prims.matmul(t2317, t_fcs_64_weight)  # t2318: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_64_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t317 = load_to_gpu(offloaded_t317, 'cuda:0')  # t317: \"cuda:0 b8[128, 16]\"\n",
       "  [t2325, t2332] = nvFusion36(t317, t2318)\n",
       "    # t2325 = prims.where(t317, t2318, 0.0)  # t2325: \"cuda:0 f32[128, 16]\"\n",
       "    # t2332 = prims.sum(t2325, (0,))  # t2332: \"cuda:0 f32[16]\"\n",
       "  del t317, t2318\n",
       "  t2326 = torch.reshape(t2325, (-1, 16))  # t2326: \"cuda:0 f32[128, 16]\"\n",
       "    # t2326 = ltorch.reshape(t2325, (-1, 16))  # t2326: \"cuda:0 f32[128, 16]\"\n",
       "      # t2326 = prims.reshape(t2325, (128, 16))  # t2326: \"cuda:0 f32[128, 16]\"\n",
       "  del t2325\n",
       "  t2327 = torch.matmul(t2326, t_fcs_63_weight)  # t2327: \"cuda:0 f32[128, 16]\"\n",
       "    # t2327 = ltorch.matmul(t2326, t_fcs_63_weight)  # t2327: \"cuda:0 f32[128, 16]\"\n",
       "      # t2327 = prims.matmul(t2326, t_fcs_63_weight)  # t2327: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_63_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t312 = load_to_gpu(offloaded_t312, 'cuda:0')  # t312: \"cuda:0 b8[128, 16]\"\n",
       "  [t2334, t2341] = nvFusion37(t312, t2327)\n",
       "    # t2334 = prims.where(t312, t2327, 0.0)  # t2334: \"cuda:0 f32[128, 16]\"\n",
       "    # t2341 = prims.sum(t2334, (0,))  # t2341: \"cuda:0 f32[16]\"\n",
       "  del t312, t2327\n",
       "  t2335 = torch.reshape(t2334, (-1, 16))  # t2335: \"cuda:0 f32[128, 16]\"\n",
       "    # t2335 = ltorch.reshape(t2334, (-1, 16))  # t2335: \"cuda:0 f32[128, 16]\"\n",
       "      # t2335 = prims.reshape(t2334, (128, 16))  # t2335: \"cuda:0 f32[128, 16]\"\n",
       "  del t2334\n",
       "  t2336 = torch.matmul(t2335, t_fcs_62_weight)  # t2336: \"cuda:0 f32[128, 16]\"\n",
       "    # t2336 = ltorch.matmul(t2335, t_fcs_62_weight)  # t2336: \"cuda:0 f32[128, 16]\"\n",
       "      # t2336 = prims.matmul(t2335, t_fcs_62_weight)  # t2336: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_62_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t307 = load_to_gpu(offloaded_t307, 'cuda:0')  # t307: \"cuda:0 b8[128, 16]\"\n",
       "  [t2343, t2350] = nvFusion38(t307, t2336)\n",
       "    # t2343 = prims.where(t307, t2336, 0.0)  # t2343: \"cuda:0 f32[128, 16]\"\n",
       "    # t2350 = prims.sum(t2343, (0,))  # t2350: \"cuda:0 f32[16]\"\n",
       "  del t307, t2336\n",
       "  t2344 = torch.reshape(t2343, (-1, 16))  # t2344: \"cuda:0 f32[128, 16]\"\n",
       "    # t2344 = ltorch.reshape(t2343, (-1, 16))  # t2344: \"cuda:0 f32[128, 16]\"\n",
       "      # t2344 = prims.reshape(t2343, (128, 16))  # t2344: \"cuda:0 f32[128, 16]\"\n",
       "  del t2343\n",
       "  t2345 = torch.matmul(t2344, t_fcs_61_weight)  # t2345: \"cuda:0 f32[128, 16]\"\n",
       "    # t2345 = ltorch.matmul(t2344, t_fcs_61_weight)  # t2345: \"cuda:0 f32[128, 16]\"\n",
       "      # t2345 = prims.matmul(t2344, t_fcs_61_weight)  # t2345: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_61_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t302 = load_to_gpu(offloaded_t302, 'cuda:0')  # t302: \"cuda:0 b8[128, 16]\"\n",
       "  [t2352, t2359] = nvFusion39(t302, t2345)\n",
       "    # t2352 = prims.where(t302, t2345, 0.0)  # t2352: \"cuda:0 f32[128, 16]\"\n",
       "    # t2359 = prims.sum(t2352, (0,))  # t2359: \"cuda:0 f32[16]\"\n",
       "  del t302, t2345\n",
       "  t2353 = torch.reshape(t2352, (-1, 16))  # t2353: \"cuda:0 f32[128, 16]\"\n",
       "    # t2353 = ltorch.reshape(t2352, (-1, 16))  # t2353: \"cuda:0 f32[128, 16]\"\n",
       "      # t2353 = prims.reshape(t2352, (128, 16))  # t2353: \"cuda:0 f32[128, 16]\"\n",
       "  del t2352\n",
       "  t2354 = torch.matmul(t2353, t_fcs_60_weight)  # t2354: \"cuda:0 f32[128, 16]\"\n",
       "    # t2354 = ltorch.matmul(t2353, t_fcs_60_weight)  # t2354: \"cuda:0 f32[128, 16]\"\n",
       "      # t2354 = prims.matmul(t2353, t_fcs_60_weight)  # t2354: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_60_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t297 = load_to_gpu(offloaded_t297, 'cuda:0')  # t297: \"cuda:0 b8[128, 16]\"\n",
       "  [t2361, t2368] = nvFusion40(t297, t2354)\n",
       "    # t2361 = prims.where(t297, t2354, 0.0)  # t2361: \"cuda:0 f32[128, 16]\"\n",
       "    # t2368 = prims.sum(t2361, (0,))  # t2368: \"cuda:0 f32[16]\"\n",
       "  del t297, t2354\n",
       "  t2362 = torch.reshape(t2361, (-1, 16))  # t2362: \"cuda:0 f32[128, 16]\"\n",
       "    # t2362 = ltorch.reshape(t2361, (-1, 16))  # t2362: \"cuda:0 f32[128, 16]\"\n",
       "      # t2362 = prims.reshape(t2361, (128, 16))  # t2362: \"cuda:0 f32[128, 16]\"\n",
       "  del t2361\n",
       "  t2363 = torch.matmul(t2362, t_fcs_59_weight)  # t2363: \"cuda:0 f32[128, 16]\"\n",
       "    # t2363 = ltorch.matmul(t2362, t_fcs_59_weight)  # t2363: \"cuda:0 f32[128, 16]\"\n",
       "      # t2363 = prims.matmul(t2362, t_fcs_59_weight)  # t2363: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_59_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t292 = load_to_gpu(offloaded_t292, 'cuda:0')  # t292: \"cuda:0 b8[128, 16]\"\n",
       "  [t2370, t2377] = nvFusion41(t292, t2363)\n",
       "    # t2370 = prims.where(t292, t2363, 0.0)  # t2370: \"cuda:0 f32[128, 16]\"\n",
       "    # t2377 = prims.sum(t2370, (0,))  # t2377: \"cuda:0 f32[16]\"\n",
       "  del t292, t2363\n",
       "  t2371 = torch.reshape(t2370, (-1, 16))  # t2371: \"cuda:0 f32[128, 16]\"\n",
       "    # t2371 = ltorch.reshape(t2370, (-1, 16))  # t2371: \"cuda:0 f32[128, 16]\"\n",
       "      # t2371 = prims.reshape(t2370, (128, 16))  # t2371: \"cuda:0 f32[128, 16]\"\n",
       "  del t2370\n",
       "  t2372 = torch.matmul(t2371, t_fcs_58_weight)  # t2372: \"cuda:0 f32[128, 16]\"\n",
       "    # t2372 = ltorch.matmul(t2371, t_fcs_58_weight)  # t2372: \"cuda:0 f32[128, 16]\"\n",
       "      # t2372 = prims.matmul(t2371, t_fcs_58_weight)  # t2372: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_58_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t287 = load_to_gpu(offloaded_t287, 'cuda:0')  # t287: \"cuda:0 b8[128, 16]\"\n",
       "  [t2379, t2386] = nvFusion42(t287, t2372)\n",
       "    # t2379 = prims.where(t287, t2372, 0.0)  # t2379: \"cuda:0 f32[128, 16]\"\n",
       "    # t2386 = prims.sum(t2379, (0,))  # t2386: \"cuda:0 f32[16]\"\n",
       "  del t287, t2372\n",
       "  t2380 = torch.reshape(t2379, (-1, 16))  # t2380: \"cuda:0 f32[128, 16]\"\n",
       "    # t2380 = ltorch.reshape(t2379, (-1, 16))  # t2380: \"cuda:0 f32[128, 16]\"\n",
       "      # t2380 = prims.reshape(t2379, (128, 16))  # t2380: \"cuda:0 f32[128, 16]\"\n",
       "  del t2379\n",
       "  t2381 = torch.matmul(t2380, t_fcs_57_weight)  # t2381: \"cuda:0 f32[128, 16]\"\n",
       "    # t2381 = ltorch.matmul(t2380, t_fcs_57_weight)  # t2381: \"cuda:0 f32[128, 16]\"\n",
       "      # t2381 = prims.matmul(t2380, t_fcs_57_weight)  # t2381: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_57_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t282 = load_to_gpu(offloaded_t282, 'cuda:0')  # t282: \"cuda:0 b8[128, 16]\"\n",
       "  [t2388, t2395] = nvFusion43(t282, t2381)\n",
       "    # t2388 = prims.where(t282, t2381, 0.0)  # t2388: \"cuda:0 f32[128, 16]\"\n",
       "    # t2395 = prims.sum(t2388, (0,))  # t2395: \"cuda:0 f32[16]\"\n",
       "  del t282, t2381\n",
       "  t2389 = torch.reshape(t2388, (-1, 16))  # t2389: \"cuda:0 f32[128, 16]\"\n",
       "    # t2389 = ltorch.reshape(t2388, (-1, 16))  # t2389: \"cuda:0 f32[128, 16]\"\n",
       "      # t2389 = prims.reshape(t2388, (128, 16))  # t2389: \"cuda:0 f32[128, 16]\"\n",
       "  del t2388\n",
       "  t2390 = torch.matmul(t2389, t_fcs_56_weight)  # t2390: \"cuda:0 f32[128, 16]\"\n",
       "    # t2390 = ltorch.matmul(t2389, t_fcs_56_weight)  # t2390: \"cuda:0 f32[128, 16]\"\n",
       "      # t2390 = prims.matmul(t2389, t_fcs_56_weight)  # t2390: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_56_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t277 = load_to_gpu(offloaded_t277, 'cuda:0')  # t277: \"cuda:0 b8[128, 16]\"\n",
       "  [t2397, t2404] = nvFusion44(t277, t2390)\n",
       "    # t2397 = prims.where(t277, t2390, 0.0)  # t2397: \"cuda:0 f32[128, 16]\"\n",
       "    # t2404 = prims.sum(t2397, (0,))  # t2404: \"cuda:0 f32[16]\"\n",
       "  del t277, t2390\n",
       "  t2398 = torch.reshape(t2397, (-1, 16))  # t2398: \"cuda:0 f32[128, 16]\"\n",
       "    # t2398 = ltorch.reshape(t2397, (-1, 16))  # t2398: \"cuda:0 f32[128, 16]\"\n",
       "      # t2398 = prims.reshape(t2397, (128, 16))  # t2398: \"cuda:0 f32[128, 16]\"\n",
       "  del t2397\n",
       "  t2399 = torch.matmul(t2398, t_fcs_55_weight)  # t2399: \"cuda:0 f32[128, 16]\"\n",
       "    # t2399 = ltorch.matmul(t2398, t_fcs_55_weight)  # t2399: \"cuda:0 f32[128, 16]\"\n",
       "      # t2399 = prims.matmul(t2398, t_fcs_55_weight)  # t2399: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_55_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t272 = load_to_gpu(offloaded_t272, 'cuda:0')  # t272: \"cuda:0 b8[128, 16]\"\n",
       "  [t2406, t2413] = nvFusion45(t272, t2399)\n",
       "    # t2406 = prims.where(t272, t2399, 0.0)  # t2406: \"cuda:0 f32[128, 16]\"\n",
       "    # t2413 = prims.sum(t2406, (0,))  # t2413: \"cuda:0 f32[16]\"\n",
       "  del t272, t2399\n",
       "  t2407 = torch.reshape(t2406, (-1, 16))  # t2407: \"cuda:0 f32[128, 16]\"\n",
       "    # t2407 = ltorch.reshape(t2406, (-1, 16))  # t2407: \"cuda:0 f32[128, 16]\"\n",
       "      # t2407 = prims.reshape(t2406, (128, 16))  # t2407: \"cuda:0 f32[128, 16]\"\n",
       "  del t2406\n",
       "  t2408 = torch.matmul(t2407, t_fcs_54_weight)  # t2408: \"cuda:0 f32[128, 16]\"\n",
       "    # t2408 = ltorch.matmul(t2407, t_fcs_54_weight)  # t2408: \"cuda:0 f32[128, 16]\"\n",
       "      # t2408 = prims.matmul(t2407, t_fcs_54_weight)  # t2408: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_54_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t267 = load_to_gpu(offloaded_t267, 'cuda:0')  # t267: \"cuda:0 b8[128, 16]\"\n",
       "  [t2415, t2422] = nvFusion46(t267, t2408)\n",
       "    # t2415 = prims.where(t267, t2408, 0.0)  # t2415: \"cuda:0 f32[128, 16]\"\n",
       "    # t2422 = prims.sum(t2415, (0,))  # t2422: \"cuda:0 f32[16]\"\n",
       "  del t267, t2408\n",
       "  t2416 = torch.reshape(t2415, (-1, 16))  # t2416: \"cuda:0 f32[128, 16]\"\n",
       "    # t2416 = ltorch.reshape(t2415, (-1, 16))  # t2416: \"cuda:0 f32[128, 16]\"\n",
       "      # t2416 = prims.reshape(t2415, (128, 16))  # t2416: \"cuda:0 f32[128, 16]\"\n",
       "  del t2415\n",
       "  t2417 = torch.matmul(t2416, t_fcs_53_weight)  # t2417: \"cuda:0 f32[128, 16]\"\n",
       "    # t2417 = ltorch.matmul(t2416, t_fcs_53_weight)  # t2417: \"cuda:0 f32[128, 16]\"\n",
       "      # t2417 = prims.matmul(t2416, t_fcs_53_weight)  # t2417: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_53_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t262 = load_to_gpu(offloaded_t262, 'cuda:0')  # t262: \"cuda:0 b8[128, 16]\"\n",
       "  [t2424, t2431] = nvFusion47(t262, t2417)\n",
       "    # t2424 = prims.where(t262, t2417, 0.0)  # t2424: \"cuda:0 f32[128, 16]\"\n",
       "    # t2431 = prims.sum(t2424, (0,))  # t2431: \"cuda:0 f32[16]\"\n",
       "  del t262, t2417\n",
       "  t2425 = torch.reshape(t2424, (-1, 16))  # t2425: \"cuda:0 f32[128, 16]\"\n",
       "    # t2425 = ltorch.reshape(t2424, (-1, 16))  # t2425: \"cuda:0 f32[128, 16]\"\n",
       "      # t2425 = prims.reshape(t2424, (128, 16))  # t2425: \"cuda:0 f32[128, 16]\"\n",
       "  del t2424\n",
       "  t2426 = torch.matmul(t2425, t_fcs_52_weight)  # t2426: \"cuda:0 f32[128, 16]\"\n",
       "    # t2426 = ltorch.matmul(t2425, t_fcs_52_weight)  # t2426: \"cuda:0 f32[128, 16]\"\n",
       "      # t2426 = prims.matmul(t2425, t_fcs_52_weight)  # t2426: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_52_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t257 = load_to_gpu(offloaded_t257, 'cuda:0')  # t257: \"cuda:0 b8[128, 16]\"\n",
       "  [t2433, t2440] = nvFusion48(t257, t2426)\n",
       "    # t2433 = prims.where(t257, t2426, 0.0)  # t2433: \"cuda:0 f32[128, 16]\"\n",
       "    # t2440 = prims.sum(t2433, (0,))  # t2440: \"cuda:0 f32[16]\"\n",
       "  del t257, t2426\n",
       "  t2434 = torch.reshape(t2433, (-1, 16))  # t2434: \"cuda:0 f32[128, 16]\"\n",
       "    # t2434 = ltorch.reshape(t2433, (-1, 16))  # t2434: \"cuda:0 f32[128, 16]\"\n",
       "      # t2434 = prims.reshape(t2433, (128, 16))  # t2434: \"cuda:0 f32[128, 16]\"\n",
       "  del t2433\n",
       "  t2435 = torch.matmul(t2434, t_fcs_51_weight)  # t2435: \"cuda:0 f32[128, 16]\"\n",
       "    # t2435 = ltorch.matmul(t2434, t_fcs_51_weight)  # t2435: \"cuda:0 f32[128, 16]\"\n",
       "      # t2435 = prims.matmul(t2434, t_fcs_51_weight)  # t2435: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_51_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t252 = load_to_gpu(offloaded_t252, 'cuda:0')  # t252: \"cuda:0 b8[128, 16]\"\n",
       "  [t2442, t2449] = nvFusion49(t252, t2435)\n",
       "    # t2442 = prims.where(t252, t2435, 0.0)  # t2442: \"cuda:0 f32[128, 16]\"\n",
       "    # t2449 = prims.sum(t2442, (0,))  # t2449: \"cuda:0 f32[16]\"\n",
       "  del t252, t2435\n",
       "  t2443 = torch.reshape(t2442, (-1, 16))  # t2443: \"cuda:0 f32[128, 16]\"\n",
       "    # t2443 = ltorch.reshape(t2442, (-1, 16))  # t2443: \"cuda:0 f32[128, 16]\"\n",
       "      # t2443 = prims.reshape(t2442, (128, 16))  # t2443: \"cuda:0 f32[128, 16]\"\n",
       "  del t2442\n",
       "  t2444 = torch.matmul(t2443, t_fcs_50_weight)  # t2444: \"cuda:0 f32[128, 16]\"\n",
       "    # t2444 = ltorch.matmul(t2443, t_fcs_50_weight)  # t2444: \"cuda:0 f32[128, 16]\"\n",
       "      # t2444 = prims.matmul(t2443, t_fcs_50_weight)  # t2444: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_50_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t247 = load_to_gpu(offloaded_t247, 'cuda:0')  # t247: \"cuda:0 b8[128, 16]\"\n",
       "  [t2451, t2458] = nvFusion50(t247, t2444)\n",
       "    # t2451 = prims.where(t247, t2444, 0.0)  # t2451: \"cuda:0 f32[128, 16]\"\n",
       "    # t2458 = prims.sum(t2451, (0,))  # t2458: \"cuda:0 f32[16]\"\n",
       "  del t247, t2444\n",
       "  t2452 = torch.reshape(t2451, (-1, 16))  # t2452: \"cuda:0 f32[128, 16]\"\n",
       "    # t2452 = ltorch.reshape(t2451, (-1, 16))  # t2452: \"cuda:0 f32[128, 16]\"\n",
       "      # t2452 = prims.reshape(t2451, (128, 16))  # t2452: \"cuda:0 f32[128, 16]\"\n",
       "  del t2451\n",
       "  t2453 = torch.matmul(t2452, t_fcs_49_weight)  # t2453: \"cuda:0 f32[128, 16]\"\n",
       "    # t2453 = ltorch.matmul(t2452, t_fcs_49_weight)  # t2453: \"cuda:0 f32[128, 16]\"\n",
       "      # t2453 = prims.matmul(t2452, t_fcs_49_weight)  # t2453: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_49_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t242 = load_to_gpu(offloaded_t242, 'cuda:0')  # t242: \"cuda:0 b8[128, 16]\"\n",
       "  [t2460, t2467] = nvFusion51(t242, t2453)\n",
       "    # t2460 = prims.where(t242, t2453, 0.0)  # t2460: \"cuda:0 f32[128, 16]\"\n",
       "    # t2467 = prims.sum(t2460, (0,))  # t2467: \"cuda:0 f32[16]\"\n",
       "  del t242, t2453\n",
       "  t2461 = torch.reshape(t2460, (-1, 16))  # t2461: \"cuda:0 f32[128, 16]\"\n",
       "    # t2461 = ltorch.reshape(t2460, (-1, 16))  # t2461: \"cuda:0 f32[128, 16]\"\n",
       "      # t2461 = prims.reshape(t2460, (128, 16))  # t2461: \"cuda:0 f32[128, 16]\"\n",
       "  del t2460\n",
       "  t2462 = torch.matmul(t2461, t_fcs_48_weight)  # t2462: \"cuda:0 f32[128, 16]\"\n",
       "    # t2462 = ltorch.matmul(t2461, t_fcs_48_weight)  # t2462: \"cuda:0 f32[128, 16]\"\n",
       "      # t2462 = prims.matmul(t2461, t_fcs_48_weight)  # t2462: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_48_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t237 = load_to_gpu(offloaded_t237, 'cuda:0')  # t237: \"cuda:0 b8[128, 16]\"\n",
       "  [t2469, t2476] = nvFusion52(t237, t2462)\n",
       "    # t2469 = prims.where(t237, t2462, 0.0)  # t2469: \"cuda:0 f32[128, 16]\"\n",
       "    # t2476 = prims.sum(t2469, (0,))  # t2476: \"cuda:0 f32[16]\"\n",
       "  del t237, t2462\n",
       "  t2470 = torch.reshape(t2469, (-1, 16))  # t2470: \"cuda:0 f32[128, 16]\"\n",
       "    # t2470 = ltorch.reshape(t2469, (-1, 16))  # t2470: \"cuda:0 f32[128, 16]\"\n",
       "      # t2470 = prims.reshape(t2469, (128, 16))  # t2470: \"cuda:0 f32[128, 16]\"\n",
       "  del t2469\n",
       "  t2471 = torch.matmul(t2470, t_fcs_47_weight)  # t2471: \"cuda:0 f32[128, 16]\"\n",
       "    # t2471 = ltorch.matmul(t2470, t_fcs_47_weight)  # t2471: \"cuda:0 f32[128, 16]\"\n",
       "      # t2471 = prims.matmul(t2470, t_fcs_47_weight)  # t2471: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_47_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t232 = load_to_gpu(offloaded_t232, 'cuda:0')  # t232: \"cuda:0 b8[128, 16]\"\n",
       "  [t2478, t2485] = nvFusion53(t232, t2471)\n",
       "    # t2478 = prims.where(t232, t2471, 0.0)  # t2478: \"cuda:0 f32[128, 16]\"\n",
       "    # t2485 = prims.sum(t2478, (0,))  # t2485: \"cuda:0 f32[16]\"\n",
       "  del t232, t2471\n",
       "  t2479 = torch.reshape(t2478, (-1, 16))  # t2479: \"cuda:0 f32[128, 16]\"\n",
       "    # t2479 = ltorch.reshape(t2478, (-1, 16))  # t2479: \"cuda:0 f32[128, 16]\"\n",
       "      # t2479 = prims.reshape(t2478, (128, 16))  # t2479: \"cuda:0 f32[128, 16]\"\n",
       "  del t2478\n",
       "  t2480 = torch.matmul(t2479, t_fcs_46_weight)  # t2480: \"cuda:0 f32[128, 16]\"\n",
       "    # t2480 = ltorch.matmul(t2479, t_fcs_46_weight)  # t2480: \"cuda:0 f32[128, 16]\"\n",
       "      # t2480 = prims.matmul(t2479, t_fcs_46_weight)  # t2480: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_46_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t227 = load_to_gpu(offloaded_t227, 'cuda:0')  # t227: \"cuda:0 b8[128, 16]\"\n",
       "  [t2487, t2494] = nvFusion54(t227, t2480)\n",
       "    # t2487 = prims.where(t227, t2480, 0.0)  # t2487: \"cuda:0 f32[128, 16]\"\n",
       "    # t2494 = prims.sum(t2487, (0,))  # t2494: \"cuda:0 f32[16]\"\n",
       "  del t227, t2480\n",
       "  t2488 = torch.reshape(t2487, (-1, 16))  # t2488: \"cuda:0 f32[128, 16]\"\n",
       "    # t2488 = ltorch.reshape(t2487, (-1, 16))  # t2488: \"cuda:0 f32[128, 16]\"\n",
       "      # t2488 = prims.reshape(t2487, (128, 16))  # t2488: \"cuda:0 f32[128, 16]\"\n",
       "  del t2487\n",
       "  t2489 = torch.matmul(t2488, t_fcs_45_weight)  # t2489: \"cuda:0 f32[128, 16]\"\n",
       "    # t2489 = ltorch.matmul(t2488, t_fcs_45_weight)  # t2489: \"cuda:0 f32[128, 16]\"\n",
       "      # t2489 = prims.matmul(t2488, t_fcs_45_weight)  # t2489: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_45_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t222 = load_to_gpu(offloaded_t222, 'cuda:0')  # t222: \"cuda:0 b8[128, 16]\"\n",
       "  [t2496, t2503] = nvFusion55(t222, t2489)\n",
       "    # t2496 = prims.where(t222, t2489, 0.0)  # t2496: \"cuda:0 f32[128, 16]\"\n",
       "    # t2503 = prims.sum(t2496, (0,))  # t2503: \"cuda:0 f32[16]\"\n",
       "  del t222, t2489\n",
       "  t2497 = torch.reshape(t2496, (-1, 16))  # t2497: \"cuda:0 f32[128, 16]\"\n",
       "    # t2497 = ltorch.reshape(t2496, (-1, 16))  # t2497: \"cuda:0 f32[128, 16]\"\n",
       "      # t2497 = prims.reshape(t2496, (128, 16))  # t2497: \"cuda:0 f32[128, 16]\"\n",
       "  del t2496\n",
       "  t2498 = torch.matmul(t2497, t_fcs_44_weight)  # t2498: \"cuda:0 f32[128, 16]\"\n",
       "    # t2498 = ltorch.matmul(t2497, t_fcs_44_weight)  # t2498: \"cuda:0 f32[128, 16]\"\n",
       "      # t2498 = prims.matmul(t2497, t_fcs_44_weight)  # t2498: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_44_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t217 = load_to_gpu(offloaded_t217, 'cuda:0')  # t217: \"cuda:0 b8[128, 16]\"\n",
       "  [t2505, t2512] = nvFusion56(t217, t2498)\n",
       "    # t2505 = prims.where(t217, t2498, 0.0)  # t2505: \"cuda:0 f32[128, 16]\"\n",
       "    # t2512 = prims.sum(t2505, (0,))  # t2512: \"cuda:0 f32[16]\"\n",
       "  del t217, t2498\n",
       "  t2506 = torch.reshape(t2505, (-1, 16))  # t2506: \"cuda:0 f32[128, 16]\"\n",
       "    # t2506 = ltorch.reshape(t2505, (-1, 16))  # t2506: \"cuda:0 f32[128, 16]\"\n",
       "      # t2506 = prims.reshape(t2505, (128, 16))  # t2506: \"cuda:0 f32[128, 16]\"\n",
       "  del t2505\n",
       "  t2507 = torch.matmul(t2506, t_fcs_43_weight)  # t2507: \"cuda:0 f32[128, 16]\"\n",
       "    # t2507 = ltorch.matmul(t2506, t_fcs_43_weight)  # t2507: \"cuda:0 f32[128, 16]\"\n",
       "      # t2507 = prims.matmul(t2506, t_fcs_43_weight)  # t2507: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_43_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t212 = load_to_gpu(offloaded_t212, 'cuda:0')  # t212: \"cuda:0 b8[128, 16]\"\n",
       "  [t2514, t2521] = nvFusion57(t212, t2507)\n",
       "    # t2514 = prims.where(t212, t2507, 0.0)  # t2514: \"cuda:0 f32[128, 16]\"\n",
       "    # t2521 = prims.sum(t2514, (0,))  # t2521: \"cuda:0 f32[16]\"\n",
       "  del t212, t2507\n",
       "  t2515 = torch.reshape(t2514, (-1, 16))  # t2515: \"cuda:0 f32[128, 16]\"\n",
       "    # t2515 = ltorch.reshape(t2514, (-1, 16))  # t2515: \"cuda:0 f32[128, 16]\"\n",
       "      # t2515 = prims.reshape(t2514, (128, 16))  # t2515: \"cuda:0 f32[128, 16]\"\n",
       "  del t2514\n",
       "  t2516 = torch.matmul(t2515, t_fcs_42_weight)  # t2516: \"cuda:0 f32[128, 16]\"\n",
       "    # t2516 = ltorch.matmul(t2515, t_fcs_42_weight)  # t2516: \"cuda:0 f32[128, 16]\"\n",
       "      # t2516 = prims.matmul(t2515, t_fcs_42_weight)  # t2516: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_42_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t207 = load_to_gpu(offloaded_t207, 'cuda:0')  # t207: \"cuda:0 b8[128, 16]\"\n",
       "  [t2523, t2530] = nvFusion58(t207, t2516)\n",
       "    # t2523 = prims.where(t207, t2516, 0.0)  # t2523: \"cuda:0 f32[128, 16]\"\n",
       "    # t2530 = prims.sum(t2523, (0,))  # t2530: \"cuda:0 f32[16]\"\n",
       "  del t207, t2516\n",
       "  t2524 = torch.reshape(t2523, (-1, 16))  # t2524: \"cuda:0 f32[128, 16]\"\n",
       "    # t2524 = ltorch.reshape(t2523, (-1, 16))  # t2524: \"cuda:0 f32[128, 16]\"\n",
       "      # t2524 = prims.reshape(t2523, (128, 16))  # t2524: \"cuda:0 f32[128, 16]\"\n",
       "  del t2523\n",
       "  t2525 = torch.matmul(t2524, t_fcs_41_weight)  # t2525: \"cuda:0 f32[128, 16]\"\n",
       "    # t2525 = ltorch.matmul(t2524, t_fcs_41_weight)  # t2525: \"cuda:0 f32[128, 16]\"\n",
       "      # t2525 = prims.matmul(t2524, t_fcs_41_weight)  # t2525: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_41_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t202 = load_to_gpu(offloaded_t202, 'cuda:0')  # t202: \"cuda:0 b8[128, 16]\"\n",
       "  [t2532, t2539] = nvFusion59(t202, t2525)\n",
       "    # t2532 = prims.where(t202, t2525, 0.0)  # t2532: \"cuda:0 f32[128, 16]\"\n",
       "    # t2539 = prims.sum(t2532, (0,))  # t2539: \"cuda:0 f32[16]\"\n",
       "  del t202, t2525\n",
       "  t2533 = torch.reshape(t2532, (-1, 16))  # t2533: \"cuda:0 f32[128, 16]\"\n",
       "    # t2533 = ltorch.reshape(t2532, (-1, 16))  # t2533: \"cuda:0 f32[128, 16]\"\n",
       "      # t2533 = prims.reshape(t2532, (128, 16))  # t2533: \"cuda:0 f32[128, 16]\"\n",
       "  del t2532\n",
       "  t2534 = torch.matmul(t2533, t_fcs_40_weight)  # t2534: \"cuda:0 f32[128, 16]\"\n",
       "    # t2534 = ltorch.matmul(t2533, t_fcs_40_weight)  # t2534: \"cuda:0 f32[128, 16]\"\n",
       "      # t2534 = prims.matmul(t2533, t_fcs_40_weight)  # t2534: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_40_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t197 = load_to_gpu(offloaded_t197, 'cuda:0')  # t197: \"cuda:0 b8[128, 16]\"\n",
       "  [t2541, t2548] = nvFusion60(t197, t2534)\n",
       "    # t2541 = prims.where(t197, t2534, 0.0)  # t2541: \"cuda:0 f32[128, 16]\"\n",
       "    # t2548 = prims.sum(t2541, (0,))  # t2548: \"cuda:0 f32[16]\"\n",
       "  del t197, t2534\n",
       "  t2542 = torch.reshape(t2541, (-1, 16))  # t2542: \"cuda:0 f32[128, 16]\"\n",
       "    # t2542 = ltorch.reshape(t2541, (-1, 16))  # t2542: \"cuda:0 f32[128, 16]\"\n",
       "      # t2542 = prims.reshape(t2541, (128, 16))  # t2542: \"cuda:0 f32[128, 16]\"\n",
       "  del t2541\n",
       "  t2543 = torch.matmul(t2542, t_fcs_39_weight)  # t2543: \"cuda:0 f32[128, 16]\"\n",
       "    # t2543 = ltorch.matmul(t2542, t_fcs_39_weight)  # t2543: \"cuda:0 f32[128, 16]\"\n",
       "      # t2543 = prims.matmul(t2542, t_fcs_39_weight)  # t2543: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_39_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t192 = load_to_gpu(offloaded_t192, 'cuda:0')  # t192: \"cuda:0 b8[128, 16]\"\n",
       "  [t2550, t2557] = nvFusion61(t192, t2543)\n",
       "    # t2550 = prims.where(t192, t2543, 0.0)  # t2550: \"cuda:0 f32[128, 16]\"\n",
       "    # t2557 = prims.sum(t2550, (0,))  # t2557: \"cuda:0 f32[16]\"\n",
       "  del t192, t2543\n",
       "  t2551 = torch.reshape(t2550, (-1, 16))  # t2551: \"cuda:0 f32[128, 16]\"\n",
       "    # t2551 = ltorch.reshape(t2550, (-1, 16))  # t2551: \"cuda:0 f32[128, 16]\"\n",
       "      # t2551 = prims.reshape(t2550, (128, 16))  # t2551: \"cuda:0 f32[128, 16]\"\n",
       "  del t2550\n",
       "  t2552 = torch.matmul(t2551, t_fcs_38_weight)  # t2552: \"cuda:0 f32[128, 16]\"\n",
       "    # t2552 = ltorch.matmul(t2551, t_fcs_38_weight)  # t2552: \"cuda:0 f32[128, 16]\"\n",
       "      # t2552 = prims.matmul(t2551, t_fcs_38_weight)  # t2552: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_38_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t187 = load_to_gpu(offloaded_t187, 'cuda:0')  # t187: \"cuda:0 b8[128, 16]\"\n",
       "  [t2559, t2566] = nvFusion62(t187, t2552)\n",
       "    # t2559 = prims.where(t187, t2552, 0.0)  # t2559: \"cuda:0 f32[128, 16]\"\n",
       "    # t2566 = prims.sum(t2559, (0,))  # t2566: \"cuda:0 f32[16]\"\n",
       "  del t187, t2552\n",
       "  t2560 = torch.reshape(t2559, (-1, 16))  # t2560: \"cuda:0 f32[128, 16]\"\n",
       "    # t2560 = ltorch.reshape(t2559, (-1, 16))  # t2560: \"cuda:0 f32[128, 16]\"\n",
       "      # t2560 = prims.reshape(t2559, (128, 16))  # t2560: \"cuda:0 f32[128, 16]\"\n",
       "  del t2559\n",
       "  t2561 = torch.matmul(t2560, t_fcs_37_weight)  # t2561: \"cuda:0 f32[128, 16]\"\n",
       "    # t2561 = ltorch.matmul(t2560, t_fcs_37_weight)  # t2561: \"cuda:0 f32[128, 16]\"\n",
       "      # t2561 = prims.matmul(t2560, t_fcs_37_weight)  # t2561: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_37_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t182 = load_to_gpu(offloaded_t182, 'cuda:0')  # t182: \"cuda:0 b8[128, 16]\"\n",
       "  [t2568, t2575] = nvFusion63(t182, t2561)\n",
       "    # t2568 = prims.where(t182, t2561, 0.0)  # t2568: \"cuda:0 f32[128, 16]\"\n",
       "    # t2575 = prims.sum(t2568, (0,))  # t2575: \"cuda:0 f32[16]\"\n",
       "  del t182, t2561\n",
       "  t2569 = torch.reshape(t2568, (-1, 16))  # t2569: \"cuda:0 f32[128, 16]\"\n",
       "    # t2569 = ltorch.reshape(t2568, (-1, 16))  # t2569: \"cuda:0 f32[128, 16]\"\n",
       "      # t2569 = prims.reshape(t2568, (128, 16))  # t2569: \"cuda:0 f32[128, 16]\"\n",
       "  del t2568\n",
       "  t2570 = torch.matmul(t2569, t_fcs_36_weight)  # t2570: \"cuda:0 f32[128, 16]\"\n",
       "    # t2570 = ltorch.matmul(t2569, t_fcs_36_weight)  # t2570: \"cuda:0 f32[128, 16]\"\n",
       "      # t2570 = prims.matmul(t2569, t_fcs_36_weight)  # t2570: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_36_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t177 = load_to_gpu(offloaded_t177, 'cuda:0')  # t177: \"cuda:0 b8[128, 16]\"\n",
       "  [t2577, t2584] = nvFusion64(t177, t2570)\n",
       "    # t2577 = prims.where(t177, t2570, 0.0)  # t2577: \"cuda:0 f32[128, 16]\"\n",
       "    # t2584 = prims.sum(t2577, (0,))  # t2584: \"cuda:0 f32[16]\"\n",
       "  del t177, t2570\n",
       "  t2578 = torch.reshape(t2577, (-1, 16))  # t2578: \"cuda:0 f32[128, 16]\"\n",
       "    # t2578 = ltorch.reshape(t2577, (-1, 16))  # t2578: \"cuda:0 f32[128, 16]\"\n",
       "      # t2578 = prims.reshape(t2577, (128, 16))  # t2578: \"cuda:0 f32[128, 16]\"\n",
       "  del t2577\n",
       "  t2579 = torch.matmul(t2578, t_fcs_35_weight)  # t2579: \"cuda:0 f32[128, 16]\"\n",
       "    # t2579 = ltorch.matmul(t2578, t_fcs_35_weight)  # t2579: \"cuda:0 f32[128, 16]\"\n",
       "      # t2579 = prims.matmul(t2578, t_fcs_35_weight)  # t2579: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_35_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t172 = load_to_gpu(offloaded_t172, 'cuda:0')  # t172: \"cuda:0 b8[128, 16]\"\n",
       "  [t2586, t2593] = nvFusion65(t172, t2579)\n",
       "    # t2586 = prims.where(t172, t2579, 0.0)  # t2586: \"cuda:0 f32[128, 16]\"\n",
       "    # t2593 = prims.sum(t2586, (0,))  # t2593: \"cuda:0 f32[16]\"\n",
       "  del t172, t2579\n",
       "  t2587 = torch.reshape(t2586, (-1, 16))  # t2587: \"cuda:0 f32[128, 16]\"\n",
       "    # t2587 = ltorch.reshape(t2586, (-1, 16))  # t2587: \"cuda:0 f32[128, 16]\"\n",
       "      # t2587 = prims.reshape(t2586, (128, 16))  # t2587: \"cuda:0 f32[128, 16]\"\n",
       "  del t2586\n",
       "  t2588 = torch.matmul(t2587, t_fcs_34_weight)  # t2588: \"cuda:0 f32[128, 16]\"\n",
       "    # t2588 = ltorch.matmul(t2587, t_fcs_34_weight)  # t2588: \"cuda:0 f32[128, 16]\"\n",
       "      # t2588 = prims.matmul(t2587, t_fcs_34_weight)  # t2588: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_34_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t167 = load_to_gpu(offloaded_t167, 'cuda:0')  # t167: \"cuda:0 b8[128, 16]\"\n",
       "  [t2595, t2602] = nvFusion66(t167, t2588)\n",
       "    # t2595 = prims.where(t167, t2588, 0.0)  # t2595: \"cuda:0 f32[128, 16]\"\n",
       "    # t2602 = prims.sum(t2595, (0,))  # t2602: \"cuda:0 f32[16]\"\n",
       "  del t167, t2588\n",
       "  t2596 = torch.reshape(t2595, (-1, 16))  # t2596: \"cuda:0 f32[128, 16]\"\n",
       "    # t2596 = ltorch.reshape(t2595, (-1, 16))  # t2596: \"cuda:0 f32[128, 16]\"\n",
       "      # t2596 = prims.reshape(t2595, (128, 16))  # t2596: \"cuda:0 f32[128, 16]\"\n",
       "  del t2595\n",
       "  t2597 = torch.matmul(t2596, t_fcs_33_weight)  # t2597: \"cuda:0 f32[128, 16]\"\n",
       "    # t2597 = ltorch.matmul(t2596, t_fcs_33_weight)  # t2597: \"cuda:0 f32[128, 16]\"\n",
       "      # t2597 = prims.matmul(t2596, t_fcs_33_weight)  # t2597: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_33_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t162 = load_to_gpu(offloaded_t162, 'cuda:0')  # t162: \"cuda:0 b8[128, 16]\"\n",
       "  [t2604, t2611] = nvFusion67(t162, t2597)\n",
       "    # t2604 = prims.where(t162, t2597, 0.0)  # t2604: \"cuda:0 f32[128, 16]\"\n",
       "    # t2611 = prims.sum(t2604, (0,))  # t2611: \"cuda:0 f32[16]\"\n",
       "  del t162, t2597\n",
       "  t2605 = torch.reshape(t2604, (-1, 16))  # t2605: \"cuda:0 f32[128, 16]\"\n",
       "    # t2605 = ltorch.reshape(t2604, (-1, 16))  # t2605: \"cuda:0 f32[128, 16]\"\n",
       "      # t2605 = prims.reshape(t2604, (128, 16))  # t2605: \"cuda:0 f32[128, 16]\"\n",
       "  del t2604\n",
       "  t2606 = torch.matmul(t2605, t_fcs_32_weight)  # t2606: \"cuda:0 f32[128, 16]\"\n",
       "    # t2606 = ltorch.matmul(t2605, t_fcs_32_weight)  # t2606: \"cuda:0 f32[128, 16]\"\n",
       "      # t2606 = prims.matmul(t2605, t_fcs_32_weight)  # t2606: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_32_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t157 = load_to_gpu(offloaded_t157, 'cuda:0')  # t157: \"cuda:0 b8[128, 16]\"\n",
       "  [t2613, t2620] = nvFusion68(t157, t2606)\n",
       "    # t2613 = prims.where(t157, t2606, 0.0)  # t2613: \"cuda:0 f32[128, 16]\"\n",
       "    # t2620 = prims.sum(t2613, (0,))  # t2620: \"cuda:0 f32[16]\"\n",
       "  del t157, t2606\n",
       "  t2614 = torch.reshape(t2613, (-1, 16))  # t2614: \"cuda:0 f32[128, 16]\"\n",
       "    # t2614 = ltorch.reshape(t2613, (-1, 16))  # t2614: \"cuda:0 f32[128, 16]\"\n",
       "      # t2614 = prims.reshape(t2613, (128, 16))  # t2614: \"cuda:0 f32[128, 16]\"\n",
       "  del t2613\n",
       "  t2615 = torch.matmul(t2614, t_fcs_31_weight)  # t2615: \"cuda:0 f32[128, 16]\"\n",
       "    # t2615 = ltorch.matmul(t2614, t_fcs_31_weight)  # t2615: \"cuda:0 f32[128, 16]\"\n",
       "      # t2615 = prims.matmul(t2614, t_fcs_31_weight)  # t2615: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_31_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t152 = load_to_gpu(offloaded_t152, 'cuda:0')  # t152: \"cuda:0 b8[128, 16]\"\n",
       "  [t2622, t2629] = nvFusion69(t152, t2615)\n",
       "    # t2622 = prims.where(t152, t2615, 0.0)  # t2622: \"cuda:0 f32[128, 16]\"\n",
       "    # t2629 = prims.sum(t2622, (0,))  # t2629: \"cuda:0 f32[16]\"\n",
       "  del t152, t2615\n",
       "  t2623 = torch.reshape(t2622, (-1, 16))  # t2623: \"cuda:0 f32[128, 16]\"\n",
       "    # t2623 = ltorch.reshape(t2622, (-1, 16))  # t2623: \"cuda:0 f32[128, 16]\"\n",
       "      # t2623 = prims.reshape(t2622, (128, 16))  # t2623: \"cuda:0 f32[128, 16]\"\n",
       "  del t2622\n",
       "  t2624 = torch.matmul(t2623, t_fcs_30_weight)  # t2624: \"cuda:0 f32[128, 16]\"\n",
       "    # t2624 = ltorch.matmul(t2623, t_fcs_30_weight)  # t2624: \"cuda:0 f32[128, 16]\"\n",
       "      # t2624 = prims.matmul(t2623, t_fcs_30_weight)  # t2624: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_30_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t147 = load_to_gpu(offloaded_t147, 'cuda:0')  # t147: \"cuda:0 b8[128, 16]\"\n",
       "  [t2631, t2638] = nvFusion70(t147, t2624)\n",
       "    # t2631 = prims.where(t147, t2624, 0.0)  # t2631: \"cuda:0 f32[128, 16]\"\n",
       "    # t2638 = prims.sum(t2631, (0,))  # t2638: \"cuda:0 f32[16]\"\n",
       "  del t147, t2624\n",
       "  t2632 = torch.reshape(t2631, (-1, 16))  # t2632: \"cuda:0 f32[128, 16]\"\n",
       "    # t2632 = ltorch.reshape(t2631, (-1, 16))  # t2632: \"cuda:0 f32[128, 16]\"\n",
       "      # t2632 = prims.reshape(t2631, (128, 16))  # t2632: \"cuda:0 f32[128, 16]\"\n",
       "  del t2631\n",
       "  t2633 = torch.matmul(t2632, t_fcs_29_weight)  # t2633: \"cuda:0 f32[128, 16]\"\n",
       "    # t2633 = ltorch.matmul(t2632, t_fcs_29_weight)  # t2633: \"cuda:0 f32[128, 16]\"\n",
       "      # t2633 = prims.matmul(t2632, t_fcs_29_weight)  # t2633: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_29_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t142 = load_to_gpu(offloaded_t142, 'cuda:0')  # t142: \"cuda:0 b8[128, 16]\"\n",
       "  [t2640, t2647] = nvFusion71(t142, t2633)\n",
       "    # t2640 = prims.where(t142, t2633, 0.0)  # t2640: \"cuda:0 f32[128, 16]\"\n",
       "    # t2647 = prims.sum(t2640, (0,))  # t2647: \"cuda:0 f32[16]\"\n",
       "  del t142, t2633\n",
       "  t2641 = torch.reshape(t2640, (-1, 16))  # t2641: \"cuda:0 f32[128, 16]\"\n",
       "    # t2641 = ltorch.reshape(t2640, (-1, 16))  # t2641: \"cuda:0 f32[128, 16]\"\n",
       "      # t2641 = prims.reshape(t2640, (128, 16))  # t2641: \"cuda:0 f32[128, 16]\"\n",
       "  del t2640\n",
       "  t2642 = torch.matmul(t2641, t_fcs_28_weight)  # t2642: \"cuda:0 f32[128, 16]\"\n",
       "    # t2642 = ltorch.matmul(t2641, t_fcs_28_weight)  # t2642: \"cuda:0 f32[128, 16]\"\n",
       "      # t2642 = prims.matmul(t2641, t_fcs_28_weight)  # t2642: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_28_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t137 = load_to_gpu(offloaded_t137, 'cuda:0')  # t137: \"cuda:0 b8[128, 16]\"\n",
       "  [t2649, t2656] = nvFusion72(t137, t2642)\n",
       "    # t2649 = prims.where(t137, t2642, 0.0)  # t2649: \"cuda:0 f32[128, 16]\"\n",
       "    # t2656 = prims.sum(t2649, (0,))  # t2656: \"cuda:0 f32[16]\"\n",
       "  del t137, t2642\n",
       "  t2650 = torch.reshape(t2649, (-1, 16))  # t2650: \"cuda:0 f32[128, 16]\"\n",
       "    # t2650 = ltorch.reshape(t2649, (-1, 16))  # t2650: \"cuda:0 f32[128, 16]\"\n",
       "      # t2650 = prims.reshape(t2649, (128, 16))  # t2650: \"cuda:0 f32[128, 16]\"\n",
       "  del t2649\n",
       "  t2651 = torch.matmul(t2650, t_fcs_27_weight)  # t2651: \"cuda:0 f32[128, 16]\"\n",
       "    # t2651 = ltorch.matmul(t2650, t_fcs_27_weight)  # t2651: \"cuda:0 f32[128, 16]\"\n",
       "      # t2651 = prims.matmul(t2650, t_fcs_27_weight)  # t2651: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_27_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t132 = load_to_gpu(offloaded_t132, 'cuda:0')  # t132: \"cuda:0 b8[128, 16]\"\n",
       "  [t2658, t2665] = nvFusion73(t132, t2651)\n",
       "    # t2658 = prims.where(t132, t2651, 0.0)  # t2658: \"cuda:0 f32[128, 16]\"\n",
       "    # t2665 = prims.sum(t2658, (0,))  # t2665: \"cuda:0 f32[16]\"\n",
       "  del t132, t2651\n",
       "  t2659 = torch.reshape(t2658, (-1, 16))  # t2659: \"cuda:0 f32[128, 16]\"\n",
       "    # t2659 = ltorch.reshape(t2658, (-1, 16))  # t2659: \"cuda:0 f32[128, 16]\"\n",
       "      # t2659 = prims.reshape(t2658, (128, 16))  # t2659: \"cuda:0 f32[128, 16]\"\n",
       "  del t2658\n",
       "  t2660 = torch.matmul(t2659, t_fcs_26_weight)  # t2660: \"cuda:0 f32[128, 16]\"\n",
       "    # t2660 = ltorch.matmul(t2659, t_fcs_26_weight)  # t2660: \"cuda:0 f32[128, 16]\"\n",
       "      # t2660 = prims.matmul(t2659, t_fcs_26_weight)  # t2660: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_26_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t127 = load_to_gpu(offloaded_t127, 'cuda:0')  # t127: \"cuda:0 b8[128, 16]\"\n",
       "  [t2667, t2674] = nvFusion74(t127, t2660)\n",
       "    # t2667 = prims.where(t127, t2660, 0.0)  # t2667: \"cuda:0 f32[128, 16]\"\n",
       "    # t2674 = prims.sum(t2667, (0,))  # t2674: \"cuda:0 f32[16]\"\n",
       "  del t127, t2660\n",
       "  t2668 = torch.reshape(t2667, (-1, 16))  # t2668: \"cuda:0 f32[128, 16]\"\n",
       "    # t2668 = ltorch.reshape(t2667, (-1, 16))  # t2668: \"cuda:0 f32[128, 16]\"\n",
       "      # t2668 = prims.reshape(t2667, (128, 16))  # t2668: \"cuda:0 f32[128, 16]\"\n",
       "  del t2667\n",
       "  t2669 = torch.matmul(t2668, t_fcs_25_weight)  # t2669: \"cuda:0 f32[128, 16]\"\n",
       "    # t2669 = ltorch.matmul(t2668, t_fcs_25_weight)  # t2669: \"cuda:0 f32[128, 16]\"\n",
       "      # t2669 = prims.matmul(t2668, t_fcs_25_weight)  # t2669: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_25_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t122 = load_to_gpu(offloaded_t122, 'cuda:0')  # t122: \"cuda:0 b8[128, 16]\"\n",
       "  [t2676, t2683] = nvFusion75(t122, t2669)\n",
       "    # t2676 = prims.where(t122, t2669, 0.0)  # t2676: \"cuda:0 f32[128, 16]\"\n",
       "    # t2683 = prims.sum(t2676, (0,))  # t2683: \"cuda:0 f32[16]\"\n",
       "  del t122, t2669\n",
       "  t2677 = torch.reshape(t2676, (-1, 16))  # t2677: \"cuda:0 f32[128, 16]\"\n",
       "    # t2677 = ltorch.reshape(t2676, (-1, 16))  # t2677: \"cuda:0 f32[128, 16]\"\n",
       "      # t2677 = prims.reshape(t2676, (128, 16))  # t2677: \"cuda:0 f32[128, 16]\"\n",
       "  del t2676\n",
       "  t2678 = torch.matmul(t2677, t_fcs_24_weight)  # t2678: \"cuda:0 f32[128, 16]\"\n",
       "    # t2678 = ltorch.matmul(t2677, t_fcs_24_weight)  # t2678: \"cuda:0 f32[128, 16]\"\n",
       "      # t2678 = prims.matmul(t2677, t_fcs_24_weight)  # t2678: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_24_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t117 = load_to_gpu(offloaded_t117, 'cuda:0')  # t117: \"cuda:0 b8[128, 16]\"\n",
       "  [t2685, t2692] = nvFusion76(t117, t2678)\n",
       "    # t2685 = prims.where(t117, t2678, 0.0)  # t2685: \"cuda:0 f32[128, 16]\"\n",
       "    # t2692 = prims.sum(t2685, (0,))  # t2692: \"cuda:0 f32[16]\"\n",
       "  del t117, t2678\n",
       "  t2686 = torch.reshape(t2685, (-1, 16))  # t2686: \"cuda:0 f32[128, 16]\"\n",
       "    # t2686 = ltorch.reshape(t2685, (-1, 16))  # t2686: \"cuda:0 f32[128, 16]\"\n",
       "      # t2686 = prims.reshape(t2685, (128, 16))  # t2686: \"cuda:0 f32[128, 16]\"\n",
       "  del t2685\n",
       "  t2687 = torch.matmul(t2686, t_fcs_23_weight)  # t2687: \"cuda:0 f32[128, 16]\"\n",
       "    # t2687 = ltorch.matmul(t2686, t_fcs_23_weight)  # t2687: \"cuda:0 f32[128, 16]\"\n",
       "      # t2687 = prims.matmul(t2686, t_fcs_23_weight)  # t2687: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_23_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t112 = load_to_gpu(offloaded_t112, 'cuda:0')  # t112: \"cuda:0 b8[128, 16]\"\n",
       "  [t2694, t2701] = nvFusion77(t112, t2687)\n",
       "    # t2694 = prims.where(t112, t2687, 0.0)  # t2694: \"cuda:0 f32[128, 16]\"\n",
       "    # t2701 = prims.sum(t2694, (0,))  # t2701: \"cuda:0 f32[16]\"\n",
       "  del t112, t2687\n",
       "  t2695 = torch.reshape(t2694, (-1, 16))  # t2695: \"cuda:0 f32[128, 16]\"\n",
       "    # t2695 = ltorch.reshape(t2694, (-1, 16))  # t2695: \"cuda:0 f32[128, 16]\"\n",
       "      # t2695 = prims.reshape(t2694, (128, 16))  # t2695: \"cuda:0 f32[128, 16]\"\n",
       "  del t2694\n",
       "  t2696 = torch.matmul(t2695, t_fcs_22_weight)  # t2696: \"cuda:0 f32[128, 16]\"\n",
       "    # t2696 = ltorch.matmul(t2695, t_fcs_22_weight)  # t2696: \"cuda:0 f32[128, 16]\"\n",
       "      # t2696 = prims.matmul(t2695, t_fcs_22_weight)  # t2696: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_22_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t107 = load_to_gpu(offloaded_t107, 'cuda:0')  # t107: \"cuda:0 b8[128, 16]\"\n",
       "  [t2703, t2710] = nvFusion78(t107, t2696)\n",
       "    # t2703 = prims.where(t107, t2696, 0.0)  # t2703: \"cuda:0 f32[128, 16]\"\n",
       "    # t2710 = prims.sum(t2703, (0,))  # t2710: \"cuda:0 f32[16]\"\n",
       "  del t107, t2696\n",
       "  t2704 = torch.reshape(t2703, (-1, 16))  # t2704: \"cuda:0 f32[128, 16]\"\n",
       "    # t2704 = ltorch.reshape(t2703, (-1, 16))  # t2704: \"cuda:0 f32[128, 16]\"\n",
       "      # t2704 = prims.reshape(t2703, (128, 16))  # t2704: \"cuda:0 f32[128, 16]\"\n",
       "  del t2703\n",
       "  t2705 = torch.matmul(t2704, t_fcs_21_weight)  # t2705: \"cuda:0 f32[128, 16]\"\n",
       "    # t2705 = ltorch.matmul(t2704, t_fcs_21_weight)  # t2705: \"cuda:0 f32[128, 16]\"\n",
       "      # t2705 = prims.matmul(t2704, t_fcs_21_weight)  # t2705: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_21_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t102 = load_to_gpu(offloaded_t102, 'cuda:0')  # t102: \"cuda:0 b8[128, 16]\"\n",
       "  [t2712, t2719] = nvFusion79(t102, t2705)\n",
       "    # t2712 = prims.where(t102, t2705, 0.0)  # t2712: \"cuda:0 f32[128, 16]\"\n",
       "    # t2719 = prims.sum(t2712, (0,))  # t2719: \"cuda:0 f32[16]\"\n",
       "  del t102, t2705\n",
       "  t2713 = torch.reshape(t2712, (-1, 16))  # t2713: \"cuda:0 f32[128, 16]\"\n",
       "    # t2713 = ltorch.reshape(t2712, (-1, 16))  # t2713: \"cuda:0 f32[128, 16]\"\n",
       "      # t2713 = prims.reshape(t2712, (128, 16))  # t2713: \"cuda:0 f32[128, 16]\"\n",
       "  del t2712\n",
       "  t2714 = torch.matmul(t2713, t_fcs_20_weight)  # t2714: \"cuda:0 f32[128, 16]\"\n",
       "    # t2714 = ltorch.matmul(t2713, t_fcs_20_weight)  # t2714: \"cuda:0 f32[128, 16]\"\n",
       "      # t2714 = prims.matmul(t2713, t_fcs_20_weight)  # t2714: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_20_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t97 = load_to_gpu(offloaded_t97, 'cuda:0')  # t97: \"cuda:0 b8[128, 16]\"\n",
       "  [t2721, t2728] = nvFusion80(t97, t2714)\n",
       "    # t2721 = prims.where(t97, t2714, 0.0)  # t2721: \"cuda:0 f32[128, 16]\"\n",
       "    # t2728 = prims.sum(t2721, (0,))  # t2728: \"cuda:0 f32[16]\"\n",
       "  del t97, t2714\n",
       "  t2722 = torch.reshape(t2721, (-1, 16))  # t2722: \"cuda:0 f32[128, 16]\"\n",
       "    # t2722 = ltorch.reshape(t2721, (-1, 16))  # t2722: \"cuda:0 f32[128, 16]\"\n",
       "      # t2722 = prims.reshape(t2721, (128, 16))  # t2722: \"cuda:0 f32[128, 16]\"\n",
       "  del t2721\n",
       "  t2723 = torch.matmul(t2722, t_fcs_19_weight)  # t2723: \"cuda:0 f32[128, 16]\"\n",
       "    # t2723 = ltorch.matmul(t2722, t_fcs_19_weight)  # t2723: \"cuda:0 f32[128, 16]\"\n",
       "      # t2723 = prims.matmul(t2722, t_fcs_19_weight)  # t2723: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_19_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t92 = load_to_gpu(offloaded_t92, 'cuda:0')  # t92: \"cuda:0 b8[128, 16]\"\n",
       "  [t2730, t2737] = nvFusion81(t92, t2723)\n",
       "    # t2730 = prims.where(t92, t2723, 0.0)  # t2730: \"cuda:0 f32[128, 16]\"\n",
       "    # t2737 = prims.sum(t2730, (0,))  # t2737: \"cuda:0 f32[16]\"\n",
       "  del t92, t2723\n",
       "  t2731 = torch.reshape(t2730, (-1, 16))  # t2731: \"cuda:0 f32[128, 16]\"\n",
       "    # t2731 = ltorch.reshape(t2730, (-1, 16))  # t2731: \"cuda:0 f32[128, 16]\"\n",
       "      # t2731 = prims.reshape(t2730, (128, 16))  # t2731: \"cuda:0 f32[128, 16]\"\n",
       "  del t2730\n",
       "  t2732 = torch.matmul(t2731, t_fcs_18_weight)  # t2732: \"cuda:0 f32[128, 16]\"\n",
       "    # t2732 = ltorch.matmul(t2731, t_fcs_18_weight)  # t2732: \"cuda:0 f32[128, 16]\"\n",
       "      # t2732 = prims.matmul(t2731, t_fcs_18_weight)  # t2732: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_18_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t87 = load_to_gpu(offloaded_t87, 'cuda:0')  # t87: \"cuda:0 b8[128, 16]\"\n",
       "  [t2739, t2746] = nvFusion82(t87, t2732)\n",
       "    # t2739 = prims.where(t87, t2732, 0.0)  # t2739: \"cuda:0 f32[128, 16]\"\n",
       "    # t2746 = prims.sum(t2739, (0,))  # t2746: \"cuda:0 f32[16]\"\n",
       "  del t87, t2732\n",
       "  t2740 = torch.reshape(t2739, (-1, 16))  # t2740: \"cuda:0 f32[128, 16]\"\n",
       "    # t2740 = ltorch.reshape(t2739, (-1, 16))  # t2740: \"cuda:0 f32[128, 16]\"\n",
       "      # t2740 = prims.reshape(t2739, (128, 16))  # t2740: \"cuda:0 f32[128, 16]\"\n",
       "  del t2739\n",
       "  t2741 = torch.matmul(t2740, t_fcs_17_weight)  # t2741: \"cuda:0 f32[128, 16]\"\n",
       "    # t2741 = ltorch.matmul(t2740, t_fcs_17_weight)  # t2741: \"cuda:0 f32[128, 16]\"\n",
       "      # t2741 = prims.matmul(t2740, t_fcs_17_weight)  # t2741: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_17_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t82 = load_to_gpu(offloaded_t82, 'cuda:0')  # t82: \"cuda:0 b8[128, 16]\"\n",
       "  [t2748, t2755] = nvFusion83(t82, t2741)\n",
       "    # t2748 = prims.where(t82, t2741, 0.0)  # t2748: \"cuda:0 f32[128, 16]\"\n",
       "    # t2755 = prims.sum(t2748, (0,))  # t2755: \"cuda:0 f32[16]\"\n",
       "  del t82, t2741\n",
       "  t2749 = torch.reshape(t2748, (-1, 16))  # t2749: \"cuda:0 f32[128, 16]\"\n",
       "    # t2749 = ltorch.reshape(t2748, (-1, 16))  # t2749: \"cuda:0 f32[128, 16]\"\n",
       "      # t2749 = prims.reshape(t2748, (128, 16))  # t2749: \"cuda:0 f32[128, 16]\"\n",
       "  del t2748\n",
       "  t2750 = torch.matmul(t2749, t_fcs_16_weight)  # t2750: \"cuda:0 f32[128, 16]\"\n",
       "    # t2750 = ltorch.matmul(t2749, t_fcs_16_weight)  # t2750: \"cuda:0 f32[128, 16]\"\n",
       "      # t2750 = prims.matmul(t2749, t_fcs_16_weight)  # t2750: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_16_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t77 = load_to_gpu(offloaded_t77, 'cuda:0')  # t77: \"cuda:0 b8[128, 16]\"\n",
       "  [t2757, t2764] = nvFusion84(t77, t2750)\n",
       "    # t2757 = prims.where(t77, t2750, 0.0)  # t2757: \"cuda:0 f32[128, 16]\"\n",
       "    # t2764 = prims.sum(t2757, (0,))  # t2764: \"cuda:0 f32[16]\"\n",
       "  del t77, t2750\n",
       "  t2758 = torch.reshape(t2757, (-1, 16))  # t2758: \"cuda:0 f32[128, 16]\"\n",
       "    # t2758 = ltorch.reshape(t2757, (-1, 16))  # t2758: \"cuda:0 f32[128, 16]\"\n",
       "      # t2758 = prims.reshape(t2757, (128, 16))  # t2758: \"cuda:0 f32[128, 16]\"\n",
       "  del t2757\n",
       "  t2759 = torch.matmul(t2758, t_fcs_15_weight)  # t2759: \"cuda:0 f32[128, 16]\"\n",
       "    # t2759 = ltorch.matmul(t2758, t_fcs_15_weight)  # t2759: \"cuda:0 f32[128, 16]\"\n",
       "      # t2759 = prims.matmul(t2758, t_fcs_15_weight)  # t2759: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_15_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t72 = load_to_gpu(offloaded_t72, 'cuda:0')  # t72: \"cuda:0 b8[128, 16]\"\n",
       "  [t2766, t2773] = nvFusion85(t72, t2759)\n",
       "    # t2766 = prims.where(t72, t2759, 0.0)  # t2766: \"cuda:0 f32[128, 16]\"\n",
       "    # t2773 = prims.sum(t2766, (0,))  # t2773: \"cuda:0 f32[16]\"\n",
       "  del t72, t2759\n",
       "  t2767 = torch.reshape(t2766, (-1, 16))  # t2767: \"cuda:0 f32[128, 16]\"\n",
       "    # t2767 = ltorch.reshape(t2766, (-1, 16))  # t2767: \"cuda:0 f32[128, 16]\"\n",
       "      # t2767 = prims.reshape(t2766, (128, 16))  # t2767: \"cuda:0 f32[128, 16]\"\n",
       "  del t2766\n",
       "  t2768 = torch.matmul(t2767, t_fcs_14_weight)  # t2768: \"cuda:0 f32[128, 16]\"\n",
       "    # t2768 = ltorch.matmul(t2767, t_fcs_14_weight)  # t2768: \"cuda:0 f32[128, 16]\"\n",
       "      # t2768 = prims.matmul(t2767, t_fcs_14_weight)  # t2768: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_14_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t67 = load_to_gpu(offloaded_t67, 'cuda:0')  # t67: \"cuda:0 b8[128, 16]\"\n",
       "  [t2775, t2782] = nvFusion86(t67, t2768)\n",
       "    # t2775 = prims.where(t67, t2768, 0.0)  # t2775: \"cuda:0 f32[128, 16]\"\n",
       "    # t2782 = prims.sum(t2775, (0,))  # t2782: \"cuda:0 f32[16]\"\n",
       "  del t67, t2768\n",
       "  t2776 = torch.reshape(t2775, (-1, 16))  # t2776: \"cuda:0 f32[128, 16]\"\n",
       "    # t2776 = ltorch.reshape(t2775, (-1, 16))  # t2776: \"cuda:0 f32[128, 16]\"\n",
       "      # t2776 = prims.reshape(t2775, (128, 16))  # t2776: \"cuda:0 f32[128, 16]\"\n",
       "  del t2775\n",
       "  t2777 = torch.matmul(t2776, t_fcs_13_weight)  # t2777: \"cuda:0 f32[128, 16]\"\n",
       "    # t2777 = ltorch.matmul(t2776, t_fcs_13_weight)  # t2777: \"cuda:0 f32[128, 16]\"\n",
       "      # t2777 = prims.matmul(t2776, t_fcs_13_weight)  # t2777: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_13_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t62 = load_to_gpu(offloaded_t62, 'cuda:0')  # t62: \"cuda:0 b8[128, 16]\"\n",
       "  [t2784, t2791] = nvFusion87(t62, t2777)\n",
       "    # t2784 = prims.where(t62, t2777, 0.0)  # t2784: \"cuda:0 f32[128, 16]\"\n",
       "    # t2791 = prims.sum(t2784, (0,))  # t2791: \"cuda:0 f32[16]\"\n",
       "  del t62, t2777\n",
       "  t2785 = torch.reshape(t2784, (-1, 16))  # t2785: \"cuda:0 f32[128, 16]\"\n",
       "    # t2785 = ltorch.reshape(t2784, (-1, 16))  # t2785: \"cuda:0 f32[128, 16]\"\n",
       "      # t2785 = prims.reshape(t2784, (128, 16))  # t2785: \"cuda:0 f32[128, 16]\"\n",
       "  del t2784\n",
       "  t2786 = torch.matmul(t2785, t_fcs_12_weight)  # t2786: \"cuda:0 f32[128, 16]\"\n",
       "    # t2786 = ltorch.matmul(t2785, t_fcs_12_weight)  # t2786: \"cuda:0 f32[128, 16]\"\n",
       "      # t2786 = prims.matmul(t2785, t_fcs_12_weight)  # t2786: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_12_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t57 = load_to_gpu(offloaded_t57, 'cuda:0')  # t57: \"cuda:0 b8[128, 16]\"\n",
       "  [t2793, t2800] = nvFusion88(t57, t2786)\n",
       "    # t2793 = prims.where(t57, t2786, 0.0)  # t2793: \"cuda:0 f32[128, 16]\"\n",
       "    # t2800 = prims.sum(t2793, (0,))  # t2800: \"cuda:0 f32[16]\"\n",
       "  del t57, t2786\n",
       "  t2794 = torch.reshape(t2793, (-1, 16))  # t2794: \"cuda:0 f32[128, 16]\"\n",
       "    # t2794 = ltorch.reshape(t2793, (-1, 16))  # t2794: \"cuda:0 f32[128, 16]\"\n",
       "      # t2794 = prims.reshape(t2793, (128, 16))  # t2794: \"cuda:0 f32[128, 16]\"\n",
       "  del t2793\n",
       "  t2795 = torch.matmul(t2794, t_fcs_11_weight)  # t2795: \"cuda:0 f32[128, 16]\"\n",
       "    # t2795 = ltorch.matmul(t2794, t_fcs_11_weight)  # t2795: \"cuda:0 f32[128, 16]\"\n",
       "      # t2795 = prims.matmul(t2794, t_fcs_11_weight)  # t2795: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_11_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t52 = load_to_gpu(offloaded_t52, 'cuda:0')  # t52: \"cuda:0 b8[128, 16]\"\n",
       "  [t2802, t2809] = nvFusion89(t52, t2795)\n",
       "    # t2802 = prims.where(t52, t2795, 0.0)  # t2802: \"cuda:0 f32[128, 16]\"\n",
       "    # t2809 = prims.sum(t2802, (0,))  # t2809: \"cuda:0 f32[16]\"\n",
       "  del t52, t2795\n",
       "  t2803 = torch.reshape(t2802, (-1, 16))  # t2803: \"cuda:0 f32[128, 16]\"\n",
       "    # t2803 = ltorch.reshape(t2802, (-1, 16))  # t2803: \"cuda:0 f32[128, 16]\"\n",
       "      # t2803 = prims.reshape(t2802, (128, 16))  # t2803: \"cuda:0 f32[128, 16]\"\n",
       "  del t2802\n",
       "  t2804 = torch.matmul(t2803, t_fcs_10_weight)  # t2804: \"cuda:0 f32[128, 16]\"\n",
       "    # t2804 = ltorch.matmul(t2803, t_fcs_10_weight)  # t2804: \"cuda:0 f32[128, 16]\"\n",
       "      # t2804 = prims.matmul(t2803, t_fcs_10_weight)  # t2804: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_10_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t47 = load_to_gpu(offloaded_t47, 'cuda:0')  # t47: \"cuda:0 b8[128, 16]\"\n",
       "  [t2811, t2818] = nvFusion90(t47, t2804)\n",
       "    # t2811 = prims.where(t47, t2804, 0.0)  # t2811: \"cuda:0 f32[128, 16]\"\n",
       "    # t2818 = prims.sum(t2811, (0,))  # t2818: \"cuda:0 f32[16]\"\n",
       "  del t47, t2804\n",
       "  t2812 = torch.reshape(t2811, (-1, 16))  # t2812: \"cuda:0 f32[128, 16]\"\n",
       "    # t2812 = ltorch.reshape(t2811, (-1, 16))  # t2812: \"cuda:0 f32[128, 16]\"\n",
       "      # t2812 = prims.reshape(t2811, (128, 16))  # t2812: \"cuda:0 f32[128, 16]\"\n",
       "  del t2811\n",
       "  t2813 = torch.matmul(t2812, t_fcs_9_weight)  # t2813: \"cuda:0 f32[128, 16]\"\n",
       "    # t2813 = ltorch.matmul(t2812, t_fcs_9_weight)  # t2813: \"cuda:0 f32[128, 16]\"\n",
       "      # t2813 = prims.matmul(t2812, t_fcs_9_weight)  # t2813: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_9_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t42 = load_to_gpu(offloaded_t42, 'cuda:0')  # t42: \"cuda:0 b8[128, 16]\"\n",
       "  [t2820, t2827] = nvFusion91(t42, t2813)\n",
       "    # t2820 = prims.where(t42, t2813, 0.0)  # t2820: \"cuda:0 f32[128, 16]\"\n",
       "    # t2827 = prims.sum(t2820, (0,))  # t2827: \"cuda:0 f32[16]\"\n",
       "  del t42, t2813\n",
       "  t2821 = torch.reshape(t2820, (-1, 16))  # t2821: \"cuda:0 f32[128, 16]\"\n",
       "    # t2821 = ltorch.reshape(t2820, (-1, 16))  # t2821: \"cuda:0 f32[128, 16]\"\n",
       "      # t2821 = prims.reshape(t2820, (128, 16))  # t2821: \"cuda:0 f32[128, 16]\"\n",
       "  del t2820\n",
       "  t2822 = torch.matmul(t2821, t_fcs_8_weight)  # t2822: \"cuda:0 f32[128, 16]\"\n",
       "    # t2822 = ltorch.matmul(t2821, t_fcs_8_weight)  # t2822: \"cuda:0 f32[128, 16]\"\n",
       "      # t2822 = prims.matmul(t2821, t_fcs_8_weight)  # t2822: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_8_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t37 = load_to_gpu(offloaded_t37, 'cuda:0')  # t37: \"cuda:0 b8[128, 16]\"\n",
       "  [t2829, t2836] = nvFusion92(t37, t2822)\n",
       "    # t2829 = prims.where(t37, t2822, 0.0)  # t2829: \"cuda:0 f32[128, 16]\"\n",
       "    # t2836 = prims.sum(t2829, (0,))  # t2836: \"cuda:0 f32[16]\"\n",
       "  del t37, t2822\n",
       "  t2830 = torch.reshape(t2829, (-1, 16))  # t2830: \"cuda:0 f32[128, 16]\"\n",
       "    # t2830 = ltorch.reshape(t2829, (-1, 16))  # t2830: \"cuda:0 f32[128, 16]\"\n",
       "      # t2830 = prims.reshape(t2829, (128, 16))  # t2830: \"cuda:0 f32[128, 16]\"\n",
       "  del t2829\n",
       "  t2831 = torch.matmul(t2830, t_fcs_7_weight)  # t2831: \"cuda:0 f32[128, 16]\"\n",
       "    # t2831 = ltorch.matmul(t2830, t_fcs_7_weight)  # t2831: \"cuda:0 f32[128, 16]\"\n",
       "      # t2831 = prims.matmul(t2830, t_fcs_7_weight)  # t2831: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_7_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t32 = load_to_gpu(offloaded_t32, 'cuda:0')  # t32: \"cuda:0 b8[128, 16]\"\n",
       "  [t2838, t2845] = nvFusion93(t32, t2831)\n",
       "    # t2838 = prims.where(t32, t2831, 0.0)  # t2838: \"cuda:0 f32[128, 16]\"\n",
       "    # t2845 = prims.sum(t2838, (0,))  # t2845: \"cuda:0 f32[16]\"\n",
       "  del t32, t2831\n",
       "  t2839 = torch.reshape(t2838, (-1, 16))  # t2839: \"cuda:0 f32[128, 16]\"\n",
       "    # t2839 = ltorch.reshape(t2838, (-1, 16))  # t2839: \"cuda:0 f32[128, 16]\"\n",
       "      # t2839 = prims.reshape(t2838, (128, 16))  # t2839: \"cuda:0 f32[128, 16]\"\n",
       "  del t2838\n",
       "  t2840 = torch.matmul(t2839, t_fcs_6_weight)  # t2840: \"cuda:0 f32[128, 16]\"\n",
       "    # t2840 = ltorch.matmul(t2839, t_fcs_6_weight)  # t2840: \"cuda:0 f32[128, 16]\"\n",
       "      # t2840 = prims.matmul(t2839, t_fcs_6_weight)  # t2840: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_6_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t27 = load_to_gpu(offloaded_t27, 'cuda:0')  # t27: \"cuda:0 b8[128, 16]\"\n",
       "  [t2847, t2854] = nvFusion94(t27, t2840)\n",
       "    # t2847 = prims.where(t27, t2840, 0.0)  # t2847: \"cuda:0 f32[128, 16]\"\n",
       "    # t2854 = prims.sum(t2847, (0,))  # t2854: \"cuda:0 f32[16]\"\n",
       "  del t27, t2840\n",
       "  t2848 = torch.reshape(t2847, (-1, 16))  # t2848: \"cuda:0 f32[128, 16]\"\n",
       "    # t2848 = ltorch.reshape(t2847, (-1, 16))  # t2848: \"cuda:0 f32[128, 16]\"\n",
       "      # t2848 = prims.reshape(t2847, (128, 16))  # t2848: \"cuda:0 f32[128, 16]\"\n",
       "  del t2847\n",
       "  t2849 = torch.matmul(t2848, t_fcs_5_weight)  # t2849: \"cuda:0 f32[128, 16]\"\n",
       "    # t2849 = ltorch.matmul(t2848, t_fcs_5_weight)  # t2849: \"cuda:0 f32[128, 16]\"\n",
       "      # t2849 = prims.matmul(t2848, t_fcs_5_weight)  # t2849: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_5_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t22 = load_to_gpu(offloaded_t22, 'cuda:0')  # t22: \"cuda:0 b8[128, 16]\"\n",
       "  [t2856, t2863] = nvFusion95(t22, t2849)\n",
       "    # t2856 = prims.where(t22, t2849, 0.0)  # t2856: \"cuda:0 f32[128, 16]\"\n",
       "    # t2863 = prims.sum(t2856, (0,))  # t2863: \"cuda:0 f32[16]\"\n",
       "  del t22, t2849\n",
       "  t2857 = torch.reshape(t2856, (-1, 16))  # t2857: \"cuda:0 f32[128, 16]\"\n",
       "    # t2857 = ltorch.reshape(t2856, (-1, 16))  # t2857: \"cuda:0 f32[128, 16]\"\n",
       "      # t2857 = prims.reshape(t2856, (128, 16))  # t2857: \"cuda:0 f32[128, 16]\"\n",
       "  del t2856\n",
       "  t2858 = torch.matmul(t2857, t_fcs_4_weight)  # t2858: \"cuda:0 f32[128, 16]\"\n",
       "    # t2858 = ltorch.matmul(t2857, t_fcs_4_weight)  # t2858: \"cuda:0 f32[128, 16]\"\n",
       "      # t2858 = prims.matmul(t2857, t_fcs_4_weight)  # t2858: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_4_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t17 = load_to_gpu(offloaded_t17, 'cuda:0')  # t17: \"cuda:0 b8[128, 16]\"\n",
       "  [t2865, t2872] = nvFusion96(t17, t2858)\n",
       "    # t2865 = prims.where(t17, t2858, 0.0)  # t2865: \"cuda:0 f32[128, 16]\"\n",
       "    # t2872 = prims.sum(t2865, (0,))  # t2872: \"cuda:0 f32[16]\"\n",
       "  del t17, t2858\n",
       "  t2866 = torch.reshape(t2865, (-1, 16))  # t2866: \"cuda:0 f32[128, 16]\"\n",
       "    # t2866 = ltorch.reshape(t2865, (-1, 16))  # t2866: \"cuda:0 f32[128, 16]\"\n",
       "      # t2866 = prims.reshape(t2865, (128, 16))  # t2866: \"cuda:0 f32[128, 16]\"\n",
       "  del t2865\n",
       "  t2867 = torch.matmul(t2866, t_fcs_3_weight)  # t2867: \"cuda:0 f32[128, 16]\"\n",
       "    # t2867 = ltorch.matmul(t2866, t_fcs_3_weight)  # t2867: \"cuda:0 f32[128, 16]\"\n",
       "      # t2867 = prims.matmul(t2866, t_fcs_3_weight)  # t2867: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_3_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t12 = load_to_gpu(offloaded_t12, 'cuda:0')  # t12: \"cuda:0 b8[128, 16]\"\n",
       "  [t2874, t2881] = nvFusion97(t12, t2867)\n",
       "    # t2874 = prims.where(t12, t2867, 0.0)  # t2874: \"cuda:0 f32[128, 16]\"\n",
       "    # t2881 = prims.sum(t2874, (0,))  # t2881: \"cuda:0 f32[16]\"\n",
       "  del t12, t2867\n",
       "  t2875 = torch.reshape(t2874, (-1, 16))  # t2875: \"cuda:0 f32[128, 16]\"\n",
       "    # t2875 = ltorch.reshape(t2874, (-1, 16))  # t2875: \"cuda:0 f32[128, 16]\"\n",
       "      # t2875 = prims.reshape(t2874, (128, 16))  # t2875: \"cuda:0 f32[128, 16]\"\n",
       "  del t2874\n",
       "  t2876 = torch.matmul(t2875, t_fcs_2_weight)  # t2876: \"cuda:0 f32[128, 16]\"\n",
       "    # t2876 = ltorch.matmul(t2875, t_fcs_2_weight)  # t2876: \"cuda:0 f32[128, 16]\"\n",
       "      # t2876 = prims.matmul(t2875, t_fcs_2_weight)  # t2876: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_2_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t7 = load_to_gpu(offloaded_t7, 'cuda:0')  # t7: \"cuda:0 b8[128, 16]\"\n",
       "  [t2883, t2890] = nvFusion98(t7, t2876)\n",
       "    # t2883 = prims.where(t7, t2876, 0.0)  # t2883: \"cuda:0 f32[128, 16]\"\n",
       "    # t2890 = prims.sum(t2883, (0,))  # t2890: \"cuda:0 f32[16]\"\n",
       "  del t7, t2876\n",
       "  t2884 = torch.reshape(t2883, (-1, 16))  # t2884: \"cuda:0 f32[128, 16]\"\n",
       "    # t2884 = ltorch.reshape(t2883, (-1, 16))  # t2884: \"cuda:0 f32[128, 16]\"\n",
       "      # t2884 = prims.reshape(t2883, (128, 16))  # t2884: \"cuda:0 f32[128, 16]\"\n",
       "  del t2883\n",
       "  t2885 = torch.matmul(t2884, t_fcs_1_weight)  # t2885: \"cuda:0 f32[128, 16]\"\n",
       "    # t2885 = ltorch.matmul(t2884, t_fcs_1_weight)  # t2885: \"cuda:0 f32[128, 16]\"\n",
       "      # t2885 = prims.matmul(t2884, t_fcs_1_weight)  # t2885: \"cuda:0 f32[128, 16]\"\n",
       "  del t_fcs_1_weight\n",
       "  # Created by CPU Offloading Transform\n",
       "  t2 = load_to_gpu(offloaded_t2, 'cuda:0')  # t2: \"cuda:0 b8[128, 16]\"\n",
       "  [t2892, t2899] = nvFusion99(t2, t2885)\n",
       "    # t2892 = prims.where(t2, t2885, 0.0)  # t2892: \"cuda:0 f32[128, 16]\"\n",
       "    # t2899 = prims.sum(t2892, (0,))  # t2899: \"cuda:0 f32[16]\"\n",
       "  del t2, t2885\n",
       "  t2895 = torch.reshape(t2892, (-1, 16))  # t2895: \"cuda:0 f32[128, 16]\"\n",
       "    # t2895 = ltorch.reshape(t2892, (-1, 16))  # t2895: \"cuda:0 f32[128, 16]\"\n",
       "      # t2895 = prims.reshape(t2892, (128, 16))  # t2895: \"cuda:0 f32[128, 16]\"\n",
       "  del t2892\n",
       "  t2896 = torch.permute(t2895, (1, 0))  # t2896: \"cuda:0 f32[16, 128]\"\n",
       "    # t2896 = ltorch.permute(t2895, (1, 0))  # t2896: \"cuda:0 f32[16, 128]\"\n",
       "      # t2896 = prims.transpose(t2895, (1, 0))  # t2896: \"cuda:0 f32[16, 128]\"\n",
       "  del t2895\n",
       "  t2897 = torch.reshape(x, (-1, 16))  # t2897: \"cuda:0 f32[128, 16]\"\n",
       "    # t2897 = ltorch.reshape(x, (-1, 16))  # t2897: \"cuda:0 f32[128, 16]\"\n",
       "      # t2897 = prims.reshape(x, (128, 16))  # t2897: \"cuda:0 f32[128, 16]\"\n",
       "  del x\n",
       "  t2898 = torch.matmul(t2896, t2897)  # t2898: \"cuda:0 f32[16, 16]\"\n",
       "    # t2898 = ltorch.matmul(t2896, t2897)  # t2898: \"cuda:0 f32[16, 16]\"\n",
       "      # t2898 = prims.matmul(t2896, t2897)  # t2898: \"cuda:0 f32[16, 16]\"\n",
       "  del t2896, t2897\n",
       "  t2887 = torch.permute(t2884, (1, 0))  # t2887: \"cuda:0 f32[16, 128]\"\n",
       "    # t2887 = ltorch.permute(t2884, (1, 0))  # t2887: \"cuda:0 f32[16, 128]\"\n",
       "      # t2887 = prims.transpose(t2884, (1, 0))  # t2887: \"cuda:0 f32[16, 128]\"\n",
       "  del t2884\n",
       "  # Created by CPU Offloading Transform\n",
       "  t4 = load_to_gpu(offloaded_t4, 'cuda:0')  # t4: \"cuda:0 f32[128, 16]\"\n",
       "  t2888 = torch.reshape(t4, (-1, 16))  # t2888: \"cuda:0 f32[128, 16]\"\n",
       "    # t2888 = ltorch.reshape(t4, (-1, 16))  # t2888: \"cuda:0 f32[128, 16]\"\n",
       "      # t2888 = prims.reshape(t4, (128, 16))  # t2888: \"cuda:0 f32[128, 16]\"\n",
       "  del t4\n",
       "  t2889 = torch.matmul(t2887, t2888)  # t2889: \"cuda:0 f32[16, 16]\"\n",
       "    # t2889 = ltorch.matmul(t2887, t2888)  # t2889: \"cuda:0 f32[16, 16]\"\n",
       "      # t2889 = prims.matmul(t2887, t2888)  # t2889: \"cuda:0 f32[16, 16]\"\n",
       "  del t2887, t2888\n",
       "  t2878 = torch.permute(t2875, (1, 0))  # t2878: \"cuda:0 f32[16, 128]\"\n",
       "    # t2878 = ltorch.permute(t2875, (1, 0))  # t2878: \"cuda:0 f32[16, 128]\"\n",
       "      # t2878 = prims.transpose(t2875, (1, 0))  # t2878: \"cuda:0 f32[16, 128]\"\n",
       "  del t2875\n",
       "  # Created by CPU Offloading Transform\n",
       "  t9 = load_to_gpu(offloaded_t9, 'cuda:0')  # t9: \"cuda:0 f32[128, 16]\"\n",
       "  t2879 = torch.reshape(t9, (-1, 16))  # t2879: \"cuda:0 f32[128, 16]\"\n",
       "    # t2879 = ltorch.reshape(t9, (-1, 16))  # t2879: \"cuda:0 f32[128, 16]\"\n",
       "      # t2879 = prims.reshape(t9, (128, 16))  # t2879: \"cuda:0 f32[128, 16]\"\n",
       "  del t9\n",
       "  t2880 = torch.matmul(t2878, t2879)  # t2880: \"cuda:0 f32[16, 16]\"\n",
       "    # t2880 = ltorch.matmul(t2878, t2879)  # t2880: \"cuda:0 f32[16, 16]\"\n",
       "      # t2880 = prims.matmul(t2878, t2879)  # t2880: \"cuda:0 f32[16, 16]\"\n",
       "  del t2878, t2879\n",
       "  t2869 = torch.permute(t2866, (1, 0))  # t2869: \"cuda:0 f32[16, 128]\"\n",
       "    # t2869 = ltorch.permute(t2866, (1, 0))  # t2869: \"cuda:0 f32[16, 128]\"\n",
       "      # t2869 = prims.transpose(t2866, (1, 0))  # t2869: \"cuda:0 f32[16, 128]\"\n",
       "  del t2866\n",
       "  # Created by CPU Offloading Transform\n",
       "  t14 = load_to_gpu(offloaded_t14, 'cuda:0')  # t14: \"cuda:0 f32[128, 16]\"\n",
       "  t2870 = torch.reshape(t14, (-1, 16))  # t2870: \"cuda:0 f32[128, 16]\"\n",
       "    # t2870 = ltorch.reshape(t14, (-1, 16))  # t2870: \"cuda:0 f32[128, 16]\"\n",
       "      # t2870 = prims.reshape(t14, (128, 16))  # t2870: \"cuda:0 f32[128, 16]\"\n",
       "  del t14\n",
       "  t2871 = torch.matmul(t2869, t2870)  # t2871: \"cuda:0 f32[16, 16]\"\n",
       "    # t2871 = ltorch.matmul(t2869, t2870)  # t2871: \"cuda:0 f32[16, 16]\"\n",
       "      # t2871 = prims.matmul(t2869, t2870)  # t2871: \"cuda:0 f32[16, 16]\"\n",
       "  del t2869, t2870\n",
       "  t2860 = torch.permute(t2857, (1, 0))  # t2860: \"cuda:0 f32[16, 128]\"\n",
       "    # t2860 = ltorch.permute(t2857, (1, 0))  # t2860: \"cuda:0 f32[16, 128]\"\n",
       "      # t2860 = prims.transpose(t2857, (1, 0))  # t2860: \"cuda:0 f32[16, 128]\"\n",
       "  del t2857\n",
       "  # Created by CPU Offloading Transform\n",
       "  t19 = load_to_gpu(offloaded_t19, 'cuda:0')  # t19: \"cuda:0 f32[128, 16]\"\n",
       "  t2861 = torch.reshape(t19, (-1, 16))  # t2861: \"cuda:0 f32[128, 16]\"\n",
       "    # t2861 = ltorch.reshape(t19, (-1, 16))  # t2861: \"cuda:0 f32[128, 16]\"\n",
       "      # t2861 = prims.reshape(t19, (128, 16))  # t2861: \"cuda:0 f32[128, 16]\"\n",
       "  del t19\n",
       "  t2862 = torch.matmul(t2860, t2861)  # t2862: \"cuda:0 f32[16, 16]\"\n",
       "    # t2862 = ltorch.matmul(t2860, t2861)  # t2862: \"cuda:0 f32[16, 16]\"\n",
       "      # t2862 = prims.matmul(t2860, t2861)  # t2862: \"cuda:0 f32[16, 16]\"\n",
       "  del t2860, t2861\n",
       "  t2851 = torch.permute(t2848, (1, 0))  # t2851: \"cuda:0 f32[16, 128]\"\n",
       "    # t2851 = ltorch.permute(t2848, (1, 0))  # t2851: \"cuda:0 f32[16, 128]\"\n",
       "      # t2851 = prims.transpose(t2848, (1, 0))  # t2851: \"cuda:0 f32[16, 128]\"\n",
       "  del t2848\n",
       "  # Created by CPU Offloading Transform\n",
       "  t24 = load_to_gpu(offloaded_t24, 'cuda:0')  # t24: \"cuda:0 f32[128, 16]\"\n",
       "  t2852 = torch.reshape(t24, (-1, 16))  # t2852: \"cuda:0 f32[128, 16]\"\n",
       "    # t2852 = ltorch.reshape(t24, (-1, 16))  # t2852: \"cuda:0 f32[128, 16]\"\n",
       "      # t2852 = prims.reshape(t24, (128, 16))  # t2852: \"cuda:0 f32[128, 16]\"\n",
       "  del t24\n",
       "  t2853 = torch.matmul(t2851, t2852)  # t2853: \"cuda:0 f32[16, 16]\"\n",
       "    # t2853 = ltorch.matmul(t2851, t2852)  # t2853: \"cuda:0 f32[16, 16]\"\n",
       "      # t2853 = prims.matmul(t2851, t2852)  # t2853: \"cuda:0 f32[16, 16]\"\n",
       "  del t2851, t2852\n",
       "  t2842 = torch.permute(t2839, (1, 0))  # t2842: \"cuda:0 f32[16, 128]\"\n",
       "    # t2842 = ltorch.permute(t2839, (1, 0))  # t2842: \"cuda:0 f32[16, 128]\"\n",
       "      # t2842 = prims.transpose(t2839, (1, 0))  # t2842: \"cuda:0 f32[16, 128]\"\n",
       "  del t2839\n",
       "  # Created by CPU Offloading Transform\n",
       "  t29 = load_to_gpu(offloaded_t29, 'cuda:0')  # t29: \"cuda:0 f32[128, 16]\"\n",
       "  t2843 = torch.reshape(t29, (-1, 16))  # t2843: \"cuda:0 f32[128, 16]\"\n",
       "    # t2843 = ltorch.reshape(t29, (-1, 16))  # t2843: \"cuda:0 f32[128, 16]\"\n",
       "      # t2843 = prims.reshape(t29, (128, 16))  # t2843: \"cuda:0 f32[128, 16]\"\n",
       "  del t29\n",
       "  t2844 = torch.matmul(t2842, t2843)  # t2844: \"cuda:0 f32[16, 16]\"\n",
       "    # t2844 = ltorch.matmul(t2842, t2843)  # t2844: \"cuda:0 f32[16, 16]\"\n",
       "      # t2844 = prims.matmul(t2842, t2843)  # t2844: \"cuda:0 f32[16, 16]\"\n",
       "  del t2842, t2843\n",
       "  t2833 = torch.permute(t2830, (1, 0))  # t2833: \"cuda:0 f32[16, 128]\"\n",
       "    # t2833 = ltorch.permute(t2830, (1, 0))  # t2833: \"cuda:0 f32[16, 128]\"\n",
       "      # t2833 = prims.transpose(t2830, (1, 0))  # t2833: \"cuda:0 f32[16, 128]\"\n",
       "  del t2830\n",
       "  # Created by CPU Offloading Transform\n",
       "  t34 = load_to_gpu(offloaded_t34, 'cuda:0')  # t34: \"cuda:0 f32[128, 16]\"\n",
       "  t2834 = torch.reshape(t34, (-1, 16))  # t2834: \"cuda:0 f32[128, 16]\"\n",
       "    # t2834 = ltorch.reshape(t34, (-1, 16))  # t2834: \"cuda:0 f32[128, 16]\"\n",
       "      # t2834 = prims.reshape(t34, (128, 16))  # t2834: \"cuda:0 f32[128, 16]\"\n",
       "  del t34\n",
       "  t2835 = torch.matmul(t2833, t2834)  # t2835: \"cuda:0 f32[16, 16]\"\n",
       "    # t2835 = ltorch.matmul(t2833, t2834)  # t2835: \"cuda:0 f32[16, 16]\"\n",
       "      # t2835 = prims.matmul(t2833, t2834)  # t2835: \"cuda:0 f32[16, 16]\"\n",
       "  del t2833, t2834\n",
       "  t2824 = torch.permute(t2821, (1, 0))  # t2824: \"cuda:0 f32[16, 128]\"\n",
       "    # t2824 = ltorch.permute(t2821, (1, 0))  # t2824: \"cuda:0 f32[16, 128]\"\n",
       "      # t2824 = prims.transpose(t2821, (1, 0))  # t2824: \"cuda:0 f32[16, 128]\"\n",
       "  del t2821\n",
       "  # Created by CPU Offloading Transform\n",
       "  t39 = load_to_gpu(offloaded_t39, 'cuda:0')  # t39: \"cuda:0 f32[128, 16]\"\n",
       "  t2825 = torch.reshape(t39, (-1, 16))  # t2825: \"cuda:0 f32[128, 16]\"\n",
       "    # t2825 = ltorch.reshape(t39, (-1, 16))  # t2825: \"cuda:0 f32[128, 16]\"\n",
       "      # t2825 = prims.reshape(t39, (128, 16))  # t2825: \"cuda:0 f32[128, 16]\"\n",
       "  del t39\n",
       "  t2826 = torch.matmul(t2824, t2825)  # t2826: \"cuda:0 f32[16, 16]\"\n",
       "    # t2826 = ltorch.matmul(t2824, t2825)  # t2826: \"cuda:0 f32[16, 16]\"\n",
       "      # t2826 = prims.matmul(t2824, t2825)  # t2826: \"cuda:0 f32[16, 16]\"\n",
       "  del t2824, t2825\n",
       "  t2815 = torch.permute(t2812, (1, 0))  # t2815: \"cuda:0 f32[16, 128]\"\n",
       "    # t2815 = ltorch.permute(t2812, (1, 0))  # t2815: \"cuda:0 f32[16, 128]\"\n",
       "      # t2815 = prims.transpose(t2812, (1, 0))  # t2815: \"cuda:0 f32[16, 128]\"\n",
       "  del t2812\n",
       "  # Created by CPU Offloading Transform\n",
       "  t44 = load_to_gpu(offloaded_t44, 'cuda:0')  # t44: \"cuda:0 f32[128, 16]\"\n",
       "  t2816 = torch.reshape(t44, (-1, 16))  # t2816: \"cuda:0 f32[128, 16]\"\n",
       "    # t2816 = ltorch.reshape(t44, (-1, 16))  # t2816: \"cuda:0 f32[128, 16]\"\n",
       "      # t2816 = prims.reshape(t44, (128, 16))  # t2816: \"cuda:0 f32[128, 16]\"\n",
       "  del t44\n",
       "  t2817 = torch.matmul(t2815, t2816)  # t2817: \"cuda:0 f32[16, 16]\"\n",
       "    # t2817 = ltorch.matmul(t2815, t2816)  # t2817: \"cuda:0 f32[16, 16]\"\n",
       "      # t2817 = prims.matmul(t2815, t2816)  # t2817: \"cuda:0 f32[16, 16]\"\n",
       "  del t2815, t2816\n",
       "  t2806 = torch.permute(t2803, (1, 0))  # t2806: \"cuda:0 f32[16, 128]\"\n",
       "    # t2806 = ltorch.permute(t2803, (1, 0))  # t2806: \"cuda:0 f32[16, 128]\"\n",
       "      # t2806 = prims.transpose(t2803, (1, 0))  # t2806: \"cuda:0 f32[16, 128]\"\n",
       "  del t2803\n",
       "  # Created by CPU Offloading Transform\n",
       "  t49 = load_to_gpu(offloaded_t49, 'cuda:0')  # t49: \"cuda:0 f32[128, 16]\"\n",
       "  t2807 = torch.reshape(t49, (-1, 16))  # t2807: \"cuda:0 f32[128, 16]\"\n",
       "    # t2807 = ltorch.reshape(t49, (-1, 16))  # t2807: \"cuda:0 f32[128, 16]\"\n",
       "      # t2807 = prims.reshape(t49, (128, 16))  # t2807: \"cuda:0 f32[128, 16]\"\n",
       "  del t49\n",
       "  t2808 = torch.matmul(t2806, t2807)  # t2808: \"cuda:0 f32[16, 16]\"\n",
       "    # t2808 = ltorch.matmul(t2806, t2807)  # t2808: \"cuda:0 f32[16, 16]\"\n",
       "      # t2808 = prims.matmul(t2806, t2807)  # t2808: \"cuda:0 f32[16, 16]\"\n",
       "  del t2806, t2807\n",
       "  t2797 = torch.permute(t2794, (1, 0))  # t2797: \"cuda:0 f32[16, 128]\"\n",
       "    # t2797 = ltorch.permute(t2794, (1, 0))  # t2797: \"cuda:0 f32[16, 128]\"\n",
       "      # t2797 = prims.transpose(t2794, (1, 0))  # t2797: \"cuda:0 f32[16, 128]\"\n",
       "  del t2794\n",
       "  # Created by CPU Offloading Transform\n",
       "  t54 = load_to_gpu(offloaded_t54, 'cuda:0')  # t54: \"cuda:0 f32[128, 16]\"\n",
       "  t2798 = torch.reshape(t54, (-1, 16))  # t2798: \"cuda:0 f32[128, 16]\"\n",
       "    # t2798 = ltorch.reshape(t54, (-1, 16))  # t2798: \"cuda:0 f32[128, 16]\"\n",
       "      # t2798 = prims.reshape(t54, (128, 16))  # t2798: \"cuda:0 f32[128, 16]\"\n",
       "  del t54\n",
       "  t2799 = torch.matmul(t2797, t2798)  # t2799: \"cuda:0 f32[16, 16]\"\n",
       "    # t2799 = ltorch.matmul(t2797, t2798)  # t2799: \"cuda:0 f32[16, 16]\"\n",
       "      # t2799 = prims.matmul(t2797, t2798)  # t2799: \"cuda:0 f32[16, 16]\"\n",
       "  del t2797, t2798\n",
       "  t2788 = torch.permute(t2785, (1, 0))  # t2788: \"cuda:0 f32[16, 128]\"\n",
       "    # t2788 = ltorch.permute(t2785, (1, 0))  # t2788: \"cuda:0 f32[16, 128]\"\n",
       "      # t2788 = prims.transpose(t2785, (1, 0))  # t2788: \"cuda:0 f32[16, 128]\"\n",
       "  del t2785\n",
       "  # Created by CPU Offloading Transform\n",
       "  t59 = load_to_gpu(offloaded_t59, 'cuda:0')  # t59: \"cuda:0 f32[128, 16]\"\n",
       "  t2789 = torch.reshape(t59, (-1, 16))  # t2789: \"cuda:0 f32[128, 16]\"\n",
       "    # t2789 = ltorch.reshape(t59, (-1, 16))  # t2789: \"cuda:0 f32[128, 16]\"\n",
       "      # t2789 = prims.reshape(t59, (128, 16))  # t2789: \"cuda:0 f32[128, 16]\"\n",
       "  del t59\n",
       "  t2790 = torch.matmul(t2788, t2789)  # t2790: \"cuda:0 f32[16, 16]\"\n",
       "    # t2790 = ltorch.matmul(t2788, t2789)  # t2790: \"cuda:0 f32[16, 16]\"\n",
       "      # t2790 = prims.matmul(t2788, t2789)  # t2790: \"cuda:0 f32[16, 16]\"\n",
       "  del t2788, t2789\n",
       "  t2779 = torch.permute(t2776, (1, 0))  # t2779: \"cuda:0 f32[16, 128]\"\n",
       "    # t2779 = ltorch.permute(t2776, (1, 0))  # t2779: \"cuda:0 f32[16, 128]\"\n",
       "      # t2779 = prims.transpose(t2776, (1, 0))  # t2779: \"cuda:0 f32[16, 128]\"\n",
       "  del t2776\n",
       "  # Created by CPU Offloading Transform\n",
       "  t64 = load_to_gpu(offloaded_t64, 'cuda:0')  # t64: \"cuda:0 f32[128, 16]\"\n",
       "  t2780 = torch.reshape(t64, (-1, 16))  # t2780: \"cuda:0 f32[128, 16]\"\n",
       "    # t2780 = ltorch.reshape(t64, (-1, 16))  # t2780: \"cuda:0 f32[128, 16]\"\n",
       "      # t2780 = prims.reshape(t64, (128, 16))  # t2780: \"cuda:0 f32[128, 16]\"\n",
       "  del t64\n",
       "  t2781 = torch.matmul(t2779, t2780)  # t2781: \"cuda:0 f32[16, 16]\"\n",
       "    # t2781 = ltorch.matmul(t2779, t2780)  # t2781: \"cuda:0 f32[16, 16]\"\n",
       "      # t2781 = prims.matmul(t2779, t2780)  # t2781: \"cuda:0 f32[16, 16]\"\n",
       "  del t2779, t2780\n",
       "  t2770 = torch.permute(t2767, (1, 0))  # t2770: \"cuda:0 f32[16, 128]\"\n",
       "    # t2770 = ltorch.permute(t2767, (1, 0))  # t2770: \"cuda:0 f32[16, 128]\"\n",
       "      # t2770 = prims.transpose(t2767, (1, 0))  # t2770: \"cuda:0 f32[16, 128]\"\n",
       "  del t2767\n",
       "  # Created by CPU Offloading Transform\n",
       "  t69 = load_to_gpu(offloaded_t69, 'cuda:0')  # t69: \"cuda:0 f32[128, 16]\"\n",
       "  t2771 = torch.reshape(t69, (-1, 16))  # t2771: \"cuda:0 f32[128, 16]\"\n",
       "    # t2771 = ltorch.reshape(t69, (-1, 16))  # t2771: \"cuda:0 f32[128, 16]\"\n",
       "      # t2771 = prims.reshape(t69, (128, 16))  # t2771: \"cuda:0 f32[128, 16]\"\n",
       "  del t69\n",
       "  t2772 = torch.matmul(t2770, t2771)  # t2772: \"cuda:0 f32[16, 16]\"\n",
       "    # t2772 = ltorch.matmul(t2770, t2771)  # t2772: \"cuda:0 f32[16, 16]\"\n",
       "      # t2772 = prims.matmul(t2770, t2771)  # t2772: \"cuda:0 f32[16, 16]\"\n",
       "  del t2770, t2771\n",
       "  t2761 = torch.permute(t2758, (1, 0))  # t2761: \"cuda:0 f32[16, 128]\"\n",
       "    # t2761 = ltorch.permute(t2758, (1, 0))  # t2761: \"cuda:0 f32[16, 128]\"\n",
       "      # t2761 = prims.transpose(t2758, (1, 0))  # t2761: \"cuda:0 f32[16, 128]\"\n",
       "  del t2758\n",
       "  # Created by CPU Offloading Transform\n",
       "  t74 = load_to_gpu(offloaded_t74, 'cuda:0')  # t74: \"cuda:0 f32[128, 16]\"\n",
       "  t2762 = torch.reshape(t74, (-1, 16))  # t2762: \"cuda:0 f32[128, 16]\"\n",
       "    # t2762 = ltorch.reshape(t74, (-1, 16))  # t2762: \"cuda:0 f32[128, 16]\"\n",
       "      # t2762 = prims.reshape(t74, (128, 16))  # t2762: \"cuda:0 f32[128, 16]\"\n",
       "  del t74\n",
       "  t2763 = torch.matmul(t2761, t2762)  # t2763: \"cuda:0 f32[16, 16]\"\n",
       "    # t2763 = ltorch.matmul(t2761, t2762)  # t2763: \"cuda:0 f32[16, 16]\"\n",
       "      # t2763 = prims.matmul(t2761, t2762)  # t2763: \"cuda:0 f32[16, 16]\"\n",
       "  del t2761, t2762\n",
       "  t2752 = torch.permute(t2749, (1, 0))  # t2752: \"cuda:0 f32[16, 128]\"\n",
       "    # t2752 = ltorch.permute(t2749, (1, 0))  # t2752: \"cuda:0 f32[16, 128]\"\n",
       "      # t2752 = prims.transpose(t2749, (1, 0))  # t2752: \"cuda:0 f32[16, 128]\"\n",
       "  del t2749\n",
       "  # Created by CPU Offloading Transform\n",
       "  t79 = load_to_gpu(offloaded_t79, 'cuda:0')  # t79: \"cuda:0 f32[128, 16]\"\n",
       "  t2753 = torch.reshape(t79, (-1, 16))  # t2753: \"cuda:0 f32[128, 16]\"\n",
       "    # t2753 = ltorch.reshape(t79, (-1, 16))  # t2753: \"cuda:0 f32[128, 16]\"\n",
       "      # t2753 = prims.reshape(t79, (128, 16))  # t2753: \"cuda:0 f32[128, 16]\"\n",
       "  del t79\n",
       "  t2754 = torch.matmul(t2752, t2753)  # t2754: \"cuda:0 f32[16, 16]\"\n",
       "    # t2754 = ltorch.matmul(t2752, t2753)  # t2754: \"cuda:0 f32[16, 16]\"\n",
       "      # t2754 = prims.matmul(t2752, t2753)  # t2754: \"cuda:0 f32[16, 16]\"\n",
       "  del t2752, t2753\n",
       "  t2743 = torch.permute(t2740, (1, 0))  # t2743: \"cuda:0 f32[16, 128]\"\n",
       "    # t2743 = ltorch.permute(t2740, (1, 0))  # t2743: \"cuda:0 f32[16, 128]\"\n",
       "      # t2743 = prims.transpose(t2740, (1, 0))  # t2743: \"cuda:0 f32[16, 128]\"\n",
       "  del t2740\n",
       "  # Created by CPU Offloading Transform\n",
       "  t84 = load_to_gpu(offloaded_t84, 'cuda:0')  # t84: \"cuda:0 f32[128, 16]\"\n",
       "  t2744 = torch.reshape(t84, (-1, 16))  # t2744: \"cuda:0 f32[128, 16]\"\n",
       "    # t2744 = ltorch.reshape(t84, (-1, 16))  # t2744: \"cuda:0 f32[128, 16]\"\n",
       "      # t2744 = prims.reshape(t84, (128, 16))  # t2744: \"cuda:0 f32[128, 16]\"\n",
       "  del t84\n",
       "  t2745 = torch.matmul(t2743, t2744)  # t2745: \"cuda:0 f32[16, 16]\"\n",
       "    # t2745 = ltorch.matmul(t2743, t2744)  # t2745: \"cuda:0 f32[16, 16]\"\n",
       "      # t2745 = prims.matmul(t2743, t2744)  # t2745: \"cuda:0 f32[16, 16]\"\n",
       "  del t2743, t2744\n",
       "  t2734 = torch.permute(t2731, (1, 0))  # t2734: \"cuda:0 f32[16, 128]\"\n",
       "    # t2734 = ltorch.permute(t2731, (1, 0))  # t2734: \"cuda:0 f32[16, 128]\"\n",
       "      # t2734 = prims.transpose(t2731, (1, 0))  # t2734: \"cuda:0 f32[16, 128]\"\n",
       "  del t2731\n",
       "  # Created by CPU Offloading Transform\n",
       "  t89 = load_to_gpu(offloaded_t89, 'cuda:0')  # t89: \"cuda:0 f32[128, 16]\"\n",
       "  t2735 = torch.reshape(t89, (-1, 16))  # t2735: \"cuda:0 f32[128, 16]\"\n",
       "    # t2735 = ltorch.reshape(t89, (-1, 16))  # t2735: \"cuda:0 f32[128, 16]\"\n",
       "      # t2735 = prims.reshape(t89, (128, 16))  # t2735: \"cuda:0 f32[128, 16]\"\n",
       "  del t89\n",
       "  t2736 = torch.matmul(t2734, t2735)  # t2736: \"cuda:0 f32[16, 16]\"\n",
       "    # t2736 = ltorch.matmul(t2734, t2735)  # t2736: \"cuda:0 f32[16, 16]\"\n",
       "      # t2736 = prims.matmul(t2734, t2735)  # t2736: \"cuda:0 f32[16, 16]\"\n",
       "  del t2734, t2735\n",
       "  t2725 = torch.permute(t2722, (1, 0))  # t2725: \"cuda:0 f32[16, 128]\"\n",
       "    # t2725 = ltorch.permute(t2722, (1, 0))  # t2725: \"cuda:0 f32[16, 128]\"\n",
       "      # t2725 = prims.transpose(t2722, (1, 0))  # t2725: \"cuda:0 f32[16, 128]\"\n",
       "  del t2722\n",
       "  # Created by CPU Offloading Transform\n",
       "  t94 = load_to_gpu(offloaded_t94, 'cuda:0')  # t94: \"cuda:0 f32[128, 16]\"\n",
       "  t2726 = torch.reshape(t94, (-1, 16))  # t2726: \"cuda:0 f32[128, 16]\"\n",
       "    # t2726 = ltorch.reshape(t94, (-1, 16))  # t2726: \"cuda:0 f32[128, 16]\"\n",
       "      # t2726 = prims.reshape(t94, (128, 16))  # t2726: \"cuda:0 f32[128, 16]\"\n",
       "  del t94\n",
       "  t2727 = torch.matmul(t2725, t2726)  # t2727: \"cuda:0 f32[16, 16]\"\n",
       "    # t2727 = ltorch.matmul(t2725, t2726)  # t2727: \"cuda:0 f32[16, 16]\"\n",
       "      # t2727 = prims.matmul(t2725, t2726)  # t2727: \"cuda:0 f32[16, 16]\"\n",
       "  del t2725, t2726\n",
       "  t2716 = torch.permute(t2713, (1, 0))  # t2716: \"cuda:0 f32[16, 128]\"\n",
       "    # t2716 = ltorch.permute(t2713, (1, 0))  # t2716: \"cuda:0 f32[16, 128]\"\n",
       "      # t2716 = prims.transpose(t2713, (1, 0))  # t2716: \"cuda:0 f32[16, 128]\"\n",
       "  del t2713\n",
       "  # Created by CPU Offloading Transform\n",
       "  t99 = load_to_gpu(offloaded_t99, 'cuda:0')  # t99: \"cuda:0 f32[128, 16]\"\n",
       "  t2717 = torch.reshape(t99, (-1, 16))  # t2717: \"cuda:0 f32[128, 16]\"\n",
       "    # t2717 = ltorch.reshape(t99, (-1, 16))  # t2717: \"cuda:0 f32[128, 16]\"\n",
       "      # t2717 = prims.reshape(t99, (128, 16))  # t2717: \"cuda:0 f32[128, 16]\"\n",
       "  del t99\n",
       "  t2718 = torch.matmul(t2716, t2717)  # t2718: \"cuda:0 f32[16, 16]\"\n",
       "    # t2718 = ltorch.matmul(t2716, t2717)  # t2718: \"cuda:0 f32[16, 16]\"\n",
       "      # t2718 = prims.matmul(t2716, t2717)  # t2718: \"cuda:0 f32[16, 16]\"\n",
       "  del t2716, t2717\n",
       "  t2707 = torch.permute(t2704, (1, 0))  # t2707: \"cuda:0 f32[16, 128]\"\n",
       "    # t2707 = ltorch.permute(t2704, (1, 0))  # t2707: \"cuda:0 f32[16, 128]\"\n",
       "      # t2707 = prims.transpose(t2704, (1, 0))  # t2707: \"cuda:0 f32[16, 128]\"\n",
       "  del t2704\n",
       "  # Created by CPU Offloading Transform\n",
       "  t104 = load_to_gpu(offloaded_t104, 'cuda:0')  # t104: \"cuda:0 f32[128, 16]\"\n",
       "  t2708 = torch.reshape(t104, (-1, 16))  # t2708: \"cuda:0 f32[128, 16]\"\n",
       "    # t2708 = ltorch.reshape(t104, (-1, 16))  # t2708: \"cuda:0 f32[128, 16]\"\n",
       "      # t2708 = prims.reshape(t104, (128, 16))  # t2708: \"cuda:0 f32[128, 16]\"\n",
       "  del t104\n",
       "  t2709 = torch.matmul(t2707, t2708)  # t2709: \"cuda:0 f32[16, 16]\"\n",
       "    # t2709 = ltorch.matmul(t2707, t2708)  # t2709: \"cuda:0 f32[16, 16]\"\n",
       "      # t2709 = prims.matmul(t2707, t2708)  # t2709: \"cuda:0 f32[16, 16]\"\n",
       "  del t2707, t2708\n",
       "  t2698 = torch.permute(t2695, (1, 0))  # t2698: \"cuda:0 f32[16, 128]\"\n",
       "    # t2698 = ltorch.permute(t2695, (1, 0))  # t2698: \"cuda:0 f32[16, 128]\"\n",
       "      # t2698 = prims.transpose(t2695, (1, 0))  # t2698: \"cuda:0 f32[16, 128]\"\n",
       "  del t2695\n",
       "  # Created by CPU Offloading Transform\n",
       "  t109 = load_to_gpu(offloaded_t109, 'cuda:0')  # t109: \"cuda:0 f32[128, 16]\"\n",
       "  t2699 = torch.reshape(t109, (-1, 16))  # t2699: \"cuda:0 f32[128, 16]\"\n",
       "    # t2699 = ltorch.reshape(t109, (-1, 16))  # t2699: \"cuda:0 f32[128, 16]\"\n",
       "      # t2699 = prims.reshape(t109, (128, 16))  # t2699: \"cuda:0 f32[128, 16]\"\n",
       "  del t109\n",
       "  t2700 = torch.matmul(t2698, t2699)  # t2700: \"cuda:0 f32[16, 16]\"\n",
       "    # t2700 = ltorch.matmul(t2698, t2699)  # t2700: \"cuda:0 f32[16, 16]\"\n",
       "      # t2700 = prims.matmul(t2698, t2699)  # t2700: \"cuda:0 f32[16, 16]\"\n",
       "  del t2698, t2699\n",
       "  t2689 = torch.permute(t2686, (1, 0))  # t2689: \"cuda:0 f32[16, 128]\"\n",
       "    # t2689 = ltorch.permute(t2686, (1, 0))  # t2689: \"cuda:0 f32[16, 128]\"\n",
       "      # t2689 = prims.transpose(t2686, (1, 0))  # t2689: \"cuda:0 f32[16, 128]\"\n",
       "  del t2686\n",
       "  # Created by CPU Offloading Transform\n",
       "  t114 = load_to_gpu(offloaded_t114, 'cuda:0')  # t114: \"cuda:0 f32[128, 16]\"\n",
       "  t2690 = torch.reshape(t114, (-1, 16))  # t2690: \"cuda:0 f32[128, 16]\"\n",
       "    # t2690 = ltorch.reshape(t114, (-1, 16))  # t2690: \"cuda:0 f32[128, 16]\"\n",
       "      # t2690 = prims.reshape(t114, (128, 16))  # t2690: \"cuda:0 f32[128, 16]\"\n",
       "  del t114\n",
       "  t2691 = torch.matmul(t2689, t2690)  # t2691: \"cuda:0 f32[16, 16]\"\n",
       "    # t2691 = ltorch.matmul(t2689, t2690)  # t2691: \"cuda:0 f32[16, 16]\"\n",
       "      # t2691 = prims.matmul(t2689, t2690)  # t2691: \"cuda:0 f32[16, 16]\"\n",
       "  del t2689, t2690\n",
       "  t2680 = torch.permute(t2677, (1, 0))  # t2680: \"cuda:0 f32[16, 128]\"\n",
       "    # t2680 = ltorch.permute(t2677, (1, 0))  # t2680: \"cuda:0 f32[16, 128]\"\n",
       "      # t2680 = prims.transpose(t2677, (1, 0))  # t2680: \"cuda:0 f32[16, 128]\"\n",
       "  del t2677\n",
       "  # Created by CPU Offloading Transform\n",
       "  t119 = load_to_gpu(offloaded_t119, 'cuda:0')  # t119: \"cuda:0 f32[128, 16]\"\n",
       "  t2681 = torch.reshape(t119, (-1, 16))  # t2681: \"cuda:0 f32[128, 16]\"\n",
       "    # t2681 = ltorch.reshape(t119, (-1, 16))  # t2681: \"cuda:0 f32[128, 16]\"\n",
       "      # t2681 = prims.reshape(t119, (128, 16))  # t2681: \"cuda:0 f32[128, 16]\"\n",
       "  del t119\n",
       "  t2682 = torch.matmul(t2680, t2681)  # t2682: \"cuda:0 f32[16, 16]\"\n",
       "    # t2682 = ltorch.matmul(t2680, t2681)  # t2682: \"cuda:0 f32[16, 16]\"\n",
       "      # t2682 = prims.matmul(t2680, t2681)  # t2682: \"cuda:0 f32[16, 16]\"\n",
       "  del t2680, t2681\n",
       "  t2671 = torch.permute(t2668, (1, 0))  # t2671: \"cuda:0 f32[16, 128]\"\n",
       "    # t2671 = ltorch.permute(t2668, (1, 0))  # t2671: \"cuda:0 f32[16, 128]\"\n",
       "      # t2671 = prims.transpose(t2668, (1, 0))  # t2671: \"cuda:0 f32[16, 128]\"\n",
       "  del t2668\n",
       "  # Created by CPU Offloading Transform\n",
       "  t124 = load_to_gpu(offloaded_t124, 'cuda:0')  # t124: \"cuda:0 f32[128, 16]\"\n",
       "  t2672 = torch.reshape(t124, (-1, 16))  # t2672: \"cuda:0 f32[128, 16]\"\n",
       "    # t2672 = ltorch.reshape(t124, (-1, 16))  # t2672: \"cuda:0 f32[128, 16]\"\n",
       "      # t2672 = prims.reshape(t124, (128, 16))  # t2672: \"cuda:0 f32[128, 16]\"\n",
       "  del t124\n",
       "  t2673 = torch.matmul(t2671, t2672)  # t2673: \"cuda:0 f32[16, 16]\"\n",
       "    # t2673 = ltorch.matmul(t2671, t2672)  # t2673: \"cuda:0 f32[16, 16]\"\n",
       "      # t2673 = prims.matmul(t2671, t2672)  # t2673: \"cuda:0 f32[16, 16]\"\n",
       "  del t2671, t2672\n",
       "  t2662 = torch.permute(t2659, (1, 0))  # t2662: \"cuda:0 f32[16, 128]\"\n",
       "    # t2662 = ltorch.permute(t2659, (1, 0))  # t2662: \"cuda:0 f32[16, 128]\"\n",
       "      # t2662 = prims.transpose(t2659, (1, 0))  # t2662: \"cuda:0 f32[16, 128]\"\n",
       "  del t2659\n",
       "  # Created by CPU Offloading Transform\n",
       "  t129 = load_to_gpu(offloaded_t129, 'cuda:0')  # t129: \"cuda:0 f32[128, 16]\"\n",
       "  t2663 = torch.reshape(t129, (-1, 16))  # t2663: \"cuda:0 f32[128, 16]\"\n",
       "    # t2663 = ltorch.reshape(t129, (-1, 16))  # t2663: \"cuda:0 f32[128, 16]\"\n",
       "      # t2663 = prims.reshape(t129, (128, 16))  # t2663: \"cuda:0 f32[128, 16]\"\n",
       "  del t129\n",
       "  t2664 = torch.matmul(t2662, t2663)  # t2664: \"cuda:0 f32[16, 16]\"\n",
       "    # t2664 = ltorch.matmul(t2662, t2663)  # t2664: \"cuda:0 f32[16, 16]\"\n",
       "      # t2664 = prims.matmul(t2662, t2663)  # t2664: \"cuda:0 f32[16, 16]\"\n",
       "  del t2662, t2663\n",
       "  t2653 = torch.permute(t2650, (1, 0))  # t2653: \"cuda:0 f32[16, 128]\"\n",
       "    # t2653 = ltorch.permute(t2650, (1, 0))  # t2653: \"cuda:0 f32[16, 128]\"\n",
       "      # t2653 = prims.transpose(t2650, (1, 0))  # t2653: \"cuda:0 f32[16, 128]\"\n",
       "  del t2650\n",
       "  # Created by CPU Offloading Transform\n",
       "  t134 = load_to_gpu(offloaded_t134, 'cuda:0')  # t134: \"cuda:0 f32[128, 16]\"\n",
       "  t2654 = torch.reshape(t134, (-1, 16))  # t2654: \"cuda:0 f32[128, 16]\"\n",
       "    # t2654 = ltorch.reshape(t134, (-1, 16))  # t2654: \"cuda:0 f32[128, 16]\"\n",
       "      # t2654 = prims.reshape(t134, (128, 16))  # t2654: \"cuda:0 f32[128, 16]\"\n",
       "  del t134\n",
       "  t2655 = torch.matmul(t2653, t2654)  # t2655: \"cuda:0 f32[16, 16]\"\n",
       "    # t2655 = ltorch.matmul(t2653, t2654)  # t2655: \"cuda:0 f32[16, 16]\"\n",
       "      # t2655 = prims.matmul(t2653, t2654)  # t2655: \"cuda:0 f32[16, 16]\"\n",
       "  del t2653, t2654\n",
       "  t2644 = torch.permute(t2641, (1, 0))  # t2644: \"cuda:0 f32[16, 128]\"\n",
       "    # t2644 = ltorch.permute(t2641, (1, 0))  # t2644: \"cuda:0 f32[16, 128]\"\n",
       "      # t2644 = prims.transpose(t2641, (1, 0))  # t2644: \"cuda:0 f32[16, 128]\"\n",
       "  del t2641\n",
       "  # Created by CPU Offloading Transform\n",
       "  t139 = load_to_gpu(offloaded_t139, 'cuda:0')  # t139: \"cuda:0 f32[128, 16]\"\n",
       "  t2645 = torch.reshape(t139, (-1, 16))  # t2645: \"cuda:0 f32[128, 16]\"\n",
       "    # t2645 = ltorch.reshape(t139, (-1, 16))  # t2645: \"cuda:0 f32[128, 16]\"\n",
       "      # t2645 = prims.reshape(t139, (128, 16))  # t2645: \"cuda:0 f32[128, 16]\"\n",
       "  del t139\n",
       "  t2646 = torch.matmul(t2644, t2645)  # t2646: \"cuda:0 f32[16, 16]\"\n",
       "    # t2646 = ltorch.matmul(t2644, t2645)  # t2646: \"cuda:0 f32[16, 16]\"\n",
       "      # t2646 = prims.matmul(t2644, t2645)  # t2646: \"cuda:0 f32[16, 16]\"\n",
       "  del t2644, t2645\n",
       "  t2635 = torch.permute(t2632, (1, 0))  # t2635: \"cuda:0 f32[16, 128]\"\n",
       "    # t2635 = ltorch.permute(t2632, (1, 0))  # t2635: \"cuda:0 f32[16, 128]\"\n",
       "      # t2635 = prims.transpose(t2632, (1, 0))  # t2635: \"cuda:0 f32[16, 128]\"\n",
       "  del t2632\n",
       "  # Created by CPU Offloading Transform\n",
       "  t144 = load_to_gpu(offloaded_t144, 'cuda:0')  # t144: \"cuda:0 f32[128, 16]\"\n",
       "  t2636 = torch.reshape(t144, (-1, 16))  # t2636: \"cuda:0 f32[128, 16]\"\n",
       "    # t2636 = ltorch.reshape(t144, (-1, 16))  # t2636: \"cuda:0 f32[128, 16]\"\n",
       "      # t2636 = prims.reshape(t144, (128, 16))  # t2636: \"cuda:0 f32[128, 16]\"\n",
       "  del t144\n",
       "  t2637 = torch.matmul(t2635, t2636)  # t2637: \"cuda:0 f32[16, 16]\"\n",
       "    # t2637 = ltorch.matmul(t2635, t2636)  # t2637: \"cuda:0 f32[16, 16]\"\n",
       "      # t2637 = prims.matmul(t2635, t2636)  # t2637: \"cuda:0 f32[16, 16]\"\n",
       "  del t2635, t2636\n",
       "  t2626 = torch.permute(t2623, (1, 0))  # t2626: \"cuda:0 f32[16, 128]\"\n",
       "    # t2626 = ltorch.permute(t2623, (1, 0))  # t2626: \"cuda:0 f32[16, 128]\"\n",
       "      # t2626 = prims.transpose(t2623, (1, 0))  # t2626: \"cuda:0 f32[16, 128]\"\n",
       "  del t2623\n",
       "  # Created by CPU Offloading Transform\n",
       "  t149 = load_to_gpu(offloaded_t149, 'cuda:0')  # t149: \"cuda:0 f32[128, 16]\"\n",
       "  t2627 = torch.reshape(t149, (-1, 16))  # t2627: \"cuda:0 f32[128, 16]\"\n",
       "    # t2627 = ltorch.reshape(t149, (-1, 16))  # t2627: \"cuda:0 f32[128, 16]\"\n",
       "      # t2627 = prims.reshape(t149, (128, 16))  # t2627: \"cuda:0 f32[128, 16]\"\n",
       "  del t149\n",
       "  t2628 = torch.matmul(t2626, t2627)  # t2628: \"cuda:0 f32[16, 16]\"\n",
       "    # t2628 = ltorch.matmul(t2626, t2627)  # t2628: \"cuda:0 f32[16, 16]\"\n",
       "      # t2628 = prims.matmul(t2626, t2627)  # t2628: \"cuda:0 f32[16, 16]\"\n",
       "  del t2626, t2627\n",
       "  t2617 = torch.permute(t2614, (1, 0))  # t2617: \"cuda:0 f32[16, 128]\"\n",
       "    # t2617 = ltorch.permute(t2614, (1, 0))  # t2617: \"cuda:0 f32[16, 128]\"\n",
       "      # t2617 = prims.transpose(t2614, (1, 0))  # t2617: \"cuda:0 f32[16, 128]\"\n",
       "  del t2614\n",
       "  # Created by CPU Offloading Transform\n",
       "  t154 = load_to_gpu(offloaded_t154, 'cuda:0')  # t154: \"cuda:0 f32[128, 16]\"\n",
       "  t2618 = torch.reshape(t154, (-1, 16))  # t2618: \"cuda:0 f32[128, 16]\"\n",
       "    # t2618 = ltorch.reshape(t154, (-1, 16))  # t2618: \"cuda:0 f32[128, 16]\"\n",
       "      # t2618 = prims.reshape(t154, (128, 16))  # t2618: \"cuda:0 f32[128, 16]\"\n",
       "  del t154\n",
       "  t2619 = torch.matmul(t2617, t2618)  # t2619: \"cuda:0 f32[16, 16]\"\n",
       "    # t2619 = ltorch.matmul(t2617, t2618)  # t2619: \"cuda:0 f32[16, 16]\"\n",
       "      # t2619 = prims.matmul(t2617, t2618)  # t2619: \"cuda:0 f32[16, 16]\"\n",
       "  del t2617, t2618\n",
       "  t2608 = torch.permute(t2605, (1, 0))  # t2608: \"cuda:0 f32[16, 128]\"\n",
       "    # t2608 = ltorch.permute(t2605, (1, 0))  # t2608: \"cuda:0 f32[16, 128]\"\n",
       "      # t2608 = prims.transpose(t2605, (1, 0))  # t2608: \"cuda:0 f32[16, 128]\"\n",
       "  del t2605\n",
       "  # Created by CPU Offloading Transform\n",
       "  t159 = load_to_gpu(offloaded_t159, 'cuda:0')  # t159: \"cuda:0 f32[128, 16]\"\n",
       "  t2609 = torch.reshape(t159, (-1, 16))  # t2609: \"cuda:0 f32[128, 16]\"\n",
       "    # t2609 = ltorch.reshape(t159, (-1, 16))  # t2609: \"cuda:0 f32[128, 16]\"\n",
       "      # t2609 = prims.reshape(t159, (128, 16))  # t2609: \"cuda:0 f32[128, 16]\"\n",
       "  del t159\n",
       "  t2610 = torch.matmul(t2608, t2609)  # t2610: \"cuda:0 f32[16, 16]\"\n",
       "    # t2610 = ltorch.matmul(t2608, t2609)  # t2610: \"cuda:0 f32[16, 16]\"\n",
       "      # t2610 = prims.matmul(t2608, t2609)  # t2610: \"cuda:0 f32[16, 16]\"\n",
       "  del t2608, t2609\n",
       "  t2599 = torch.permute(t2596, (1, 0))  # t2599: \"cuda:0 f32[16, 128]\"\n",
       "    # t2599 = ltorch.permute(t2596, (1, 0))  # t2599: \"cuda:0 f32[16, 128]\"\n",
       "      # t2599 = prims.transpose(t2596, (1, 0))  # t2599: \"cuda:0 f32[16, 128]\"\n",
       "  del t2596\n",
       "  # Created by CPU Offloading Transform\n",
       "  t164 = load_to_gpu(offloaded_t164, 'cuda:0')  # t164: \"cuda:0 f32[128, 16]\"\n",
       "  t2600 = torch.reshape(t164, (-1, 16))  # t2600: \"cuda:0 f32[128, 16]\"\n",
       "    # t2600 = ltorch.reshape(t164, (-1, 16))  # t2600: \"cuda:0 f32[128, 16]\"\n",
       "      # t2600 = prims.reshape(t164, (128, 16))  # t2600: \"cuda:0 f32[128, 16]\"\n",
       "  del t164\n",
       "  t2601 = torch.matmul(t2599, t2600)  # t2601: \"cuda:0 f32[16, 16]\"\n",
       "    # t2601 = ltorch.matmul(t2599, t2600)  # t2601: \"cuda:0 f32[16, 16]\"\n",
       "      # t2601 = prims.matmul(t2599, t2600)  # t2601: \"cuda:0 f32[16, 16]\"\n",
       "  del t2599, t2600\n",
       "  t2590 = torch.permute(t2587, (1, 0))  # t2590: \"cuda:0 f32[16, 128]\"\n",
       "    # t2590 = ltorch.permute(t2587, (1, 0))  # t2590: \"cuda:0 f32[16, 128]\"\n",
       "      # t2590 = prims.transpose(t2587, (1, 0))  # t2590: \"cuda:0 f32[16, 128]\"\n",
       "  del t2587\n",
       "  # Created by CPU Offloading Transform\n",
       "  t169 = load_to_gpu(offloaded_t169, 'cuda:0')  # t169: \"cuda:0 f32[128, 16]\"\n",
       "  t2591 = torch.reshape(t169, (-1, 16))  # t2591: \"cuda:0 f32[128, 16]\"\n",
       "    # t2591 = ltorch.reshape(t169, (-1, 16))  # t2591: \"cuda:0 f32[128, 16]\"\n",
       "      # t2591 = prims.reshape(t169, (128, 16))  # t2591: \"cuda:0 f32[128, 16]\"\n",
       "  del t169\n",
       "  t2592 = torch.matmul(t2590, t2591)  # t2592: \"cuda:0 f32[16, 16]\"\n",
       "    # t2592 = ltorch.matmul(t2590, t2591)  # t2592: \"cuda:0 f32[16, 16]\"\n",
       "      # t2592 = prims.matmul(t2590, t2591)  # t2592: \"cuda:0 f32[16, 16]\"\n",
       "  del t2590, t2591\n",
       "  t2581 = torch.permute(t2578, (1, 0))  # t2581: \"cuda:0 f32[16, 128]\"\n",
       "    # t2581 = ltorch.permute(t2578, (1, 0))  # t2581: \"cuda:0 f32[16, 128]\"\n",
       "      # t2581 = prims.transpose(t2578, (1, 0))  # t2581: \"cuda:0 f32[16, 128]\"\n",
       "  del t2578\n",
       "  # Created by CPU Offloading Transform\n",
       "  t174 = load_to_gpu(offloaded_t174, 'cuda:0')  # t174: \"cuda:0 f32[128, 16]\"\n",
       "  t2582 = torch.reshape(t174, (-1, 16))  # t2582: \"cuda:0 f32[128, 16]\"\n",
       "    # t2582 = ltorch.reshape(t174, (-1, 16))  # t2582: \"cuda:0 f32[128, 16]\"\n",
       "      # t2582 = prims.reshape(t174, (128, 16))  # t2582: \"cuda:0 f32[128, 16]\"\n",
       "  del t174\n",
       "  t2583 = torch.matmul(t2581, t2582)  # t2583: \"cuda:0 f32[16, 16]\"\n",
       "    # t2583 = ltorch.matmul(t2581, t2582)  # t2583: \"cuda:0 f32[16, 16]\"\n",
       "      # t2583 = prims.matmul(t2581, t2582)  # t2583: \"cuda:0 f32[16, 16]\"\n",
       "  del t2581, t2582\n",
       "  t2572 = torch.permute(t2569, (1, 0))  # t2572: \"cuda:0 f32[16, 128]\"\n",
       "    # t2572 = ltorch.permute(t2569, (1, 0))  # t2572: \"cuda:0 f32[16, 128]\"\n",
       "      # t2572 = prims.transpose(t2569, (1, 0))  # t2572: \"cuda:0 f32[16, 128]\"\n",
       "  del t2569\n",
       "  # Created by CPU Offloading Transform\n",
       "  t179 = load_to_gpu(offloaded_t179, 'cuda:0')  # t179: \"cuda:0 f32[128, 16]\"\n",
       "  t2573 = torch.reshape(t179, (-1, 16))  # t2573: \"cuda:0 f32[128, 16]\"\n",
       "    # t2573 = ltorch.reshape(t179, (-1, 16))  # t2573: \"cuda:0 f32[128, 16]\"\n",
       "      # t2573 = prims.reshape(t179, (128, 16))  # t2573: \"cuda:0 f32[128, 16]\"\n",
       "  del t179\n",
       "  t2574 = torch.matmul(t2572, t2573)  # t2574: \"cuda:0 f32[16, 16]\"\n",
       "    # t2574 = ltorch.matmul(t2572, t2573)  # t2574: \"cuda:0 f32[16, 16]\"\n",
       "      # t2574 = prims.matmul(t2572, t2573)  # t2574: \"cuda:0 f32[16, 16]\"\n",
       "  del t2572, t2573\n",
       "  t2563 = torch.permute(t2560, (1, 0))  # t2563: \"cuda:0 f32[16, 128]\"\n",
       "    # t2563 = ltorch.permute(t2560, (1, 0))  # t2563: \"cuda:0 f32[16, 128]\"\n",
       "      # t2563 = prims.transpose(t2560, (1, 0))  # t2563: \"cuda:0 f32[16, 128]\"\n",
       "  del t2560\n",
       "  # Created by CPU Offloading Transform\n",
       "  t184 = load_to_gpu(offloaded_t184, 'cuda:0')  # t184: \"cuda:0 f32[128, 16]\"\n",
       "  t2564 = torch.reshape(t184, (-1, 16))  # t2564: \"cuda:0 f32[128, 16]\"\n",
       "    # t2564 = ltorch.reshape(t184, (-1, 16))  # t2564: \"cuda:0 f32[128, 16]\"\n",
       "      # t2564 = prims.reshape(t184, (128, 16))  # t2564: \"cuda:0 f32[128, 16]\"\n",
       "  del t184\n",
       "  t2565 = torch.matmul(t2563, t2564)  # t2565: \"cuda:0 f32[16, 16]\"\n",
       "    # t2565 = ltorch.matmul(t2563, t2564)  # t2565: \"cuda:0 f32[16, 16]\"\n",
       "      # t2565 = prims.matmul(t2563, t2564)  # t2565: \"cuda:0 f32[16, 16]\"\n",
       "  del t2563, t2564\n",
       "  t2554 = torch.permute(t2551, (1, 0))  # t2554: \"cuda:0 f32[16, 128]\"\n",
       "    # t2554 = ltorch.permute(t2551, (1, 0))  # t2554: \"cuda:0 f32[16, 128]\"\n",
       "      # t2554 = prims.transpose(t2551, (1, 0))  # t2554: \"cuda:0 f32[16, 128]\"\n",
       "  del t2551\n",
       "  # Created by CPU Offloading Transform\n",
       "  t189 = load_to_gpu(offloaded_t189, 'cuda:0')  # t189: \"cuda:0 f32[128, 16]\"\n",
       "  t2555 = torch.reshape(t189, (-1, 16))  # t2555: \"cuda:0 f32[128, 16]\"\n",
       "    # t2555 = ltorch.reshape(t189, (-1, 16))  # t2555: \"cuda:0 f32[128, 16]\"\n",
       "      # t2555 = prims.reshape(t189, (128, 16))  # t2555: \"cuda:0 f32[128, 16]\"\n",
       "  del t189\n",
       "  t2556 = torch.matmul(t2554, t2555)  # t2556: \"cuda:0 f32[16, 16]\"\n",
       "    # t2556 = ltorch.matmul(t2554, t2555)  # t2556: \"cuda:0 f32[16, 16]\"\n",
       "      # t2556 = prims.matmul(t2554, t2555)  # t2556: \"cuda:0 f32[16, 16]\"\n",
       "  del t2554, t2555\n",
       "  t2545 = torch.permute(t2542, (1, 0))  # t2545: \"cuda:0 f32[16, 128]\"\n",
       "    # t2545 = ltorch.permute(t2542, (1, 0))  # t2545: \"cuda:0 f32[16, 128]\"\n",
       "      # t2545 = prims.transpose(t2542, (1, 0))  # t2545: \"cuda:0 f32[16, 128]\"\n",
       "  del t2542\n",
       "  # Created by CPU Offloading Transform\n",
       "  t194 = load_to_gpu(offloaded_t194, 'cuda:0')  # t194: \"cuda:0 f32[128, 16]\"\n",
       "  t2546 = torch.reshape(t194, (-1, 16))  # t2546: \"cuda:0 f32[128, 16]\"\n",
       "    # t2546 = ltorch.reshape(t194, (-1, 16))  # t2546: \"cuda:0 f32[128, 16]\"\n",
       "      # t2546 = prims.reshape(t194, (128, 16))  # t2546: \"cuda:0 f32[128, 16]\"\n",
       "  del t194\n",
       "  t2547 = torch.matmul(t2545, t2546)  # t2547: \"cuda:0 f32[16, 16]\"\n",
       "    # t2547 = ltorch.matmul(t2545, t2546)  # t2547: \"cuda:0 f32[16, 16]\"\n",
       "      # t2547 = prims.matmul(t2545, t2546)  # t2547: \"cuda:0 f32[16, 16]\"\n",
       "  del t2545, t2546\n",
       "  t2536 = torch.permute(t2533, (1, 0))  # t2536: \"cuda:0 f32[16, 128]\"\n",
       "    # t2536 = ltorch.permute(t2533, (1, 0))  # t2536: \"cuda:0 f32[16, 128]\"\n",
       "      # t2536 = prims.transpose(t2533, (1, 0))  # t2536: \"cuda:0 f32[16, 128]\"\n",
       "  del t2533\n",
       "  # Created by CPU Offloading Transform\n",
       "  t199 = load_to_gpu(offloaded_t199, 'cuda:0')  # t199: \"cuda:0 f32[128, 16]\"\n",
       "  t2537 = torch.reshape(t199, (-1, 16))  # t2537: \"cuda:0 f32[128, 16]\"\n",
       "    # t2537 = ltorch.reshape(t199, (-1, 16))  # t2537: \"cuda:0 f32[128, 16]\"\n",
       "      # t2537 = prims.reshape(t199, (128, 16))  # t2537: \"cuda:0 f32[128, 16]\"\n",
       "  del t199\n",
       "  t2538 = torch.matmul(t2536, t2537)  # t2538: \"cuda:0 f32[16, 16]\"\n",
       "    # t2538 = ltorch.matmul(t2536, t2537)  # t2538: \"cuda:0 f32[16, 16]\"\n",
       "      # t2538 = prims.matmul(t2536, t2537)  # t2538: \"cuda:0 f32[16, 16]\"\n",
       "  del t2536, t2537\n",
       "  t2527 = torch.permute(t2524, (1, 0))  # t2527: \"cuda:0 f32[16, 128]\"\n",
       "    # t2527 = ltorch.permute(t2524, (1, 0))  # t2527: \"cuda:0 f32[16, 128]\"\n",
       "      # t2527 = prims.transpose(t2524, (1, 0))  # t2527: \"cuda:0 f32[16, 128]\"\n",
       "  del t2524\n",
       "  # Created by CPU Offloading Transform\n",
       "  t204 = load_to_gpu(offloaded_t204, 'cuda:0')  # t204: \"cuda:0 f32[128, 16]\"\n",
       "  t2528 = torch.reshape(t204, (-1, 16))  # t2528: \"cuda:0 f32[128, 16]\"\n",
       "    # t2528 = ltorch.reshape(t204, (-1, 16))  # t2528: \"cuda:0 f32[128, 16]\"\n",
       "      # t2528 = prims.reshape(t204, (128, 16))  # t2528: \"cuda:0 f32[128, 16]\"\n",
       "  del t204\n",
       "  t2529 = torch.matmul(t2527, t2528)  # t2529: \"cuda:0 f32[16, 16]\"\n",
       "    # t2529 = ltorch.matmul(t2527, t2528)  # t2529: \"cuda:0 f32[16, 16]\"\n",
       "      # t2529 = prims.matmul(t2527, t2528)  # t2529: \"cuda:0 f32[16, 16]\"\n",
       "  del t2527, t2528\n",
       "  t2518 = torch.permute(t2515, (1, 0))  # t2518: \"cuda:0 f32[16, 128]\"\n",
       "    # t2518 = ltorch.permute(t2515, (1, 0))  # t2518: \"cuda:0 f32[16, 128]\"\n",
       "      # t2518 = prims.transpose(t2515, (1, 0))  # t2518: \"cuda:0 f32[16, 128]\"\n",
       "  del t2515\n",
       "  # Created by CPU Offloading Transform\n",
       "  t209 = load_to_gpu(offloaded_t209, 'cuda:0')  # t209: \"cuda:0 f32[128, 16]\"\n",
       "  t2519 = torch.reshape(t209, (-1, 16))  # t2519: \"cuda:0 f32[128, 16]\"\n",
       "    # t2519 = ltorch.reshape(t209, (-1, 16))  # t2519: \"cuda:0 f32[128, 16]\"\n",
       "      # t2519 = prims.reshape(t209, (128, 16))  # t2519: \"cuda:0 f32[128, 16]\"\n",
       "  del t209\n",
       "  t2520 = torch.matmul(t2518, t2519)  # t2520: \"cuda:0 f32[16, 16]\"\n",
       "    # t2520 = ltorch.matmul(t2518, t2519)  # t2520: \"cuda:0 f32[16, 16]\"\n",
       "      # t2520 = prims.matmul(t2518, t2519)  # t2520: \"cuda:0 f32[16, 16]\"\n",
       "  del t2518, t2519\n",
       "  t2509 = torch.permute(t2506, (1, 0))  # t2509: \"cuda:0 f32[16, 128]\"\n",
       "    # t2509 = ltorch.permute(t2506, (1, 0))  # t2509: \"cuda:0 f32[16, 128]\"\n",
       "      # t2509 = prims.transpose(t2506, (1, 0))  # t2509: \"cuda:0 f32[16, 128]\"\n",
       "  del t2506\n",
       "  # Created by CPU Offloading Transform\n",
       "  t214 = load_to_gpu(offloaded_t214, 'cuda:0')  # t214: \"cuda:0 f32[128, 16]\"\n",
       "  t2510 = torch.reshape(t214, (-1, 16))  # t2510: \"cuda:0 f32[128, 16]\"\n",
       "    # t2510 = ltorch.reshape(t214, (-1, 16))  # t2510: \"cuda:0 f32[128, 16]\"\n",
       "      # t2510 = prims.reshape(t214, (128, 16))  # t2510: \"cuda:0 f32[128, 16]\"\n",
       "  del t214\n",
       "  t2511 = torch.matmul(t2509, t2510)  # t2511: \"cuda:0 f32[16, 16]\"\n",
       "    # t2511 = ltorch.matmul(t2509, t2510)  # t2511: \"cuda:0 f32[16, 16]\"\n",
       "      # t2511 = prims.matmul(t2509, t2510)  # t2511: \"cuda:0 f32[16, 16]\"\n",
       "  del t2509, t2510\n",
       "  t2500 = torch.permute(t2497, (1, 0))  # t2500: \"cuda:0 f32[16, 128]\"\n",
       "    # t2500 = ltorch.permute(t2497, (1, 0))  # t2500: \"cuda:0 f32[16, 128]\"\n",
       "      # t2500 = prims.transpose(t2497, (1, 0))  # t2500: \"cuda:0 f32[16, 128]\"\n",
       "  del t2497\n",
       "  # Created by CPU Offloading Transform\n",
       "  t219 = load_to_gpu(offloaded_t219, 'cuda:0')  # t219: \"cuda:0 f32[128, 16]\"\n",
       "  t2501 = torch.reshape(t219, (-1, 16))  # t2501: \"cuda:0 f32[128, 16]\"\n",
       "    # t2501 = ltorch.reshape(t219, (-1, 16))  # t2501: \"cuda:0 f32[128, 16]\"\n",
       "      # t2501 = prims.reshape(t219, (128, 16))  # t2501: \"cuda:0 f32[128, 16]\"\n",
       "  del t219\n",
       "  t2502 = torch.matmul(t2500, t2501)  # t2502: \"cuda:0 f32[16, 16]\"\n",
       "    # t2502 = ltorch.matmul(t2500, t2501)  # t2502: \"cuda:0 f32[16, 16]\"\n",
       "      # t2502 = prims.matmul(t2500, t2501)  # t2502: \"cuda:0 f32[16, 16]\"\n",
       "  del t2500, t2501\n",
       "  t2491 = torch.permute(t2488, (1, 0))  # t2491: \"cuda:0 f32[16, 128]\"\n",
       "    # t2491 = ltorch.permute(t2488, (1, 0))  # t2491: \"cuda:0 f32[16, 128]\"\n",
       "      # t2491 = prims.transpose(t2488, (1, 0))  # t2491: \"cuda:0 f32[16, 128]\"\n",
       "  del t2488\n",
       "  # Created by CPU Offloading Transform\n",
       "  t224 = load_to_gpu(offloaded_t224, 'cuda:0')  # t224: \"cuda:0 f32[128, 16]\"\n",
       "  t2492 = torch.reshape(t224, (-1, 16))  # t2492: \"cuda:0 f32[128, 16]\"\n",
       "    # t2492 = ltorch.reshape(t224, (-1, 16))  # t2492: \"cuda:0 f32[128, 16]\"\n",
       "      # t2492 = prims.reshape(t224, (128, 16))  # t2492: \"cuda:0 f32[128, 16]\"\n",
       "  del t224\n",
       "  t2493 = torch.matmul(t2491, t2492)  # t2493: \"cuda:0 f32[16, 16]\"\n",
       "    # t2493 = ltorch.matmul(t2491, t2492)  # t2493: \"cuda:0 f32[16, 16]\"\n",
       "      # t2493 = prims.matmul(t2491, t2492)  # t2493: \"cuda:0 f32[16, 16]\"\n",
       "  del t2491, t2492\n",
       "  t2482 = torch.permute(t2479, (1, 0))  # t2482: \"cuda:0 f32[16, 128]\"\n",
       "    # t2482 = ltorch.permute(t2479, (1, 0))  # t2482: \"cuda:0 f32[16, 128]\"\n",
       "      # t2482 = prims.transpose(t2479, (1, 0))  # t2482: \"cuda:0 f32[16, 128]\"\n",
       "  del t2479\n",
       "  # Created by CPU Offloading Transform\n",
       "  t229 = load_to_gpu(offloaded_t229, 'cuda:0')  # t229: \"cuda:0 f32[128, 16]\"\n",
       "  t2483 = torch.reshape(t229, (-1, 16))  # t2483: \"cuda:0 f32[128, 16]\"\n",
       "    # t2483 = ltorch.reshape(t229, (-1, 16))  # t2483: \"cuda:0 f32[128, 16]\"\n",
       "      # t2483 = prims.reshape(t229, (128, 16))  # t2483: \"cuda:0 f32[128, 16]\"\n",
       "  del t229\n",
       "  t2484 = torch.matmul(t2482, t2483)  # t2484: \"cuda:0 f32[16, 16]\"\n",
       "    # t2484 = ltorch.matmul(t2482, t2483)  # t2484: \"cuda:0 f32[16, 16]\"\n",
       "      # t2484 = prims.matmul(t2482, t2483)  # t2484: \"cuda:0 f32[16, 16]\"\n",
       "  del t2482, t2483\n",
       "  t2473 = torch.permute(t2470, (1, 0))  # t2473: \"cuda:0 f32[16, 128]\"\n",
       "    # t2473 = ltorch.permute(t2470, (1, 0))  # t2473: \"cuda:0 f32[16, 128]\"\n",
       "      # t2473 = prims.transpose(t2470, (1, 0))  # t2473: \"cuda:0 f32[16, 128]\"\n",
       "  del t2470\n",
       "  # Created by CPU Offloading Transform\n",
       "  t234 = load_to_gpu(offloaded_t234, 'cuda:0')  # t234: \"cuda:0 f32[128, 16]\"\n",
       "  t2474 = torch.reshape(t234, (-1, 16))  # t2474: \"cuda:0 f32[128, 16]\"\n",
       "    # t2474 = ltorch.reshape(t234, (-1, 16))  # t2474: \"cuda:0 f32[128, 16]\"\n",
       "      # t2474 = prims.reshape(t234, (128, 16))  # t2474: \"cuda:0 f32[128, 16]\"\n",
       "  del t234\n",
       "  t2475 = torch.matmul(t2473, t2474)  # t2475: \"cuda:0 f32[16, 16]\"\n",
       "    # t2475 = ltorch.matmul(t2473, t2474)  # t2475: \"cuda:0 f32[16, 16]\"\n",
       "      # t2475 = prims.matmul(t2473, t2474)  # t2475: \"cuda:0 f32[16, 16]\"\n",
       "  del t2473, t2474\n",
       "  t2464 = torch.permute(t2461, (1, 0))  # t2464: \"cuda:0 f32[16, 128]\"\n",
       "    # t2464 = ltorch.permute(t2461, (1, 0))  # t2464: \"cuda:0 f32[16, 128]\"\n",
       "      # t2464 = prims.transpose(t2461, (1, 0))  # t2464: \"cuda:0 f32[16, 128]\"\n",
       "  del t2461\n",
       "  # Created by CPU Offloading Transform\n",
       "  t239 = load_to_gpu(offloaded_t239, 'cuda:0')  # t239: \"cuda:0 f32[128, 16]\"\n",
       "  t2465 = torch.reshape(t239, (-1, 16))  # t2465: \"cuda:0 f32[128, 16]\"\n",
       "    # t2465 = ltorch.reshape(t239, (-1, 16))  # t2465: \"cuda:0 f32[128, 16]\"\n",
       "      # t2465 = prims.reshape(t239, (128, 16))  # t2465: \"cuda:0 f32[128, 16]\"\n",
       "  del t239\n",
       "  t2466 = torch.matmul(t2464, t2465)  # t2466: \"cuda:0 f32[16, 16]\"\n",
       "    # t2466 = ltorch.matmul(t2464, t2465)  # t2466: \"cuda:0 f32[16, 16]\"\n",
       "      # t2466 = prims.matmul(t2464, t2465)  # t2466: \"cuda:0 f32[16, 16]\"\n",
       "  del t2464, t2465\n",
       "  t2455 = torch.permute(t2452, (1, 0))  # t2455: \"cuda:0 f32[16, 128]\"\n",
       "    # t2455 = ltorch.permute(t2452, (1, 0))  # t2455: \"cuda:0 f32[16, 128]\"\n",
       "      # t2455 = prims.transpose(t2452, (1, 0))  # t2455: \"cuda:0 f32[16, 128]\"\n",
       "  del t2452\n",
       "  # Created by CPU Offloading Transform\n",
       "  t244 = load_to_gpu(offloaded_t244, 'cuda:0')  # t244: \"cuda:0 f32[128, 16]\"\n",
       "  t2456 = torch.reshape(t244, (-1, 16))  # t2456: \"cuda:0 f32[128, 16]\"\n",
       "    # t2456 = ltorch.reshape(t244, (-1, 16))  # t2456: \"cuda:0 f32[128, 16]\"\n",
       "      # t2456 = prims.reshape(t244, (128, 16))  # t2456: \"cuda:0 f32[128, 16]\"\n",
       "  del t244\n",
       "  t2457 = torch.matmul(t2455, t2456)  # t2457: \"cuda:0 f32[16, 16]\"\n",
       "    # t2457 = ltorch.matmul(t2455, t2456)  # t2457: \"cuda:0 f32[16, 16]\"\n",
       "      # t2457 = prims.matmul(t2455, t2456)  # t2457: \"cuda:0 f32[16, 16]\"\n",
       "  del t2455, t2456\n",
       "  t2446 = torch.permute(t2443, (1, 0))  # t2446: \"cuda:0 f32[16, 128]\"\n",
       "    # t2446 = ltorch.permute(t2443, (1, 0))  # t2446: \"cuda:0 f32[16, 128]\"\n",
       "      # t2446 = prims.transpose(t2443, (1, 0))  # t2446: \"cuda:0 f32[16, 128]\"\n",
       "  del t2443\n",
       "  # Created by CPU Offloading Transform\n",
       "  t249 = load_to_gpu(offloaded_t249, 'cuda:0')  # t249: \"cuda:0 f32[128, 16]\"\n",
       "  t2447 = torch.reshape(t249, (-1, 16))  # t2447: \"cuda:0 f32[128, 16]\"\n",
       "    # t2447 = ltorch.reshape(t249, (-1, 16))  # t2447: \"cuda:0 f32[128, 16]\"\n",
       "      # t2447 = prims.reshape(t249, (128, 16))  # t2447: \"cuda:0 f32[128, 16]\"\n",
       "  del t249\n",
       "  t2448 = torch.matmul(t2446, t2447)  # t2448: \"cuda:0 f32[16, 16]\"\n",
       "    # t2448 = ltorch.matmul(t2446, t2447)  # t2448: \"cuda:0 f32[16, 16]\"\n",
       "      # t2448 = prims.matmul(t2446, t2447)  # t2448: \"cuda:0 f32[16, 16]\"\n",
       "  del t2446, t2447\n",
       "  t2437 = torch.permute(t2434, (1, 0))  # t2437: \"cuda:0 f32[16, 128]\"\n",
       "    # t2437 = ltorch.permute(t2434, (1, 0))  # t2437: \"cuda:0 f32[16, 128]\"\n",
       "      # t2437 = prims.transpose(t2434, (1, 0))  # t2437: \"cuda:0 f32[16, 128]\"\n",
       "  del t2434\n",
       "  # Created by CPU Offloading Transform\n",
       "  t254 = load_to_gpu(offloaded_t254, 'cuda:0')  # t254: \"cuda:0 f32[128, 16]\"\n",
       "  t2438 = torch.reshape(t254, (-1, 16))  # t2438: \"cuda:0 f32[128, 16]\"\n",
       "    # t2438 = ltorch.reshape(t254, (-1, 16))  # t2438: \"cuda:0 f32[128, 16]\"\n",
       "      # t2438 = prims.reshape(t254, (128, 16))  # t2438: \"cuda:0 f32[128, 16]\"\n",
       "  del t254\n",
       "  t2439 = torch.matmul(t2437, t2438)  # t2439: \"cuda:0 f32[16, 16]\"\n",
       "    # t2439 = ltorch.matmul(t2437, t2438)  # t2439: \"cuda:0 f32[16, 16]\"\n",
       "      # t2439 = prims.matmul(t2437, t2438)  # t2439: \"cuda:0 f32[16, 16]\"\n",
       "  del t2437, t2438\n",
       "  t2428 = torch.permute(t2425, (1, 0))  # t2428: \"cuda:0 f32[16, 128]\"\n",
       "    # t2428 = ltorch.permute(t2425, (1, 0))  # t2428: \"cuda:0 f32[16, 128]\"\n",
       "      # t2428 = prims.transpose(t2425, (1, 0))  # t2428: \"cuda:0 f32[16, 128]\"\n",
       "  del t2425\n",
       "  # Created by CPU Offloading Transform\n",
       "  t259 = load_to_gpu(offloaded_t259, 'cuda:0')  # t259: \"cuda:0 f32[128, 16]\"\n",
       "  t2429 = torch.reshape(t259, (-1, 16))  # t2429: \"cuda:0 f32[128, 16]\"\n",
       "    # t2429 = ltorch.reshape(t259, (-1, 16))  # t2429: \"cuda:0 f32[128, 16]\"\n",
       "      # t2429 = prims.reshape(t259, (128, 16))  # t2429: \"cuda:0 f32[128, 16]\"\n",
       "  del t259\n",
       "  t2430 = torch.matmul(t2428, t2429)  # t2430: \"cuda:0 f32[16, 16]\"\n",
       "    # t2430 = ltorch.matmul(t2428, t2429)  # t2430: \"cuda:0 f32[16, 16]\"\n",
       "      # t2430 = prims.matmul(t2428, t2429)  # t2430: \"cuda:0 f32[16, 16]\"\n",
       "  del t2428, t2429\n",
       "  t2419 = torch.permute(t2416, (1, 0))  # t2419: \"cuda:0 f32[16, 128]\"\n",
       "    # t2419 = ltorch.permute(t2416, (1, 0))  # t2419: \"cuda:0 f32[16, 128]\"\n",
       "      # t2419 = prims.transpose(t2416, (1, 0))  # t2419: \"cuda:0 f32[16, 128]\"\n",
       "  del t2416\n",
       "  # Created by CPU Offloading Transform\n",
       "  t264 = load_to_gpu(offloaded_t264, 'cuda:0')  # t264: \"cuda:0 f32[128, 16]\"\n",
       "  t2420 = torch.reshape(t264, (-1, 16))  # t2420: \"cuda:0 f32[128, 16]\"\n",
       "    # t2420 = ltorch.reshape(t264, (-1, 16))  # t2420: \"cuda:0 f32[128, 16]\"\n",
       "      # t2420 = prims.reshape(t264, (128, 16))  # t2420: \"cuda:0 f32[128, 16]\"\n",
       "  del t264\n",
       "  t2421 = torch.matmul(t2419, t2420)  # t2421: \"cuda:0 f32[16, 16]\"\n",
       "    # t2421 = ltorch.matmul(t2419, t2420)  # t2421: \"cuda:0 f32[16, 16]\"\n",
       "      # t2421 = prims.matmul(t2419, t2420)  # t2421: \"cuda:0 f32[16, 16]\"\n",
       "  del t2419, t2420\n",
       "  t2410 = torch.permute(t2407, (1, 0))  # t2410: \"cuda:0 f32[16, 128]\"\n",
       "    # t2410 = ltorch.permute(t2407, (1, 0))  # t2410: \"cuda:0 f32[16, 128]\"\n",
       "      # t2410 = prims.transpose(t2407, (1, 0))  # t2410: \"cuda:0 f32[16, 128]\"\n",
       "  del t2407\n",
       "  # Created by CPU Offloading Transform\n",
       "  t269 = load_to_gpu(offloaded_t269, 'cuda:0')  # t269: \"cuda:0 f32[128, 16]\"\n",
       "  t2411 = torch.reshape(t269, (-1, 16))  # t2411: \"cuda:0 f32[128, 16]\"\n",
       "    # t2411 = ltorch.reshape(t269, (-1, 16))  # t2411: \"cuda:0 f32[128, 16]\"\n",
       "      # t2411 = prims.reshape(t269, (128, 16))  # t2411: \"cuda:0 f32[128, 16]\"\n",
       "  del t269\n",
       "  t2412 = torch.matmul(t2410, t2411)  # t2412: \"cuda:0 f32[16, 16]\"\n",
       "    # t2412 = ltorch.matmul(t2410, t2411)  # t2412: \"cuda:0 f32[16, 16]\"\n",
       "      # t2412 = prims.matmul(t2410, t2411)  # t2412: \"cuda:0 f32[16, 16]\"\n",
       "  del t2410, t2411\n",
       "  t2401 = torch.permute(t2398, (1, 0))  # t2401: \"cuda:0 f32[16, 128]\"\n",
       "    # t2401 = ltorch.permute(t2398, (1, 0))  # t2401: \"cuda:0 f32[16, 128]\"\n",
       "      # t2401 = prims.transpose(t2398, (1, 0))  # t2401: \"cuda:0 f32[16, 128]\"\n",
       "  del t2398\n",
       "  # Created by CPU Offloading Transform\n",
       "  t274 = load_to_gpu(offloaded_t274, 'cuda:0')  # t274: \"cuda:0 f32[128, 16]\"\n",
       "  t2402 = torch.reshape(t274, (-1, 16))  # t2402: \"cuda:0 f32[128, 16]\"\n",
       "    # t2402 = ltorch.reshape(t274, (-1, 16))  # t2402: \"cuda:0 f32[128, 16]\"\n",
       "      # t2402 = prims.reshape(t274, (128, 16))  # t2402: \"cuda:0 f32[128, 16]\"\n",
       "  del t274\n",
       "  t2403 = torch.matmul(t2401, t2402)  # t2403: \"cuda:0 f32[16, 16]\"\n",
       "    # t2403 = ltorch.matmul(t2401, t2402)  # t2403: \"cuda:0 f32[16, 16]\"\n",
       "      # t2403 = prims.matmul(t2401, t2402)  # t2403: \"cuda:0 f32[16, 16]\"\n",
       "  del t2401, t2402\n",
       "  t2392 = torch.permute(t2389, (1, 0))  # t2392: \"cuda:0 f32[16, 128]\"\n",
       "    # t2392 = ltorch.permute(t2389, (1, 0))  # t2392: \"cuda:0 f32[16, 128]\"\n",
       "      # t2392 = prims.transpose(t2389, (1, 0))  # t2392: \"cuda:0 f32[16, 128]\"\n",
       "  del t2389\n",
       "  # Created by CPU Offloading Transform\n",
       "  t279 = load_to_gpu(offloaded_t279, 'cuda:0')  # t279: \"cuda:0 f32[128, 16]\"\n",
       "  t2393 = torch.reshape(t279, (-1, 16))  # t2393: \"cuda:0 f32[128, 16]\"\n",
       "    # t2393 = ltorch.reshape(t279, (-1, 16))  # t2393: \"cuda:0 f32[128, 16]\"\n",
       "      # t2393 = prims.reshape(t279, (128, 16))  # t2393: \"cuda:0 f32[128, 16]\"\n",
       "  del t279\n",
       "  t2394 = torch.matmul(t2392, t2393)  # t2394: \"cuda:0 f32[16, 16]\"\n",
       "    # t2394 = ltorch.matmul(t2392, t2393)  # t2394: \"cuda:0 f32[16, 16]\"\n",
       "      # t2394 = prims.matmul(t2392, t2393)  # t2394: \"cuda:0 f32[16, 16]\"\n",
       "  del t2392, t2393\n",
       "  t2383 = torch.permute(t2380, (1, 0))  # t2383: \"cuda:0 f32[16, 128]\"\n",
       "    # t2383 = ltorch.permute(t2380, (1, 0))  # t2383: \"cuda:0 f32[16, 128]\"\n",
       "      # t2383 = prims.transpose(t2380, (1, 0))  # t2383: \"cuda:0 f32[16, 128]\"\n",
       "  del t2380\n",
       "  # Created by CPU Offloading Transform\n",
       "  t284 = load_to_gpu(offloaded_t284, 'cuda:0')  # t284: \"cuda:0 f32[128, 16]\"\n",
       "  t2384 = torch.reshape(t284, (-1, 16))  # t2384: \"cuda:0 f32[128, 16]\"\n",
       "    # t2384 = ltorch.reshape(t284, (-1, 16))  # t2384: \"cuda:0 f32[128, 16]\"\n",
       "      # t2384 = prims.reshape(t284, (128, 16))  # t2384: \"cuda:0 f32[128, 16]\"\n",
       "  del t284\n",
       "  t2385 = torch.matmul(t2383, t2384)  # t2385: \"cuda:0 f32[16, 16]\"\n",
       "    # t2385 = ltorch.matmul(t2383, t2384)  # t2385: \"cuda:0 f32[16, 16]\"\n",
       "      # t2385 = prims.matmul(t2383, t2384)  # t2385: \"cuda:0 f32[16, 16]\"\n",
       "  del t2383, t2384\n",
       "  t2374 = torch.permute(t2371, (1, 0))  # t2374: \"cuda:0 f32[16, 128]\"\n",
       "    # t2374 = ltorch.permute(t2371, (1, 0))  # t2374: \"cuda:0 f32[16, 128]\"\n",
       "      # t2374 = prims.transpose(t2371, (1, 0))  # t2374: \"cuda:0 f32[16, 128]\"\n",
       "  del t2371\n",
       "  # Created by CPU Offloading Transform\n",
       "  t289 = load_to_gpu(offloaded_t289, 'cuda:0')  # t289: \"cuda:0 f32[128, 16]\"\n",
       "  t2375 = torch.reshape(t289, (-1, 16))  # t2375: \"cuda:0 f32[128, 16]\"\n",
       "    # t2375 = ltorch.reshape(t289, (-1, 16))  # t2375: \"cuda:0 f32[128, 16]\"\n",
       "      # t2375 = prims.reshape(t289, (128, 16))  # t2375: \"cuda:0 f32[128, 16]\"\n",
       "  del t289\n",
       "  t2376 = torch.matmul(t2374, t2375)  # t2376: \"cuda:0 f32[16, 16]\"\n",
       "    # t2376 = ltorch.matmul(t2374, t2375)  # t2376: \"cuda:0 f32[16, 16]\"\n",
       "      # t2376 = prims.matmul(t2374, t2375)  # t2376: \"cuda:0 f32[16, 16]\"\n",
       "  del t2374, t2375\n",
       "  t2365 = torch.permute(t2362, (1, 0))  # t2365: \"cuda:0 f32[16, 128]\"\n",
       "    # t2365 = ltorch.permute(t2362, (1, 0))  # t2365: \"cuda:0 f32[16, 128]\"\n",
       "      # t2365 = prims.transpose(t2362, (1, 0))  # t2365: \"cuda:0 f32[16, 128]\"\n",
       "  del t2362\n",
       "  # Created by CPU Offloading Transform\n",
       "  t294 = load_to_gpu(offloaded_t294, 'cuda:0')  # t294: \"cuda:0 f32[128, 16]\"\n",
       "  t2366 = torch.reshape(t294, (-1, 16))  # t2366: \"cuda:0 f32[128, 16]\"\n",
       "    # t2366 = ltorch.reshape(t294, (-1, 16))  # t2366: \"cuda:0 f32[128, 16]\"\n",
       "      # t2366 = prims.reshape(t294, (128, 16))  # t2366: \"cuda:0 f32[128, 16]\"\n",
       "  del t294\n",
       "  t2367 = torch.matmul(t2365, t2366)  # t2367: \"cuda:0 f32[16, 16]\"\n",
       "    # t2367 = ltorch.matmul(t2365, t2366)  # t2367: \"cuda:0 f32[16, 16]\"\n",
       "      # t2367 = prims.matmul(t2365, t2366)  # t2367: \"cuda:0 f32[16, 16]\"\n",
       "  del t2365, t2366\n",
       "  t2356 = torch.permute(t2353, (1, 0))  # t2356: \"cuda:0 f32[16, 128]\"\n",
       "    # t2356 = ltorch.permute(t2353, (1, 0))  # t2356: \"cuda:0 f32[16, 128]\"\n",
       "      # t2356 = prims.transpose(t2353, (1, 0))  # t2356: \"cuda:0 f32[16, 128]\"\n",
       "  del t2353\n",
       "  # Created by CPU Offloading Transform\n",
       "  t299 = load_to_gpu(offloaded_t299, 'cuda:0')  # t299: \"cuda:0 f32[128, 16]\"\n",
       "  t2357 = torch.reshape(t299, (-1, 16))  # t2357: \"cuda:0 f32[128, 16]\"\n",
       "    # t2357 = ltorch.reshape(t299, (-1, 16))  # t2357: \"cuda:0 f32[128, 16]\"\n",
       "      # t2357 = prims.reshape(t299, (128, 16))  # t2357: \"cuda:0 f32[128, 16]\"\n",
       "  del t299\n",
       "  t2358 = torch.matmul(t2356, t2357)  # t2358: \"cuda:0 f32[16, 16]\"\n",
       "    # t2358 = ltorch.matmul(t2356, t2357)  # t2358: \"cuda:0 f32[16, 16]\"\n",
       "      # t2358 = prims.matmul(t2356, t2357)  # t2358: \"cuda:0 f32[16, 16]\"\n",
       "  del t2356, t2357\n",
       "  t2347 = torch.permute(t2344, (1, 0))  # t2347: \"cuda:0 f32[16, 128]\"\n",
       "    # t2347 = ltorch.permute(t2344, (1, 0))  # t2347: \"cuda:0 f32[16, 128]\"\n",
       "      # t2347 = prims.transpose(t2344, (1, 0))  # t2347: \"cuda:0 f32[16, 128]\"\n",
       "  del t2344\n",
       "  # Created by CPU Offloading Transform\n",
       "  t304 = load_to_gpu(offloaded_t304, 'cuda:0')  # t304: \"cuda:0 f32[128, 16]\"\n",
       "  t2348 = torch.reshape(t304, (-1, 16))  # t2348: \"cuda:0 f32[128, 16]\"\n",
       "    # t2348 = ltorch.reshape(t304, (-1, 16))  # t2348: \"cuda:0 f32[128, 16]\"\n",
       "      # t2348 = prims.reshape(t304, (128, 16))  # t2348: \"cuda:0 f32[128, 16]\"\n",
       "  del t304\n",
       "  t2349 = torch.matmul(t2347, t2348)  # t2349: \"cuda:0 f32[16, 16]\"\n",
       "    # t2349 = ltorch.matmul(t2347, t2348)  # t2349: \"cuda:0 f32[16, 16]\"\n",
       "      # t2349 = prims.matmul(t2347, t2348)  # t2349: \"cuda:0 f32[16, 16]\"\n",
       "  del t2347, t2348\n",
       "  t2338 = torch.permute(t2335, (1, 0))  # t2338: \"cuda:0 f32[16, 128]\"\n",
       "    # t2338 = ltorch.permute(t2335, (1, 0))  # t2338: \"cuda:0 f32[16, 128]\"\n",
       "      # t2338 = prims.transpose(t2335, (1, 0))  # t2338: \"cuda:0 f32[16, 128]\"\n",
       "  del t2335\n",
       "  # Created by CPU Offloading Transform\n",
       "  t309 = load_to_gpu(offloaded_t309, 'cuda:0')  # t309: \"cuda:0 f32[128, 16]\"\n",
       "  t2339 = torch.reshape(t309, (-1, 16))  # t2339: \"cuda:0 f32[128, 16]\"\n",
       "    # t2339 = ltorch.reshape(t309, (-1, 16))  # t2339: \"cuda:0 f32[128, 16]\"\n",
       "      # t2339 = prims.reshape(t309, (128, 16))  # t2339: \"cuda:0 f32[128, 16]\"\n",
       "  del t309\n",
       "  t2340 = torch.matmul(t2338, t2339)  # t2340: \"cuda:0 f32[16, 16]\"\n",
       "    # t2340 = ltorch.matmul(t2338, t2339)  # t2340: \"cuda:0 f32[16, 16]\"\n",
       "      # t2340 = prims.matmul(t2338, t2339)  # t2340: \"cuda:0 f32[16, 16]\"\n",
       "  del t2338, t2339\n",
       "  t2329 = torch.permute(t2326, (1, 0))  # t2329: \"cuda:0 f32[16, 128]\"\n",
       "    # t2329 = ltorch.permute(t2326, (1, 0))  # t2329: \"cuda:0 f32[16, 128]\"\n",
       "      # t2329 = prims.transpose(t2326, (1, 0))  # t2329: \"cuda:0 f32[16, 128]\"\n",
       "  del t2326\n",
       "  # Created by CPU Offloading Transform\n",
       "  t314 = load_to_gpu(offloaded_t314, 'cuda:0')  # t314: \"cuda:0 f32[128, 16]\"\n",
       "  t2330 = torch.reshape(t314, (-1, 16))  # t2330: \"cuda:0 f32[128, 16]\"\n",
       "    # t2330 = ltorch.reshape(t314, (-1, 16))  # t2330: \"cuda:0 f32[128, 16]\"\n",
       "      # t2330 = prims.reshape(t314, (128, 16))  # t2330: \"cuda:0 f32[128, 16]\"\n",
       "  del t314\n",
       "  t2331 = torch.matmul(t2329, t2330)  # t2331: \"cuda:0 f32[16, 16]\"\n",
       "    # t2331 = ltorch.matmul(t2329, t2330)  # t2331: \"cuda:0 f32[16, 16]\"\n",
       "      # t2331 = prims.matmul(t2329, t2330)  # t2331: \"cuda:0 f32[16, 16]\"\n",
       "  del t2329, t2330\n",
       "  t2320 = torch.permute(t2317, (1, 0))  # t2320: \"cuda:0 f32[16, 128]\"\n",
       "    # t2320 = ltorch.permute(t2317, (1, 0))  # t2320: \"cuda:0 f32[16, 128]\"\n",
       "      # t2320 = prims.transpose(t2317, (1, 0))  # t2320: \"cuda:0 f32[16, 128]\"\n",
       "  del t2317\n",
       "  # Created by CPU Offloading Transform\n",
       "  t319 = load_to_gpu(offloaded_t319, 'cuda:0')  # t319: \"cuda:0 f32[128, 16]\"\n",
       "  t2321 = torch.reshape(t319, (-1, 16))  # t2321: \"cuda:0 f32[128, 16]\"\n",
       "    # t2321 = ltorch.reshape(t319, (-1, 16))  # t2321: \"cuda:0 f32[128, 16]\"\n",
       "      # t2321 = prims.reshape(t319, (128, 16))  # t2321: \"cuda:0 f32[128, 16]\"\n",
       "  del t319\n",
       "  t2322 = torch.matmul(t2320, t2321)  # t2322: \"cuda:0 f32[16, 16]\"\n",
       "    # t2322 = ltorch.matmul(t2320, t2321)  # t2322: \"cuda:0 f32[16, 16]\"\n",
       "      # t2322 = prims.matmul(t2320, t2321)  # t2322: \"cuda:0 f32[16, 16]\"\n",
       "  del t2320, t2321\n",
       "  t2311 = torch.permute(t2308, (1, 0))  # t2311: \"cuda:0 f32[16, 128]\"\n",
       "    # t2311 = ltorch.permute(t2308, (1, 0))  # t2311: \"cuda:0 f32[16, 128]\"\n",
       "      # t2311 = prims.transpose(t2308, (1, 0))  # t2311: \"cuda:0 f32[16, 128]\"\n",
       "  del t2308\n",
       "  # Created by CPU Offloading Transform\n",
       "  t324 = load_to_gpu(offloaded_t324, 'cuda:0')  # t324: \"cuda:0 f32[128, 16]\"\n",
       "  t2312 = torch.reshape(t324, (-1, 16))  # t2312: \"cuda:0 f32[128, 16]\"\n",
       "    # t2312 = ltorch.reshape(t324, (-1, 16))  # t2312: \"cuda:0 f32[128, 16]\"\n",
       "      # t2312 = prims.reshape(t324, (128, 16))  # t2312: \"cuda:0 f32[128, 16]\"\n",
       "  del t324\n",
       "  t2313 = torch.matmul(t2311, t2312)  # t2313: \"cuda:0 f32[16, 16]\"\n",
       "    # t2313 = ltorch.matmul(t2311, t2312)  # t2313: \"cuda:0 f32[16, 16]\"\n",
       "      # t2313 = prims.matmul(t2311, t2312)  # t2313: \"cuda:0 f32[16, 16]\"\n",
       "  del t2311, t2312\n",
       "  t2302 = torch.permute(t2299, (1, 0))  # t2302: \"cuda:0 f32[16, 128]\"\n",
       "    # t2302 = ltorch.permute(t2299, (1, 0))  # t2302: \"cuda:0 f32[16, 128]\"\n",
       "      # t2302 = prims.transpose(t2299, (1, 0))  # t2302: \"cuda:0 f32[16, 128]\"\n",
       "  del t2299\n",
       "  # Created by CPU Offloading Transform\n",
       "  t329 = load_to_gpu(offloaded_t329, 'cuda:0')  # t329: \"cuda:0 f32[128, 16]\"\n",
       "  t2303 = torch.reshape(t329, (-1, 16))  # t2303: \"cuda:0 f32[128, 16]\"\n",
       "    # t2303 = ltorch.reshape(t329, (-1, 16))  # t2303: \"cuda:0 f32[128, 16]\"\n",
       "      # t2303 = prims.reshape(t329, (128, 16))  # t2303: \"cuda:0 f32[128, 16]\"\n",
       "  del t329\n",
       "  t2304 = torch.matmul(t2302, t2303)  # t2304: \"cuda:0 f32[16, 16]\"\n",
       "    # t2304 = ltorch.matmul(t2302, t2303)  # t2304: \"cuda:0 f32[16, 16]\"\n",
       "      # t2304 = prims.matmul(t2302, t2303)  # t2304: \"cuda:0 f32[16, 16]\"\n",
       "  del t2302, t2303\n",
       "  t2293 = torch.permute(t2290, (1, 0))  # t2293: \"cuda:0 f32[16, 128]\"\n",
       "    # t2293 = ltorch.permute(t2290, (1, 0))  # t2293: \"cuda:0 f32[16, 128]\"\n",
       "      # t2293 = prims.transpose(t2290, (1, 0))  # t2293: \"cuda:0 f32[16, 128]\"\n",
       "  del t2290\n",
       "  # Created by CPU Offloading Transform\n",
       "  t334 = load_to_gpu(offloaded_t334, 'cuda:0')  # t334: \"cuda:0 f32[128, 16]\"\n",
       "  t2294 = torch.reshape(t334, (-1, 16))  # t2294: \"cuda:0 f32[128, 16]\"\n",
       "    # t2294 = ltorch.reshape(t334, (-1, 16))  # t2294: \"cuda:0 f32[128, 16]\"\n",
       "      # t2294 = prims.reshape(t334, (128, 16))  # t2294: \"cuda:0 f32[128, 16]\"\n",
       "  del t334\n",
       "  t2295 = torch.matmul(t2293, t2294)  # t2295: \"cuda:0 f32[16, 16]\"\n",
       "    # t2295 = ltorch.matmul(t2293, t2294)  # t2295: \"cuda:0 f32[16, 16]\"\n",
       "      # t2295 = prims.matmul(t2293, t2294)  # t2295: \"cuda:0 f32[16, 16]\"\n",
       "  del t2293, t2294\n",
       "  t2284 = torch.permute(t2281, (1, 0))  # t2284: \"cuda:0 f32[16, 128]\"\n",
       "    # t2284 = ltorch.permute(t2281, (1, 0))  # t2284: \"cuda:0 f32[16, 128]\"\n",
       "      # t2284 = prims.transpose(t2281, (1, 0))  # t2284: \"cuda:0 f32[16, 128]\"\n",
       "  del t2281\n",
       "  # Created by CPU Offloading Transform\n",
       "  t339 = load_to_gpu(offloaded_t339, 'cuda:0')  # t339: \"cuda:0 f32[128, 16]\"\n",
       "  t2285 = torch.reshape(t339, (-1, 16))  # t2285: \"cuda:0 f32[128, 16]\"\n",
       "    # t2285 = ltorch.reshape(t339, (-1, 16))  # t2285: \"cuda:0 f32[128, 16]\"\n",
       "      # t2285 = prims.reshape(t339, (128, 16))  # t2285: \"cuda:0 f32[128, 16]\"\n",
       "  del t339\n",
       "  t2286 = torch.matmul(t2284, t2285)  # t2286: \"cuda:0 f32[16, 16]\"\n",
       "    # t2286 = ltorch.matmul(t2284, t2285)  # t2286: \"cuda:0 f32[16, 16]\"\n",
       "      # t2286 = prims.matmul(t2284, t2285)  # t2286: \"cuda:0 f32[16, 16]\"\n",
       "  del t2284, t2285\n",
       "  t2275 = torch.permute(t2272, (1, 0))  # t2275: \"cuda:0 f32[16, 128]\"\n",
       "    # t2275 = ltorch.permute(t2272, (1, 0))  # t2275: \"cuda:0 f32[16, 128]\"\n",
       "      # t2275 = prims.transpose(t2272, (1, 0))  # t2275: \"cuda:0 f32[16, 128]\"\n",
       "  del t2272\n",
       "  # Created by CPU Offloading Transform\n",
       "  t344 = load_to_gpu(offloaded_t344, 'cuda:0')  # t344: \"cuda:0 f32[128, 16]\"\n",
       "  t2276 = torch.reshape(t344, (-1, 16))  # t2276: \"cuda:0 f32[128, 16]\"\n",
       "    # t2276 = ltorch.reshape(t344, (-1, 16))  # t2276: \"cuda:0 f32[128, 16]\"\n",
       "      # t2276 = prims.reshape(t344, (128, 16))  # t2276: \"cuda:0 f32[128, 16]\"\n",
       "  del t344\n",
       "  t2277 = torch.matmul(t2275, t2276)  # t2277: \"cuda:0 f32[16, 16]\"\n",
       "    # t2277 = ltorch.matmul(t2275, t2276)  # t2277: \"cuda:0 f32[16, 16]\"\n",
       "      # t2277 = prims.matmul(t2275, t2276)  # t2277: \"cuda:0 f32[16, 16]\"\n",
       "  del t2275, t2276\n",
       "  t2266 = torch.permute(t2263, (1, 0))  # t2266: \"cuda:0 f32[16, 128]\"\n",
       "    # t2266 = ltorch.permute(t2263, (1, 0))  # t2266: \"cuda:0 f32[16, 128]\"\n",
       "      # t2266 = prims.transpose(t2263, (1, 0))  # t2266: \"cuda:0 f32[16, 128]\"\n",
       "  del t2263\n",
       "  # Created by CPU Offloading Transform\n",
       "  t349 = load_to_gpu(offloaded_t349, 'cuda:0')  # t349: \"cuda:0 f32[128, 16]\"\n",
       "  t2267 = torch.reshape(t349, (-1, 16))  # t2267: \"cuda:0 f32[128, 16]\"\n",
       "    # t2267 = ltorch.reshape(t349, (-1, 16))  # t2267: \"cuda:0 f32[128, 16]\"\n",
       "      # t2267 = prims.reshape(t349, (128, 16))  # t2267: \"cuda:0 f32[128, 16]\"\n",
       "  del t349\n",
       "  t2268 = torch.matmul(t2266, t2267)  # t2268: \"cuda:0 f32[16, 16]\"\n",
       "    # t2268 = ltorch.matmul(t2266, t2267)  # t2268: \"cuda:0 f32[16, 16]\"\n",
       "      # t2268 = prims.matmul(t2266, t2267)  # t2268: \"cuda:0 f32[16, 16]\"\n",
       "  del t2266, t2267\n",
       "  t2257 = torch.permute(t2254, (1, 0))  # t2257: \"cuda:0 f32[16, 128]\"\n",
       "    # t2257 = ltorch.permute(t2254, (1, 0))  # t2257: \"cuda:0 f32[16, 128]\"\n",
       "      # t2257 = prims.transpose(t2254, (1, 0))  # t2257: \"cuda:0 f32[16, 128]\"\n",
       "  del t2254\n",
       "  # Created by CPU Offloading Transform\n",
       "  t354 = load_to_gpu(offloaded_t354, 'cuda:0')  # t354: \"cuda:0 f32[128, 16]\"\n",
       "  t2258 = torch.reshape(t354, (-1, 16))  # t2258: \"cuda:0 f32[128, 16]\"\n",
       "    # t2258 = ltorch.reshape(t354, (-1, 16))  # t2258: \"cuda:0 f32[128, 16]\"\n",
       "      # t2258 = prims.reshape(t354, (128, 16))  # t2258: \"cuda:0 f32[128, 16]\"\n",
       "  del t354\n",
       "  t2259 = torch.matmul(t2257, t2258)  # t2259: \"cuda:0 f32[16, 16]\"\n",
       "    # t2259 = ltorch.matmul(t2257, t2258)  # t2259: \"cuda:0 f32[16, 16]\"\n",
       "      # t2259 = prims.matmul(t2257, t2258)  # t2259: \"cuda:0 f32[16, 16]\"\n",
       "  del t2257, t2258\n",
       "  t2248 = torch.permute(t2245, (1, 0))  # t2248: \"cuda:0 f32[16, 128]\"\n",
       "    # t2248 = ltorch.permute(t2245, (1, 0))  # t2248: \"cuda:0 f32[16, 128]\"\n",
       "      # t2248 = prims.transpose(t2245, (1, 0))  # t2248: \"cuda:0 f32[16, 128]\"\n",
       "  del t2245\n",
       "  # Created by CPU Offloading Transform\n",
       "  t359 = load_to_gpu(offloaded_t359, 'cuda:0')  # t359: \"cuda:0 f32[128, 16]\"\n",
       "  t2249 = torch.reshape(t359, (-1, 16))  # t2249: \"cuda:0 f32[128, 16]\"\n",
       "    # t2249 = ltorch.reshape(t359, (-1, 16))  # t2249: \"cuda:0 f32[128, 16]\"\n",
       "      # t2249 = prims.reshape(t359, (128, 16))  # t2249: \"cuda:0 f32[128, 16]\"\n",
       "  del t359\n",
       "  t2250 = torch.matmul(t2248, t2249)  # t2250: \"cuda:0 f32[16, 16]\"\n",
       "    # t2250 = ltorch.matmul(t2248, t2249)  # t2250: \"cuda:0 f32[16, 16]\"\n",
       "      # t2250 = prims.matmul(t2248, t2249)  # t2250: \"cuda:0 f32[16, 16]\"\n",
       "  del t2248, t2249\n",
       "  t2239 = torch.permute(t2236, (1, 0))  # t2239: \"cuda:0 f32[16, 128]\"\n",
       "    # t2239 = ltorch.permute(t2236, (1, 0))  # t2239: \"cuda:0 f32[16, 128]\"\n",
       "      # t2239 = prims.transpose(t2236, (1, 0))  # t2239: \"cuda:0 f32[16, 128]\"\n",
       "  del t2236\n",
       "  # Created by CPU Offloading Transform\n",
       "  t364 = load_to_gpu(offloaded_t364, 'cuda:0')  # t364: \"cuda:0 f32[128, 16]\"\n",
       "  t2240 = torch.reshape(t364, (-1, 16))  # t2240: \"cuda:0 f32[128, 16]\"\n",
       "    # t2240 = ltorch.reshape(t364, (-1, 16))  # t2240: \"cuda:0 f32[128, 16]\"\n",
       "      # t2240 = prims.reshape(t364, (128, 16))  # t2240: \"cuda:0 f32[128, 16]\"\n",
       "  del t364\n",
       "  t2241 = torch.matmul(t2239, t2240)  # t2241: \"cuda:0 f32[16, 16]\"\n",
       "    # t2241 = ltorch.matmul(t2239, t2240)  # t2241: \"cuda:0 f32[16, 16]\"\n",
       "      # t2241 = prims.matmul(t2239, t2240)  # t2241: \"cuda:0 f32[16, 16]\"\n",
       "  del t2239, t2240\n",
       "  t2230 = torch.permute(t2227, (1, 0))  # t2230: \"cuda:0 f32[16, 128]\"\n",
       "    # t2230 = ltorch.permute(t2227, (1, 0))  # t2230: \"cuda:0 f32[16, 128]\"\n",
       "      # t2230 = prims.transpose(t2227, (1, 0))  # t2230: \"cuda:0 f32[16, 128]\"\n",
       "  del t2227\n",
       "  # Created by CPU Offloading Transform\n",
       "  t369 = load_to_gpu(offloaded_t369, 'cuda:0')  # t369: \"cuda:0 f32[128, 16]\"\n",
       "  t2231 = torch.reshape(t369, (-1, 16))  # t2231: \"cuda:0 f32[128, 16]\"\n",
       "    # t2231 = ltorch.reshape(t369, (-1, 16))  # t2231: \"cuda:0 f32[128, 16]\"\n",
       "      # t2231 = prims.reshape(t369, (128, 16))  # t2231: \"cuda:0 f32[128, 16]\"\n",
       "  del t369\n",
       "  t2232 = torch.matmul(t2230, t2231)  # t2232: \"cuda:0 f32[16, 16]\"\n",
       "    # t2232 = ltorch.matmul(t2230, t2231)  # t2232: \"cuda:0 f32[16, 16]\"\n",
       "      # t2232 = prims.matmul(t2230, t2231)  # t2232: \"cuda:0 f32[16, 16]\"\n",
       "  del t2230, t2231\n",
       "  t2221 = torch.permute(t2218, (1, 0))  # t2221: \"cuda:0 f32[16, 128]\"\n",
       "    # t2221 = ltorch.permute(t2218, (1, 0))  # t2221: \"cuda:0 f32[16, 128]\"\n",
       "      # t2221 = prims.transpose(t2218, (1, 0))  # t2221: \"cuda:0 f32[16, 128]\"\n",
       "  del t2218\n",
       "  # Created by CPU Offloading Transform\n",
       "  t374 = load_to_gpu(offloaded_t374, 'cuda:0')  # t374: \"cuda:0 f32[128, 16]\"\n",
       "  t2222 = torch.reshape(t374, (-1, 16))  # t2222: \"cuda:0 f32[128, 16]\"\n",
       "    # t2222 = ltorch.reshape(t374, (-1, 16))  # t2222: \"cuda:0 f32[128, 16]\"\n",
       "      # t2222 = prims.reshape(t374, (128, 16))  # t2222: \"cuda:0 f32[128, 16]\"\n",
       "  del t374\n",
       "  t2223 = torch.matmul(t2221, t2222)  # t2223: \"cuda:0 f32[16, 16]\"\n",
       "    # t2223 = ltorch.matmul(t2221, t2222)  # t2223: \"cuda:0 f32[16, 16]\"\n",
       "      # t2223 = prims.matmul(t2221, t2222)  # t2223: \"cuda:0 f32[16, 16]\"\n",
       "  del t2221, t2222\n",
       "  t2212 = torch.permute(t2209, (1, 0))  # t2212: \"cuda:0 f32[16, 128]\"\n",
       "    # t2212 = ltorch.permute(t2209, (1, 0))  # t2212: \"cuda:0 f32[16, 128]\"\n",
       "      # t2212 = prims.transpose(t2209, (1, 0))  # t2212: \"cuda:0 f32[16, 128]\"\n",
       "  del t2209\n",
       "  # Created by CPU Offloading Transform\n",
       "  t379 = load_to_gpu(offloaded_t379, 'cuda:0')  # t379: \"cuda:0 f32[128, 16]\"\n",
       "  t2213 = torch.reshape(t379, (-1, 16))  # t2213: \"cuda:0 f32[128, 16]\"\n",
       "    # t2213 = ltorch.reshape(t379, (-1, 16))  # t2213: \"cuda:0 f32[128, 16]\"\n",
       "      # t2213 = prims.reshape(t379, (128, 16))  # t2213: \"cuda:0 f32[128, 16]\"\n",
       "  del t379\n",
       "  t2214 = torch.matmul(t2212, t2213)  # t2214: \"cuda:0 f32[16, 16]\"\n",
       "    # t2214 = ltorch.matmul(t2212, t2213)  # t2214: \"cuda:0 f32[16, 16]\"\n",
       "      # t2214 = prims.matmul(t2212, t2213)  # t2214: \"cuda:0 f32[16, 16]\"\n",
       "  del t2212, t2213\n",
       "  t2203 = torch.permute(t2200, (1, 0))  # t2203: \"cuda:0 f32[16, 128]\"\n",
       "    # t2203 = ltorch.permute(t2200, (1, 0))  # t2203: \"cuda:0 f32[16, 128]\"\n",
       "      # t2203 = prims.transpose(t2200, (1, 0))  # t2203: \"cuda:0 f32[16, 128]\"\n",
       "  del t2200\n",
       "  # Created by CPU Offloading Transform\n",
       "  t384 = load_to_gpu(offloaded_t384, 'cuda:0')  # t384: \"cuda:0 f32[128, 16]\"\n",
       "  t2204 = torch.reshape(t384, (-1, 16))  # t2204: \"cuda:0 f32[128, 16]\"\n",
       "    # t2204 = ltorch.reshape(t384, (-1, 16))  # t2204: \"cuda:0 f32[128, 16]\"\n",
       "      # t2204 = prims.reshape(t384, (128, 16))  # t2204: \"cuda:0 f32[128, 16]\"\n",
       "  del t384\n",
       "  t2205 = torch.matmul(t2203, t2204)  # t2205: \"cuda:0 f32[16, 16]\"\n",
       "    # t2205 = ltorch.matmul(t2203, t2204)  # t2205: \"cuda:0 f32[16, 16]\"\n",
       "      # t2205 = prims.matmul(t2203, t2204)  # t2205: \"cuda:0 f32[16, 16]\"\n",
       "  del t2203, t2204\n",
       "  t2194 = torch.permute(t2191, (1, 0))  # t2194: \"cuda:0 f32[16, 128]\"\n",
       "    # t2194 = ltorch.permute(t2191, (1, 0))  # t2194: \"cuda:0 f32[16, 128]\"\n",
       "      # t2194 = prims.transpose(t2191, (1, 0))  # t2194: \"cuda:0 f32[16, 128]\"\n",
       "  del t2191\n",
       "  # Created by CPU Offloading Transform\n",
       "  t389 = load_to_gpu(offloaded_t389, 'cuda:0')  # t389: \"cuda:0 f32[128, 16]\"\n",
       "  t2195 = torch.reshape(t389, (-1, 16))  # t2195: \"cuda:0 f32[128, 16]\"\n",
       "    # t2195 = ltorch.reshape(t389, (-1, 16))  # t2195: \"cuda:0 f32[128, 16]\"\n",
       "      # t2195 = prims.reshape(t389, (128, 16))  # t2195: \"cuda:0 f32[128, 16]\"\n",
       "  del t389\n",
       "  t2196 = torch.matmul(t2194, t2195)  # t2196: \"cuda:0 f32[16, 16]\"\n",
       "    # t2196 = ltorch.matmul(t2194, t2195)  # t2196: \"cuda:0 f32[16, 16]\"\n",
       "      # t2196 = prims.matmul(t2194, t2195)  # t2196: \"cuda:0 f32[16, 16]\"\n",
       "  del t2194, t2195\n",
       "  t2185 = torch.permute(t2182, (1, 0))  # t2185: \"cuda:0 f32[16, 128]\"\n",
       "    # t2185 = ltorch.permute(t2182, (1, 0))  # t2185: \"cuda:0 f32[16, 128]\"\n",
       "      # t2185 = prims.transpose(t2182, (1, 0))  # t2185: \"cuda:0 f32[16, 128]\"\n",
       "  del t2182\n",
       "  # Created by CPU Offloading Transform\n",
       "  t394 = load_to_gpu(offloaded_t394, 'cuda:0')  # t394: \"cuda:0 f32[128, 16]\"\n",
       "  t2186 = torch.reshape(t394, (-1, 16))  # t2186: \"cuda:0 f32[128, 16]\"\n",
       "    # t2186 = ltorch.reshape(t394, (-1, 16))  # t2186: \"cuda:0 f32[128, 16]\"\n",
       "      # t2186 = prims.reshape(t394, (128, 16))  # t2186: \"cuda:0 f32[128, 16]\"\n",
       "  del t394\n",
       "  t2187 = torch.matmul(t2185, t2186)  # t2187: \"cuda:0 f32[16, 16]\"\n",
       "    # t2187 = ltorch.matmul(t2185, t2186)  # t2187: \"cuda:0 f32[16, 16]\"\n",
       "      # t2187 = prims.matmul(t2185, t2186)  # t2187: \"cuda:0 f32[16, 16]\"\n",
       "  del t2185, t2186\n",
       "  t2176 = torch.permute(t2173, (1, 0))  # t2176: \"cuda:0 f32[16, 128]\"\n",
       "    # t2176 = ltorch.permute(t2173, (1, 0))  # t2176: \"cuda:0 f32[16, 128]\"\n",
       "      # t2176 = prims.transpose(t2173, (1, 0))  # t2176: \"cuda:0 f32[16, 128]\"\n",
       "  del t2173\n",
       "  # Created by CPU Offloading Transform\n",
       "  t399 = load_to_gpu(offloaded_t399, 'cuda:0')  # t399: \"cuda:0 f32[128, 16]\"\n",
       "  t2177 = torch.reshape(t399, (-1, 16))  # t2177: \"cuda:0 f32[128, 16]\"\n",
       "    # t2177 = ltorch.reshape(t399, (-1, 16))  # t2177: \"cuda:0 f32[128, 16]\"\n",
       "      # t2177 = prims.reshape(t399, (128, 16))  # t2177: \"cuda:0 f32[128, 16]\"\n",
       "  del t399\n",
       "  t2178 = torch.matmul(t2176, t2177)  # t2178: \"cuda:0 f32[16, 16]\"\n",
       "    # t2178 = ltorch.matmul(t2176, t2177)  # t2178: \"cuda:0 f32[16, 16]\"\n",
       "      # t2178 = prims.matmul(t2176, t2177)  # t2178: \"cuda:0 f32[16, 16]\"\n",
       "  del t2176, t2177\n",
       "  t2167 = torch.permute(t2164, (1, 0))  # t2167: \"cuda:0 f32[16, 128]\"\n",
       "    # t2167 = ltorch.permute(t2164, (1, 0))  # t2167: \"cuda:0 f32[16, 128]\"\n",
       "      # t2167 = prims.transpose(t2164, (1, 0))  # t2167: \"cuda:0 f32[16, 128]\"\n",
       "  del t2164\n",
       "  # Created by CPU Offloading Transform\n",
       "  t404 = load_to_gpu(offloaded_t404, 'cuda:0')  # t404: \"cuda:0 f32[128, 16]\"\n",
       "  t2168 = torch.reshape(t404, (-1, 16))  # t2168: \"cuda:0 f32[128, 16]\"\n",
       "    # t2168 = ltorch.reshape(t404, (-1, 16))  # t2168: \"cuda:0 f32[128, 16]\"\n",
       "      # t2168 = prims.reshape(t404, (128, 16))  # t2168: \"cuda:0 f32[128, 16]\"\n",
       "  del t404\n",
       "  t2169 = torch.matmul(t2167, t2168)  # t2169: \"cuda:0 f32[16, 16]\"\n",
       "    # t2169 = ltorch.matmul(t2167, t2168)  # t2169: \"cuda:0 f32[16, 16]\"\n",
       "      # t2169 = prims.matmul(t2167, t2168)  # t2169: \"cuda:0 f32[16, 16]\"\n",
       "  del t2167, t2168\n",
       "  t2158 = torch.permute(t2155, (1, 0))  # t2158: \"cuda:0 f32[16, 128]\"\n",
       "    # t2158 = ltorch.permute(t2155, (1, 0))  # t2158: \"cuda:0 f32[16, 128]\"\n",
       "      # t2158 = prims.transpose(t2155, (1, 0))  # t2158: \"cuda:0 f32[16, 128]\"\n",
       "  del t2155\n",
       "  # Created by CPU Offloading Transform\n",
       "  t409 = load_to_gpu(offloaded_t409, 'cuda:0')  # t409: \"cuda:0 f32[128, 16]\"\n",
       "  t2159 = torch.reshape(t409, (-1, 16))  # t2159: \"cuda:0 f32[128, 16]\"\n",
       "    # t2159 = ltorch.reshape(t409, (-1, 16))  # t2159: \"cuda:0 f32[128, 16]\"\n",
       "      # t2159 = prims.reshape(t409, (128, 16))  # t2159: \"cuda:0 f32[128, 16]\"\n",
       "  del t409\n",
       "  t2160 = torch.matmul(t2158, t2159)  # t2160: \"cuda:0 f32[16, 16]\"\n",
       "    # t2160 = ltorch.matmul(t2158, t2159)  # t2160: \"cuda:0 f32[16, 16]\"\n",
       "      # t2160 = prims.matmul(t2158, t2159)  # t2160: \"cuda:0 f32[16, 16]\"\n",
       "  del t2158, t2159\n",
       "  t2149 = torch.permute(t2146, (1, 0))  # t2149: \"cuda:0 f32[16, 128]\"\n",
       "    # t2149 = ltorch.permute(t2146, (1, 0))  # t2149: \"cuda:0 f32[16, 128]\"\n",
       "      # t2149 = prims.transpose(t2146, (1, 0))  # t2149: \"cuda:0 f32[16, 128]\"\n",
       "  del t2146\n",
       "  # Created by CPU Offloading Transform\n",
       "  t414 = load_to_gpu(offloaded_t414, 'cuda:0')  # t414: \"cuda:0 f32[128, 16]\"\n",
       "  t2150 = torch.reshape(t414, (-1, 16))  # t2150: \"cuda:0 f32[128, 16]\"\n",
       "    # t2150 = ltorch.reshape(t414, (-1, 16))  # t2150: \"cuda:0 f32[128, 16]\"\n",
       "      # t2150 = prims.reshape(t414, (128, 16))  # t2150: \"cuda:0 f32[128, 16]\"\n",
       "  del t414\n",
       "  t2151 = torch.matmul(t2149, t2150)  # t2151: \"cuda:0 f32[16, 16]\"\n",
       "    # t2151 = ltorch.matmul(t2149, t2150)  # t2151: \"cuda:0 f32[16, 16]\"\n",
       "      # t2151 = prims.matmul(t2149, t2150)  # t2151: \"cuda:0 f32[16, 16]\"\n",
       "  del t2149, t2150\n",
       "  t2140 = torch.permute(t2137, (1, 0))  # t2140: \"cuda:0 f32[16, 128]\"\n",
       "    # t2140 = ltorch.permute(t2137, (1, 0))  # t2140: \"cuda:0 f32[16, 128]\"\n",
       "      # t2140 = prims.transpose(t2137, (1, 0))  # t2140: \"cuda:0 f32[16, 128]\"\n",
       "  del t2137\n",
       "  # Created by CPU Offloading Transform\n",
       "  t419 = load_to_gpu(offloaded_t419, 'cuda:0')  # t419: \"cuda:0 f32[128, 16]\"\n",
       "  t2141 = torch.reshape(t419, (-1, 16))  # t2141: \"cuda:0 f32[128, 16]\"\n",
       "    # t2141 = ltorch.reshape(t419, (-1, 16))  # t2141: \"cuda:0 f32[128, 16]\"\n",
       "      # t2141 = prims.reshape(t419, (128, 16))  # t2141: \"cuda:0 f32[128, 16]\"\n",
       "  del t419\n",
       "  t2142 = torch.matmul(t2140, t2141)  # t2142: \"cuda:0 f32[16, 16]\"\n",
       "    # t2142 = ltorch.matmul(t2140, t2141)  # t2142: \"cuda:0 f32[16, 16]\"\n",
       "      # t2142 = prims.matmul(t2140, t2141)  # t2142: \"cuda:0 f32[16, 16]\"\n",
       "  del t2140, t2141\n",
       "  t2131 = torch.permute(t2128, (1, 0))  # t2131: \"cuda:0 f32[16, 128]\"\n",
       "    # t2131 = ltorch.permute(t2128, (1, 0))  # t2131: \"cuda:0 f32[16, 128]\"\n",
       "      # t2131 = prims.transpose(t2128, (1, 0))  # t2131: \"cuda:0 f32[16, 128]\"\n",
       "  del t2128\n",
       "  # Created by CPU Offloading Transform\n",
       "  t424 = load_to_gpu(offloaded_t424, 'cuda:0')  # t424: \"cuda:0 f32[128, 16]\"\n",
       "  t2132 = torch.reshape(t424, (-1, 16))  # t2132: \"cuda:0 f32[128, 16]\"\n",
       "    # t2132 = ltorch.reshape(t424, (-1, 16))  # t2132: \"cuda:0 f32[128, 16]\"\n",
       "      # t2132 = prims.reshape(t424, (128, 16))  # t2132: \"cuda:0 f32[128, 16]\"\n",
       "  del t424\n",
       "  t2133 = torch.matmul(t2131, t2132)  # t2133: \"cuda:0 f32[16, 16]\"\n",
       "    # t2133 = ltorch.matmul(t2131, t2132)  # t2133: \"cuda:0 f32[16, 16]\"\n",
       "      # t2133 = prims.matmul(t2131, t2132)  # t2133: \"cuda:0 f32[16, 16]\"\n",
       "  del t2131, t2132\n",
       "  t2122 = torch.permute(t2119, (1, 0))  # t2122: \"cuda:0 f32[16, 128]\"\n",
       "    # t2122 = ltorch.permute(t2119, (1, 0))  # t2122: \"cuda:0 f32[16, 128]\"\n",
       "      # t2122 = prims.transpose(t2119, (1, 0))  # t2122: \"cuda:0 f32[16, 128]\"\n",
       "  del t2119\n",
       "  # Created by CPU Offloading Transform\n",
       "  t429 = load_to_gpu(offloaded_t429, 'cuda:0')  # t429: \"cuda:0 f32[128, 16]\"\n",
       "  t2123 = torch.reshape(t429, (-1, 16))  # t2123: \"cuda:0 f32[128, 16]\"\n",
       "    # t2123 = ltorch.reshape(t429, (-1, 16))  # t2123: \"cuda:0 f32[128, 16]\"\n",
       "      # t2123 = prims.reshape(t429, (128, 16))  # t2123: \"cuda:0 f32[128, 16]\"\n",
       "  del t429\n",
       "  t2124 = torch.matmul(t2122, t2123)  # t2124: \"cuda:0 f32[16, 16]\"\n",
       "    # t2124 = ltorch.matmul(t2122, t2123)  # t2124: \"cuda:0 f32[16, 16]\"\n",
       "      # t2124 = prims.matmul(t2122, t2123)  # t2124: \"cuda:0 f32[16, 16]\"\n",
       "  del t2122, t2123\n",
       "  t2113 = torch.permute(t2110, (1, 0))  # t2113: \"cuda:0 f32[16, 128]\"\n",
       "    # t2113 = ltorch.permute(t2110, (1, 0))  # t2113: \"cuda:0 f32[16, 128]\"\n",
       "      # t2113 = prims.transpose(t2110, (1, 0))  # t2113: \"cuda:0 f32[16, 128]\"\n",
       "  del t2110\n",
       "  # Created by CPU Offloading Transform\n",
       "  t434 = load_to_gpu(offloaded_t434, 'cuda:0')  # t434: \"cuda:0 f32[128, 16]\"\n",
       "  t2114 = torch.reshape(t434, (-1, 16))  # t2114: \"cuda:0 f32[128, 16]\"\n",
       "    # t2114 = ltorch.reshape(t434, (-1, 16))  # t2114: \"cuda:0 f32[128, 16]\"\n",
       "      # t2114 = prims.reshape(t434, (128, 16))  # t2114: \"cuda:0 f32[128, 16]\"\n",
       "  del t434\n",
       "  t2115 = torch.matmul(t2113, t2114)  # t2115: \"cuda:0 f32[16, 16]\"\n",
       "    # t2115 = ltorch.matmul(t2113, t2114)  # t2115: \"cuda:0 f32[16, 16]\"\n",
       "      # t2115 = prims.matmul(t2113, t2114)  # t2115: \"cuda:0 f32[16, 16]\"\n",
       "  del t2113, t2114\n",
       "  t2104 = torch.permute(t2101, (1, 0))  # t2104: \"cuda:0 f32[16, 128]\"\n",
       "    # t2104 = ltorch.permute(t2101, (1, 0))  # t2104: \"cuda:0 f32[16, 128]\"\n",
       "      # t2104 = prims.transpose(t2101, (1, 0))  # t2104: \"cuda:0 f32[16, 128]\"\n",
       "  del t2101\n",
       "  # Created by CPU Offloading Transform\n",
       "  t439 = load_to_gpu(offloaded_t439, 'cuda:0')  # t439: \"cuda:0 f32[128, 16]\"\n",
       "  t2105 = torch.reshape(t439, (-1, 16))  # t2105: \"cuda:0 f32[128, 16]\"\n",
       "    # t2105 = ltorch.reshape(t439, (-1, 16))  # t2105: \"cuda:0 f32[128, 16]\"\n",
       "      # t2105 = prims.reshape(t439, (128, 16))  # t2105: \"cuda:0 f32[128, 16]\"\n",
       "  del t439\n",
       "  t2106 = torch.matmul(t2104, t2105)  # t2106: \"cuda:0 f32[16, 16]\"\n",
       "    # t2106 = ltorch.matmul(t2104, t2105)  # t2106: \"cuda:0 f32[16, 16]\"\n",
       "      # t2106 = prims.matmul(t2104, t2105)  # t2106: \"cuda:0 f32[16, 16]\"\n",
       "  del t2104, t2105\n",
       "  t2095 = torch.permute(t2092, (1, 0))  # t2095: \"cuda:0 f32[16, 128]\"\n",
       "    # t2095 = ltorch.permute(t2092, (1, 0))  # t2095: \"cuda:0 f32[16, 128]\"\n",
       "      # t2095 = prims.transpose(t2092, (1, 0))  # t2095: \"cuda:0 f32[16, 128]\"\n",
       "  del t2092\n",
       "  # Created by CPU Offloading Transform\n",
       "  t444 = load_to_gpu(offloaded_t444, 'cuda:0')  # t444: \"cuda:0 f32[128, 16]\"\n",
       "  t2096 = torch.reshape(t444, (-1, 16))  # t2096: \"cuda:0 f32[128, 16]\"\n",
       "    # t2096 = ltorch.reshape(t444, (-1, 16))  # t2096: \"cuda:0 f32[128, 16]\"\n",
       "      # t2096 = prims.reshape(t444, (128, 16))  # t2096: \"cuda:0 f32[128, 16]\"\n",
       "  del t444\n",
       "  t2097 = torch.matmul(t2095, t2096)  # t2097: \"cuda:0 f32[16, 16]\"\n",
       "    # t2097 = ltorch.matmul(t2095, t2096)  # t2097: \"cuda:0 f32[16, 16]\"\n",
       "      # t2097 = prims.matmul(t2095, t2096)  # t2097: \"cuda:0 f32[16, 16]\"\n",
       "  del t2095, t2096\n",
       "  t2086 = torch.permute(t2083, (1, 0))  # t2086: \"cuda:0 f32[16, 128]\"\n",
       "    # t2086 = ltorch.permute(t2083, (1, 0))  # t2086: \"cuda:0 f32[16, 128]\"\n",
       "      # t2086 = prims.transpose(t2083, (1, 0))  # t2086: \"cuda:0 f32[16, 128]\"\n",
       "  del t2083\n",
       "  # Created by CPU Offloading Transform\n",
       "  t449 = load_to_gpu(offloaded_t449, 'cuda:0')  # t449: \"cuda:0 f32[128, 16]\"\n",
       "  t2087 = torch.reshape(t449, (-1, 16))  # t2087: \"cuda:0 f32[128, 16]\"\n",
       "    # t2087 = ltorch.reshape(t449, (-1, 16))  # t2087: \"cuda:0 f32[128, 16]\"\n",
       "      # t2087 = prims.reshape(t449, (128, 16))  # t2087: \"cuda:0 f32[128, 16]\"\n",
       "  del t449\n",
       "  t2088 = torch.matmul(t2086, t2087)  # t2088: \"cuda:0 f32[16, 16]\"\n",
       "    # t2088 = ltorch.matmul(t2086, t2087)  # t2088: \"cuda:0 f32[16, 16]\"\n",
       "      # t2088 = prims.matmul(t2086, t2087)  # t2088: \"cuda:0 f32[16, 16]\"\n",
       "  del t2086, t2087\n",
       "  t2077 = torch.permute(t2074, (1, 0))  # t2077: \"cuda:0 f32[16, 128]\"\n",
       "    # t2077 = ltorch.permute(t2074, (1, 0))  # t2077: \"cuda:0 f32[16, 128]\"\n",
       "      # t2077 = prims.transpose(t2074, (1, 0))  # t2077: \"cuda:0 f32[16, 128]\"\n",
       "  del t2074\n",
       "  # Created by CPU Offloading Transform\n",
       "  t454 = load_to_gpu(offloaded_t454, 'cuda:0')  # t454: \"cuda:0 f32[128, 16]\"\n",
       "  t2078 = torch.reshape(t454, (-1, 16))  # t2078: \"cuda:0 f32[128, 16]\"\n",
       "    # t2078 = ltorch.reshape(t454, (-1, 16))  # t2078: \"cuda:0 f32[128, 16]\"\n",
       "      # t2078 = prims.reshape(t454, (128, 16))  # t2078: \"cuda:0 f32[128, 16]\"\n",
       "  del t454\n",
       "  t2079 = torch.matmul(t2077, t2078)  # t2079: \"cuda:0 f32[16, 16]\"\n",
       "    # t2079 = ltorch.matmul(t2077, t2078)  # t2079: \"cuda:0 f32[16, 16]\"\n",
       "      # t2079 = prims.matmul(t2077, t2078)  # t2079: \"cuda:0 f32[16, 16]\"\n",
       "  del t2077, t2078\n",
       "  t2068 = torch.permute(t2065, (1, 0))  # t2068: \"cuda:0 f32[16, 128]\"\n",
       "    # t2068 = ltorch.permute(t2065, (1, 0))  # t2068: \"cuda:0 f32[16, 128]\"\n",
       "      # t2068 = prims.transpose(t2065, (1, 0))  # t2068: \"cuda:0 f32[16, 128]\"\n",
       "  del t2065\n",
       "  # Created by CPU Offloading Transform\n",
       "  t459 = load_to_gpu(offloaded_t459, 'cuda:0')  # t459: \"cuda:0 f32[128, 16]\"\n",
       "  t2069 = torch.reshape(t459, (-1, 16))  # t2069: \"cuda:0 f32[128, 16]\"\n",
       "    # t2069 = ltorch.reshape(t459, (-1, 16))  # t2069: \"cuda:0 f32[128, 16]\"\n",
       "      # t2069 = prims.reshape(t459, (128, 16))  # t2069: \"cuda:0 f32[128, 16]\"\n",
       "  del t459\n",
       "  t2070 = torch.matmul(t2068, t2069)  # t2070: \"cuda:0 f32[16, 16]\"\n",
       "    # t2070 = ltorch.matmul(t2068, t2069)  # t2070: \"cuda:0 f32[16, 16]\"\n",
       "      # t2070 = prims.matmul(t2068, t2069)  # t2070: \"cuda:0 f32[16, 16]\"\n",
       "  del t2068, t2069\n",
       "  t2059 = torch.permute(t2056, (1, 0))  # t2059: \"cuda:0 f32[16, 128]\"\n",
       "    # t2059 = ltorch.permute(t2056, (1, 0))  # t2059: \"cuda:0 f32[16, 128]\"\n",
       "      # t2059 = prims.transpose(t2056, (1, 0))  # t2059: \"cuda:0 f32[16, 128]\"\n",
       "  del t2056\n",
       "  # Created by CPU Offloading Transform\n",
       "  t464 = load_to_gpu(offloaded_t464, 'cuda:0')  # t464: \"cuda:0 f32[128, 16]\"\n",
       "  t2060 = torch.reshape(t464, (-1, 16))  # t2060: \"cuda:0 f32[128, 16]\"\n",
       "    # t2060 = ltorch.reshape(t464, (-1, 16))  # t2060: \"cuda:0 f32[128, 16]\"\n",
       "      # t2060 = prims.reshape(t464, (128, 16))  # t2060: \"cuda:0 f32[128, 16]\"\n",
       "  del t464\n",
       "  t2061 = torch.matmul(t2059, t2060)  # t2061: \"cuda:0 f32[16, 16]\"\n",
       "    # t2061 = ltorch.matmul(t2059, t2060)  # t2061: \"cuda:0 f32[16, 16]\"\n",
       "      # t2061 = prims.matmul(t2059, t2060)  # t2061: \"cuda:0 f32[16, 16]\"\n",
       "  del t2059, t2060\n",
       "  t2050 = torch.permute(t2047, (1, 0))  # t2050: \"cuda:0 f32[16, 128]\"\n",
       "    # t2050 = ltorch.permute(t2047, (1, 0))  # t2050: \"cuda:0 f32[16, 128]\"\n",
       "      # t2050 = prims.transpose(t2047, (1, 0))  # t2050: \"cuda:0 f32[16, 128]\"\n",
       "  del t2047\n",
       "  # Created by CPU Offloading Transform\n",
       "  t469 = load_to_gpu(offloaded_t469, 'cuda:0')  # t469: \"cuda:0 f32[128, 16]\"\n",
       "  t2051 = torch.reshape(t469, (-1, 16))  # t2051: \"cuda:0 f32[128, 16]\"\n",
       "    # t2051 = ltorch.reshape(t469, (-1, 16))  # t2051: \"cuda:0 f32[128, 16]\"\n",
       "      # t2051 = prims.reshape(t469, (128, 16))  # t2051: \"cuda:0 f32[128, 16]\"\n",
       "  del t469\n",
       "  t2052 = torch.matmul(t2050, t2051)  # t2052: \"cuda:0 f32[16, 16]\"\n",
       "    # t2052 = ltorch.matmul(t2050, t2051)  # t2052: \"cuda:0 f32[16, 16]\"\n",
       "      # t2052 = prims.matmul(t2050, t2051)  # t2052: \"cuda:0 f32[16, 16]\"\n",
       "  del t2050, t2051\n",
       "  t2041 = torch.permute(t2038, (1, 0))  # t2041: \"cuda:0 f32[16, 128]\"\n",
       "    # t2041 = ltorch.permute(t2038, (1, 0))  # t2041: \"cuda:0 f32[16, 128]\"\n",
       "      # t2041 = prims.transpose(t2038, (1, 0))  # t2041: \"cuda:0 f32[16, 128]\"\n",
       "  del t2038\n",
       "  # Created by CPU Offloading Transform\n",
       "  t474 = load_to_gpu(offloaded_t474, 'cuda:0')  # t474: \"cuda:0 f32[128, 16]\"\n",
       "  t2042 = torch.reshape(t474, (-1, 16))  # t2042: \"cuda:0 f32[128, 16]\"\n",
       "    # t2042 = ltorch.reshape(t474, (-1, 16))  # t2042: \"cuda:0 f32[128, 16]\"\n",
       "      # t2042 = prims.reshape(t474, (128, 16))  # t2042: \"cuda:0 f32[128, 16]\"\n",
       "  del t474\n",
       "  t2043 = torch.matmul(t2041, t2042)  # t2043: \"cuda:0 f32[16, 16]\"\n",
       "    # t2043 = ltorch.matmul(t2041, t2042)  # t2043: \"cuda:0 f32[16, 16]\"\n",
       "      # t2043 = prims.matmul(t2041, t2042)  # t2043: \"cuda:0 f32[16, 16]\"\n",
       "  del t2041, t2042\n",
       "  t2032 = torch.permute(t2029, (1, 0))  # t2032: \"cuda:0 f32[16, 128]\"\n",
       "    # t2032 = ltorch.permute(t2029, (1, 0))  # t2032: \"cuda:0 f32[16, 128]\"\n",
       "      # t2032 = prims.transpose(t2029, (1, 0))  # t2032: \"cuda:0 f32[16, 128]\"\n",
       "  del t2029\n",
       "  # Created by CPU Offloading Transform\n",
       "  t479 = load_to_gpu(offloaded_t479, 'cuda:0')  # t479: \"cuda:0 f32[128, 16]\"\n",
       "  t2033 = torch.reshape(t479, (-1, 16))  # t2033: \"cuda:0 f32[128, 16]\"\n",
       "    # t2033 = ltorch.reshape(t479, (-1, 16))  # t2033: \"cuda:0 f32[128, 16]\"\n",
       "      # t2033 = prims.reshape(t479, (128, 16))  # t2033: \"cuda:0 f32[128, 16]\"\n",
       "  del t479\n",
       "  t2034 = torch.matmul(t2032, t2033)  # t2034: \"cuda:0 f32[16, 16]\"\n",
       "    # t2034 = ltorch.matmul(t2032, t2033)  # t2034: \"cuda:0 f32[16, 16]\"\n",
       "      # t2034 = prims.matmul(t2032, t2033)  # t2034: \"cuda:0 f32[16, 16]\"\n",
       "  del t2032, t2033\n",
       "  t2023 = torch.permute(t2020, (1, 0))  # t2023: \"cuda:0 f32[16, 128]\"\n",
       "    # t2023 = ltorch.permute(t2020, (1, 0))  # t2023: \"cuda:0 f32[16, 128]\"\n",
       "      # t2023 = prims.transpose(t2020, (1, 0))  # t2023: \"cuda:0 f32[16, 128]\"\n",
       "  del t2020\n",
       "  # Created by CPU Offloading Transform\n",
       "  t484 = load_to_gpu(offloaded_t484, 'cuda:0')  # t484: \"cuda:0 f32[128, 16]\"\n",
       "  t2024 = torch.reshape(t484, (-1, 16))  # t2024: \"cuda:0 f32[128, 16]\"\n",
       "    # t2024 = ltorch.reshape(t484, (-1, 16))  # t2024: \"cuda:0 f32[128, 16]\"\n",
       "      # t2024 = prims.reshape(t484, (128, 16))  # t2024: \"cuda:0 f32[128, 16]\"\n",
       "  del t484\n",
       "  t2025 = torch.matmul(t2023, t2024)  # t2025: \"cuda:0 f32[16, 16]\"\n",
       "    # t2025 = ltorch.matmul(t2023, t2024)  # t2025: \"cuda:0 f32[16, 16]\"\n",
       "      # t2025 = prims.matmul(t2023, t2024)  # t2025: \"cuda:0 f32[16, 16]\"\n",
       "  del t2023, t2024\n",
       "  t2014 = torch.permute(t2011, (1, 0))  # t2014: \"cuda:0 f32[16, 128]\"\n",
       "    # t2014 = ltorch.permute(t2011, (1, 0))  # t2014: \"cuda:0 f32[16, 128]\"\n",
       "      # t2014 = prims.transpose(t2011, (1, 0))  # t2014: \"cuda:0 f32[16, 128]\"\n",
       "  del t2011\n",
       "  # Created by CPU Offloading Transform\n",
       "  t489 = load_to_gpu(offloaded_t489, 'cuda:0')  # t489: \"cuda:0 f32[128, 16]\"\n",
       "  t2015 = torch.reshape(t489, (-1, 16))  # t2015: \"cuda:0 f32[128, 16]\"\n",
       "    # t2015 = ltorch.reshape(t489, (-1, 16))  # t2015: \"cuda:0 f32[128, 16]\"\n",
       "      # t2015 = prims.reshape(t489, (128, 16))  # t2015: \"cuda:0 f32[128, 16]\"\n",
       "  del t489\n",
       "  t2016 = torch.matmul(t2014, t2015)  # t2016: \"cuda:0 f32[16, 16]\"\n",
       "    # t2016 = ltorch.matmul(t2014, t2015)  # t2016: \"cuda:0 f32[16, 16]\"\n",
       "      # t2016 = prims.matmul(t2014, t2015)  # t2016: \"cuda:0 f32[16, 16]\"\n",
       "  del t2014, t2015\n",
       "  t2005 = torch.permute(t2002, (1, 0))  # t2005: \"cuda:0 f32[16, 128]\"\n",
       "    # t2005 = ltorch.permute(t2002, (1, 0))  # t2005: \"cuda:0 f32[16, 128]\"\n",
       "      # t2005 = prims.transpose(t2002, (1, 0))  # t2005: \"cuda:0 f32[16, 128]\"\n",
       "  del t2002\n",
       "  # Created by CPU Offloading Transform\n",
       "  t494 = load_to_gpu(offloaded_t494, 'cuda:0')  # t494: \"cuda:0 f32[128, 16]\"\n",
       "  t2006 = torch.reshape(t494, (-1, 16))  # t2006: \"cuda:0 f32[128, 16]\"\n",
       "    # t2006 = ltorch.reshape(t494, (-1, 16))  # t2006: \"cuda:0 f32[128, 16]\"\n",
       "      # t2006 = prims.reshape(t494, (128, 16))  # t2006: \"cuda:0 f32[128, 16]\"\n",
       "  del t494\n",
       "  t2007 = torch.matmul(t2005, t2006)  # t2007: \"cuda:0 f32[16, 16]\"\n",
       "    # t2007 = ltorch.matmul(t2005, t2006)  # t2007: \"cuda:0 f32[16, 16]\"\n",
       "      # t2007 = prims.matmul(t2005, t2006)  # t2007: \"cuda:0 f32[16, 16]\"\n",
       "  del t2005, t2006\n",
       "  return (None, t2899, t2898, t2890, t2889, t2881, t2880, t2872, t2871, t2863, t2862, t2854, t2853, t2845, t2844, t2836, t2835, t2827, t2826, t2818, t2817, t2809, t2808, t2800, t2799, t2791, t2790, t2782, t2781, t2773, t2772, t2764, t2763, t2755, t2754, t2746, t2745, t2737, t2736, t2728, t2727, t2719, t2718, t2710, t2709, t2701, t2700, t2692, t2691, t2683, t2682, t2674, t2673, t2665, t2664, t2656, t2655, t2647, t2646, t2638, t2637, t2629, t2628, t2620, t2619, t2611, t2610, t2602, t2601, t2593, t2592, t2584, t2583, t2575, t2574, t2566, t2565, t2557, t2556, t2548, t2547, t2539, t2538, t2530, t2529, t2521, t2520, t2512, t2511, t2503, t2502, t2494, t2493, t2485, t2484, t2476, t2475, t2467, t2466, t2458, t2457, t2449, t2448, t2440, t2439, t2431, t2430, t2422, t2421, t2413, t2412, t2404, t2403, t2395, t2394, t2386, t2385, t2377, t2376, t2368, t2367, t2359, t2358, t2350, t2349, t2341, t2340, t2332, t2331, t2323, t2322, t2314, t2313, t2305, t2304, t2296, t2295, t2287, t2286, t2278, t2277, t2269, t2268, t2260, t2259, t2251, t2250, t2242, t2241, t2233, t2232, t2224, t2223, t2215, t2214, t2206, t2205, t2197, t2196, t2188, t2187, t2179, t2178, t2170, t2169, t2161, t2160, t2152, t2151, t2143, t2142, t2134, t2133, t2125, t2124, t2116, t2115, t2107, t2106, t2098, t2097, t2089, t2088, t2080, t2079, t2071, t2070, t2062, t2061, t2053, t2052, t2044, t2043, t2035, t2034, t2026, t2025, t2017, t2016, t2008, t2007)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bw_traces[-1]  # Note the calls to `load_to_gpu` and verify that they before the first usage of the tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark `thunder` vs `thunder + CPU Offloading` on Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated Memory after cleaning 8.192e-06 GB\n"
     ]
    }
   ],
   "source": [
    "model, args, kwargs = get_model_and_args()\n",
    "\n",
    "measurement_thunder = benchmark(thunder.jit(model), model, args, kwargs)\n",
    "measurement_thunder_offload = benchmark(thunder.jit(model, transforms=[CPUOffloading()]), model, args, kwargs)\n",
    "\n",
    "del model, args, kwargs\n",
    "\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.benchmark.utils.common.Measurement object at 0x7059ba637f70>\n",
       "stmt:\n",
       "  # Use the optimized model for prediction and backward\n",
       "  o = jmodel(*args, **kwargs)\n",
       "  o.sum().backward()\n",
       "  for param in model.parameters():  # use original model for clear grads\n",
       "      param.grad = None\n",
       "\n",
       "  8.73 ms\n",
       "  1 measurement, 10 runs , 1 thread"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measurement_thunder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.benchmark.utils.common.Measurement object at 0x7059b55642e0>\n",
       "stmt:\n",
       "  # Use the optimized model for prediction and backward\n",
       "  o = jmodel(*args, **kwargs)\n",
       "  o.sum().backward()\n",
       "  for param in model.parameters():  # use original model for clear grads\n",
       "      param.grad = None\n",
       "\n",
       "  12.84 ms\n",
       "  1 measurement, 10 runs , 1 thread"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measurement_thunder_offload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try it on a real-life model Llama-3. We will run it on a smaller Llama-3. Feel free to update `N_LAYER` and `BLOCK_SIZE` based on the available device memory.\n",
    "\n",
    "**NOTE**: Running the cell below requires `litgpt` installed. Use `pip install litgpt` if it is not available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from litgpt import Config, GPT\n",
    "from functools import partial\n",
    "from torch.testing import make_tensor\n",
    "\n",
    "N_LAYER = 7\n",
    "BLOCK_SIZE = 1024\n",
    "\n",
    "def get_model_and_args(batchdims=8):\n",
    "    with torch.device(\"cuda\"):\n",
    "        cfg: Config = Config.from_name(\"Llama-3-8B\")\n",
    "        # Smaller configuration\n",
    "        cfg.n_layer = N_LAYER\n",
    "        cfg.block_size = BLOCK_SIZE\n",
    "\n",
    "        model = GPT(cfg)\n",
    "        make = partial(make_tensor, low=0, high=255, device='cuda', dtype=torch.int64, requires_grad=False)\n",
    "        shape = (batchdims,) + (cfg.block_size,)\n",
    "\n",
    "        x = make(shape)\n",
    "        args, kwargs = (x,), {}\n",
    "\n",
    "    return model, args, kwargs, cfg\n",
    "\n",
    "def print_memory_usage_and_benchmark(name):\n",
    "    print(f\"{name} took -\")\n",
    "    model, args, kwargs, cfg = get_model_and_args()\n",
    "\n",
    "    if name == 'thunder':\n",
    "        jmodel = thunder.jit(model)\n",
    "    elif name == 'thunder_offload':\n",
    "        jmodel = thunder.jit(model, transforms=[CPUOffloading()])\n",
    "    else:\n",
    "        raise RuntimeError(\"Received invalid value for `name` - try `thunder` or `thunder_offload`.\")\n",
    "\n",
    "    memory_after_model_load = torch.cuda.max_memory_allocated() / 1e9\n",
    "    print(f\"Peak memory after loading the model : {memory_after_model_load} GB\")\n",
    "\n",
    "    a = jmodel(*args, **kwargs)\n",
    "\n",
    "    memory_after_forward = torch.cuda.max_memory_allocated() / 1e9\n",
    "    print(f\"Peak memory after forward the model : {memory_after_forward} GB\")\n",
    "\n",
    "    g = torch.rand_like(a)\n",
    "    actual_grads = torch.autograd.grad(a, model.parameters(), g)\n",
    "\n",
    "    memory_after_backward = torch.cuda.max_memory_allocated() / 1e9\n",
    "    print(f\"Peak memory after backward the model : {memory_after_backward} GB\")\n",
    "\n",
    "    del a, g, actual_grads  # Clear data which is not required for benchmark to free some memory.\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    measurement = benchmark(jmodel, model, args, kwargs)\n",
    "    print(f\"Benchmark Timings - mean : {measurement.mean} - median {measurement.median}\")\n",
    "\n",
    "    del jmodel, model, cfg, args, kwargs\n",
    "\n",
    "    clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thunder took -\n",
      "Peak memory after loading the model : 10.311430656 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W815 16:01:03.874001433 sdp_utils.cpp:455] Warning: 1Torch was not compiled with flash attention. (function operator())\n",
      "[W815 16:01:03.874014103 sdp_utils.cpp:504] Warning: 1Torch was not compiled with memory efficient attention. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak memory after forward the model : 39.679975424 GB\n",
      "Peak memory after backward the model : 47.024185344 GB\n",
      "Benchmark Timings - mean : 3.9535594288026914 - median 3.9535594288026914\n",
      "CUDA Memory has been cleared and currently allocated memory is  8.192e-06\n"
     ]
    }
   ],
   "source": [
    "# print_memory_usage_and_benchmark(\"thunder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thunder_offload took -\n",
      "Peak memory after loading the model : 10.311430656 GB\n",
      "Peak memory after forward the model : 14.647877632 GB\n",
      "Peak memory after backward the model : 32.004153344 GB\n",
      "Benchmark Timings - mean : 5.257712998200441 - median 5.257712998200441\n",
      "CUDA Memory has been cleared and currently allocated memory is  8.192e-06\n"
     ]
    }
   ],
   "source": [
    "# print_memory_usage_and_benchmark(\"thunder_offload\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "\n",
    "In this notebook, we have understood how to write our own `Transform` in `thunder`. As an example, we wrote an `CPUOffloading` transform to implement CPU offloading technique to decrease peak memory usage during training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
