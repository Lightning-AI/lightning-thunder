from __future__ import annotations
from typing import TYPE_CHECKING
import subprocess
import sys
from pathlib import Path
import json

from thunder.dynamo.compiler import thunderfx

if TYPE_CHECKING:
    from thunder.dynamo.utils import SubgraphInfo
    from os import PathLike
    from collections.abc import Callable


def get_thunder_graph_names(subgraph_infos: list[SubgraphInfo]):
    thunder_graph_names = []
    for graph_idx, subgraph_info in enumerate(subgraph_infos):
        for node in subgraph_info.split_graph_module.graph.nodes:
            target = node.target
            if isinstance(target, str) and target.startswith("thunder_"):
                thunder_graph_names.append(f"graph{graph_idx}_{target}")
    return thunder_graph_names


def thunderfx_save_report(
    fn: Callable,
    *args,
    compile_kwargs: dict = None,
    folder_path: str | PathLike = "/tmp/thunderfx_report",
    check_consistency: bool = True,
    check_benchmark: bool = True,
    **kwargs,
):
    try:
        compiled = thunderfx(fn, **compile_kwargs) if compile_kwargs is not None else thunderfx(fn)
        compiled(*args, **kwargs)
    except Exception as e:
        print(f"Failed to run the function using ThunderFX with exception: {e}")
        try:
            compiled._backend.save_reproducer_to_folder(folder_path)
        except Exception as repro_e:
            print(f"Failed to save reproducer due to {repro_e}")
            return
        print(f"The reproducer file is saved in {folder_path}")
        return

    if not check_benchmark and not check_consistency:
        return

    thunder_graph_names = get_thunder_graph_names(compiled._backend.subgraph_infos)
    EXECUTOR_NAMES = ("eager", "thunder", "torch_inductor")

    report_result: dict[str, list] = {}
    for g_name in thunder_graph_names:
        for ex in EXECUTOR_NAMES:
            # Sets the consistency field to None for eager
            report_result[f"{g_name}[{ex}]"] = [None] if ex == "eager" else []

    folder = Path(folder_path)
    # NOTE If the input folder path contains subfolders named 'benchmark' or 'consistency', they will be overwritten.
    folder.mkdir(exist_ok=True)
    # Checks consistency with Torch eager
    if check_consistency:
        consistency_folder = folder / "consistency"
        consistency_folder.mkdir(exist_ok=True)
        compiled._backend.save_reproducer_to_folder(consistency_folder, check_consistency=True)
        for file in consistency_folder.glob("*.py"):
            # The consistency results generated by the script are passed here via stdout
            consistency_result = eval(
                subprocess.run([sys.executable, folder / file], capture_output=True, text=True).stdout
            )
            for g_name, consistency in consistency_result.items():
                g_ex_name = f"{file.name.rstrip('.py')}[{g_name}]"
                assert g_ex_name in report_result
                report_result[g_ex_name] = ["yes" if consistency is None else str(consistency)]

    # Benchmark
    if check_benchmark:
        benchmark_folder = folder / "benchmark"
        benchmark_folder.mkdir(exist_ok=True)
        compiled._backend.save_reproducer_to_folder(benchmark_folder, save_input_tensor=True, use_pytest_benchmark=True)

        benchmark_json_files = []
        for file in benchmark_folder.glob("*.py"):
            benchmark_json_files.append(str(benchmark_folder / f"{file.name.replace('.py', '.json')}"))
            subprocess.run(
                [
                    sys.executable,
                    "-m",
                    "pytest",
                    benchmark_folder / file,
                    "--benchmark-timer=torch.utils.benchmark.utils.timer.timer",
                    "--benchmark-warmup=on",
                    f"--benchmark-json={benchmark_json_files[-1]}",
                ],
                capture_output=True,
                text=True,
            )

        for tmp_json in benchmark_json_files:
            with open(tmp_json) as file:
                data = json.load(file)
                for bk in data["benchmarks"]:
                    cur_name = bk["name"].lstrip("test_")
                    if cur_name in report_result:
                        report_result[cur_name].append(bk["stats"]["mean"])
                        report_result[cur_name].append(bk["extra_info"]["max_allocated_memory_MB"])

    list_data: list[dict] = []
    for g_name, values in report_result.items():
        list_data.append({})
        list_data[-1]["name"] = g_name
        if check_consistency:
            list_data[-1]["consistency"] = values[0]
        if check_benchmark:
            base = check_benchmark + check_consistency - 1
            list_data[-1]["performance_mean"] = values[base]
            list_data[-1]["max_allocated_memory_MB"] = values[base + 1]
    json_data = {"report": list_data}

    with open(folder / "report.json", "w") as f:
        json.dump(json_data, f, indent=4)
