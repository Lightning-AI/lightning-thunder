from collections.abc import Callable

import numpy as np
import pytest
import torch
from torch.testing import assert_close

import thunder
import thunder.core.dtypes as dtypes
from thunder.core.pytree import tree_map
from thunder.tests.framework import assert_closer, ops, run_snippet, requiresJAX, requiresCUDA
from thunder.tests.opinfos import OpInfo, SampleInput, opinfos
import thunder.tests.bf16

#
# Generic test templates for all operators
#


# NOTE err_msg_match=None will match any error message
def snippet_errors(op, sample, ex_type, err_msg_match=None):
    with pytest.raises(ex_type, match=err_msg_match):
        op(*sample.args, **sample.kwargs)


@ops(tuple(op for op in opinfos if op.error_input_generator is not None))
def test_errors(op, device, dtype, executor, comp):
    for sample, ex_type, err_msg in op.error_inputs(device):
        result = run_snippet(snippet_errors, op, device, None, executor.make_callable(op.op), sample, ex_type, err_msg)
        if result is not None:
            return result


# Snippets run a single test using a single sample
# TODO: should snippets be able to access the original opinfo? -- No?
# TODO: revisit atol/rtol, maybe be more selective about which ops need a more permissive check
def snippet_torch_consistency(op: OpInfo, torch_op, sample: SampleInput, comp: Callable):
    args, kwargs = sample.args, sample.kwargs

    thunder_result = op(*args, **kwargs)
    torch_result = torch_op(*args, **kwargs)

    # TODO Review how thunder.jit returns Exception information
    if isinstance(thunder_result, Exception):
        raise thunder_result

    # Try checking strictly, if that does not work, check against reference.
    try:
        comp(thunder_result, torch_result)
    except AssertionError:

        def upcast_tensors(x):
            if isinstance(x, torch.Tensor) and torch.is_floating_point(x):
                return x.to(torch.double)
            return x

        reference_args = tree_map(upcast_tensors, args)
        reference_kwargs = tree_map(upcast_tensors, kwargs)
        reference_result = torch_op(*reference_args, **reference_kwargs)

        assert_closer(
            reference=(reference_result,), candidate=(thunder_result,), competitor=(torch_result,), comparator=comp
        )


# TODO consider structuring tests like this to be autogenerated
#   using a snippet and an "extractor" that constructs the args and kwargs for the snippet
# TODO The name of this test is misleading as it may test operators from a variety of languages,
#   maybe we should cut it up so developers can test just torch operators or just core lang operators
# TODO Extend this test with some reproducible randomness (maybe using hypothesis)
# TODO Remove the atol and rtol defaults and rely on the given comparator to set them
@ops(tuple(op for op in opinfos if op.torch_reference is not None))
def test_core_vs_torch_consistency(op, device: str, dtype: dtypes.dtype, executor, comp):
    if dtypes.is_complex_dtype(dtype) and not op.instantiate_complex_tests:
        pytest.skip("Skipping complex operator tests in CI for speed")
    if (
        torch.device(device).type == "cuda"
        and dtype is dtypes.bfloat16
        and not thunder.tests.bf16.device_supports_bf16(device)
    ):
        pytest.skip("Your CUDA device does not support bfloat16")

    for sample in op.sample_inputs(device, dtype):
        comp = sample.comp if sample.comp is not None else comp

        tfn: Callable
        tfn = thunder.jit(
            op.op,
            executors=executor.executors_list(),
            cache="no caching",
            disable_torch_autograd=True,
        )

        result = run_snippet(
            snippet_torch_consistency,
            op,
            device,
            dtype,
            tfn,
            op.torch_reference,
            sample,
            lambda a, b, **kwargs: comp(a, b, equal_nan=True, **kwargs),
        )

        # See [NOTE] dynamo reset
        if any("torchcompile" in ex.name for ex in executor.executors_list()):
            torch._dynamo.reset()

        if result is not None:
            return result


def snippet_jax_consistency(op, jax_op, sample, comp):
    import jax.numpy as jnp

    jax_sample = sample.jax()

    thunder_result = op(*sample.args, **sample.kwargs)
    jax_result = jax_op(*jax_sample.args, **jax_sample.kwargs)

    # NOTE This strange unpacking is to handle NumPy's and JAX's sometimes odd
    #   number vs. array representation. In particular, NumPy can mimic
    #   Python numbers, but `asarray` doesn't understand this mimicry
    def convert_to_torch(x):
        if not isinstance(x, jnp.ndarray):
            return x

        np_array = np.array(x)
        if np_array.shape == ():
            return torch.tensor(np_array.item(), device=thunder_result.device)
        else:
            return torch.asarray(np_array, device=thunder_result.device)

    jax_result = tree_map(convert_to_torch, jax_result)

    comp(thunder_result, jax_result)


# TODO Consider structuring tests like this to be autogenerated
#   using a snippet and an "extractor" that constructs the args and kwargs for the snippet
# TODO Extend this test with some reproducible randomness (maybe using hypothesis)
@ops(tuple(op for op in opinfos if op.jax_reference is not None))
@requiresJAX
def test_core_vs_jax_consistency(op, device: str, dtype: dtypes.dtype, executor, comp):
    if dtypes.is_complex_dtype(dtype) and not op.instantiate_complex_tests:
        pytest.skip("Skipping complex operator tests in CI for speed")
    if dtype is dtypes.complex32:
        pytest.skip("jax doesn't support complex32!")
    if dtype is dtypes.bfloat16:
        pytest.skip("jax bfloat16 support is spotty (at least on CPU)")

    for sample in op.sample_inputs(device, dtype):
        comp = sample.comp if sample.comp is not None else comp

        result = run_snippet(
            snippet_jax_consistency,
            op,
            device,
            dtype,
            executor.make_callable(op.op),
            op.jax_reference,
            sample,
            # NOTE: dtype is not checked because jax will translate
            # int64, float64, and complex128 to int32, float32 and complex64
            lambda a, b: comp(a, b, equal_nan=True, check_dtype=False),
        )
        if result is not None:
            return result


def snippet_numpy_consistency(op: OpInfo, np_op, sample: SampleInput, comp: Callable):
    np_sample = sample.numpy()

    thunder_result = op(*sample.args, **sample.kwargs)
    np_result = np_op(*np_sample.args, **np_sample.kwargs)

    # Converts NumPy results to PyTorch.
    # NOTE This assumes PyTorch will return tensors where NumPy is aggressive about returning `np.number` objects.
    def convert_to_torch(x):
        if not isinstance(x, (np.ndarray, np.number, np.bool_)):
            return x

        if isinstance(x, (np.number, np.bool_)):
            return torch.tensor(x, device=thunder_result.device)
        elif x.shape == ():
            return torch.tensor(x.item(), device=thunder_result.device)
        else:
            return torch.asarray(x, device=thunder_result.device)

    np_result = tree_map(convert_to_torch, np_result)

    comp(thunder_result, np_result)


@ops(tuple(op for op in opinfos if op.numpy_reference is not None))
def test_core_vs_numpy_consistency(op: OpInfo, device: str, dtype: dtypes.dtype, executor, comp):
    if dtypes.is_complex_dtype(dtype):
        pytest.skip("Skipping complex operator tests in CI for speed")
    if dtype == dtypes.complex32:
        pytest.skip("NumPy does not support complex32")
    if dtype == dtypes.bfloat16:
        pytest.skip("NumPy does not support bfloat16")

    for sample in op.sample_inputs(device, dtype):
        comp = sample.comp if sample.comp is not None else comp

        result = run_snippet(
            snippet_numpy_consistency,
            op,
            device,
            dtype,
            executor.make_callable(op.op),
            op.numpy_reference,
            sample,
            # NOTE dtype is intentionally not checked because NumPy sometimes has slight dtype variances
            lambda a, b: comp(a, b, equal_nan=True, check_dtype=False),
        )
        if result is not None:
            return result


def test_interpolate_nearest_vs_nearest_exact():
    t0 = torch.tensor([[[1, 2, 3], [4, 5, 6]], [[1, 2, 3], [4, 5, 6]]], dtype=torch.float16)

    def foo(mode):
        t1 = torch.nn.functional.interpolate(
            t0,
            scale_factor=1.5,
            mode=mode,
        )
        return t1

    def bar(mode):
        t1 = torch.nn.functional.interpolate(
            t0,
            size=4,
            mode=mode,
        )
        return t1

    tfoo = thunder.jit(foo)
    assert not torch.equal(tfoo("nearest"), tfoo("nearest-exact"))
    tbar = thunder.jit(bar)
    assert not torch.equal(tbar("nearest"), tbar("nearest-exact"))


def test_notimplemented_interpolate_align():
    def foo():
        t0 = torch.randn((22, 288, 15, 20), dtype=torch.float16)
        t1 = torch.nn.functional.interpolate(t0, scale_factor=2.0, mode="nearest", align_corners=True)
        return t1

    with pytest.raises(NotImplementedError, match="not yet support"):
        tfoo = thunder.jit(foo)
        tfoo()


def test_notimplemented_interpolate_recompute_scale():
    def foo():
        t0 = torch.randn((22, 288, 15, 20), dtype=torch.float16)
        t1 = torch.nn.functional.interpolate(t0, scale_factor=2.0, mode="nearest", recompute_scale_factor=True)
        return t1

    with pytest.raises(NotImplementedError, match="not yet support"):
        tfoo = thunder.jit(foo)
        tfoo()


def test_notimplemented_interpolate_antialias():
    def foo():
        t0 = torch.randn((22, 288, 15, 20), dtype=torch.float16)
        t1 = torch.nn.functional.interpolate(
            t0,
            scale_factor=2.0,
            mode="nearest",
            antialias=True,
        )
        return t1

    with pytest.raises(NotImplementedError, match="not yet support"):
        tfoo = thunder.jit(foo)
        tfoo()


def test_notimplemented_interpolate_modes():
    def foo(mode):
        t0 = torch.randn((22, 288, 15, 20), dtype=torch.float16)
        t1 = torch.nn.functional.interpolate(
            t0,
            scale_factor=2.0,
            mode=mode,
        )
        return t1

    tfoo = thunder.jit(foo)
    for mode in ["linear", "bilinear", "bicubic", "trilinear", "area"]:
        match = f"only modes 'nearest' and 'nearest-exact' are supported at the moment, but got mode='{mode}'"
        with pytest.raises(NotImplementedError, match=match):
            tfoo(mode)


@pytest.mark.parametrize("requires_grad", (True, False))
def test_setitem(requires_grad):

    def _test_forward_and_backward(fn, a, value):
        a_ref = a.detach().clone()
        a_ref.requires_grad_(a.requires_grad)

        if isinstance(value, torch.Tensor):
            value_ref = value.detach().clone()
            value_ref.requires_grad_(value.requires_grad)
        else:
            value_ref = value

        out_ref = fn(a_ref, value_ref)
        jf = thunder.jit(fn)
        out = jf(a, value)
        assert_close(a, a_ref)
        assert_close(out, out_ref)

        if requires_grad:
            g = torch.randn_like(out)
            inputs = (a, value) if isinstance(value, torch.Tensor) else (a,)
            actual_grad = torch.autograd.grad(out, inputs, g)

            inputs_ref = (a_ref, value_ref) if isinstance(value, torch.Tensor) else (a_ref,)
            expected_grad = torch.autograd.grad(out_ref, inputs_ref, g)
            assert_close(actual_grad, expected_grad)

    def clone_if_requires_grad(a):
        if requires_grad:
            # Withou the clone
            # PyTorch eager errors with
            # `RuntimeError: a view of a leaf Variable that requires grad is being used in an in-place operation.`
            # and thunder has silent correctness issue - https://github.com/Lightning-AI/lightning-thunder/issues/1284
            return a.clone()
        return a

    def fn(a, value):
        a = clone_if_requires_grad(a)
        a[:3] = value
        return a * 2

    # set value: scalar
    _test_forward_and_backward(fn, torch.randn(5, requires_grad=requires_grad), 2.0)

    # set value: tensor which needs to be broadcasted
    _test_forward_and_backward(
        fn, torch.randn(5, requires_grad=requires_grad), torch.tensor(2.0, requires_grad=requires_grad)
    )

    def bcast_fn(a, value):
        a = clone_if_requires_grad(a)
        a[..., :3] = value
        return a * 2

    _test_forward_and_backward(
        bcast_fn, torch.randn(5, 3, 5, requires_grad=requires_grad), torch.randn(1, 3, requires_grad=requires_grad)
    )

    # set value: tensor of same rank
    _test_forward_and_backward(
        fn, torch.randn(5, requires_grad=requires_grad), torch.tensor([1.0, 2.0, 3.0], requires_grad=requires_grad)
    )


# TODO: Add random operator support to OpInfo
# https://github.com/Lightning-AI/lightning-thunder/issues/1163
@requiresCUDA
def test_exponential():
    def fn(a):
        return a.exponential_(1)

    size = 10
    seed = 1234

    # on cpu, aten.exponential_ is not decomposed to ops used in Thunder exponential_.
    with torch.device("cuda"):
        a_ref = torch.ones(size)
        b_ref = torch.ones((size, size, size))
        torch.manual_seed(seed)
        a_ref = fn(a_ref)
        b_ref = fn(b_ref)

        a = torch.ones(size)
        b = torch.ones((size, size, size))
        torch.manual_seed(seed)

        # nvfuser fuses prims.uniform, which is used by our exponential resulting in differing numerics.
        jf = thunder.jit(fn, executors={})
        a = jf(a)
        b = jf(b)

        assert_close(a, a_ref)
        assert_close(b, b_ref)


# https://github.com/Lightning-AI/lightning-thunder/issues/1857
def test_max_with_int():
    def f(x, ids):
        x + x
        return ids[0].max()

    x = torch.rand([2, 2], requires_grad=True)
    ids = torch.randint(0, 10, size=(1, 512))

    thunder.jit(f)(x, ids)


def test_ltorch_cumsum_result_dtype_for_int_input():

    def f(a):
        return torch.cumsum(a, dim=0)

    x = torch.randint(0, 128, (4,), dtype=torch.int32)
    jitted = thunder.jit(f)
    out = jitted(x)
    # runtime check
    assert out.dtype is torch.int64

    trc = thunder.last_traces(jitted)[0]
    bsym_of_cumsum = trc.bound_symbols[1]
    assert bsym_of_cumsum.output.dtype is dtypes.int64


def test_ltorch_maximum_result_dtype_for_scalar_tensors():
    def foo(a, b):
        return torch.maximum(a, b)

    x = torch.rand(2, 2, dtype=torch.bfloat16)
    y = torch.tensor(0.12345)

    jfoo = thunder.jit(foo)
    out = jfoo(x, y)

    assert out.dtype == torch.bfloat16

    trc = thunder.last_traces(jfoo)[-1]

    bsym = trc.bound_symbols[2]
    assert bsym.output.dtype is dtypes.bfloat16


def test_multi_dot_optimization():
    def fn(tensors):
        return torch.linalg.multi_dot(tensors)

    # three matrices

    a = torch.randn(10, 100)
    b = torch.randn(100, 10)
    c = torch.randn(10, 100)

    jfn = thunder.jit(fn)
    out = jfn([a, b, c])
    trc = thunder.last_traces(jfn)[-1]

    # make sure that there is no (100 x 100) intermediates
    for bsym in trc.bound_symbols:
        if bsym.sym.id == "matmul":
            for flat_out in bsym.flat_outs:
                assert flat_out.shape != (100, 100)

    out2 = jfn([a.T, b.T, c.T])
    trc2 = thunder.last_traces(jfn)[-1]

    # make sure that there is no (100 x 100) intermediates
    for bsym in trc2.bound_symbols:
        if bsym.sym.id == "matmul":
            for flat_out in bsym.flat_outs:
                assert flat_out.shape != (100, 100)

    # five matrices

    d = torch.randn(100, 10)
    e = torch.randn(10, 100)

    jfn = thunder.jit(fn)
    out = jfn([a, b, c, d, e])
    trc = thunder.last_traces(jfn)[-1]

    # make sure that there is no (100 x 100) intermediates
    for bsym in trc.bound_symbols:
        if bsym.sym.id == "matmul":
            for flat_out in bsym.flat_outs:
                assert flat_out.shape != (100, 100)

    out2 = jfn([a.T, b.T, c.T, d.T, e.T])
    trc2 = thunder.last_traces(jfn)[-1]

    # make sure that there is no (100 x 100) intermediates
    for bsym in trc2.bound_symbols:
        if bsym.sym.id == "matmul":
            for flat_out in bsym.flat_outs:
                assert flat_out.shape != (100, 100)
