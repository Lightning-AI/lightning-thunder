"""Inference benchmark focusing on throughput and latency metrics of prefill and decode phases.

AutoModelForCausalLM from Hugging Face transformers is used for model implementation.

Key metrics:
- Throughput (tokens/second)
- Latency (ms/token)
- Time to First Token (TTFT)
- Time Between Output Tokens (TBOT)
"""

from __future__ import annotations
import argparse
from contextlib import contextmanager
from dataclasses import dataclass, field
import json
import os
import time
from typing import TYPE_CHECKING

import numpy as np
import torch
from torch.distributed.device_mesh import init_device_mesh
from torch.distributed.distributed_c10d import destroy_process_group
from torch.distributed.tensor.parallel import parallelize_module, RowwiseParallel, ColwiseParallel
from tqdm import tqdm
from transformers import AutoConfig, AutoModelForCausalLM
from transformers.cache_utils import HybridChunkedCache

import thunder
from thunder.dynamo.compiler import thunderfx
from thunder.dynamo.report import thunderfx_benchmark_report

if TYPE_CHECKING:
    from typing import Any


RANK = int(os.environ.get("RANK", 0))
LOCAL_RANK = int(os.environ.get("LOCAL_RANK", 0))
WORLD_SIZE = int(os.environ.get("WORLD_SIZE", 1))
MASTER_ADDR = os.environ.get("MASTER_ADDR", "localhost")
MASTER_PORT = os.environ.get("MASTER_PORT", "29500")
os.environ["TORCH_NCCL_ASYNC_ERROR_HANDLING"] = "1"

DEVICE = torch.device("cuda", LOCAL_RANK)
torch.cuda.set_device(DEVICE)

if WORLD_SIZE > 1:
    mesh = init_device_mesh("cuda", (WORLD_SIZE,), mesh_dim_names=("tp",))

LLAMA4_MAVERICK_MODEL_ID: str = "meta-llama/Llama-4-Maverick-17B-128E"


@contextmanager
def timer():
    torch.cuda.synchronize()
    t1 = t2 = time.perf_counter()
    yield lambda: (t2 - t1) * 1000  # Convert to ms
    torch.cuda.synchronize()
    t2 = time.perf_counter()


# Standard benchmark scenarios following the three-scenario methodology
BENCHMARK_SCENARIOS = {
    "summarization": {
        "name": "Summarization (Prefill-Heavy)",
        "input_length": 4000,
        "output_length": 1000,
        "description": "4,000 input → 1,000 output tokens (80% prefill, 20% decode)",
    },
    "chat": {
        "name": "Chat (Balanced)",
        "input_length": 1000,
        "output_length": 1000,
        "description": "1,000 input → 1,000 output tokens (50% prefill, 50% decode)",
    },
    "reasoning": {
        "name": "Reasoning (Decode-Heavy)",
        "input_length": 1000,
        "output_length": 4000,
        "description": "1,000 input → 4,000 output tokens (20% prefill, 80% decode)",
    },
}

WARMUP_ITERATIONS = 2


@dataclass
class InferenceBenchmarkConfig:
    """Configuration for inference benchmarking"""

    model_name: str
    batch_size: int
    input_length: int
    output_length: int
    num_layers: int | None
    num_iterations: int
    scenario: str | None  # Standard scenario name if using predefined configurations
    dtensor_single_gpu: bool
    load_nvfp4: bool  # Enable NVFP4 quantization
    fx_report_folder: str | None
    enable_nv_linear: bool
    mode: str

    # Memory bandwidth and compute specs
    # TODO check correctness of numbers (generated by AI)
    gpu_specs: dict[str, dict[str, float]] = field(
        default_factory=lambda: {
            "H100": {"memory_bandwidth_gb": 3350, "fp16_tflops": 1979, "fp8_tflops": 3958},
            "H200": {"memory_bandwidth_gb": 4800, "fp16_tflops": 1979, "fp8_tflops": 3958},
            "B200": {"memory_bandwidth_gb": 8000, "fp16_tflops": 2529, "fp8_tflops": 5058},
        },
        init=False,
    )


@dataclass
class InferenceMetrics:
    """Metrics collected during inference benchmarking"""

    throughput_tokens_per_sec: float = 0.0
    latency_ms_per_token: float = 0.0
    time_to_first_token_ms: float = 0.0
    time_between_output_tokens_ms: float = 0.0
    total_time_ms: float = 0.0
    memory_used_gb: float = 0.0
    peak_memory_gb: float = 0.0

    # Separate prefill and decode metrics
    prefill_throughput_tokens_per_sec: float = 0.0
    decode_throughput_tokens_per_sec: float = 0.0
    prefill_time_ms: float = 0.0
    decode_time_ms: float = 0.0

    # Per-iteration metrics for variance analysis
    iteration_times: list[float] = field(default_factory=list)
    ttft_times: list[float] = field(default_factory=list)
    prefill_times: list[float] = field(default_factory=list)
    decode_times: list[float] = field(default_factory=list)


class InferenceBenchmark:
    """Main benchmark class"""

    def __init__(self, config: InferenceBenchmarkConfig):
        self.config = config
        self.metrics = InferenceMetrics()

        self.model = self._load_model()

        tp_plan = {
            "*.layers.*.self_attn.q_proj": ColwiseParallel(use_local_output=True),
            "*.layers.*.self_attn.k_proj": ColwiseParallel(use_local_output=True),
            "*.layers.*.self_attn.v_proj": ColwiseParallel(use_local_output=True),
            "*.layers.*.self_attn.o_proj": RowwiseParallel(use_local_output=True),
            "*.layers.*.feed_forward.gate_proj": ColwiseParallel(use_local_output=False),
            "*.layers.*.feed_forward.up_proj": ColwiseParallel(use_local_output=False),
            "*.layers.*.feed_forward.down_proj": RowwiseParallel(use_local_output=True),
            "*.layers.*.feed_forward.shared_expert.gate_proj": ColwiseParallel(use_local_output=False),
            "*.layers.*.feed_forward.shared_expert.up_proj": ColwiseParallel(use_local_output=False),
            "*.layers.*.feed_forward.shared_expert.down_proj": RowwiseParallel(use_local_output=True),
        }

        if self.config.dtensor_single_gpu or WORLD_SIZE > 1:
            self.model = parallelize_module(self.model, mesh, tp_plan)

            # Required as that doesn't understand inference mode
            for p in self.model.parameters():
                p.requires_grad_(False)

        self.model = self._compile_model(self.model)

    @property
    def _thunder_jit_options(self) -> dict[str, Any]:
        # `nv_enable_linear=True` might fail with distributed run
        # ref: https://github.com/NVIDIA/Fuser/issues/4507
        if self.config.enable_nv_linear:
            return {"nv_enable_linear": True, "nv_enable_matmul": True}
        return {}

    def _compile_model(self, model):
        if self.config.fx_report_folder is not None:
            return model
        match self.config.mode:
            case "eager":
                return model
            case "inductor":
                return torch.compile(model, mode="reduce-overhead")
            case "thunder":
                return thunderfx(model, **self._thunder_jit_options)
            case "thunderjit":
                return thunder.jit(model, **self._thunder_jit_options)
            case _:
                raise ValueError(f"Unknown mode: {self.config.mode}")

    def _load_model(self) -> torch.nn.Module:
        """Load the model based on configuration"""
        model_id = self.config.model_name
        config = AutoConfig.from_pretrained(model_id)

        if hasattr(config, "text_config"):
            config = config.text_config
        if self.config.num_layers:
            config.num_hidden_layers = self.config.num_layers

        self.hf_config = config

        # TODO: Apply NVFP4 quantization to weights if requested
        with DEVICE:
            model = AutoModelForCausalLM.from_config(config, torch_dtype=torch.bfloat16)

        return model

    def generate_batch(self) -> tuple[torch.Tensor, HybridChunkedCache]:
        """Generate a batch of input tokens"""
        batch_size = self.config.batch_size
        input_length = self.config.input_length

        if hasattr(self.model, "vocab_size"):
            vocab_size = self.model.vocab_size
        elif hasattr(self.model, "config") and hasattr(self.model.config, "vocab_size"):
            vocab_size = self.model.config.vocab_size
        else:
            # Default vocabulary size for older models
            vocab_size = 32000

        input_ids = torch.randint(0, vocab_size, (batch_size, input_length), device=DEVICE)
        past_key_values = HybridChunkedCache(
            self.hf_config, input_ids.shape[0], input_ids.shape[1] + self.config.output_length
        )
        for layer_idx in range(self.hf_config.num_hidden_layers):
            # key_states.shape[1] is used to retrieve the number of key value heads, all other dimensions can be 1 and ignored
            # https://github.com/huggingface/transformers/blob/9300728665aaeb0ebf4db99f9d9fbce916b4a183/src/transformers/cache_utils.py#L1822
            past_key_values.initialise_cache_layer(
                layer_idx, torch.empty(1, self.hf_config.num_key_value_heads // WORLD_SIZE, 1, 1, device=DEVICE)
            )

        return input_ids, past_key_values

    def get_next_token(self, input_ids: torch.Tensor, past_key_values: HybridChunkedCache) -> torch.Tensor:
        outputs = self.model(input_ids, past_key_values=past_key_values, use_cache=True)
        logits = outputs.logits  # [B, seq_len, vocab_size]
        next_token_logits = logits[:, -1, :]
        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)
        return next_token

    def prefill(self, input_ids: torch.Tensor, past_key_values: HybridChunkedCache) -> torch.Tensor:
        """
        Prefill phase: Process the entire input prompt at once.
        Returns the next token.

        Similar to: https://github.com/pytorch-labs/gpt-fast/blob/main/generate.py#L68-L82
        """
        return self.get_next_token(input_ids, past_key_values)

    def decode_one_token(self, input_ids: torch.Tensor, past_key_values: HybridChunkedCache) -> torch.Tensor:
        """
        Decode phase: Generate a single token given the current sequence.
        Returns the next token.
        """
        # input_pos: [B, 1] One token at the time
        assert input_ids.shape[-1] == 1, f"Expected shape (B, 1), but found {input_ids.shape}"
        return self.get_next_token(input_ids, past_key_values)

    @torch.inference_mode()
    def generate(
        self, input_ids: torch.Tensor, max_new_tokens: int, past_key_values: HybridChunkedCache
    ) -> dict[str, Any]:
        """
        Generate tokens using separate prefill and decode phases.
        Returns detailed metrics for both phases.
        """
        # Prefill phase - process the entire prompt
        with timer() as prefill_timer:
            first_token = self.prefill(input_ids, past_key_values)
        prefill_time = prefill_timer()
        generated_tokens = [first_token]

        # Decode phase - generate remaining tokens one by one
        next_token = first_token
        with timer() as decode_timer:
            for _ in range(max_new_tokens - 1):
                next_token = self.decode_one_token(next_token, past_key_values)
                generated_tokens.append(next_token)

        total_decode_time = decode_timer()

        return {
            "prefill_time_ms": prefill_time,
            "decode_time_ms": total_decode_time,
            "generated_tokens": generated_tokens,
            "total_tokens": max_new_tokens,
        }

    def measure_inference_step(
        self, input_ids: torch.Tensor, past_key_values: HybridChunkedCache, max_new_tokens: int
    ) -> dict[str, float]:
        """Measure a single inference step with detailed timing using separate prefill/decode"""
        with timer() as total_timer:
            # Generate tokens with separate prefill/decode tracking
            generation_result = self.generate(input_ids, max_new_tokens, past_key_values)
        total_time = total_timer()

        # Extract metrics
        ttft = generation_result["prefill_time_ms"]  # Time to first token is the prefill time
        total_decode_time = generation_result["decode_time_ms"]
        avg_tbot = total_decode_time / (max_new_tokens - 1) if max_new_tokens > 1 else 0

        # Calculate throughput
        total_tokens = self.config.output_length * self.config.batch_size
        throughput = (total_tokens / total_time) * 1000  # tokens/second

        # Calculate separate prefill and decode throughput
        prefill_tokens = self.config.input_length * self.config.batch_size
        prefill_throughput = (prefill_tokens / generation_result["prefill_time_ms"]) * 1000

        decode_tokens = (self.config.output_length - 1) * self.config.batch_size
        decode_throughput = (decode_tokens / total_decode_time) * 1000 if total_decode_time > 0 else 0

        return {
            "ttft": ttft,
            "avg_tbot": avg_tbot,
            "total_time": total_time,
            "throughput": throughput,
            "prefill_throughput": prefill_throughput,
            "decode_throughput": decode_throughput,
            "prefill_time": generation_result["prefill_time_ms"],
            "total_decode_time": total_decode_time,
        }

    def _run_thunderfx_benchmark_report(self):
        print(f"Running thunderfx benchmark report for {self.config.model_name} to {self.config.fx_report_folder}")
        print(f"Batch size: {self.config.batch_size}")
        print(f"Input length: {self.config.input_length}")
        print(f"Output length: {self.config.output_length}")
        input_ids, past_key_values = self.generate_batch()
        thunderfx_benchmark_report(
            self.model,
            folder_path=self.config.fx_report_folder,
            compare_fusion=True,
        )(input_ids, past_key_values)

    def run_benchmark(self) -> InferenceMetrics:
        """Run the full benchmark and collect metrics"""
        if self.config.fx_report_folder is not None:
            self._run_thunderfx_benchmark_report()
            return
        print(f"Running inference benchmark for {self.config.model_name}")

        print(f"Batch size: {self.config.batch_size}")
        print(f"Input length: {self.config.input_length}")
        print(f"Output length: {self.config.output_length}")
        print(f"Mode: {self.config.mode}")

        print(f"\nWarming up with {WARMUP_ITERATIONS} iterations...")
        input_ids, past_key_values = self.generate_batch()

        for _ in tqdm(range(WARMUP_ITERATIONS)):
            past_key_values.reset()
            _ = self.measure_inference_step(input_ids, past_key_values, max_new_tokens=1)

        print(f"\nRunning {self.config.num_iterations} benchmark iterations...")
        all_metrics = []

        for _ in tqdm(range(self.config.num_iterations)):
            past_key_values.reset()
            iter_metrics = self.measure_inference_step(input_ids, past_key_values, self.config.output_length)
            all_metrics.append(iter_metrics)

            # Track metrics
            self.metrics.iteration_times.append(iter_metrics["total_time"])
            self.metrics.ttft_times.append(iter_metrics["ttft"])
            self.metrics.prefill_times.append(iter_metrics["prefill_time"])
            self.metrics.decode_times.append(iter_metrics["total_decode_time"])

        self._calculate_aggregate_metrics(all_metrics)

        if torch.cuda.is_available():
            self.metrics.memory_used_gb = torch.cuda.memory_allocated() / 1e9
            self.metrics.peak_memory_gb = torch.cuda.max_memory_allocated() / 1e9

        return self.metrics

    def _calculate_aggregate_metrics(self, all_metrics: list[dict[str, Any]]):
        """Calculate aggregate metrics from individual iterations"""
        # Average throughput
        throughputs = [m["throughput"] for m in all_metrics]
        self.metrics.throughput_tokens_per_sec = np.mean(throughputs)

        # Average latency
        total_times = [m["total_time"] for m in all_metrics]
        total_tokens = self.config.output_length * self.config.batch_size
        self.metrics.latency_ms_per_token = np.mean(total_times) / total_tokens

        # TTFT
        ttfts = [m["ttft"] for m in all_metrics]
        self.metrics.time_to_first_token_ms = np.mean(ttfts)

        # TBOT
        self.metrics.time_between_output_tokens_ms = np.mean([m["avg_tbot"] for m in all_metrics])

        # Total time
        self.metrics.total_time_ms = np.mean(total_times)

        # Prefill metrics
        prefill_throughputs = [m["prefill_throughput"] for m in all_metrics]
        self.metrics.prefill_throughput_tokens_per_sec = np.mean(prefill_throughputs)
        prefill_times = [m["prefill_time"] for m in all_metrics]
        self.metrics.prefill_time_ms = np.mean(prefill_times)

        # Decode metrics
        decode_throughputs = [m["decode_throughput"] for m in all_metrics]
        self.metrics.decode_throughput_tokens_per_sec = np.mean(decode_throughputs)
        decode_times = [m["total_decode_time"] for m in all_metrics]
        self.metrics.decode_time_ms = np.mean(decode_times)

    def print_results(self):
        """Print benchmark results in a formatted way"""
        print("\n" + "=" * 60)
        print(f"BENCHMARK RESULTS - {self.config.model_name} {self.config.mode}")
        if self.config.scenario:
            scenario_config = BENCHMARK_SCENARIOS[self.config.scenario]
            print(f"SCENARIO: {scenario_config['name']}")
        print("=" * 60)

        print("\nThroughput Metrics:")
        print(f"  Overall Throughput: {self.metrics.throughput_tokens_per_sec:.2f} tokens/sec")
        print(f"  Prefill Throughput: {self.metrics.prefill_throughput_tokens_per_sec:.2f} tokens/sec")
        print(f"  Decode Throughput: {self.metrics.decode_throughput_tokens_per_sec:.2f} tokens/sec")
        print(f"  Latency: {self.metrics.latency_ms_per_token:.2f} ms/token")

        print("\nLatency Breakdown:")
        print(f"  Time to First Token (TTFT): {self.metrics.time_to_first_token_ms:.2f} ms")
        print(f"  Time Between Output Tokens (TBOT): {self.metrics.time_between_output_tokens_ms:.2f} ms")
        print(f"  Prefill Time: {self.metrics.prefill_time_ms:.2f} ms")
        print(f"  Decode Time: {self.metrics.decode_time_ms:.2f} ms")
        print(f"  Total Generation Time: {self.metrics.total_time_ms:.2f} ms")

        print("\nMemory Usage:")
        print(f"  Current Memory: {self.metrics.memory_used_gb:.2f} GB")
        print(f"  Peak Memory: {self.metrics.peak_memory_gb:.2f} GB")

        if self.metrics.iteration_times:
            print("\nVariance Analysis:")
            print(f"  Throughput Std Dev: {np.std([t for t in self.metrics.iteration_times]):.2f} ms")
            print(f"  TTFT Std Dev: {np.std(self.metrics.ttft_times):.2f} ms")

    def save_results(self, filename: str):
        """Save results to JSON file"""
        results = {
            "config": self.config.__dict__,
            "metrics": {
                "throughput_tokens_per_sec": self.metrics.throughput_tokens_per_sec,
                "prefill_throughput_tokens_per_sec": self.metrics.prefill_throughput_tokens_per_sec,
                "decode_throughput_tokens_per_sec": self.metrics.decode_throughput_tokens_per_sec,
                "latency_ms_per_token": self.metrics.latency_ms_per_token,
                "time_to_first_token_ms": self.metrics.time_to_first_token_ms,
                "time_between_output_tokens_ms": self.metrics.time_between_output_tokens_ms,
                "prefill_time_ms": self.metrics.prefill_time_ms,
                "decode_time_ms": self.metrics.decode_time_ms,
                "total_time_ms": self.metrics.total_time_ms,
                "memory_used_gb": self.metrics.memory_used_gb,
                "peak_memory_gb": self.metrics.peak_memory_gb,
            },
            "detailed_metrics": {
                "iteration_times": self.metrics.iteration_times,
                "ttft_times": self.metrics.ttft_times,
                "prefill_times": self.metrics.prefill_times,
                "decode_times": self.metrics.decode_times,
            },
        }

        with open(filename, "w") as f:
            json.dump(results, f, indent=2)

        print(f"\nResults saved to {filename}")


def run_benchmark(
    *,
    model_name: str,
    batch_size: int,
    input_length: int,
    output_length: int,
    num_iterations: int,
    num_layers: int | None,
    mode: str,
    save_results: bool,
    scenario: str | None,
    dtensor_single_gpu: bool,
    load_nvfp4: bool,
    fx_report_folder: str | None,
    enable_nv_linear: bool,
):
    """Main function to run the benchmark"""

    if scenario is not None:
        if scenario not in BENCHMARK_SCENARIOS:
            raise ValueError(f"Unknown scenario '{scenario}'. Available scenarios: {list(BENCHMARK_SCENARIOS.keys())}")

        scenario_config = BENCHMARK_SCENARIOS[scenario]
        input_length = scenario_config["input_length"]
        output_length = scenario_config["output_length"]

        print(f"\nUsing standardized scenario: {scenario_config['name']}")
        print(f"Configuration: {scenario_config['description']}")

    config = InferenceBenchmarkConfig(
        model_name=model_name,
        batch_size=batch_size,
        input_length=input_length,
        output_length=output_length,
        num_layers=num_layers,
        num_iterations=num_iterations,
        mode=mode,
        scenario=scenario,
        dtensor_single_gpu=dtensor_single_gpu,
        load_nvfp4=load_nvfp4,
        fx_report_folder=fx_report_folder,
        enable_nv_linear=enable_nv_linear,
    )

    benchmark = InferenceBenchmark(config)

    metrics = benchmark.run_benchmark()
    benchmark.print_results()

    if save_results:
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        scenario_suffix = f"_{scenario}" if scenario else ""
        filename = f"thunder_inference_{model_name}_{scenario_suffix}_{timestamp}.json"
        benchmark.save_results(filename)

    return metrics


class CustomFormatter(argparse.RawDescriptionHelpFormatter, argparse.ArgumentDefaultsHelpFormatter):
    pass


def parse_args() -> argparse.Namespace:
    """Command line interface for the benchmark"""
    parser = argparse.ArgumentParser(
        description="Thunder Inference Benchmark",
        formatter_class=CustomFormatter,
        epilog="""
Standard Benchmark Scenarios:
  summarization  - Prefill-Heavy: 4,000 input → 1,000 output tokens
  chat          - Balanced: 1,000 input → 1,000 output tokens
  reasoning     - Decode-Heavy: 1,000 input → 4,000 output tokens

Examples:
  python benchmark_inference.py --input-length 2048 --output-length 512 --model-name meta-llama/Llama-4-Maverick-17B-128E --mode eager
  python benchmark_inference.py --scenario chat --model-name meta-llama/Llama-4-Maverick-17B-128E --load-nvfp4
        """,
    )

    parser.add_argument(
        "--model-name",
        type=str,
        default=LLAMA4_MAVERICK_MODEL_ID,
        help="Model to benchmark",
    )

    parser.add_argument(
        "--scenario",
        type=str,
        choices=list(BENCHMARK_SCENARIOS.keys()),
        help="Use standardized benchmark scenario. Available: "
        + ", ".join([f"{k} ({v['description'].replace('%', '%%')})" for k, v in BENCHMARK_SCENARIOS.items()])
        + ". If specified, overrides --input-length and --output-length.",
    )

    # Benchmark configuration (for custom experimentation when not using scenarios)
    parser.add_argument("--batch-size", type=int, default=1, help="Batch size for inference")
    parser.add_argument(
        "--input-length",
        type=int,
        default=2048,
        help="Input sequence length (ignored if --scenario is used)",
    )
    parser.add_argument(
        "--output-length",
        type=int,
        default=128,
        help="Output sequence length (ignored if --scenario is used)",
    )
    parser.add_argument("--num-iterations", type=int, default=100, help="Number of benchmark iterations")
    parser.add_argument("--warmup-iterations", type=int, default=10, help="Number of warmup iterations")
    parser.add_argument(
        "--num-layers",
        default=2,
        type=int,
        help="Number of layers of the moddel. Llama4 Maverick has 48 hidden layers, which could be too memory hungry",
    )

    # Execution configuration
    parser.add_argument(
        "--mode",
        type=str,
        default="eager",
        choices=("thunder", "eager", "inductor", "thunderjit"),
        help="Compilation mode: thunder, eager (default), or inductor. thunder runs thunderfx.",
    )
    parser.add_argument(
        "--fx-report-folder",
        default=None,
        type=str,
        help="Specify the folder for thunderfx_benchmark_report.",
    )

    parser.add_argument(
        "--dtensor-single-gpu",
        action="store_true",
        help="Use DTensor for single GPU",
    )
    parser.add_argument("--load-nvfp4", action="store_true", help="Enable NVFP4 quantization for linear layers")
    parser.add_argument(
        "--enable-nv-linear",
        action="store_true",
        help="let nvfuser take care of linear and matmul, note that this might fail with distributed run. See: https://github.com/NVIDIA/Fuser/issues/4507",
    )

    parser.add_argument("--save-results", action="store_true", help="Save results to JSON file")
    parser.add_argument("--output-dir", type=str, default="./results", help="Directory to save results")

    args = parser.parse_args()

    if args.load_nvfp4:
        raise NotImplementedError("Currently NVFP4 is not supported")

    return args


def main():
    args = parse_args()
    if args.save_results:
        os.makedirs(args.output_dir, exist_ok=True)

    run_benchmark(
        model_name=args.model_name,
        batch_size=args.batch_size,
        input_length=args.input_length,
        output_length=args.output_length,
        num_iterations=args.num_iterations,
        num_layers=args.num_layers,
        mode=args.mode,
        save_results=args.save_results,
        scenario=args.scenario,
        dtensor_single_gpu=args.dtensor_single_gpu,
        load_nvfp4=args.load_nvfp4,
        fx_report_folder=args.fx_report_folder,
        enable_nv_linear=args.enable_nv_linear,
    )


if __name__ == "__main__":
    try:
        main()
    except Exception:
        raise
    finally:
        if WORLD_SIZE > 1:
            for process_group in mesh.get_all_groups():
                destroy_process_group(process_group)
