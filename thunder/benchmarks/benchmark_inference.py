"""Thunder Inference Benchmark following SemiAnalysis Methodology

This benchmark implements the methodology from the SemiAnalysis article:
"AMD vs NVIDIA Inference Benchmark: Who Wins? - Performance & Cost Per Million Tokens"
https://semianalysis.com/2025/05/23/amd-vs-nvidia-inference-benchmark-who-wins-performance-cost-per-million-tokens

Models:
- DeepSeekV3 670B
- Llama 4 Scout 17B
- Llama 4 Maverick

Key metrics:
- Throughput (tokens/second)
- Latency (ms/token)
- Time to First Token (TTFT)
- Time Between Output Tokens (TBOT)
- Cost per Million Tokens
"""

from __future__ import annotations
import argparse
from contextlib import contextmanager
from dataclasses import dataclass, field
import json
import os
import time
from typing import TYPE_CHECKING

import numpy as np
import torch
from torch.distributed.device_mesh import init_device_mesh
from torch.distributed.distributed_c10d import destroy_process_group
from torch.distributed.tensor.parallel import parallelize_module, RowwiseParallel, ColwiseParallel
from tqdm import tqdm
from transformers import AutoConfig, AutoModelForCausalLM
from transformers.cache_utils import HybridChunkedCache

import thunder
from thunder.dynamo.report import thunderfx_benchmark_report

if TYPE_CHECKING:
    from typing import Any


RANK = int(os.environ.get("RANK", 0))
LOCAL_RANK = int(os.environ.get("LOCAL_RANK", 0))
WORLD_SIZE = int(os.environ.get("WORLD_SIZE", 1))
MASTER_ADDR = os.environ.get("MASTER_ADDR", "localhost")
MASTER_PORT = os.environ.get("MASTER_PORT", "29500")
os.environ["TORCH_NCCL_ASYNC_ERROR_HANDLING"] = "1"

if WORLD_SIZE > 1:
    mesh = init_device_mesh("cuda", (WORLD_SIZE,), mesh_dim_names=("tp",))
    device = torch.device("cuda", LOCAL_RANK)
    torch.cuda.set_device(device)

LLAMA4_MAVERICK_MODEL_ID: str = "meta-llama/Llama-4-Maverick-17B-128E"


@contextmanager
def timer():
    torch.cuda.synchronize()
    t1 = t2 = time.perf_counter()
    yield lambda: (t2 - t1) * 1000  # Convert to ms
    torch.cuda.synchronize()
    t2 = time.perf_counter()


# Standard benchmark scenarios following the three-scenario methodology
BENCHMARK_SCENARIOS = {
    "summarization": {
        "name": "Summarization (Prefill-Heavy)",
        "input_length": 4000,
        "output_length": 1000,
        "description": "4,000 input → 1,000 output tokens (80% prefill, 20% decode)",
        "workload_balance": "80% prefill, 20% decode computational cost",
        "hardware_focus": "Compute optimization provides maximum impact",
    },
    "chat": {
        "name": "Chat (Balanced)",
        "input_length": 1000,
        "output_length": 1000,
        "description": "1,000 input → 1,000 output tokens (50% prefill, 50% decode)",
        "workload_balance": "50% prefill, 50% decode computational cost",
        "hardware_focus": "Mixed optimization requirements",
    },
    "reasoning": {
        "name": "Reasoning (Decode-Heavy)",
        "input_length": 1000,
        "output_length": 4000,
        "description": "1,000 input → 4,000 output tokens (20% prefill, 80% decode)",
        "workload_balance": "20% prefill, 80% decode computational cost",
        "hardware_focus": "Memory bandwidth optimization dominates",
    },
}


@dataclass
class InferenceBenchmarkConfig:
    """Configuration for inference benchmarking following SemiAnalysis methodology"""

    model_name: str
    # Expected GPU memory requirements (FP16):
    # - 8B: ~16GB (suitable for local experimentation on consumer GPUs)
    # - 70B: ~140GB (requires multi-GPU setup or high-end datacenter GPUs)
    # - 405B: ~810GB (requires large multi-GPU clusters)
    # - 670B: ~1340GB (requires very large multi-GPU clusters)
    batch_size: int = 1
    input_length: int = 1024
    output_length: int = 1024
    num_layers: int | None = None
    num_iterations: int = 10
    warmup_iterations: int = 2
    device: str = "cuda"
    mode: str = "thunder"
    scenario: str | None = None  # Standard scenario name if using predefined configurations
    dtensor_single_gpu: bool = False
    load_nvfp4: bool = False  # Enable NVFP4 quantization
    fx_report_folder: str | None = None
    measure_ttft: bool = field(default=True, init=False)
    measure_tbot: bool = field(default=True, init=False)

    # Cost calculation parameters (per GPU hour) # optional
    h100_cost_per_hour: float = field(default=1.58, init=False)
    h200_cost_per_hour: float = field(default=1.63, init=False)
    b200_cost_per_hour: float = field(default=2.23, init=False)

    # Memory bandwidth and compute specs
    # TODO check correctness of numbers (generated by AI)
    gpu_specs: dict[str, dict[str, float]] = field(
        default_factory=lambda: {
            "H100": {"memory_bandwidth_gb": 3350, "fp16_tflops": 1979, "fp8_tflops": 3958},
            "H200": {"memory_bandwidth_gb": 4800, "fp16_tflops": 1979, "fp8_tflops": 3958},
            "B200": {"memory_bandwidth_gb": 8000, "fp16_tflops": 2529, "fp8_tflops": 5058},
        },
        init=False,
    )


@dataclass
class InferenceMetrics:
    """Metrics collected during inference benchmarking"""

    throughput_tokens_per_sec: float = 0.0
    latency_ms_per_token: float = 0.0
    time_to_first_token_ms: float = 0.0
    time_between_output_tokens_ms: float = 0.0
    total_time_ms: float = 0.0
    memory_used_gb: float = 0.0
    peak_memory_gb: float = 0.0
    cost_per_million_tokens: float = 0.0

    # Separate prefill and decode metrics
    prefill_throughput_tokens_per_sec: float = 0.0
    decode_throughput_tokens_per_sec: float = 0.0
    prefill_time_ms: float = 0.0
    decode_time_ms: float = 0.0

    # Per-iteration metrics for variance analysis
    iteration_times: list[float] = field(default_factory=list)
    ttft_times: list[float] = field(default_factory=list)
    prefill_times: list[float] = field(default_factory=list)
    decode_times: list[float] = field(default_factory=list)


class SemiAnalysisInferenceBenchmark:
    """Main benchmark class following SemiAnalysis methodology"""

    def __init__(self, config: InferenceBenchmarkConfig):
        self.config = config
        self.device = torch.device(config.device)
        self.metrics = InferenceMetrics()

        self.model = self._load_model()

        tp_plan = {
            "*.layers.*.self_attn.q_proj": ColwiseParallel(use_local_output=True),
            "*.layers.*.self_attn.k_proj": ColwiseParallel(use_local_output=True),
            "*.layers.*.self_attn.v_proj": ColwiseParallel(use_local_output=True),
            "*.layers.*.self_attn.o_proj": RowwiseParallel(use_local_output=True),
            "*.layers.*.feed_forward.gate_proj": ColwiseParallel(use_local_output=False),
            "*.layers.*.feed_forward.up_proj": ColwiseParallel(use_local_output=False),
            "*.layers.*.feed_forward.down_proj": RowwiseParallel(use_local_output=True),
            "*.layers.*.feed_forward.shared_expert.gate_proj": ColwiseParallel(use_local_output=False),
            "*.layers.*.feed_forward.shared_expert.up_proj": ColwiseParallel(use_local_output=False),
            "*.layers.*.feed_forward.shared_expert.down_proj": RowwiseParallel(use_local_output=True),
        }

        if self.config.dtensor_single_gpu or WORLD_SIZE > 1:
            self.model = parallelize_module(self.model, mesh, tp_plan)

            # Required as that doesn't understand inference mode
            for p in self.model.parameters():
                p.requires_grad_(False)

        self.model = self._compile_model(self.model)

    def _compile_model(self, model):
        if self.config.fx_report_folder is not None:
            return model
        match self.config.mode:
            case "eager":
                return model
            case "inductor":
                return torch.compile(model, mode="reduce-overhead")
            case "thunder":
                from thunder.dynamo import thunderfx

                # Set `nv_enable_linear` to True
                # once workaround is added for https://github.com/NVIDIA/Fuser/issues/4507
                return thunderfx(model, nv_enable_linear=False)
            case "thunderjit":
                # Set `nv_enable_linear` to True
                # once workaround is added for https://github.com/NVIDIA/Fuser/issues/4507
                return thunder.jit(model, nv_enable_linear=False)
            case _:
                raise ValueError(f"Unknown mode: {self.config.mode}")

    def _load_model(self) -> torch.nn.Module:
        """Load the model based on configuration"""
        model_id = self.config.model_name
        config = AutoConfig.from_pretrained(model_id)

        if hasattr(config, "text_config"):
            config = config.text_config
        if self.config.num_layers:
            config.num_hidden_layers = self.config.num_layers

        self.hf_config = config

        # TODO: Apply NVFP4 quantization to weights if requested
        with torch.device(self.device):
            model = AutoModelForCausalLM.from_config(config, torch_dtype=torch.bfloat16)

        return model

    def generate_batch(self) -> tuple[torch.Tensor, HybridChunkedCache]:
        """Generate a batch of input tokens"""
        batch_size = self.config.batch_size
        input_length = self.config.input_length

        if hasattr(self.model, "vocab_size"):
            vocab_size = self.model.vocab_size
        elif hasattr(self.model, "config") and hasattr(self.model.config, "vocab_size"):
            vocab_size = self.model.config.vocab_size
        else:
            # Default vocabulary size for older models
            vocab_size = 32000

        input_ids = torch.randint(0, vocab_size, (batch_size, input_length), device=self.device)
        past_key_values = HybridChunkedCache(
            self.hf_config, input_ids.shape[0], input_ids.shape[1] + self.config.output_length
        )
        for layer_idx in range(self.hf_config.num_hidden_layers):
            # key_states.shape[1] is used to retrieve the number of key value heads, all other dimensions can be 1 and ignored
            # https://github.com/huggingface/transformers/blob/9300728665aaeb0ebf4db99f9d9fbce916b4a183/src/transformers/cache_utils.py#L1822
            past_key_values.initialise_cache_layer(
                layer_idx, torch.empty(1, self.hf_config.num_key_value_heads // WORLD_SIZE, 1, 1, device=self.device)
            )

        return input_ids, past_key_values

    def get_next_token(self, input_ids: torch.Tensor, past_key_values: HybridChunkedCache) -> torch.Tensor:
        outputs = self.model(input_ids, past_key_values=past_key_values, use_cache=True)
        logits = outputs.logits  # [B, seq_len, vocab_size]
        next_token_logits = logits[:, -1, :]
        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)
        return next_token

    def prefill(self, input_ids: torch.Tensor, past_key_values: HybridChunkedCache) -> torch.Tensor:
        """
        Prefill phase: Process the entire input prompt at once.
        Returns the next token.

        Similar to: https://github.com/pytorch-labs/gpt-fast/blob/main/generate.py#L68-L82
        """
        return self.get_next_token(input_ids, past_key_values)

    def decode_one_token(self, input_ids: torch.Tensor, past_key_values: HybridChunkedCache) -> torch.Tensor:
        """
        Decode phase: Generate a single token given the current sequence.
        Returns the next token.
        """
        # input_pos: [B, 1] One token at the time
        assert input_ids.shape[-1] == 1, f"Expected shape (B, 1), but found {input_ids.shape}"
        return self.get_next_token(input_ids, past_key_values)

    @torch.inference_mode()
    def generate(
        self, input_ids: torch.Tensor, max_new_tokens: int, past_key_values: HybridChunkedCache
    ) -> dict[str, Any]:
        """
        Generate tokens using separate prefill and decode phases.
        Returns detailed metrics for both phases.
        """
        # Prefill phase - process the entire prompt
        with timer() as prefill_timer:
            first_token = self.prefill(input_ids, past_key_values)
        prefill_time = prefill_timer()
        generated_tokens = [first_token]

        # Decode phase - generate remaining tokens one by one
        next_token = first_token
        with timer() as decode_timer:
            for _ in range(max_new_tokens - 1):
                next_token = self.decode_one_token(next_token, past_key_values)
                generated_tokens.append(next_token)

        total_decode_time = decode_timer()

        return {
            "prefill_time_ms": prefill_time,
            "decode_time_ms": total_decode_time,
            "generated_tokens": generated_tokens,
            "total_tokens": max_new_tokens,
        }

    def measure_inference_step(
        self, input_ids: torch.Tensor, past_key_values: HybridChunkedCache, max_new_tokens: int
    ) -> dict[str, float]:
        """Measure a single inference step with detailed timing using separate prefill/decode"""
        with timer() as total_timer:
            # Generate tokens with separate prefill/decode tracking
            generation_result = self.generate(input_ids, max_new_tokens, past_key_values)
        total_time = total_timer()

        # Extract metrics
        ttft = generation_result["prefill_time_ms"]  # Time to first token is the prefill time
        total_decode_time = generation_result["decode_time_ms"]
        avg_tbot = total_decode_time / (max_new_tokens - 1) if max_new_tokens > 1 else 0

        # Calculate throughput
        total_tokens = self.config.output_length * self.config.batch_size
        throughput = (total_tokens / total_time) * 1000  # tokens/second

        # Calculate separate prefill and decode throughput
        prefill_tokens = self.config.input_length * self.config.batch_size
        prefill_throughput = (prefill_tokens / generation_result["prefill_time_ms"]) * 1000

        decode_tokens = (self.config.output_length - 1) * self.config.batch_size
        decode_throughput = (decode_tokens / total_decode_time) * 1000 if total_decode_time > 0 else 0

        return {
            "ttft": ttft,
            "avg_tbot": avg_tbot,
            "total_time": total_time,
            "throughput": throughput,
            "prefill_throughput": prefill_throughput,
            "decode_throughput": decode_throughput,
            "prefill_time": generation_result["prefill_time_ms"],
            "total_decode_time": total_decode_time,
        }

    def _run_thunderfx_benchmark_report(self):
        print(f"Running thunderfx benchmark report for {self.config.model_name} to {self.config.fx_report_folder}")
        print(f"Batch size: {self.config.batch_size}")
        print(f"Input length: {self.config.input_length}")
        print(f"Output length: {self.config.output_length}")
        print(f"Device: {self.device}")
        input_ids, past_key_values = self.generate_batch()
        thunderfx_benchmark_report(
            self.model,
            folder_path=self.config.fx_report_folder,
            compare_fusion=True,
        )(input_ids, past_key_values)

    def run_benchmark(self) -> InferenceMetrics:
        """Run the full benchmark and collect metrics"""
        if self.config.fx_report_folder is not None:
            self._run_thunderfx_benchmark_report()
            return
        print(f"Running inference benchmark for {self.config.model_name}")

        print(f"Batch size: {self.config.batch_size}")
        print(f"Input length: {self.config.input_length}")
        print(f"Output length: {self.config.output_length}")
        print(f"Device: {self.device}")
        print(f"Mode: {self.config.mode}")

        print(f"\nWarming up with {self.config.warmup_iterations} iterations...")
        input_ids, past_key_values = self.generate_batch()

        for _ in tqdm(range(self.config.warmup_iterations)):
            past_key_values.reset()
            _ = self.measure_inference_step(input_ids, past_key_values, max_new_tokens=1)

        print(f"\nRunning {self.config.num_iterations} benchmark iterations...")
        all_metrics = []

        for _ in tqdm(range(self.config.num_iterations)):
            past_key_values.reset()
            iter_metrics = self.measure_inference_step(input_ids, past_key_values, self.config.output_length)
            all_metrics.append(iter_metrics)

            # Track metrics
            self.metrics.iteration_times.append(iter_metrics["total_time"])
            self.metrics.ttft_times.append(iter_metrics["ttft"])
            self.metrics.prefill_times.append(iter_metrics["prefill_time"])
            self.metrics.decode_times.append(iter_metrics["total_decode_time"])

        self._calculate_aggregate_metrics(all_metrics)

        self._calculate_cost_metrics()

        if torch.cuda.is_available():
            self.metrics.memory_used_gb = torch.cuda.memory_allocated() / 1e9
            self.metrics.peak_memory_gb = torch.cuda.max_memory_allocated() / 1e9

        return self.metrics

    def _calculate_aggregate_metrics(self, all_metrics: list[dict[str, Any]]):
        """Calculate aggregate metrics from individual iterations"""
        # Average throughput
        throughputs = [m["throughput"] for m in all_metrics]
        self.metrics.throughput_tokens_per_sec = np.mean(throughputs)

        # Average latency
        total_times = [m["total_time"] for m in all_metrics]
        total_tokens = self.config.output_length * self.config.batch_size
        self.metrics.latency_ms_per_token = np.mean(total_times) / total_tokens

        # TTFT
        ttfts = [m["ttft"] for m in all_metrics]
        self.metrics.time_to_first_token_ms = np.mean(ttfts)

        # TBOT
        self.metrics.time_between_output_tokens_ms = np.mean([m["avg_tbot"] for m in all_metrics])

        # Total time
        self.metrics.total_time_ms = np.mean(total_times)

        # Prefill metrics
        prefill_throughputs = [m["prefill_throughput"] for m in all_metrics]
        self.metrics.prefill_throughput_tokens_per_sec = np.mean(prefill_throughputs)
        prefill_times = [m["prefill_time"] for m in all_metrics]
        self.metrics.prefill_time_ms = np.mean(prefill_times)

        # Decode metrics
        decode_throughputs = [m["decode_throughput"] for m in all_metrics]
        self.metrics.decode_throughput_tokens_per_sec = np.mean(decode_throughputs)
        decode_times = [m["total_decode_time"] for m in all_metrics]
        self.metrics.decode_time_ms = np.mean(decode_times)

    def _calculate_cost_metrics(self):
        """Calculate cost per million tokens based on GPU type and usage"""
        # Detect GPU type (simplified - in real scenario would use actual detection)
        gpu_name = torch.cuda.get_device_name() if torch.cuda.is_available() else "Unknown"

        # Map to cost (simplified mapping)
        if "H100" in gpu_name:
            cost_per_hour = self.config.h100_cost_per_hour
        elif "H200" in gpu_name:
            cost_per_hour = self.config.h200_cost_per_hour
        elif "B200" in gpu_name:
            cost_per_hour = self.config.b200_cost_per_hour
        else:
            cost_per_hour = self.config.h100_cost_per_hour  # Default

        # Calculate cost per million tokens
        tokens_per_hour = self.metrics.throughput_tokens_per_sec * 3600
        if tokens_per_hour > 0:
            self.metrics.cost_per_million_tokens = (cost_per_hour / tokens_per_hour) * 1_000_000

    def print_results(self):
        """Print benchmark results in a formatted way"""
        print("\n" + "=" * 60)
        print(f"BENCHMARK RESULTS - {self.config.model_name} {self.config.mode}")
        if self.config.scenario:
            scenario_config = BENCHMARK_SCENARIOS[self.config.scenario]
            print(f"SCENARIO: {scenario_config['name']}")
        print("=" * 60)

        print("\nThroughput Metrics:")
        print(f"  Overall Throughput: {self.metrics.throughput_tokens_per_sec:.2f} tokens/sec")
        print(f"  Prefill Throughput: {self.metrics.prefill_throughput_tokens_per_sec:.2f} tokens/sec")
        print(f"  Decode Throughput: {self.metrics.decode_throughput_tokens_per_sec:.2f} tokens/sec")
        print(f"  Latency: {self.metrics.latency_ms_per_token:.2f} ms/token")

        print("\nLatency Breakdown:")
        print(f"  Time to First Token (TTFT): {self.metrics.time_to_first_token_ms:.2f} ms")
        print(f"  Time Between Output Tokens (TBOT): {self.metrics.time_between_output_tokens_ms:.2f} ms")
        print(f"  Prefill Time: {self.metrics.prefill_time_ms:.2f} ms")
        print(f"  Decode Time: {self.metrics.decode_time_ms:.2f} ms")
        print(f"  Total Generation Time: {self.metrics.total_time_ms:.2f} ms")

        print("\nMemory Usage:")
        print(f"  Current Memory: {self.metrics.memory_used_gb:.2f} GB")
        print(f"  Peak Memory: {self.metrics.peak_memory_gb:.2f} GB")

        print("\nCost Analysis:")
        print(f"  Cost per Million Tokens: ${self.metrics.cost_per_million_tokens:.4f}")

        if self.metrics.iteration_times:
            print("\nVariance Analysis:")
            print(f"  Throughput Std Dev: {np.std([t for t in self.metrics.iteration_times]):.2f} ms")
            print(f"  TTFT Std Dev: {np.std(self.metrics.ttft_times):.2f} ms")

    def save_results(self, filename: str):
        """Save results to JSON file"""
        results = {
            "config": self.config.__dict__,
            "metrics": {
                "throughput_tokens_per_sec": self.metrics.throughput_tokens_per_sec,
                "prefill_throughput_tokens_per_sec": self.metrics.prefill_throughput_tokens_per_sec,
                "decode_throughput_tokens_per_sec": self.metrics.decode_throughput_tokens_per_sec,
                "latency_ms_per_token": self.metrics.latency_ms_per_token,
                "time_to_first_token_ms": self.metrics.time_to_first_token_ms,
                "time_between_output_tokens_ms": self.metrics.time_between_output_tokens_ms,
                "prefill_time_ms": self.metrics.prefill_time_ms,
                "decode_time_ms": self.metrics.decode_time_ms,
                "total_time_ms": self.metrics.total_time_ms,
                "memory_used_gb": self.metrics.memory_used_gb,
                "peak_memory_gb": self.metrics.peak_memory_gb,
                "cost_per_million_tokens": self.metrics.cost_per_million_tokens,
            },
            "detailed_metrics": {
                "iteration_times": self.metrics.iteration_times,
                "ttft_times": self.metrics.ttft_times,
                "prefill_times": self.metrics.prefill_times,
                "decode_times": self.metrics.decode_times,
            },
        }

        with open(filename, "w") as f:
            json.dump(results, f, indent=2)

        print(f"\nResults saved to {filename}")


def run_semianalysis_benchmark(
    model_name: str = LLAMA4_MAVERICK_MODEL_ID,
    batch_size: int = 1,
    input_length: int = 1024,  # default 1k -> 1k
    output_length: int = 1024,  # default 1k -> 1k
    num_iterations: int = 100,
    num_layers: int | None = None,
    mode: str = "thunder",
    save_results: bool = True,
    scenario: str | None = None,
    dtensor_single_gpu: bool = False,
    load_nvfp4: bool = False,
    fx_report_folder: str | None = None,
):
    """Main function to run the benchmark"""

    if scenario is not None:
        if scenario not in BENCHMARK_SCENARIOS:
            raise ValueError(f"Unknown scenario '{scenario}'. Available scenarios: {list(BENCHMARK_SCENARIOS.keys())}")

        scenario_config = BENCHMARK_SCENARIOS[scenario]
        input_length = scenario_config["input_length"]
        output_length = scenario_config["output_length"]

        print(f"\nUsing standardized scenario: {scenario_config['name']}")
        print(f"Configuration: {scenario_config['description']}")
        print(f"Workload balance: {scenario_config['workload_balance']}")
        print(f"Hardware focus: {scenario_config['hardware_focus']}")

    config = InferenceBenchmarkConfig(
        model_name=model_name,
        batch_size=batch_size,
        input_length=input_length,
        output_length=output_length,
        num_layers=num_layers,
        num_iterations=num_iterations,
        mode=mode,
        scenario=scenario,
        dtensor_single_gpu=dtensor_single_gpu,
        load_nvfp4=load_nvfp4,
        fx_report_folder=fx_report_folder,
    )

    benchmark = SemiAnalysisInferenceBenchmark(config)

    metrics = benchmark.run_benchmark()
    benchmark.print_results()

    if save_results:
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        scenario_suffix = f"_{scenario}" if scenario else ""
        filename = f"thunder_semianalysis_{model_name}_{scenario_suffix}_{timestamp}.json"
        benchmark.save_results(filename)

    return metrics


def list_scenarios():
    """Print available benchmark scenarios"""
    print("\nAvailable Standard Benchmark Scenarios:")
    print("=" * 50)
    for key, config in BENCHMARK_SCENARIOS.items():
        print(f"\n{key.upper()}:")
        print(f"  Name: {config['name']}")
        print(f"  Configuration: {config['description']}")
        print(f"  Workload Balance: {config['workload_balance']}")
        print(f"  Hardware Focus: {config['hardware_focus']}")
    print("\n" + "=" * 50)
    print("Use --scenario <scenario_name> to select a standard scenario")
    print("Or use --input-length and --output-length for custom configurations")


class CustomFormatter(argparse.RawDescriptionHelpFormatter, argparse.ArgumentDefaultsHelpFormatter):
    pass


def parse_args() -> argparse.Namespace:
    """Command line interface for the benchmark"""
    parser = argparse.ArgumentParser(
        description="Thunder Inference Benchmark following SemiAnalysis Methodology",
        formatter_class=CustomFormatter,
        epilog="""
Standard Benchmark Scenarios:
  summarization  - Prefill-Heavy: 4,000 input → 1,000 output tokens
  chat          - Balanced: 1,000 input → 1,000 output tokens
  reasoning     - Decode-Heavy: 1,000 input → 4,000 output tokens

Use --list-scenarios for detailed scenario descriptions.

Examples:
  python benchmark_inference.py --input-length 2048 --output-length 512 --model-name meta-llama/Llama-4-Maverick-17B-128E --mode eager
  python benchmark_inference.py --scenario chat --model-name meta-llama/Llama-4-Maverick-17B-128E --load-nvfp4
        """,
    )

    parser.add_argument(
        "--model-name",
        type=str,
        default=LLAMA4_MAVERICK_MODEL_ID,  # Small model so it's easier to iterate locally.
        help="Model to benchmark",
    )

    parser.add_argument(
        "--scenario",
        type=str,
        choices=list(BENCHMARK_SCENARIOS.keys()),
        help="Use standardized benchmark scenario. Available: "
        + ", ".join([f"{k} ({v['description'].replace('%', '%%')})" for k, v in BENCHMARK_SCENARIOS.items()])
        + ". If specified, overrides --input-length and --output-length.",
    )

    # Benchmark configuration (for custom experimentation when not using scenarios)
    parser.add_argument("--batch-size", type=int, default=1, help="Batch size for inference")
    parser.add_argument(
        "--input-length",
        type=int,
        default=2048,
        help="Input sequence length (ignored if --scenario is used)",
    )
    parser.add_argument(
        "--output-length",
        type=int,
        default=128,
        help="Output sequence length (ignored if --scenario is used)",
    )
    parser.add_argument("--num-iterations", type=int, default=100, help="Number of benchmark iterations")
    parser.add_argument("--warmup-iterations", type=int, default=10, help="Number of warmup iterations")
    parser.add_argument(
        "--num-layers",
        default=2,
        type=int,
        help="Number of layers of the moddel. Llama4 Maverick has 48 hidden layers, which could be too memory hungry",
    )

    # Execution configuration
    parser.add_argument(
        "--mode",
        type=str,
        default="eager",
        choices=("thunder", "eager", "inductor", "thunderjit"),
        help="Compilation mode: thunder, eager (default), or inductor. thunder runs thunderfx.",
    )
    parser.add_argument(
        "--fx-report-folder",
        default=None,
        type=str,
        help="Specify the folder for thunderfx_benchmark_report.",
    )

    parser.add_argument(
        "--dtensor-single-gpu",
        action="store_true",
        help="Use DTensor for single GPU",
    )
    parser.add_argument("--load-nvfp4", action="store_true", help="Enable NVFP4 quantization for linear layers")

    parser.add_argument("--save-results", action="store_true", help="Save results to JSON file")
    parser.add_argument("--output-dir", type=str, default="./results", help="Directory to save results")
    parser.add_argument(
        "--list-scenarios",
        action="store_true",
        help="List available standard benchmark scenarios and exit",
    )

    args = parser.parse_args()

    if args.load_nvfp4:
        raise NotImplementedError("Currently NVFP4 is not supported")

    if args.list_scenarios:
        list_scenarios()
        return None
    return args


def main():
    args = parse_args()
    if args.save_results:
        os.makedirs(args.output_dir, exist_ok=True)

    run_semianalysis_benchmark(
        model_name=args.model_name,
        batch_size=args.batch_size,
        input_length=args.input_length,
        output_length=args.output_length,
        num_iterations=args.num_iterations,
        num_layers=args.num_layers,
        mode=args.mode,
        save_results=args.save_results,
        scenario=args.scenario,
        dtensor_single_gpu=args.dtensor_single_gpu,
        load_nvfp4=args.load_nvfp4,
        fx_report_folder=args.fx_report_folder,
    )


if __name__ == "__main__":
    try:
        main()
    except Exception:
        raise
    finally:
        if WORLD_SIZE > 1:
            for process_group in mesh.get_all_groups():
                destroy_process_group(process_group)
