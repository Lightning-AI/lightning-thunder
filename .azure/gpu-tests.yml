trigger:
  tags:
    include: ["*"]
  branches:
    include:
      - "main"
      - "release/*"
      - "refs/tags/*"

pr:
  branches:
    include: ["*"]

jobs:
  - job: testing
    strategy:
      matrix:
        # CUDA 12.1
        "ubuntu22.04 | cuda 12.1 | python 3.10 | torch 2.5.1 | main":
          docker-image: "ubuntu22.04-cuda12.4.1-cudnn-fe1.10.0-py3.10-pt_2.6.0-dev"
          CUDA_VERSION_MM: "121"
          testing: "main"
        "ubuntu22.04 | cuda 12.1 | python 3.10 | torch 2.5.1 | ops":
          docker-image: "ubuntu22.04-cuda12.4.1-cudnn-fe1.10.0-py3.10-pt_2.6.0-dev"
          CUDA_VERSION_MM: "121"
          testing: "ops"
        "ubuntu22.04 | cuda 12.1 | python 3.10 | torch 2.5.1 | grads":
          docker-image: "ubuntu22.04-cuda12.4.1-cudnn-fe1.10.0-py3.10-pt_2.6.0-dev"
          CUDA_VERSION_MM: "121"
          testing: "grads"
        "ubuntu22.04 | cuda 12.1 | python 3.10 | torch 2.5.1 | distributed":
          docker-image: "ubuntu22.04-cuda12.4.1-cudnn-fe1.10.0-py3.10-pt_2.6.0-dev"
          CUDA_VERSION_MM: "121"
          testing: "distributed"
        "ubuntu22.04 | cuda 12.1 | python 3.10 | torch-nightly | main":
          docker-image: "ubuntu22.04-cuda12.4.1-cudnn-fe1.10.0-py3.10-pt_main-dev"
          CUDA_VERSION_MM: "121"
          testing: "main"
        "ubuntu22.04 | cuda 12.1 | python 3.10 | torch-nightly | ops":
          docker-image: "ubuntu22.04-cuda12.4.1-cudnn-fe1.10.0-py3.10-pt_main-dev"
          CUDA_VERSION_MM: "121"
          testing: "ops"
        "ubuntu22.04 | cuda 12.1 | python 3.10 | torch-nightly | grads":
          docker-image: "ubuntu22.04-cuda12.4.1-cudnn-fe1.10.0-py3.10-pt_main-dev"
          CUDA_VERSION_MM: "121"
          testing: "grads"
        "ubuntu22.04 | cuda 12.1 | python 3.10 | torch-nightly | distributed":
          docker-image: "ubuntu22.04-cuda12.4.1-cudnn-fe1.10.0-py3.10-pt_main-dev"
          CUDA_VERSION_MM: "121"
          testing: "distributed"
    # how much time to give 'run always even if cancelled tasks' before stopping them
    cancelTimeoutInMinutes: "2"
    pool: "lit-rtx-3090"
    variables:
      DEVICES: $( python -c 'name = "$(Agent.Name)" ; gpus = name.split("_")[-1] if "_" in name else "0"; print(gpus)' )
      TORCH_HOME: "/var/tmp/torch"
      PIP_CACHE_DIR: "/var/tmp/pip"
      PYTHONHASHSEED: "0"
      CI: "true"
    container:
      image: "pytorchlightning/lightning-thunder:$(docker-image)"
      options: "--gpus=all --shm-size=16g -v /var/tmp:/var/tmp"
    workspace:
      clean: all
    steps:
      - bash: |
          echo $(DEVICES)
          echo "CUDA_VERSION_MM=$CUDA_VERSION_MM"
          lspci | egrep 'VGA|3D'
          whereis nvidia
          nvidia-smi
          which python && which pip
          python --version
          pip --version
          pip list
          echo "##vso[task.setvariable variable=CUDA_VISIBLE_DEVICES]$(DEVICES)"
        displayName: "Image info & NVIDIA"

      - bash: |
          # drop pt from requirements so not to interfere with the existing one
          bash .azure/remove-torch-lines.sh requirements/base.txt
          cat requirements/base.txt

          # double check on test requirements
          pip install -r requirements/test.txt

          # https://docs.codecov.com/docs/codecov-uploader
          curl -Os https://uploader.codecov.io/latest/linux/codecov
          chmod +x codecov

          # install this package
          python setup.py develop
        displayName: "Install package & ..."

      - bash: bash .azure/sanity-check.sh
        displayName: "Sanity check / details"

      - bash: |
          set -ex
          export CUDA_LAUNCH_BLOCKING=1
          coverage run --source thunder -m \
            pytest thunder/tests/ \
              -m "not standalone" \
              -v --datefmt="%Y%m%d-%H:%M:%S.%f" \
              --random-order-seed=42 \
              --durations=250 \
              --timeout=240 \
              --numprocesses=9 \
              --ignore=thunder/tests/distributed --ignore=thunder/tests/test_networks.py \
              --ignore=thunder/tests/test_ops.py --ignore=thunder/tests/test_grad.py
          # compile coverage results
          python -m coverage report
          python -m coverage xml
          # upload to codecov
          ./codecov --token=$(CODECOV_TOKEN) --commit=$(Build.SourceVersion) \
            --flags=gpu,pytest,regular --name="GPU-coverage" --env=linux,azure
        condition: eq(variables['testing'], 'main')
        timeoutInMinutes: "40"
        displayName: "Testing: main"

      - bash: |
          set -ex
          # these test need to run in single thread as they occurs with CUDA OOM
          coverage run --source thunder -m \
             pytest \
               thunder/tests/test_networks.py \
               -m "not standalone" \
               -v --durations=0 \
               --random-order-seed=42 \
               --numprocesses=3
          # compile coverage results
          python -m coverage report
          python -m coverage xml
          # upload to codecov
          ./codecov --token=$(CODECOV_TOKEN) --commit=$(Build.SourceVersion) \
            --flags=gpu,pytest,networks --name="GPU-coverage" --env=linux,azure
        condition: eq(variables['testing'], 'main')
        timeoutInMinutes: "15"
        displayName: "Testing: networks"

      - bash: |
          set -ex
          export CUDA_LAUNCH_BLOCKING=1
          coverage run --source thunder -m \
            pytest thunder/tests/test_ops.py \
              -m "not standalone" \
              -v --datefmt="%Y%m%d-%H:%M:%S.%f" \
              --random-order-seed=42 \
              --durations=250 \
              --timeout=240 \
              --numprocesses=9
          # compile coverage results
          python -m coverage report
          python -m coverage xml
          # upload to codecov
          ./codecov --token=$(CODECOV_TOKEN) --commit=$(Build.SourceVersion) \
            --flags=gpu,pytest,regular --name="GPU-coverage" --env=linux,azure
        condition: eq(variables['testing'], 'ops')
        timeoutInMinutes: "40"
        displayName: "Testing: ops"

      - bash: |
          set -ex
          export CUDA_LAUNCH_BLOCKING=1
          coverage run --source thunder -m \
            pytest thunder/tests/test_grad.py \
              -m "not standalone" \
              -v --datefmt="%Y%m%d-%H:%M:%S.%f" \
              --random-order-seed=42 \
              --durations=250 \
              --timeout=240 \
              --numprocesses=9
          # compile coverage results
          python -m coverage report
          python -m coverage xml
          # upload to codecov
          ./codecov --token=$(CODECOV_TOKEN) --commit=$(Build.SourceVersion) \
            --flags=gpu,pytest,regular --name="GPU-coverage" --env=linux,azure
        condition: eq(variables['testing'], 'grads')
        timeoutInMinutes: "40"
        displayName: "Testing: grads"

      - bash: |
          set -ex
          # run all found tests in given past as standalone
          pytest \
             thunder/tests/distributed \
             -v --durations=0 \
             --random-order-seed=42
          # compile coverage results
          # TODO: collect and merge reports
          #  python -m coverage report
          #  python -m coverage xml
          #  # upload to codecov
          #  ./codecov --token=$(CODECOV_TOKEN) --commit=$(Build.SourceVersion) \
          #    --flags=gpu,pytest,distributed --name="GPU-coverage" --env=linux,azure
        condition: eq(variables['testing'], 'distributed')
        timeoutInMinutes: "30"
        displayName: "Testing: distributed"
