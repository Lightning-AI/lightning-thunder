trigger:
  tags:
    include: ['*']
  branches:
    include:
      - "main"
      - "release/*"
      - "refs/tags/*"

pr:
  branches:
    include: ['*']

jobs:
  - job: testing
    strategy:
      matrix:
        # CUDA 12.1
        'ubuntu22.04 | cuda 12.1 | python 3.10 | torch 2.2 | regular':
          docker-image: 'ubuntu22.04-cuda12.1.1-cudnn-fe1.2.1-py3.10-pt_2.2.1-apex'
          CUDA_VERSION_MM: '121'
        'ubuntu22.04 | cuda 12.1 | python 3.10 | torch 2.2 | distributed':
          docker-image: 'ubuntu22.04-cuda12.1.1-cudnn-fe1.2.1-py3.10-pt_2.2.1-apex'
          CUDA_VERSION_MM: '121'
          testing: 'distributed'
        'ubuntu22.04 | cuda 12.1 | python 3.10 | torch 2.3 | regular':
          docker-image: 'ubuntu22.04-cuda12.1.1-cudnn-fe1.2.1-py3.10-pt_2.3.0-apex'
          CUDA_VERSION_MM: '121'
        'ubuntu22.04 | cuda 12.1 | python 3.10 | torch 2.3 | distributed':
          docker-image: 'ubuntu22.04-cuda12.1.1-cudnn-fe1.2.1-py3.10-pt_2.3.0-apex'
          CUDA_VERSION_MM: '121'
          testing: 'distributed'
        'ubuntu22.04 | cuda 12.1 | python 3.10 | torch-nightly | regular':
          docker-image: 'ubuntu22.04-cuda12.1.1-cudnn-fe1.2.1-py3.10-pt_main-apex'
          CUDA_VERSION_MM: '121'
        'ubuntu22.04 | cuda 12.1 | python 3.10 | torch-nightly | distributed':
          docker-image: 'ubuntu22.04-cuda12.1.1-cudnn-fe1.2.1-py3.10-pt_main-apex'
          CUDA_VERSION_MM: '121'
          testing: 'distributed'
    # how much time to give 'run always even if cancelled tasks' before stopping them
    cancelTimeoutInMinutes: "2"
    pool: "lit-rtx-3090"
    variables:
      DEVICES: $( python -c 'name = "$(Agent.Name)" ; gpus = name.split("_")[-1] if "_" in name else "0"; print(gpus)' )
      TORCH_HOME: "/var/tmp/torch"
      PIP_CACHE_DIR: "/var/tmp/pip"
      PYTHONHASHSEED: "0"
      CI: "true"
    container:
      image: "pytorchlightning/lightning-thunder:$(docker-image)"
      options: "--gpus=all --shm-size=16g -v /var/tmp:/var/tmp"
    workspace:
      clean: all
    steps:

    - bash: |
        echo $(DEVICES)
        echo "CUDA_VERSION_MM=$CUDA_VERSION_MM"
        lspci | egrep 'VGA|3D'
        whereis nvidia
        nvidia-smi
        which python && which pip
        python --version
        pip --version
        pip list
        echo "##vso[task.setvariable variable=CUDA_VISIBLE_DEVICES]$(DEVICES)"
      displayName: 'Image info & NVIDIA'

    - bash: |
        # drop pt from requirements so not to interfere with the existing one
        bash .azure/remove-torch-lines.sh requirements/base.txt
        cat requirements/base.txt

        # double check on test requirements
        pip install -r requirements/test.txt

        # https://docs.codecov.com/docs/codecov-uploader
        curl -Os https://uploader.codecov.io/latest/linux/codecov
        chmod +x codecov

        # install this package
        python setup.py develop
      displayName: 'Install package & ...'

    - bash: bash .azure/sanity-check.sh
      displayName: 'Sanity check / details'

    - bash: |
        set -ex
        coverage run --source thunder -m \
          pytest thunder/tests/ \
            -m "not standalone" \
            -v --datefmt="%Y%m%d-%H:%M:%S.%f" \
            --timeout=240 \
            --random-order-seed=42 \
            --durations=250 \
            --timeout=240 \
            --numprocesses=9 \
            --ignore=thunder/tests/distributed --ignore=thunder/tests/test_networks.py
        # compile coverage results
        python -m coverage report
        python -m coverage xml
        # upload to codecov
        ./codecov --token=$(CODECOV_TOKEN) --commit=$(Build.SourceVersion) \
          --flags=gpu,pytest,regular --name="GPU-coverage" --env=linux,azure
      condition: ne(variables['testing'], 'distributed')
      timeoutInMinutes: "30"
      displayName: 'Testing: regular'

    - bash: |
        set -ex
        # these test need to run in single thread as they occurs with CUDA OOM
        coverage run --source thunder -m \
           pytest \
             thunder/tests/test_networks.py \
             -m "not standalone" \
             -v --durations=0 \
             --random-order-seed=42 \
             --numprocesses=3
        # compile coverage results
        python -m coverage report
        python -m coverage xml
        # upload to codecov
        ./codecov --token=$(CODECOV_TOKEN) --commit=$(Build.SourceVersion) \
          --flags=gpu,pytest,networks --name="GPU-coverage" --env=linux,azure
      condition: ne(variables['testing'], 'distributed')
      timeoutInMinutes: "15"
      displayName: 'Testing: networks'

    #- bash: |
    #    bash .azure/run_standalone_tests.sh \
    #      "thunder/tests" \
    #      "-m standalone --ignore=thunder/tests/distributed"
    #  condition: ne(variables['testing'], 'distributed')
    #  displayName: 'Testing: standalone'

    - bash: |
        set -ex
        # run all found tests in given past as standalone
        bash scripts/run_standalone_tests.sh "thunder/tests/distributed"
        # compile coverage results
        # TODO: collect and merge reports
        #  python -m coverage report
        #  python -m coverage xml
        #  # upload to codecov
        #  ./codecov --token=$(CODECOV_TOKEN) --commit=$(Build.SourceVersion) \
        #    --flags=gpu,pytest,distributed --name="GPU-coverage" --env=linux,azure
      condition: eq(variables['testing'], 'distributed')
      timeoutInMinutes: "25"
      displayName: 'Testing: distributed'

    # todo (mruberry): decide whether this should be here or in another workflow
    #- bash: |
    #     python benchmarks/ops_benchmark.py nanogpt-gelu
    #     python benchmarks/nvfuser_benchmarks.py nanogpt-mlp -x thunder
    #     python benchmarks/nvfuser_benchmarks.py nanogpt-gelu -x thunder
    #  displayName: 'Benchmarks'
