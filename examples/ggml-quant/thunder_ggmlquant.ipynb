{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2db72f9",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Loading GGML / Ollama weights into LitGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3b8246e",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import thunder\n",
    "import torch\n",
    "import ggmltensor\n",
    "\n",
    "\n",
    "def load_ggml_weights(model, fn):\n",
    "    ggml_quant = ggmltensor.GgmlDataReader(fn)\n",
    "\n",
    "    for n, p in model.named_parameters():\n",
    "        qw, (typ, shape) = ggml_quant.get_parameter(n)\n",
    "        with torch.no_grad():\n",
    "            w = ggmltensor.dequantize(qw, typ, shape, dtype=p.dtype).to(p.device)\n",
    "            p.copy_(w.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51d82e38-ddcc-443b-b846-1d607404d7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright Lightning AI. Licensed under the Apache License 2.0, see LICENSE file.\n",
    "\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, Literal, Optional\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "import torch._dynamo.config\n",
    "import torch._inductor.config\n",
    "\n",
    "# from lightning.fabric.plugins import BitsandbytesPrecision\n",
    "\n",
    "from litgpt import GPT, Config, PromptStyle, Tokenizer\n",
    "from litgpt.prompts import has_prompt_style, load_prompt_style\n",
    "from litgpt.utils import CLI, check_valid_checkpoint_dir, get_default_supported_precision, load_checkpoint\n",
    "\n",
    "\n",
    "def multinomial_num_samples_1(probs: torch.Tensor) -> torch.Tensor:\n",
    "    if torch._dynamo.is_compiling():\n",
    "        # Faster alternative to `torch.multinomial(probs, num_samples=1)` that is also CUDAGraph friendly\n",
    "        distribution = torch.empty_like(probs).exponential_(1)\n",
    "        return torch.argmax(probs / distribution, dim=-1, keepdim=True)\n",
    "    return torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "\n",
    "def sample_top_p(logits: torch.Tensor, top_p: float) -> torch.Tensor:\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=False)\n",
    "    cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
    "    # Example:\n",
    "    # sorted_probs=[0.1, 0.15, 0.2, 0.25, 0.3] -> sorted_cumprobs=[0.1, 0.25, 0.45, 0.7, 1.0]\n",
    "    # sorted_indices_to_remove = [1, 1, 0, 0, 0] if top_p=0.7\n",
    "    sorted_indices_to_remove = cumulative_probs <= (1 - top_p)\n",
    "    # Keep at least 1 token always to prevent the case where no token is selected\n",
    "    # In this case the most probable one is always kept\n",
    "    sorted_indices_to_remove[-1:] = 0\n",
    "    indices_to_remove = sorted_indices_to_remove.scatter(0, sorted_indices, sorted_indices_to_remove)\n",
    "    logits = logits.masked_fill(indices_to_remove, float(\"-inf\"))\n",
    "    return logits\n",
    "\n",
    "\n",
    "def sample(\n",
    "    logits: torch.Tensor, temperature: float = 1.0, top_k: Optional[int] = None, top_p: float = 1.0\n",
    ") -> torch.Tensor:\n",
    "    if top_p < 0.0 or top_p > 1.0:\n",
    "        raise ValueError(f\"top_p must be in [0, 1], got {top_p}\")\n",
    "    logits = logits[0, -1]\n",
    "    # optionally crop the logits to only the top k options\n",
    "    if top_k is not None:\n",
    "        v, i = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "        # do not use `torch.where` as in nanogpt because it will repeat top-k collisions\n",
    "        logits = torch.full_like(logits, float(\"-inf\")).scatter_(-1, i, v)\n",
    "    # optionally scale the logits and sample from a probability distribution\n",
    "    if temperature > 0.0 or top_p > 0.0:\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "        # optionally crop the logits to smallest set of logits with a cumulative probability above top_p\n",
    "        if top_p < 1.0:\n",
    "            logits = sample_top_p(logits, top_p)\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        return multinomial_num_samples_1(probs)\n",
    "    return torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "def next_token(model: GPT, input_pos: torch.Tensor, x: torch.Tensor, **kwargs: Any) -> torch.Tensor:\n",
    "    logits = model(x, input_pos)\n",
    "    next = sample(logits, **kwargs)\n",
    "    return next.to(dtype=x.dtype)\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate(\n",
    "    model: GPT,\n",
    "    prompt: torch.Tensor,\n",
    "    max_returned_tokens: int,\n",
    "    *,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: Optional[int] = None,\n",
    "    top_p: float = 1.0,\n",
    "    eos_id: Optional[int] = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Takes a conditioning sequence (prompt) as input and continues to generate as many tokens as requested.\n",
    "\n",
    "    The implementation of this function is modified from A. Karpathy's nanoGPT.\n",
    "\n",
    "    Args:\n",
    "        model: The model to use.\n",
    "        prompt: Tensor of shape (T) with indices of the prompt sequence.\n",
    "        max_returned_tokens: The maximum number of tokens to return (given plus generated).\n",
    "        temperature: Scales the predicted logits by 1 / temperature.\n",
    "        top_k: If specified, only sample among the tokens with the k highest probabilities.\n",
    "        top_p: If specified, it represents the cumulative probability threshold to consider in the sampling process.\n",
    "            In top-p sampling, the next token is sampled from the highest probability tokens\n",
    "            whose cumulative probability exceeds the threshold `top_p`. When specified,\n",
    "            it must be `0 <= top_p <= 1`. Here, `top_p=0` is equivalent\n",
    "            to sampling the most probable token, while `top_p=1` samples from the whole distribution.\n",
    "            It can be used in conjunction with `top_k` and `temperature` with the following order\n",
    "            of application:\n",
    "\n",
    "            1. `top_k` sampling\n",
    "            2. `temperature` scaling\n",
    "            3. `top_p` sampling\n",
    "\n",
    "            For more details, see https://arxiv.org/abs/1904.09751\n",
    "            or https://huyenchip.com/2024/01/16/sampling.html#top_p\n",
    "        eos_id: If specified, stop generating any more token once the <eos> token is triggered.\n",
    "    \"\"\"\n",
    "    T = prompt.size(0)\n",
    "    assert max_returned_tokens > T\n",
    "    if model.max_seq_length < max_returned_tokens - 1:\n",
    "        # rolling the kv cache based on the `input_pos` value would be necessary. However, doing so would introduce a\n",
    "        # data dependency on the `input_pos` tensor and impact model compilation. Since this setting is uncommon, we do\n",
    "        # not support it to avoid negatively impacting the overall speed\n",
    "        raise NotImplementedError(f\"max_seq_length {model.max_seq_length} needs to be >= {max_returned_tokens - 1}\")\n",
    "\n",
    "    device = prompt.device\n",
    "    tokens = [prompt]\n",
    "    input_pos = torch.tensor([T], device=device)\n",
    "    token = next_token(\n",
    "        model, torch.arange(0, T, device=device), prompt.view(1, -1), temperature=temperature, top_k=top_k, top_p=top_p\n",
    "    ).clone()\n",
    "    tokens.append(token)\n",
    "    for _ in range(2, max_returned_tokens - T + 1):\n",
    "        token = next_token(\n",
    "            model, input_pos, token.view(1, -1), temperature=temperature, top_k=top_k, top_p=top_p\n",
    "        ).clone()\n",
    "        tokens.append(token)\n",
    "        if token == eos_id:\n",
    "            break\n",
    "        input_pos = input_pos.add_(1)\n",
    "    return torch.cat(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39943398-afaa-40bb-8de1-39d021914245",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    prompt: str = \"What food do llamas eat?\"\n",
    "    num_samples: int = 1\n",
    "    max_new_tokens: int = 256\n",
    "    top_k: Optional[int] = 50\n",
    "    top_p: float = 1.0\n",
    "    temperature: float = 0.8\n",
    "    checkpoint_dir: Path = Path(\n",
    "        \"/home/tv/data/firma/grid/thunder/litgpt/checkpoints/meta-llama/Meta-Llama-3-8B-Instruct/\"\n",
    "    )\n",
    "    quantize: Optional[Literal[\"bnb.nf4\", \"bnb.nf4-dq\", \"bnb.fp4\", \"bnb.fp4-dq\", \"bnb.int8\"]] = \"bnb.nf4\"\n",
    "    precision: Optional[str] = \"bf16-true\"\n",
    "    compile: bool = False\n",
    "    # litgpt generate base --quantize bnb.nf4 --checkpoint_dir checkpoints/tiiuae/falcon-7b --precision bf16-true --max_new_tokens 256\n",
    "\n",
    "    \"\"\"Generates text samples based on a pre-trained model and tokenizer.\n",
    "\n",
    "    Args:\n",
    "        prompt: The prompt string to use for generating the samples.\n",
    "        num_samples: The number of text samples to generate.\n",
    "        max_new_tokens: The number of generation steps to take.\n",
    "        top_k: The number of top most probable tokens to consider in the sampling process.\n",
    "        top_p: If specified, it represents the cumulative probability threshold to consider in the sampling process.\n",
    "            In top-p sampling, the next token is sampled from the highest probability tokens\n",
    "            whose cumulative probability exceeds the threshold `top_p`. When specified,\n",
    "            it must be `0 <= top_p <= 1`. Here, `top_p=0` is equivalent\n",
    "            to sampling the most probable token, while `top_p=1` samples from the whole distribution.\n",
    "            It can be used in conjunction with `top_k` and `temperature` with the following order\n",
    "            of application:\n",
    "\n",
    "            1. `top_k` sampling\n",
    "            2. `temperature` scaling\n",
    "            3. `top_p` sampling\n",
    "\n",
    "            For more details, see https://arxiv.org/abs/1904.09751\n",
    "            or https://huyenchip.com/2024/01/16/sampling.html#top_p\n",
    "        temperature: A value controlling the randomness of the sampling process. Higher values result in more random\n",
    "            samples.\n",
    "        checkpoint_dir: The checkpoint directory to load.\n",
    "        quantize: Whether to quantize the model and using which method:\n",
    "            - bnb.nf4, bnb.nf4-dq, bnb.fp4, bnb.fp4-dq: 4-bit quantization from bitsandbytes\n",
    "            - bnb.int8: 8-bit quantization from bitsandbytes\n",
    "            for more details, see https://github.com/Lightning-AI/litgpt/blob/main/tutorials/quantize.md\n",
    "        precision: Indicates the Fabric precision setting to use.\n",
    "        compile: Whether to compile the model.\n",
    "    \"\"\"\n",
    "    precision = precision or get_default_supported_precision(training=False)\n",
    "\n",
    "    # plugins = BitsandbytesPrecision(mode='nf4', dtype=torch.bfloat16)\n",
    "\n",
    "    precision = \"bf16-true\"\n",
    "\n",
    "    fabric = L.Fabric(devices=1, precision=precision)  # , plugins=plugins)\n",
    "\n",
    "    check_valid_checkpoint_dir(checkpoint_dir)\n",
    "    config = Config.from_file(checkpoint_dir / \"model_config.yaml\")\n",
    "\n",
    "    checkpoint_path = checkpoint_dir / \"lit_model.pth\"\n",
    "\n",
    "    tokenizer = Tokenizer(checkpoint_dir)\n",
    "    prompt_style = (\n",
    "        load_prompt_style(checkpoint_dir) if has_prompt_style(checkpoint_dir) else PromptStyle.from_config(config)\n",
    "    )\n",
    "\n",
    "    prompt = prompt_style.apply(prompt)\n",
    "    encoded = tokenizer.encode(prompt, device=fabric.device)\n",
    "    prompt_length = encoded.size(0)\n",
    "    max_returned_tokens = prompt_length + max_new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7afe7f82-d468-4570-83f5-9bfc4817183c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    fabric.print(f\"Loading model {str(checkpoint_path)!r} with {config.__dict__}\", file=sys.stderr)\n",
    "    t0 = time.perf_counter()\n",
    "    with fabric.init_module(empty_init=True):\n",
    "        model = GPT(config)\n",
    "    fabric.print(f\"Time to instantiate model: {time.perf_counter() - t0:.02f} seconds.\", file=sys.stderr)\n",
    "    with fabric.init_tensor():\n",
    "        # set the max_seq_length to limit the memory usage to what we need\n",
    "        model.max_seq_length = max_returned_tokens\n",
    "        # enable the kv cache\n",
    "        model.set_kv_cache(batch_size=1)\n",
    "    model.eval()\n",
    "\n",
    "    if compile:\n",
    "        torch._dynamo.config.automatic_dynamic_shapes = True\n",
    "        torch._inductor.config.triton.unique_kernel_names = True\n",
    "        torch._inductor.config.coordinate_descent_tuning = True\n",
    "        global next_token\n",
    "        next_token = torch.compile(next_token, mode=\"reduce-overhead\")\n",
    "\n",
    "    model = fabric.setup_module(model)\n",
    "\n",
    "    ggml_fn = \"~/.ollama/models/manifests/registry.ollama.ai/library/llama3/latest\"\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    # load_checkpoint(fabric, model, checkpoint_path)\n",
    "    load_ggml_weights(model._original_module, ggml_fn)\n",
    "\n",
    "    fabric.print(f\"Time to load the model weights: {time.perf_counter() - t0:.02f} seconds.\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "483cc7da-4db8-4953-b5c7-657b709cc8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    with torch.inference_mode():\n",
    "        L.seed_everything(1234)\n",
    "        for i in range(num_samples):\n",
    "            t0 = time.perf_counter()\n",
    "            y = generate(\n",
    "                model,\n",
    "                encoded,\n",
    "                max_returned_tokens,\n",
    "                temperature=temperature,\n",
    "                top_k=top_k,\n",
    "                top_p=top_p,\n",
    "                eos_id=tokenizer.eos_id,\n",
    "            )\n",
    "            t = time.perf_counter() - t0\n",
    "            for block in model.transformer.h:\n",
    "                block.attn.kv_cache.reset_parameters()\n",
    "            fabric.print(tokenizer.decode(y))\n",
    "            tokens_generated = y.size(0) - prompt_length\n",
    "            fabric.print(\n",
    "                f\"Time for inference {i + 1}: {t:.02f} sec total, {tokens_generated / t:.02f} tokens/sec\",\n",
    "                file=sys.stderr,\n",
    "            )\n",
    "        if fabric.device.type == \"cuda\":\n",
    "            fabric.print(f\"Memory used: {torch.cuda.max_memory_allocated() / 1e9:.02f} GB\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8705fb23-8b24-488f-ad85-72e4cdd98c25",
   "metadata": {},
   "source": [
    "# Thunder transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a76d44e0-dd65-4e6c-a747-96f2ef1e33fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Sequence\n",
    "\n",
    "import thunder\n",
    "from thunder.core.transform_common import Transform\n",
    "from thunder.core import utils\n",
    "from thunder.core import prims\n",
    "import torch\n",
    "\n",
    "from thunder.transforms.utils import (\n",
    "    get_orig_and_thunder_module_proxies_from_prologue,\n",
    "    get_checks,\n",
    "    add_trace_output,\n",
    ")\n",
    "\n",
    "import ggmltensor\n",
    "\n",
    "ggmlquant_executor = thunder.extend.OperatorExecutor(\"quant_ggml\", version=0.1)\n",
    "\n",
    "\n",
    "def ggmlquant_matmul_meta(x, qweight, ggmltype: int, shape):\n",
    "    assert isinstance(shape, Sequence) and len(shape) == 2\n",
    "    assert x.shape[-1] == shape[1], f\"{x.shape=}, rhs {shape=}\"\n",
    "    return thunder.TensorProxy(like=x, shape=(*x.shape[:-1], shape[0]))\n",
    "\n",
    "\n",
    "def ggmlquant_matmul_impl(x, qweight, ggmltype: int, shape):\n",
    "    w = ggmltensor.dequantize(qweight, ggmltensor.GgmlType(ggmltype), shape, dtype=x.dtype)\n",
    "    return x @ w\n",
    "\n",
    "\n",
    "def ggmlquant_embed_meta(x, qweight, ggmltype: int, shape):\n",
    "    assert isinstance(shape, Sequence) and len(shape) == 2\n",
    "    # checks for mul\n",
    "    return thunder.TensorProxy(like=x, shape=(*x.shape, shape[1]))\n",
    "\n",
    "\n",
    "def ggmlquant_embed_impl(x, qweight, ggmltype: int, shape):\n",
    "    w = ggmltensor.dequantize(qweight, ggmltensor.GgmlType(ggmltype), shape, dtype=torch.bfloat16)\n",
    "    return torch.nn.functional.embedding(x, w.t())\n",
    "\n",
    "\n",
    "ggmlquant_matmul = ggmlquant_executor.register_operator(\n",
    "    \"ggmlquant_matmul\", meta=ggmlquant_matmul_meta, fn=ggmlquant_matmul_impl\n",
    ")\n",
    "\n",
    "ggmlquant_embed = ggmlquant_executor.register_operator(\n",
    "    \"ggmlquant_embed\", meta=ggmlquant_embed_meta, fn=ggmlquant_embed_impl\n",
    ")\n",
    "\n",
    "\n",
    "class GGMLQuantTransform(Transform):\n",
    "    def __init__(self, model_file_name, device):\n",
    "        self.quant_states = {}\n",
    "        self.quantized_submodule_names = set()\n",
    "        self.device = device\n",
    "        self.model_file_name = model_file_name\n",
    "\n",
    "    def transform_module(self, model: thunder.ThunderModule):\n",
    "        ggml_quant = ggmltensor.GgmlDataReader(self.model_file_name)\n",
    "        self.thunder_module = model\n",
    "\n",
    "        def convert_layer_with_weight(tm, name):\n",
    "            self.quantized_submodule_names.add(name)\n",
    "            weight_name = f\"{name}.weight\"\n",
    "            w = tm.get_parameter(weight_name)\n",
    "            qw, (typ, shape) = ggml_quant.get_parameter(weight_name)\n",
    "            tm._overrides_parameters[weight_name] = qw.to(self.device)\n",
    "            if not qw.is_floating_point():\n",
    "                self.quant_states[weight_name] = {\"typ\": typ, \"shape\": shape}\n",
    "\n",
    "        for n, submodule in model._model.named_modules():\n",
    "            if hasattr(submodule, \"weight\"):\n",
    "                convert_layer_with_weight(model, n)\n",
    "        ggml_quant.close()\n",
    "\n",
    "    def transform_state_dict_for_submodule(self, model: thunder.ThunderModule, submodule_name: str, state_dict: dict):\n",
    "        raise NotImplementedError(\"load weights ...\")\n",
    "\n",
    "    def transform_traces_pre_prologue(self, prologue_trace, computation_trace, epilogue_trace, **kwargs):\n",
    "        tm = self.thunder_module\n",
    "        from thunder.core.trace import tracectx\n",
    "\n",
    "        checks = get_checks(prologue_trace)\n",
    "\n",
    "        compute_producers, compute_consumers = utils.producers_and_consumers(computation_trace)\n",
    "\n",
    "        proglogue_to_compute_outputs = prologue_trace.output[0]\n",
    "\n",
    "        output_idxes = {id(o): i for i, o in enumerate(proglogue_to_compute_outputs)}\n",
    "\n",
    "        computation_trace.push_scope([])\n",
    "        quantized_proxies: dict[int, str] = {}  # id -> name\n",
    "\n",
    "        for n, qs in self.quant_states.items():\n",
    "            param = tm.get_parameter(n)\n",
    "            check, get_param = checks[n]\n",
    "            quantized_proxies[id(get_param.output)] = n\n",
    "            # check has args: tensor, shape, device, dtype, requires_grad\n",
    "            proxy, _, _, _, requires_grad = check.args\n",
    "            thunder_device = thunder.devices.to_device(param.device)\n",
    "            thunder_device_str = thunder_device.device_str()\n",
    "            check.args = (proxy, (*param.shape,), thunder_device_str, param.dtype, False)\n",
    "        for n, param in tm.named_parameters():\n",
    "            if n not in self.quant_states:\n",
    "                check, get_param = checks[n]\n",
    "                proxy, _, _, _, requires_grad = check.args\n",
    "                thunder_device = thunder.devices.to_device(param.device)\n",
    "                thunder_device_str = thunder_device.device_str()\n",
    "                check.args = (proxy, (*param.shape,), thunder_device_str, param.dtype, False)\n",
    "\n",
    "        new_computation_trace = thunder.core.trace.from_trace(computation_trace)\n",
    "\n",
    "        proxies_to_replace = {}\n",
    "        for bsym in computation_trace.bound_symbols:\n",
    "            if bsym.sym == thunder.torch.linear and id(bsym.args[1]) in quantized_proxies:\n",
    "                assert len(bsym.args) == 3  # torch.linear(input, weight, bias)\n",
    "                assert bsym.args[2] is None\n",
    "                n = quantized_proxies[id(bsym.args[1])]\n",
    "                qs = self.quant_states[n]\n",
    "                # signature of the new symbol:\n",
    "                # bnb_matmul_nf4(x, qweight, bias, absmax, quant_map, blocksize, dtype, shape)\n",
    "                new_args = (\n",
    "                    *bsym.args[:2],\n",
    "                    qs[\"typ\"].value,  # integer value\n",
    "                    qs[\"shape\"],\n",
    "                )\n",
    "                mm_bsym = bsym.from_bsym(\n",
    "                    sym=ggmlquant_matmul,\n",
    "                    subsymbols=[],\n",
    "                    args=new_args,\n",
    "                )\n",
    "\n",
    "                new_computation_trace.bound_symbols.append(mm_bsym)\n",
    "                # we need the postprocess to set the internal state (call_ctx) because we do not bind / execute the new symbol to\n",
    "                # preserve the \"meta\"-info like source location, header, etc.\n",
    "                # TODO: switch to a better solution when it is there\n",
    "                ggmlquant_matmul._bind_postprocess(mm_bsym)\n",
    "            elif bsym.sym == thunder.torch.embedding and id(bsym.args[1]) in quantized_proxies:\n",
    "                assert len(bsym.args) == 7  # torch.linear(input, weight, bias)\n",
    "                assert bsym.args[2] is None and bsym.args[3] is None\n",
    "                assert bsym.args[5] is False and bsym.args[6] is False\n",
    "                n = quantized_proxies[id(bsym.args[1])]\n",
    "                qs = self.quant_states[n]\n",
    "                new_args = (\n",
    "                    *bsym.args[:2],\n",
    "                    qs[\"typ\"].value,  # integer value\n",
    "                    qs[\"shape\"],\n",
    "                )\n",
    "                emb_bsym = bsym.from_bsym(\n",
    "                    sym=ggmlquant_embed,\n",
    "                    subsymbols=[],\n",
    "                    args=new_args,\n",
    "                )\n",
    "\n",
    "                new_computation_trace.bound_symbols.append(emb_bsym)\n",
    "                # we need the postprocess to set the internal state (call_ctx) because we do not bind / execute the new symbol to\n",
    "                # preserve the \"meta\"-info like source location, header, etc.\n",
    "                # TODO: switch to a better solution when it is there\n",
    "                ggmlquant_embed._bind_postprocess(emb_bsym)\n",
    "            else:\n",
    "                new_computation_trace.bound_symbols.append(bsym.from_bsym())\n",
    "\n",
    "        new_computation_trace.set_provenance(thunder.core.trace.TraceProvenance(\"quant pass\"))\n",
    "        return prologue_trace, new_computation_trace, epilogue_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c08de032-8db4-49ce-9382-868263d00d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import thunder, torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c070756-4857-4741-9658-ee83b9dceeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import thunder.tests.litgpt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "250b5edd-31d0-456c-8bf1-96fc73e4e5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import litgpt\n",
    "\n",
    "with torch.device(\"meta\"):\n",
    "    m = thunder.tests.litgpt_model.GPT.from_name(\"Llama-3-8B-Instruct\")\n",
    "    m.requires_grad_(False)\n",
    "    # del m.transformer.h[2:]\n",
    "# enable the kv cache\n",
    "device = \"cuda\"\n",
    "with torch.device(device):\n",
    "    m.max_seq_length = max_returned_tokens\n",
    "    m.set_kv_cache(batch_size=1)\n",
    "m.cos, m.sin = m.rope_cache(device=torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39264594-703f-4393-ac2d-6c88e50b5764",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_name = \"~/.ollama/models/manifests/registry.ollama.ai/library/llama3/latest\"\n",
    "\n",
    "quant_transform = GGMLQuantTransform(model_file_name, torch.device(\"cuda\"))\n",
    "tm = thunder.jit(m, transforms=[quant_transform])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a87b6c62-afa7-4537-90ac-de99b2a7c4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.randint(1, 100, (1, 64), device=\"cuda\")\n",
    "# tm(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7a0c755-77a3-44a4-8f11-0b3d308c75f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d8666d-387f-4676-a01b-04c8567a9c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1234\n"
     ]
    }
   ],
   "source": [
    "for block in model.transformer.h:\n",
    "    block.attn.kv_cache.reset_parameters()\n",
    "num_samples = 1\n",
    "with torch.inference_mode():\n",
    "    L.seed_everything(1234)\n",
    "    for i in range(num_samples):\n",
    "        t0 = time.perf_counter()\n",
    "        y = generate(\n",
    "            model,\n",
    "            encoded,\n",
    "            max_returned_tokens,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            eos_id=tokenizer.eos_id,\n",
    "        )\n",
    "        t = time.perf_counter() - t0\n",
    "        for block in model.transformer.h:\n",
    "            block.attn.kv_cache.reset_parameters()\n",
    "        fabric.print(tokenizer.decode(y))\n",
    "        tokens_generated = y.size(0) - prompt_length\n",
    "        fabric.print(\n",
    "            f\"Time  for inference {i + 1}: {t:.02f} sec total, {tokens_generated / t:.02f} tokens/sec\", file=sys.stderr\n",
    "        )\n",
    "    if fabric.device.type == \"cuda\":\n",
    "        fabric.print(f\"Memory used: {torch.cuda.max_memory_allocated() / 1e9:.02f} GB\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cdfcae-4fc1-434e-acfc-27f9d5c53942",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ad6791-a375-4cbe-b74b-2bf08b202d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transformer.h[0].attn.kv_cache.v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278af111-7b12-4579-8efb-f8a03582c355",
   "metadata": {},
   "outputs": [],
   "source": [
    "model._forward_module.max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33c1b1c-45b4-42d6-b363-9adcbeb8b017",
   "metadata": {},
   "outputs": [],
   "source": [
    "model._forward_module.config.n_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c7e96a-8794-4219-bd9f-f4dc0e18fde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model._model.mask_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e8c0c4-48cc-44c0-ad21-77a870f14dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = thunder.tests.litgpt_model.OverridenKVCache((1, 32, 286, 128), (1, 32, 286, 128), device=torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd6e24a-3c16-4d48-94e3-c1da897ea04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tm2 = thunder.jit(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a4cd5f-a4d2-458d-b51c-7cb679049655",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pos = torch.tensor([2], device=\"cuda\")\n",
    "k, v = torch.randn(2, 1, 32, 1, 128, device=\"cuda\")\n",
    "tm2(input_pos, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a80452-8852-45ea-bad9-04048336d0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90b8686-678b-4982-abc9-6bff19abfc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor.index_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97917740-cd76-4471-b1fd-284c3b159c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "?? m.transformer.h[0].attn.kv_cache.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daf3945-91e3-4836-806f-ef8dd972515f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
